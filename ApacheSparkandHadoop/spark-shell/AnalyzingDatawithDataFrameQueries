//------------------
//Hands-On Exercise: Analyzing Data with DataFrame Queries
//------------------
//spark-shell
spark
val accountsDF = spark.read.table("devsh.accounts")
accountsDF.select(accountsDF("first_name")).show
accountsDF.select(($"first_name")).show
val fnCol = accountsDF("first_name")
//fnCol.
val lucyCol = (fnCol === "Lucy")
lucyCol.desc
accountsDF.select($"first_name",$"last_name",lucyCol).show
accountsDF.select($"first_name",$"last_name",lucyCol).where(lucyCol).show()
accountsDF.select($"first_name", $"last_name", lucyCol).where($"first_name" === "Lucy").show()
accountsDF.select($"first_name", $"last_name", lucyCol).where(fnCol === "Lucy").show()
accountsDF.where(fnCol === "Lucy").show()
accountsDF.select($"city", $"state", $"phone_number".substr(1,3)).show(5)
accountsDF.select($"city", $"state", $"phone_number".substr(1,3).alias("area_code")).show(5)
accountsDF.where($"first_name".substr(1,2) === $"last_name".substr(1,2)).select("first_name","last_name").show(5)
accountsDF.groupBy("last_name").count.show(5)
accountsDF.groupBy("last_name","first_name").count.show(5)
//training@dev ~]$ parquet-tools schema $DEVDATA/base_stations.parquet
//message spark_schema{
//  optional int32 id;
//  optional binary zip (UTF8);
//  optional binary city (UTF8);
//  optional binary state (UTF8);
//  optional double lat;
//  optional double lon;
//
//training@dev ~]$ parquet-tools head $DEVDATA/base_stations.parquet

//scelisdev02@cca175-m:~/training_materials/devsh/exercises/analyze/solution$  hdfs dfs -ls /devsh_loudacre/base*

val baseDF = spark.read.parquet("/devsh_loudacre/base_stations.parquet")

baseDF.printSchema

accountsDF.select("acct_num","first_name","last_name","zipcode").join(baseDF,$"zip" === $"zipcode").show()

//scelisdev02@cca175-m:~/training_materials/devsh/data$ ls

//scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -put accountdevice /devsh_loudacre/accountdevice

//scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -ls /devsh_loudacre/accountdevice

val accountDeviceDF = spark.read.options(Map("inferSchema"->"true", "header"->"true")).csv("/devsh_loudacre/accountdevice")

accountDeviceDF.printSchema

//scelisdev02@cca175-m:~/training_materials/devsh/exercises$ hdfs dfs -ls /devsh_loudacre/

//scelisdev02@cca175-m:~/training_materials/devsh/exercises$ hdfs dfs -get /devsh_loudacre/top_devices /tmp/
//scelisdev02@cca175-m:~/training_materials/devsh/exercises$ ls -l /tmp/top_devices/

//training@dev ~]$ parquet-tools head /tmp/top_devices

val accountDeviceDF = spark.read.options(Map("inferSchema"->"true", "header"->"true")).csv("/devsh_loudacre/accountdevice")

accountDeviceDF.printSchema
val activeAccountsDF = accountsDF.where(accountsDF("acct_close_dt").isNull)

val activeAcctDevsDF = activeAccountsDF.join(accountDeviceDF,  accountDeviceDF("account_id") === activeAccountsDF("acct_num")).select("device_id")

val sumDevicesDF = activeAcctDevsDF.groupBy("device_id").count().withColumnRenamed("count", "active_num")

val orderDevicesDF = sumDevicesDF.orderBy($"active_num".desc)

val devDF = spark.read.json("/devsh_loudacre/devices.json")

val joinDevicesDF = orderDevicesDF.join(devDF, orderDevicesDF("device_id") === devDF("devnum"))

joinDevicesDF.select("device_id", "make", "model", "active_num").write.mode("overwrite").save("/devsh_loudacre/top_devices")

//scelisdev02@cca175-m:~/training_materials/devsh/exercises$  rm -r /tmp/top_devices
//scelisdev02@cca175-m:~/training_materials/devsh/exercises$ hdfs dfs -get /devsh_loudacre/top_devices /tmp/
//scelisdev02@cca175-m:~/training_materials/devsh/exercises$  rm -r /tmp/top_devices
//scelisdev02@cca175-m:~/training_materials/devsh/exercises$ hdfs dfs -get /devsh_loudacre/top_devices /tmp/
//scelisdev03@cca175-m:~/training_materials/devsh/exercises$  ls -l /tmp/top_devices