#------------------
#Hands-On Exercise: Joining Data Using Pair RDDs
#------------------

#export PYSPARK_DRIVER_PYTHON=ipython
#PYSPARK_DRIVER_PYTHON_OPTS=ipython
#pyspark

#scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -put weblogs /devsh_loudacre/weblogs/
#scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -cat weblogs /devsh_loudacre/weblogs/2014-03-15.log |more

weblogfiles = ("/devsh_loudacre/weblogs/*2.log")
#type(weblogfiles)

weblogsRDD = sc. textFile(weblogfiles)
for i in weblogsRDD. take(10): print(i)
#type(weblogsRDD)

weblogs1RDD = weblogsRDD. map( lambda i: i.split(' ') )
for i in weblogs1RDD. take(10): print(i)
#type(weblogs1RDD)

# map to create a pair RDD with the user ID as the key and the integer 1 as the value # ('67858', 1)
weblogs2RDD = weblogs1RDD. map( lambda i: (i[2], 1) )
for i in weblogs2RDD. take(10): print(i)
#type(weblogs2RDD)

# Use reduceByKey to sum the values for each Key  - user/count - # ('91', 174) - 
weblogs3RDD = weblogs2RDD. reduceByKey( lambda x,y: x + y )
for i in weblogs3RDD. take(10): print(i)
#type(weblogs3RDD)

# Use map to reverse the key and value, swap, reverse
weblogs4RDD = weblogs3RDD. map( lambda x: (x[1], x[0]) )
for i in weblogs4RDD. take(10) : print ( i )
#type(weblogs4RDD)

# countByKey devuelve un dictionary - es ineficiente
# requency - cuantos (keys), users visited the site for each
weblogs4dict = weblogs4RDD. countByKey()
# type(weblogs4dict)
# print(weblogs4dict)

for k, v in weblogs4dict.items():
    print( "k, v: ", (k, v) )

weblogsRDD.getNumPartitions()
for i in weblogsRDD. take(10): print(i)

freq = weblogsRDD. map( lambda x: ( x[1], x[0] ) ). countByKey()
#type(freq)
#for i in freq: print(i)

freq = weblogsRDD. countByKey(). items()
#type(freq) --> dict_items

#for i in freq: print(i)
for k, v in freq:
    print( "k, v: ", (k, v) )
# or

weblogsRDD = sc. textFile(weblogfiles).  map( lambda i: i.split(' ')) .map( lambda i: (i[2], 1) ). reduceByKey( lambda x,y: x+y ) . map( lambda x: (x[1], x[0]) )
for i in weblogsRDD. take(10): print(i)

weblogsDict = weblogsRDD. countByKey()

for k, v in weblogsDict.items():
    print( "k, v: ", (k, v) )

for k, v in weblogsDict.items():
    print( "k: ", k, ", ", "v: ", v )

for k in weblogsDict.keys(): 
    print(k)

# map to (userid,ipaddress) and then use groupByKey
weblogsRDD = sc. textFile(weblogfiles).  map( lambda i: i.split(' ')). map( lambda i: (i[2], i[0]) ). groupByKey().  map( lambda i: i )
for i in weblogsRDD. take(10): print(i)

for (k, v) in weblogsRDD. take(10):
    print(k, ":")
    for i in v: print("\t",i)

# collect devuelve una lista
weblogsList = weblogsRDD. collect()
for k,v in weblogsList:
    print("\n k: ", k)
    for i in v: print("\t v: ", i)
#type(weblogsList)

# or
weblogsRDD = sc. textFile(weblogfiles)
userIPRDD = weblogsRDD. map( lambda i: i.split(' ') ). map( lambda i: ( i[2], i[0] ) )

# ('73079', <pyspark.resultiterable.ResultIterable object at 0x7f5fd693f7c0>)
groupRDD = userIPRDD. groupByKey()
for i in groupRDD. take(2) : print(i)

# ('44794', ['237.190.50.80', '237.190.50.80', '224.110.132.187', '224.110.132.187', '26.189.206.212', '26.189.206.212'])
UserIPsRDD = groupRDD. map( lambda x : ( x[0], list( x[1] ) ))
for i in UserIPsRDD. take(2) : print(i)

UserlistIPs = UserIPsRDD. collect()
#for i in UserlistIPs : print(i)

#--hasta aquí el ejercicio finalizado en pyspark, dejo esta nota para recordar que el flatMap es diferente al map

weblogsRDDFlat = weblogsRDD. flatMap( lambda i: i.split(' ') )
for i in weblogsRDDFlat. take(10): print(i)

weblogsRDDMap = weblogsRDD. map( lambda i : i.split(' ') )
for i in weblogsRDDMap. take(2): print(i)

#------------------------

accountsdata = "/user/hive/warehouse/devsh.db/accounts"
#type(accountsdata)

accountsRDD = sc. textFile( accountsdata )
for i in accountsRDD. take(10): print(i)
#type(accountsRDD)

accounts1RDD = accountsRDD. map( lambda i: i.split(',') )
for i in accounts1RDD. take(10): print(i)
#type(accounts1RDD)

# key/value-array pairs:
accounts2RDD = accounts1RDD.map( lambda i: (i[0], i) )
for i in accounts2RDD. take(10): print(i)
#type(accounts2RDD)

# Join the pair RDD with the set of user-id/hit-count pairs
accountsjoinRDD = accounts2RDD. join(weblogs3RDD)
for i in accountsjoinRDD. take(10): print(i)
#type(accountsjoinRDD)

# print tupla donde el segundo elemento tambien es una tupla: (   , ( [, ........ , ],    ) )
# ( k, ( [v.... values], other) )
# ('48', (['48', '2008-11-15 12:22:34.0', ........ , '2014-03-18 13:29:47.0'], 188))
for (k,(v, o)) in accountsjoinRDD. take(10) :
     print(k, o, v[3], v[4])


# Bonus Exercises
# RDD filed-pair of account data with the postal code 9th as the key and a list of names (Last Name,First Name)
accountsdata = "/user/hive/warehouse/devsh.db/accounts"
accountsRDD = sc. textFile( accountsdata )
for i in accountsRDD. take(10): print(i)

accounts1RDD = accountsRDD. map( lambda i: i.split(',') )
for i in accounts1RDD. take(10): print(i)

# ( k, [v.... values] ) - devuelve un pairRDD
accounts2RDD = accounts1RDD. keyBy( lambda i: i[8] )
for i in accounts2RDD. take(10): print(i)
type(accounts1RDD)

# gnora la key, la funcion aplica sólo a los valores('94660', 'Becton,Donald')
accounts3RDD = accounts2RDD. mapValues( lambda v: v[4] + ',' + v[3] ) 
for i in accounts3RDD. take(10): print(i)


# or

filename = "/user/hive/warehouse/devsh.db/accounts" 
bonusRDD = sc. textFile( filename ). map( lambda i : i.split(',') ) . keyBy( lambda i : i[8] ). mapValues( lambda v: v[4] + ',' + v[3] ) 
for i in bonusRDD. take(10) :print(i) 

# ('94143', <pyspark.resultiterable.ResultIterable object at 0x7f5fd77179d0>)
bonusRDD2 = bonusRDD.groupByKey() 
for i in bonusRDD2. take(10) : print(i) 

# print por cada k la lista de values
for (k, v) in bonusRDD2.sortByKey(). take(10):
    print ("\n---", k) 
    for i in v: print(i)
