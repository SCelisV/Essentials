#export PYSPARK_DRIVER_PYTHON=ipython
#PYSPARK_DRIVER_PYTHON_OPTS=ipython
#pyspark

spark

tempDF = spark.read.option("inferSchema", "true").csv("/user/scelisdev02/1800.csv") 
tempDF.show() 
tempDF.printSchema() 

from pyspark.sql.types import *

devColumns = [
StructField("estacion",StringType()),
StructField("fecha",StringType()),
StructField("tipo",StringType()),
StructField("temperatura",DoubleType())]

devSchema = StructType(devColumns)

temp01DF = spark.read.schema(devSchema).csv("/user/scelisdev02/1800.csv")
temp01DF.printSchema()


# sólo en este caso porque nos dan los grados centigrados múltiplicado por 100, por lo tanto tenemos 
# que dividir por 100
def CtoF(celsius): return (0.01* celsius*9/5) + 32

from pyspark.sql.functions import *

fnCol = temp01DF.tipo
tminCol = (fnCol == "TMIN")    
temp01DF.select( "estacion", "fecha", "tipo", "temperatura").where(tminCol).show()

temp01DF.where(tminCol).select( "estacion", "fecha", "tipo", "temperatura").show()

temp02DF = temp01DF.where(tminCol).select( "estacion", "temperatura").groupBy( "estacion").min("temperatura")
temp02DF = temp01DF.where(tminCol).select( "estacion", "temperatura").groupBy( "estacion").min("temperatura").withColumnRenamed("min(temperatura)", "min_temp")
temp02DF.show()

temp02DF.select( "estacion", format_number(temp02DF["min_temp"], 2)).show()  

temp02DF.select( "estacion", CtoF(format_number(temp02DF["min_temp"], 2))).show()  
temp02DF.select( "estacion", CtoF(format_number(temp02DF["min_temp"], 2))).withColumnRenamed("((((format_number(min_temp, 2) * 0.01) * 9) / 5) + 32)", "min_temp").show()

# en una sóla línea sería:


from pyspark.sql.types import *

devColumns = [
StructField("estacion",StringType()),
StructField("fecha",StringType()),
StructField("tipo",StringType()),
StructField("temperatura",DoubleType())]

devSchema = StructType(devColumns)

temp01DF = spark.read.schema(devSchema).csv("/user/scelisdev02/1800.csv")
temp01DF.printSchema()
temp01DF.show(5)   


schema = """
  estacion string,
  fecha string,
  tipo string,
  temperatura double
  """

temp01DF = spark.read.schema(schema).csv("/user/scelisdev02/1800.csv")
temp01DF.printSchema()
temp01DF.show(5)   

fnCol = temp01DF.tipo
tminCol = (fnCol == "TMIN")   

from pyspark.sql.functions import *

# or

temp01DF.\
where(tminCol).\
select("estacion", "temperatura").\
groupBy("estacion").\
min("temperatura").\
withColumnRenamed("min(temperatura)", "min_temp").\
select( temp01DF.estacion, CtoF(format_number("min_temp", 2))).\
withColumnRenamed("((((format_number(min_temp, 2) * 0.01) * 9) / 5) + 32)", "minima").\
show() 

temp01DF.where(tminCol).  select("estacion", "temperatura").  groupBy("estacion").  min("temperatura").  withColumnRenamed("min(temperatura)", "min_temp").  select( temp01DF.estacion, CtoF(format_number("min_temp", 2))).  withColumnRenamed("((((format_number(min_temp, 2) * 0.01) * 9) / 5) + 32)", "minima").  show() 

# or

temp01DF.\
where(temp01DF.tipo == "TMIN").\
select("estacion", "temperatura").\
groupBy("estacion").\
min("temperatura").\
withColumnRenamed("min(temperatura)", "min_temp").\
select( temp01DF.estacion, CtoF(format_number("min_temp", 2))).\
withColumnRenamed("((((format_number(min_temp, 2) * 0.01) * 9) / 5) + 32)", "minima").\
show() 

# or

temp01DF.\
where(temp01DF.tipo == "TMIN").\
select("estacion", "temperatura").\
groupBy("estacion").\
min("temperatura").\
withColumnRenamed("min(temperatura)", "min_temp").\
select( temp01DF.estacion, (col("min_temp") * 0.01 * (9.0 / 5.0) + 32.0).alias("minima")).\
show() 

