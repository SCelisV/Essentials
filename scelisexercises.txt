========================================================================================================================================================================
https://cloud.google.com/
========================================================================================================================================================================
Exercices CCA 159 - Hands-On Exercises
========================================================================================================================================================================
------------------
Hands-On Exercise: Data Ingest with Hadoop Tools
------------------
-----
MYSQL
-----
mysql -u root -p cloudera
crear la db analyst_dualcore utilizando el fichero dbsetup.sql para la creación de la db y las tablas
y los ficheros:customers.txt, employees.txt, order_details.txt, orders.txt, products.txt, suppliers.txt para poblarlas
solucion del manual: source dbsetup.sql
---------------------------------------
# Setup database for Data Analyst Exercises
DROP TABLE IF EXISTS analyst_dualcore.employees;
DROP TABLE IF EXISTS analyst_dualcore.customers;
DROP TABLE IF EXISTS analyst_dualcore.products;
DROP TABLE IF EXISTS analyst_dualcore.orders;
DROP TABLE IF EXISTS analyst_dualcore.order_details;
DROP TABLE IF EXISTS analyst_dualcore.suppliers;

DROP DATABASE IF EXISTS analyst_dualcore;
CREATE DATABASE analyst_dualcore;

USE analyst_dualcore;

-- # setup employees table
CREATE TABLE employees (
	emp_id CHAR(9) PRIMARY KEY NOT NULL,
	fname VARCHAR(15) NULL,
	lname VARCHAR(20) NULL,
	address VARCHAR(40) NULL,
	city VARCHAR(30) NULL,
	state CHAR(2) NULL,
	zipcode CHAR(5) NULL,
	job_title VARCHAR(35) NULL,
	email VARCHAR(25) NULL,
	active CHAR(1) NOT NULL default 'Y',
	salary INT NULL
);

LOAD DATA LOCAL INFILE 'employees.txt' INTO TABLE employees;


-- # setup products table
CREATE TABLE products (
	prod_id INT PRIMARY KEY NOT NULL,
	brand VARCHAR(20) NULL,
	name VARCHAR(75) NULL,
	price INT NULL,
	cost INT NULL,
	shipping_wt SMALLINT NULL
);

LOAD DATA LOCAL INFILE 'products.txt' INTO TABLE products;


-- # setup customers table
CREATE TABLE customers (
	cust_id INT PRIMARY KEY NOT NULL,
	fname VARCHAR(15) NULL,
	lname VARCHAR(20) NULL,
	address VARCHAR(40) NULL,
	city VARCHAR(30) NULL,
	state CHAR(2) NULL,
	zipcode CHAR(5) NULL
);

LOAD DATA LOCAL INFILE 'customers.txt' INTO TABLE customers;


-- # setup orders table
CREATE TABLE orders (
	order_id INT PRIMARY KEY NOT NULL,
	cust_id INT NOT NULL,
	order_date DATETIME NOT NULL
);

LOAD DATA LOCAL INFILE 'orders.txt'
	INTO TABLE orders (order_id, cust_id, @order_date)
	SET order_date = STR_TO_DATE(@order_date,'%m-%d-%Y %H:%i:%s');

-- # setup order_details table
CREATE TABLE order_details (
	order_id INT NOT NULL,
	prod_id INT NOT NULL
);

LOAD DATA LOCAL INFILE 'order_details.txt' INTO TABLE order_details;

-- # setup suppliers table
CREATE TABLE suppliers (
    supp_id INT PRIMARY KEY NOT NULL,
    company VARCHAR(60) NULL,
    contact VARCHAR(60) NULL,
    address VARCHAR(50) NULL,
    city VARCHAR(30) NULL,
    state CHAR(2) NULL,
    zipcode CHAR(2) NULL,
    phone CHAR(14) NULL
);

LOAD DATA LOCAL INFILE 'suppliers.txt' INTO TABLE suppliers;
-----
SQOOP
-----
importar todas las tablas a la vez

sqoop import-all-tables \
 -m 1  \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore  \
--username=root  \
--password=cloudera  \
--compression-codec=snappy  \
--as-parquetfile  \
--warehouse-dir=/user/hive/warehouse  \
--hive-import

sqoop import-all-tables 
 -m 1 \
--connect jdbc:mysql://quickstart:3306/sc_test \
--username=root \
--password=cloudera \
--compression-codec=snappy \
--as-parquetfile \
--warehouse-dir=/user/hive/warehouse/test \
--hive-import

importar tabla a tabla

solucion del manual: import_products.sh
---------------------------------------
#!/bin/bash

sqoop import \
 --connect jdbc:mysql://localhost/analyst_dualcore \
 --username training --password training \
 --fields-terminated-by '\t' \
 --warehouse-dir /analyst/dualcore \
 --table products
 
solucion del manual: import_orders.sh
-------------------------------------
#!/bin/bash

sqoop import \
 --connect jdbc:mysql://localhost/analyst_dualcore \
 --username training --password training \
 --fields-terminated-by '\t' \
 --warehouse-dir /analyst/dualcore \
 --table orders
 
solucion del manual: import_customers.sh
----------------------------------------
#!/bin/bash

sqoop import \
 --connect jdbc:mysql://localhost/analyst_dualcore \
 --username training --password training \
 --fields-terminated-by '\t' \
 --warehouse-dir /analyst/dualcore \
 --table customers
 
solucion del manual: import_order_details.sh
--------------------------------------------
#!/bin/bash

sqoop import \
 --connect jdbc:mysql://localhost/analyst_dualcore \
 --username training --password training \
 --fields-terminated-by '\t' \
 --warehouse-dir /analyst/dualcore \
 --table order_details \
 --split-by order_id
 
solucion del manual: import_employees.sh
----------------------------------------
#!/bin/bash

sqoop import \
 -Dorg.apache.sqoop.splitter.allow_text_splitter=true \
 --connect jdbc:mysql://localhost/analyst_dualcore \
 --username training --password training \
 --fields-terminated-by '\t' \
 --warehouse-dir /analyst/dualcore \
 --table employees
 
------------------
Hands-On Exercise: Running Queries from Shells Scripts and Hue
------------------
------
IMPALA
------
[quickstart.cloudera:21000] > 

-- search some customer
SELECT cust_id, fname, lname, city 
FROM default.customers 
WHERE city = 'Kansas City' 
  AND fname LIKE 'Brid%' 
LIMIT 10;

-- Top 3 products
SELECT * 
FROM products 
ORDER BY price 
DESC LIMIT 3;

-- Query to find the products
-- que corresponden con 'Kansas City' AND c.fname LIKE 'Brid%' AND p.name LIKE '%Table%'
-- sin tratamiento de fechas
SELECT c.cust_id, fname, lname, city, o.order_id, (o.order_date), p.prod_id, p.name
FROM customers c
INNER JOIN orders o ON(o.cust_id = c.cust_id)
INNER JOIN order_details od ON (od.order_id = o.order_id)
INNER JOIN products p ON(p.prod_id = od.prod_id)
WHERE c.city = 'Kansas City' 
  AND c.fname LIKE 'Brid%' 
  AND p.name LIKE '%Table%';

describe orders;
Query: describe orders
+------------+--------+---------+
| name       | type   | comment |
+------------+--------+---------+
| order_id   | int    |         |
| cust_id    | int    |         |
| order_date | bigint |         |
+------------+--------+---------+

-- Query to find the products
-- que corresponden con 'Kansas City' AND c.fname LIKE 'Brid%' AND p.name LIKE '%Table%'
-- y que compro entre el 01 May 2013 and 31 May 2013
SELECT c.cust_id, fname, lname, city, o.order_id, (o.order_date), p.prod_id, p.name
FROM customers c
INNER JOIN orders o ON(o.cust_id = c.cust_id)
INNER JOIN order_details od ON (od.order_id = o.order_id)
INNER JOIN products p ON(p.prod_id = od.prod_id)
WHERE c.city = 'Kansas City' AND c.fname LIKE 'Brid%' AND p.name LIKE '%Table%'
  AND to_date(o.order_date) BETWEEN 01May2013 AND 31May2013; --esto no me ha funcionado

-O-J-O-- al probar todo esto.. - WRONG - yo creo que es problema del campo .. :/P

SELECT order_date, unix_timestamp(order_date, 'MM/dd/yyyy')
FROM orders
LIMIT 10;
Query: select order_date, unix_timestamp(order_date, 'MM/dd/yyyy')
FROM orders
LIMIT 10
Query submitted at: 2021-03-20 05:55:42 (Coordinator: http://quickstart.cloudera:25000)
ERROR: AnalysisException: No matching function with signature: unix_timestamp(BIGINT, STRING).

SELECT order_date, to_date(order_date)
FROM orders
LIMIT 10;
Query: select order_date, to_date(order_date)
FROM orders LIMIT 10
Query submitted at: 2021-03-20 05:51:39 (Coordinator: http://quickstart.cloudera:25000)
ERROR: AnalysisException: No matching function with signature: to_date(BIGINT).

SELECT order_date, from_unixtime(order_date, 'dd/MMM/yyyy') 
FROM orders
LIMIT 10;
Query: select order_date, from_unixtime(order_date, 'dd/MMM/yyyy')
FROM orders
LIMIT 10
Query submitted at: 2021-03-20 05:58:37 (Coordinator: http://quickstart.cloudera:25000)
Query progress can be monitored at: http://quickstart.cloudera:25000/query_plan?query_id=2948ab34b4fc19b0:b753035d00000000
+---------------+------------------------------------------+
| order_date    | from_unixtime(order_date, 'dd/mmm/yyyy') |
+---------------+------------------------------------------+
| 1212303815000 | NULL                                     |
| 1212305262000 | NULL                                     |
| 1212306577000 | NULL                                     |
| 1212307528000 | NULL                                     |
| 1212307716000 | NULL                                     |
| 1212307869000 | NULL                                     |
| 1212309395000 | NULL                                     |
| 1212309412000 | NULL                                     |
| 1212310186000 | NULL                                     |
| 1212314834000 | NULL                                     |
+---------------+------------------------------------------+

SELECT order_date, to_timestamp(order_date)
FROM orders
LIMIT 10;
Query: select order_date, to_timestamp(order_date)
FROM orders
LIMIT 10
+---------------+--------------------------+
| order_date    | to_timestamp(order_date) |
+---------------+--------------------------+
| 1212303815000 | NULL                     |
| 1212305262000 | NULL                     |
| 1212306577000 | NULL                     |
| 1212307528000 | NULL                     |
| 1212307716000 | NULL                     |
| 1212307869000 | NULL                     |
| 1212309395000 | NULL                     |
| 1212309412000 | NULL                     |
| 1212310186000 | NULL                     |
| 1212314834000 | NULL                     |
+---------------+--------------------------+

-O-J-O-- al probar todo esto.. - WRONG - yo creo que es problema del campo .. :/P


solucion del manual: gigabux_under_10.sql
-----------------------------------------
-- products of Gigabux and  price is less than 1000(less than $10)
SELECT * 
FROM analyst_dualcore.products 
WHERE brand='Gigabux' 
  AND price < 1000;

-------
BEELINE
-------
0: jdbc:hive2://localhost:10000> 

-- search some customer
SELECT cust_id, fname, lname, city 
FROM default.customers 
WHERE city = 'Kansas City' 
  AND fname 
LIKE 'Brid%' LIMIT 10;

-- Top 3 products
SELECT * FROM products 
ORDER BY price DESC 
LIMIT 3;

-- Query to find the products
-- que corresponden con 'Kansas City' AND c.fname LIKE 'Brid%' AND p.name LIKE '%Table%'
-- sin tratamiento de fechas
SELECT c.cust_id, fname, lname, city, o.order_id, (o.order_date), p.prod_id, p.name
FROM customers c
INNER JOIN orders o ON(o.cust_id = c.cust_id)
INNER JOIN order_details od ON (od.order_id = o.order_id)
INNER JOIN products p ON(p.prod_id = od.prod_id)
WHERE c.city = 'Kansas City' 
  AND c.fname LIKE 'Brid%' 
  AND p.name LIKE '%Table%';


describe orders;
+-------------+------------+----------+--+
|  col_name   | data_type  | comment  |
+-------------+------------+----------+--+
| order_id    | int        |          |
| cust_id     | int        |          |
| order_date  | bigint     |          |
+-------------+------------+----------+--+

-- Query to find the products
-- que corresponden con 'Kansas City' AND c.fname LIKE 'Brid%' AND p.name LIKE '%Table%'
-- y que compro entre el 01 May 2013 and 31 May 2013
SELECT c.cust_id, fname, lname, city, o.order_id, (o.order_date), p.prod_id, p.name
FROM customers c
INNER JOIN orders o ON(o.cust_id = c.cust_id)
INNER JOIN order_details od ON (od.order_id = o.order_id)
INNER JOIN products p ON(p.prod_id = od.prod_id)
WHERE c.city = 'Kansas City' AND c.fname LIKE 'Brid%' AND p.name LIKE '%Table%';

solucion del manual: gigabux_under_10.sql
-----------------------------------------
-- products of Gigabux and  price is less than 1000(less than $10)
SELECT * 
FROM default.products 
WHERE brand LIKE '%Gigabux%' 
  AND price < 1000;

-----
MYSQL
-----
mysql> 

-- search some customer
SELECT cust_id, fname, lname, city 
FROM analyst_dualcore.customers 
WHERE city = 'Kansas City' 
  AND fname LIKE 'Brid%' LIMIT 10;

-- Top 3 products
SELECT * 
FROM products 
ORDER BY price 
DESC LIMIT 3;

solucion del manual: verify_tablet_order.sql
--------------------------------------------
-- Query to find the order for the
-- tablet (product ID 1274348) from the 
-- contest winner (customer ID 1139477)
SELECT o.order_id, fname, lname, o.order_date 
FROM default.customers c 
JOIN default.orders o ON (c.cust_id = o.cust_id) 
JOIN default.order_details d ON (o.order_id = d.order_id) 
WHERE d.prod_id=1274348
  AND c.cust_id=1139477;

-- Query to find the products
-- que corresponden con 'Kansas City' AND c.fname LIKE 'Brid%' AND p.name LIKE '%Table%'
-- sin tratamiento de fechas
SELECT c.cust_id, fname, lname, city, o.order_id, (o.order_date), p.prod_id, p.name
FROM customers c
INNER JOIN orders o ON(o.cust_id = c.cust_id)
INNER JOIN order_details od ON (od.order_id = o.order_id)
INNER JOIN products p ON(p.prod_id = od.prod_id)
WHERE c.city = 'Kansas City' AND c.fname LIKE 'Brid%' AND p.name LIKE '%Table%';

describe orders;
+------------+----------+------+-----+---------+-------+
| Field      | Type     | Null | Key | Default | Extra |
+------------+----------+------+-----+---------+-------+
| order_id   | int(11)  | NO   | PRI | NULL    |       |
| cust_id    | int(11)  | NO   |     | NULL    |       |
| order_date | datetime | NO   |     | NULL    |       |
+------------+----------+------+-----+---------+-------+

-- Query to find the products
-- que corresponden con 'Kansas City' AND c.fname LIKE 'Brid%' AND p.name LIKE '%Table%'
-- y que compro entre el 01 May 2013 and 31 May 2013
SELECT c.cust_id, fname, lname, city, o.order_id, (o.order_date), p.prod_id, p.name
FROM customers c
INNER JOIN orders o ON(o.cust_id = c.cust_id)
INNER JOIN order_details od ON (od.order_id = o.order_id)
INNER JOIN products p ON(p.prod_id = od.prod_id)
WHERE c.city = 'Kansas City' AND c.fname LIKE 'Brid%' AND p.name LIKE '%Table%';

solucion del manual: gigabux_under_10.sql
-----------------------------------------
-- products of Gigabux and  price is less than 1000(less than $10)
SELECT * 
FROM analyst_dualcore.products 
WHERE brand LIKE '%Gigabux%' 
  AND price < 1000;

------------------
Hands-On Exercise: Using Functions
------------------
-------------------
HIVE, IMPALA, MYSQL
-------------------
Listar el order_id y definir si es par o impar:
SELECT order_id, if(order_id %2=0, 'par', 'impar') paridad 
FROM orders 
LIMIT 10;

Realizar el modulo 10 de order_id, si el resultado es menor que 4 devolvera 'P', si es menor que 7 devolvera 'M', de resto devolvera 'G'
SELECT order_id, 
case 
 when order_id%10 < 4 then 'P' 
 when order_id%10 < 7 then 'M' 
 else 'G' 
end 
FROM orders 
LIMIT 10;

Realizar el modulo 10 de order_id, si el resultado es menor que 4 devolvera 'P', si es menor que 7 devolvera 'M', de resto devolvera 'G' quitar el else y ver el resultado (como no hay else escribe NULL)
SELECT order_id, 
case 
 when order_id%10 < 4 then 'P' 
 when order_id%10 < 7 then 'M' 
end 
FROM orders 
LIMIT 10;

Busy Times
Find a function for extracting the hour from a timestamp type. 
SELECT order_date, hour(from_unixtime(order_date))  
FROM orders
LIMIT 10;

Test the function by finding the distinct values for hours in the order_date column. (You should have only 0 to 23.
0: jdbc:hive2://localhost:10000> SELECT DISTINCT hour(from_unixtime(order_date))  FROM orders;
mysql> SELECT DISTINCT hour((order_date))  FROM orders;

Which are the three most active hours, and the three least active hours, for Dualcore orders?
mysql> SELECT count(o.order_id) cantidad, hour((o.order_date)) hours 
FROM orders o 
GROUP BY hours 
ORDER BY cantidad DESC;

mysql> SELECT count(o.order_id) cantidad, hour((o.order_date)) hours 
FROM orders o 
GROUP BY hours 
ORDER BY cantidad DESC 
LIMIT 3;
+----------+-------+
| cantidad | hours |
+----------+-------+
|    86883 |    15 |
|    86778 |    12 |
|    86741 |     9 |
+----------+-------+

mysql> SELECT count(o.order_id) cantidad, hour((o.order_date)) hours 
FROM orders o 
GROUP BY hours 
ORDER BY hours 
LIMIT 3;
+----------+-------+
| cantidad | hours |
+----------+-------+
|    22745 |     2 |
|    26483 |     3 |
|    29606 |     4 |
+----------+-------+

0: jdbc:hive2://localhost:10000> SELECT h.hours, count(h.order_id) cantidad 
FROM (SELECT hour(from_unixtime(o.order_date)) hours, o.order_id 
      FROM orders o) h 
GROUP BY h.hours 
ORDER BY cantidad DESC 
LIMIT 3;

solucion del manual: active_hours.sql
-------------------------------------
-- Most active hours
SELECT hour(order_date) AS hour, COUNT(*) AS hr_count
FROM orders
GROUP BY hour(order_date)
ORDER BY hr_count DESC
LIMIT 3;

-- Least active hours
SELECT hour(order_date) AS hour, COUNT(*) AS hr_count
FROM orders
GROUP BY hour(order_date)
ORDER BY hr_count
LIMIT 3;

Finding Profit
Find the five items that provide the largest profit. (They should all be servers.)
profit = price - cost
------------------------
mysql - beeline - impala
------------------------
SELECT prod_id, name, price, cost, (price - cost) profit
FROM products
ORDER BY profit DESC
LIMIT 5;

What items provide no profit, or actually cost Dualcore more than they charge for it? 
Use a filter to return only the rows for which this is the case. 
(Your query should return three rows.)
------------------------
mysql - beeline - impala
------------------------
SELECT prod_id, name, price, cost, (price - cost) profit 
FROM products 
WHERE (price - cost) <= 0;

If either of the price or the cost column has a NULL value for a row, then that row would not be included in the previous queries. 
Do the following to check how complete this data is.

• First check either of the columns for NULL values using a null operator. Does the column have any NULL values?
SELECT prod_id, name, price, cost, 
FROM products
WHERE price IS NULL 

SELECT prod_id, name, price, cost, 
FROM products
WHERE cost IS NULL

• Now write a single query that will check both columns for NULL values. Are there any
SELECT prod_id, name, price, cost, 
FROM products
WHERE price IS NULL
   OR cost IS NULL

Averages
Write a query to find the average cost, average price, and average profit of all the items Dualcore carries. 
(The average profit is around $30.04.)

------------------------
mysql - beeline - impala
------------------------
SELECT AVG(cost), AVG(price), AVG(price-cost)
FROM products

Modify the previous query to find the same averages for each brand and round the averages to the nearest penny (that is, the nearest integer). 
(Your query should return 47 rows.)

------------------------
mysql - beeline - impala
------------------------
SELECT brand, round(avg(price)), round(avg(cost)), round(avg(price - cost)) AS avg_profit
FROM products 
GROUP BY brand;

mysql> SELECT brand, round(AVG(price), 0), round(AVG(cost), 0), round(AVG(price-cost) , 0) 
FROM products 
GROUP BY brand;

[quickstart.cloudera:21000] > SELECT brand, ceil(AVG(price)), ceil(round(AVG(cost), 2)), ceil(AVG(price - cost)) 
FROM products 
GROUP BY brand;

0: jdbc:hive2://localhost:10000> SELECT brand, ceil(AVG(price)), ceil(AVG(cost)), ceil(AVG(price-cost)) 
FROM products 
GROUP BY brand;

Modify the query to filter the results, so you only get those brands with an average price greater than $1000 (100000). 
Write down the largest average price for the next problem. (Your query should return only two rows.)

------------------------
mysql - beeline - impala
------------------------
SELECT brand, AVG(price) avg_price, AVG(cost) avg_cost, AVG(price-cost) avg_profit 
FROM products 
GROUP BY brand 
HAVING avg_price > 100000;

Esto nos devuelve el max precio medio de los items.
CREATE VIEW max_avg_price AS
SELECT MAX(avg.price)
FROM
(SELECT brand, round(avg(price)) price
 FROM products
 GROUP BY brand) avg

Find the items whose price is larger than the largest average price that you found for the previous problem. 
(Your query should return 20 rows.)

SELECT brand, name, price, cost, price - cost 
FROM products
WHERE price > 184237 
ORDER BY price DESC

Which products are similar to those expensive ones but are also significantly cheaper?
solucion del manual: similar_cheaper.sql
----------------------------------------
-- Interpretations of "similar" can be more or less
-- strict than the following

SELECT brand, name, price 
FROM products
WHERE (name LIKE 'Server%' 
   OR  name LIKE 'Hadoop Cluster%' 
   OR  name = 'All-in-one Desktop (24 in. display)' 
   OR  name LIKE 'VPN Appliance (250%') 
  AND price < 184237;

Bonus Exercise: Price-to-Profit Ratio
How much does the price-to-profit ratio(relación) vary among items? If you get an odd result, check your query and think about what might give that result.
ratio = price/(price-cost) how many times one number contains another, asumir que el costo es extrictamente inferior al precio
SELECT MIN(if(price > cost,price/(price - cost),NULL)), MAX(if(price > cost,price/(price - cost),NULL))
  FROM products;

considerar lo siguiente: por lo tanto lo mejor sería hacer un UNION entre los positivos y los negativos
(price - cost) > 0 --> para sacar el que me deja mayor beneficio MAX()
(price - cost) = 0 --> no tengo beneficio
(price - cost) < 0 --> perdidas

How much does it change within a brand?
solucion del manual: price_to_profit_brands.sql
------------------------------------------------
SELECT MIN(if(price > cost,price/(price - cost),NULL)), MAX(if(price > cost,price/(price - cost),NULL))
FROM products GROUP BY brand;

What items have the largest price-to-profit ratio? (That is, they are relatively expensive for the amount of money they bring in to the company.)
SELECT prod_id, brand, name, price, cost, price/(price - cost) AS ratio
FROM products
ORDER BY ratio DESC
LIMIT 10;

Compare price-to-profit ratios for items similar to those. Purely from a price-to-profit perspective, 
would you recommend Dualcore stop carrying any particular items from a particular brand?
What other things might Dualcore need to consider before agreeing to stop carrying those items?
solucion del manual: price_to_profit_similar.sql
------------------------------------------------

SELECT prod_id, brand, name, price, cost, price/(price - cost) AS ratio
FROM analyst.products
WHERE name LIKE '8GB%Dual Channel Desktop Memory Kit%';

------------------
Hands-On Exercise: Data Management
------------------
[cloudera@quickstart data]$ hdfs dfs -put ratings_2012.txt /analyst/dualcore/
importar la tabla con HUE
/user/hive/warehouse/analyst.db/ratings

Cargando datos from HUE genera este querie:

LOAD DATA INPATH '/analyst/dualcore/ratings_2012.txt' INTO TABLE `analyst`.`ratings`

0: jdbc:hive2://localhost:10000> select count(*) from analyst.ratings;

[quickstart.cloudera:21000] > refresh analyst.ratings;
[quickstart.cloudera:21000] > select count (*) from analyst.ratings;
+----------+
| count(*) |
+----------+
| 9        |
+----------+

0: jdbc:hive2://localhost:10000> LOAD DATA INPATH '/analyst/dualcore/ratings_2013.txt' INTO TABLE analyst.ratings;
0: jdbc:hive2://localhost:10000> select count(*) from analyst.ratings;
+------+--+
| _c0  |
+------+--+
| 19   |
+------+--+

solucion del manual: create_ratings_table.sql 
---------------------------------------------
-- This table is created using Hue in the exercise,
-- but it could be created using the command below

DROP TABLE IF EXISTS analyst.ratings;

CREATE TABLE analyst.ratings
   (posted TIMESTAMP, 
    cust_id INT,
    prod_id INT,
    rating TINYINT,
    message STRING)       
  ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '\t';


Write and execute a CREATE TABLE statement to create an external table for the tab-delimited records in HDFS at /analyst/dualcore/employees2.txt. 
The format is shown below:
Field Name Field Type
emp_id STRING
fname STRING
lname STRING
address STRING
city STRING
state STRING
zipcode STRING
job_title STRING
email STRING
active STRING
salary INT

solucion del manual: create_employees_table.sql 
-----------------------------------------------
DROP TABLE IF EXISTS analyst.employees;

CREATE EXTERNAL TABLE analyst.employees 
   (emp_id STRING, 
    fname STRING,
    lname STRING,
    address STRING,
    city STRING,
    state STRING,
    zipcode STRING,
    job_title STRING,
    email STRING,
    active STRING,
    salary INT)
   ROW FORMAT DELIMITED 
      FIELDS TERMINATED BY '\t'
      LOCATION '/analyst/dualcore/employees';


CREATE EXTERNAL TABLE analyst.employees2(
     emp_id STRING, 
     fname STRING,
     lname STRING,
     address STRING,
     city STRING,
     state STRING,
     zipcode STRING,
     job_title STRING,
     email STRING,
     active STRING,
     salary INT)
 COMMENT 'This is the employees2 table'
 ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
 LOCATION '/user/hive/warehouse/analyst.db/employees2';

[cloudera@quickstart mysql]$ hdfs dfs -put employees2.txt /user/hive/warehouse/analyst.db/employees2/
[cloudera@quickstart mysql]$ hdfs dfs -ls /user/hive/warehouse/analyst.db/employees2/
Found 1 items
-rw-r--r--   1 cloudera supergroup    6707049 2021-03-29 03:35 /user/hive/warehouse/analyst.db/employees2/employees2.txt

REFRESH employees2;

solucion del manual: top_three_job_titles.sql 
---------------------------------------------
SELECT job_title, COUNT(*) AS num
    FROM analyst.employees
    GROUP BY job_title
    ORDER BY num DESC


[quickstart.cloudera:21000] > SELECT job_title, count(*) num 
FROM analyst.employees2 
GROUP BY job_title 
ORDER BY num DESC 
LIMIT 3;

Query: select job_title, count(*) num FROM analyst.employees2 GROUP BY job_title ORDER BY num desc limit 3
Query submitted at: 2021-03-29 03:49:28 (Coordinator: http://quickstart.cloudera:25000)
Query progress can be monitored at: http://quickstart.cloudera:25000/query_plan?query_id=984be365544463ae:6609e67f00000000
+-------------------+-------+
| job_title         | num   |
+-------------------+-------+
| Sales Associate   | 39747 |
| Cashier           | 13621 |
| Assistant Manager | 2055  |
+-------------------+-------+

Bonus Exercise 1: Using Apache Sqoop’s Hive Import Option to Create a Table

sqoop import \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root --password cloudera \
--fields-terminated-by '\t' \
--table suppliers \
--hive-import --hive-table analyst.suppliers2

ó

sqoop import \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root --password cloudera \
--fields-terminated-by '\t' \
--table suppliers \
--hive-import 
--hive-database analyst
--hive-table suppliers2

[quickstart.cloudera:21000] > invalidate metadata suppliers2;

[quickstart.cloudera:21000] > SELECT STATE, COUNT(supp_id)  
FROM analyst.suppliers2 
WHERE STATE = 'TX' 
GROUP BY STATE;
Query: select STATE, COUNT(supp_id)  FROM analyst.suppliers2 WHERE STATE = 'TX' GROUP BY STATE
Query submitted at: 2021-03-29 04:26:01 (Coordinator: http://quickstart.cloudera:25000)
Query progress can be monitored at: http://quickstart.cloudera:25000/query_plan?query_id=8a4f02ef6b4ee11f:940dd22100000000
+-------+----------------+
| state | count(supp_id) |
+-------+----------------+
| TX    | 9              |
+-------+----------------+

Bonus Exercise 2: Altering a Table

[quickstart.cloudera:21000] >  describe analyst.suppliers2;
Query: describe analyst.suppliers2
+---------+--------+---------+
| name    | type   | comment |
+---------+--------+---------+
| supp_id | int    |         |
| company | string |         |
| contact | string |         |
| address | string |         |
| city    | string |         |
| state   | string |         |
| zipcode | string |         |
| phone   | string |         |
+---------+--------+---------+

1. Use ALTER TABLE to rename the company column to name.

solucion del manual: alter_suppliers_rename_column.sql 
------------------------------------------------------

ALTER TABLE analyst.suppliers CHANGE company name STRING;

[quickstart.cloudera:21000] > ALTER TABLE analyst.suppliers2 CHANGE company name string;
Query: alter TABLE analyst.suppliers2 CHANGE company name string

2. Use the DESCRIBE command on the suppliers table to verify the change.
[quickstart.cloudera:21000] >  describe analyst.suppliers2;
Query: describe analyst.suppliers2
+---------+--------+---------+
| name    | type   | comment |
+---------+--------+---------+
| supp_id | int    |         |
| name    | string |         |
| contact | string |         |
| address | string |         |
| city    | string |         |
| state   | string |         |
| zipcode | string |         |
| phone   | string |         |
+---------+--------+---------+

3. Use ALTER TABLE to rename the entire table to vendors.
solucion del manual: alter_suppliers_rename_table.sql 
-----------------------------------------------------
ALTER TABLE analyst.suppliers RENAME TO vendors;

[quickstart.cloudera:21000] > ALTER TABLE analyst.suppliers2 RENAME TO analyst.vendors;
Query: alter TABLE analyst.suppliers2 RENAME TO analyst.vendors
Fetched 0 row(s) in 0.75s
[quickstart.cloudera:21000] > show tables;
Query: show tables
+---------------+
| name          |
+---------------+
| customers     |
| employees     |
| employees2    |
| order_details |
| orders        |
| products      |
| ratings       |
| suppliers     |
| vendors       |
+---------------+

4. Although the ALTER TABLE command often requires that we make a corresponding change to the data in HDFS, renaming a table or column does not.
You can verify this by running a query on the table using the new names, for example:
[quickstart.cloudera:21000] > SELECT supp_id, name FROM vendors LIMIT 10;
Query: select supp_id, name FROM vendors LIMIT 10
Query submitted at: 2021-03-29 04:45:40 (Coordinator: http://quickstart.cloudera:25000)
Query progress can be monitored at: http://quickstart.cloudera:25000/query_plan?query_id=7a464e6d8e77640f:af0c81000000000
+---------+-------------------------------------+
| supp_id | name                                |
+---------+-------------------------------------+
| 1000    | Smithco Equipment Manufacturing     |
| 1001    | Simian Tech Systems, Inc.           |
| 1002    | Division Zero Corp.                 |
| 1003    | General Diode Co.                   |
| 1004    | JPD Companies                       |
| 1005    | Harvey Mettle Electrical Supply Co. |
| 1006    | The Connector Company               |
| 1007    | Sturdy Widgets, Inc.                |
| 1008    | TBD Distribution, Inc.              |
| 1009    | Fuzzy Display Systems, Inc.         |
+---------+-------------------------------------+

[quickstart.cloudera:21000] > SELECT supp_id, company FROM suppliers2 LIMIT 10;
Query: select supp_id, company FROM suppliers2 LIMIT 10
Query submitted at: 2021-03-29 04:46:30 (Coordinator: http://quickstart.cloudera:25000)
ERROR: AnalysisException: Could not resolve table reference: 'suppliers2'


------------------
Hands-On Exercise: Data Storage and Performance
------------------
hdfs dfs -put whatever...  ad_data1 /analyst/dualcore
hdfs dfs -put whatever...  ad_data1 /analyst/dualcore

Creating and Loading a Static Partitioned Table

The following HDFS directories contain processed data about the ad networks:
Ad Network 1: /analyst/dualcore/ad_data1
Ad Network 2: /analyst/dualcore/ad_data2

The ad networks are not identified by a field in the data; instead, the networks are distinguished by keeping the data from the different networks in separate files. 
Now, however, we would like to be able to query the ad data in the same table without losing the ability to distinguish between the two networks. 
We can do this by creating a partition for each network.

1. Optional: View the first few lines in the data files for both networks.

[cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/ad_data1/
-rw-r--r--   1 cloudera supergroup   28123117 2021-04-01 03:40 /analyst/dualcore/ad_data1/part-m-00000

[cloudera@quickstart data]$ hdfs dfs -cat /analyst/dualcore/ad_data1/part-m-00000 |more

D8	05/01/2013	00:00:08	LIGHTWEIGHT	gamersite.example.com	SIDE	0	72
B1	05/01/2013	00:00:10	ACCELEROMETER	datawire.example.com	INLINE	0	78
D3	05/01/2013	00:00:16	PC	datasnap.example.com	BOTTOM	0	49
D7	05/01/2013	00:00:22	DUALCORE	datawire.example.com	SIDE	0	58
D7	05/01/2013	00:00:37	REVIEW	amateurcoder.example.com	SIDE	0	66
D5	05/01/2013	00:00:39	BROWSER	datascientist.example.com	INLINE	0	79
B2	05/01/2013	00:00:47	TOUCHSCREEN	burritofinder.example.com	SIDE	0	84
A2	05/01/2013	00:01:06	SOCIAL	photosite.example.com	TOP	0	82
C5	05/01/2013	00:01:16	ACCELEROMETER	linuxlife.example.com	TOP	0	92
D6	05/01/2013	00:01:23	SOCIAL	datasnap.example.com	INLINE	0	73


[cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/ad_data2/
-rw-r--r--   1 cloudera supergroup   22394686 2021-04-01 03:44 /analyst/dualcore/ad_data2/part-r-00000

[cloudera@quickstart data]$ hdfs dfs -cat /analyst/dualcore/ad_data2/part-r-00000 |more

A1	05/01/2013	00:00:47	CAMERA	techwiz.example.com	SIDE	0	62
A1	05/01/2013	00:00:55	E-BOOK	techfire.example.com	SIDE	0	62
A1	05/01/2013	00:01:21	DRAWING	techtips.example.com	SIDE	0	66
A1	05/01/2013	00:01:34	LAPTOP	technews.example.com	TOP	0	72
A1	05/01/2013	00:05:55	LCD	techpack.example.com	INLINE	0	61
A1	05/01/2013	00:07:31	SKETCH	techwire.example.com	TOP	0	76
A1	05/01/2013	00:10:52	CHAT	techpack.example.com	SIDE	0	58
A1	05/01/2013	00:11:57	VIRTUAL	trumpeter.example.com	SIDE	0	64
A1	05/01/2013	00:12:45	UI	siliconwire.example.com	SIDE	0	58
A1	05/01/2013	00:13:42	BLUETOOTH	techline.example.com	BOTTOM	0	64

2. Define a partitioned table in the analyst database called ads, with the following characteristics:
Type: EXTERNAL
Partition column: network (type TINYINT)
Field Terminator: \t (tab)
Location: /analyst/dualcore/ads
Columns:
        Field Name Field Type
	---------- ----------
        campaign_id STRING
        display_date STRING
        display_time STRING
        keyword STRING
        display_site STRING
        placement STRING
        was_clicked TINYINT
        cpc INT

CREATE EXTERNAL TABLE IF NOT EXISTS analyst.ads
  ( campaign_id STRING,
    display_date STRING,
    display_time STRING,
    keyword STRING,
    display_site STRING,
    placement STRING,
    was_clicked TINYINT,
    cpc INT
  )
  PARTITIONED BY (network TINYINT COMMENT 'network')
   COMMENT 'table partitioned by column network TINYINT'
   ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
  LOCATION '/analyst/dualcore/ads'


solucion del manual: create_ads_table.sql
-----------------------------------------
DROP TABLE IF EXISTS analyst.ads;

CREATE EXTERNAL TABLE analyst.ads ( 
    campaign_id STRING, 
    display_date STRING,
    display_time STRING, 
    keyword STRING,
    display_site STRING, 
    placement STRING,
    was_clicked TINYINT,
    cpc INT
  ) 
  PARTITIONED BY (network TINYINT)
  ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '\t'
  LOCATION '/analyst/dualcore/ads';

3. Alter the ads table to add two partitions, one for network 1 and one for network 2.

ALTER TABLE analyst.ads 
ADD IF NOT EXISTS PARTITION (network=1);

ALTER TABLE analyst.ads 
ADD IF NOT EXISTS PARTITION (network=2);

[cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/ads/
drwxr-xr-x   - impala supergroup          0 2021-04-01 04:23 /analyst/dualcore/ads/network=1
drwxr-xr-x   - impala supergroup          0 2021-04-01 04:23 /analyst/dualcore/ads/network=2

4. Load the data in /analyst/dualcore/ad_data1 into the Network 1 partition.

LOAD DATA INPATH '/analyst/dualcore/ad_data1/'
INTO TABLE analyst.ads
PARTITION (network=1);

0: jdbc:hive2://localhost:10000> LOAD DATA INPATH '/analyst/dualcore/ad_data1/'
0: jdbc:hive2://localhost:10000> INTO TABLE analyst.ads
0: jdbc:hive2://localhost:10000> PARTITION (network=1);

-- a "movido el fichero a la partición correspondiente y ya no esta aquí:"
[cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/ad_data1/

6. Verify that the data for both ad networks were correctly loaded by counting the
0: jdbc:hive2://localhost:10000> SELECT COUNT(*) FROM analyst.ads WHERE network=1 ;
+---------+--+
|   _c0   |
+---------+--+
| 438389  |


5. Load the data in /analyst/dualcore/ad_data2 into the Network 2 partition.

[cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/ad_data2/
-rw-r--r--   1 cloudera supergroup   22394686 2021-04-01 03:44 /analyst/dualcore/ad_data2/part-r-00000

0: jdbc:hive2://localhost:10000> LOAD DATA INPATH '/analyst/dualcore/ad_data2/part-r-00000' INTO TABLE analyst.ads PARTITION (network=2);

6. Verify that the data for both ad networks were correctly loaded by counting the
0: jdbc:hive2://localhost:10000> SELECT COUNT(*) FROM analyst.ads WHERE network=2 ;
+---------+--+
|   _c0   |
+---------+--+
| 350563  |
+---------+--+

-- a "movido el fichero a la partición correspondiente y ya no esta aquí:"
[cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/ad_data2/

Note: If you are using Hue and put multiple commands in the Query Editor at the
same time, Hue runs them one at a time. After it runs a command, Hue will report
the results, then wait for another click of the Execute button before continuing.
Network 1 should have 438,389 records and Network 2 should have 350,563
records.

solucion del manual: load_ad_data.sql 
-------------------------------------
ALTER TABLE analyst.ads ADD PARTITION (network=1);
ALTER TABLE analyst.ads ADD PARTITION (network=2);

LOAD DATA INPATH '/analyst/dualcore/ad_data1' 
  INTO TABLE analyst.ads
  PARTITION(network=1);
  
LOAD DATA INPATH '/analyst/dualcore/ad_data2' 
  INTO TABLE analyst.ads
  PARTITION(network=2);

Bonus Exercise 1: Accessing Data in Parquet File Format
For this exercise, you have been provided data in Parquet format that is output from another tool—in this example, the output from an ETL process that maps US zip codes
to latitude and longitude. 
You need to be able to query this data in Impala.

1. The ETL data output directory is provided as $ADIR/data/latlon. Copy the data directory to the /analyst/dualcore directory in HDFS. You can use the Hue File

Browser, or the following hdfs command:
$ hdfs dfs -put /home/cloudera/training_materials/data/latlon /analyst/dualcore/

2. Using Impala, create an external Parquet-based table in the analyst database pointing to the latlon data directory.

• Hint: The actual data in the data directory is in a MapReduce output file called part-m-000000_0.parquet. 
Use the LIKE PARQUET command to use the existing column definitions in the data file.

DROP TABLE IF EXISTS analyst.latlon;
CREATE EXTERNAL TABLE analyst.latlon
LIKE PARQUET '/analyst/dualcore/latlon/part-m-000000_0.parquet'
STORED AS PARQUET
LOCATION '/analyst/dualcore/laton/';

3. Review the table in Impala to confirm it was correctly created with columns zip, latitude and longitude.
DESCRIBE EXTENDED analyst.latlon;
+------------------------------+----------------------------------------------------------------+-----------------------------+
| name                         | type                                                           | comment                     |
+------------------------------+----------------------------------------------------------------+-----------------------------+
| # col_name                   | data_type                                                      | comment                     |
|                              | NULL                                                           | NULL                        |
| zip                          | string                                                         | Inferred from Parquet file. |
| latitude                     | decimal(8,5)                                                   | Inferred from Parquet file. |
| longitude                    | decimal(8,5)                                                   | Inferred from Parquet file. |
|                              | NULL                                                           | NULL                        |

4. Perform a query or preview the data in the Impala Query Editor to confirm that the data in the data file is being accessed correctly
SELECT * 
FROM analyst.latlon;

solucion del manual: create_latlon_table.sql
--------------------------------------------
DROP TABLE IF EXISTS analyst.latlon;
CREATE EXTERNAL TABLE analyst.latlon 
  LIKE PARQUET '/analyst/dualcore/latlon/part-m-000000_0.parquet' 
  STORED AS PARQUET 
  LOCATION '/analyst/dualcore/latlon';  

------------------
Hands-On Exercise: Working with Multiple Datasets
------------------
Calculating the Top N Products
• Which top three products have been sold more than any other products? 
(Hint: Remember that if you use a GROUP BY clause, you must group by all fields listed in the SELECT clause that are not part of an aggregate function.)

SELECT p.prod_id, count(p.prod_id) n_sold
FROM analyst.products p
JOIN analyst.order_details od
ON (p.prod_id=od.prod_id)
GROUP BY p.prod_id
ORDER BY n_sold DESC
LIMIT 3
;

-------------------------------------------
solucion del manual: top_three_products.sql
-------------------------------------------
-- Note: Whether p.prod_id should be included in the GROUP BY
-- is arguable. There are items in the products table with 
-- the same brand and name but different prod_id, and different
-- price and cost. Whether these should be kept separate or
-- combined is a matter of interpretation.

SELECT brand, name, COUNT(p.prod_id) AS sold 
  FROM analyst.products p
    JOIN analyst.order_details d
      ON (p.prod_id = d.prod_id)
  GROUP BY brand, name, p.prod_id
  ORDER BY sold DESC
  LIMIT 3;

Calculating the Order Total
• Which ten orders had the highest total dollar amounts?

SELECT o.order_id, sum(p.price) total
FROM analyst.products p
JOIN analyst.order_details od ON (p.prod_id=od.prod_id)
JOIN analyst.orders o ON (o.order_id=od.order_id)
GROUP BY o.order_id
ORDER BY total DESC
LIMIT 10
;
---------------------------------------
solucion del manual: top_ten_orders.sql
---------------------------------------
SELECT od.order_id, SUM(p.price) AS total 
  FROM analyst.order_details od
    JOIN analyst.products p 
      ON (od.prod_id = p.prod_id)
  GROUP BY order_id 
  ORDER BY total DESC
  LIMIT 10;

Calculating Revenue and Profit
• Run a query to show Dualcore’s revenue (total price of products sold) and profit (price minus cost) by date.
◦ Hint: The order_date column in the orders table is of type TIMESTAMP. Use the function to_date to get just the date portion of the value.

SELECT to_date(o.order_date), sum(p.price) revenue, sum(p.price - p.cost) profit
FROM analyst.products p
JOIN analyst.order_details od ON (p.prod_id=od.prod_id)
JOIN analyst.orders o ON (o.order_id=od.order_id)
GROUP BY to_date(o.order_date)
ORDER BY to_date(o.order_date) 
LIMIT 10
;

SELECT to_date(o.order_date) o_date, sum(p.price) revenue, sum(p.price - p.cost) profit
FROM analyst.products p
JOIN analyst.order_details od ON (p.prod_id=od.prod_id)
JOIN analyst.orders o ON (o.order_id=od.order_id)
GROUP BY o_date
ORDER BY o_date
LIMIT 10
;

--------------------------------------
solucion del manual: daily_profits.sql 
--------------------------------------

SELECT to_date(order_date) AS date_ordered, sum(price) AS revenue, sum(price-cost) AS profit
  FROM analyst.orders o
    JOIN analyst.order_details d ON o.order_id = d.order_id
    JOIN analyst.products p ON d.prod_id = p.prod_id
    GROUP BY to_date(order_date) LIMIT 10;
    
• Run a query to show each day’s profit. Limit the results to 20 rows.

SELECT to_date(o.order_date), sum(p.price - p.cost) profit
FROM analyst.products p
JOIN analyst.order_details od ON (p.prod_id=od.prod_id)
JOIN analyst.orders o ON (o.order_id=od.order_id)
GROUP BY to_date(o.order_date)
LIMIT 20
;

--------------------------------------
solucion del manual: daily_profits.sql
--------------------------------------
SELECT to_date(order_date) AS date_ordered, sum(price-cost) AS profit
  FROM analyst.orders o
    JOIN analyst.order_details d ON o.order_id = d.order_id
    JOIN analyst.products p ON d.prod_id = p.prod_id
    GROUP BY to_date(order_date) LIMIT 20;

• Run a query to give only the daily profits for May, 2013, in chronological order.
Hint: You’ll need to use the HAVING clause. There are several ways to match the date.

SELECT to_date(o.order_date), sum(p.price - p.cost) profit
FROM analyst.products p
JOIN analyst.order_details od ON (p.prod_id=od.prod_id)
JOIN analyst.orders o ON (o.order_id=od.order_id)
WHERE year(o.order_date) = 2013
  AND month(o.order_date) = 05
GROUP BY to_date(o.order_date)
ORDER BY to_date(o.order_date)
;

-----------------------------------------------
solucion del manual: daily_profits_May_2013.sql
-----------------------------------------------

SELECT to_date(order_date) AS date_ordered, sum(price-cost) AS profit
  FROM analyst.orders o
    JOIN analyst.order_details d ON o.order_id = d.order_id
    JOIN analyst.products p ON d.prod_id = p.prod_id
    GROUP BY to_date(order_date)
      HAVING cast(to_date(order_date) AS STRING)
        BETWEEN '2013-05-01' AND '2013-05-31'
    ORDER BY date_ordered;

For example, you can cast the date as a STRING or use to_timestamp(string, string pattern). In both cases, you might try using BETWEEN ... 
AND for the comparison operator, or use <= and >=. You could also extract and match the month and year separately and use the = operator.

• Run a query to show the profits by month, in order chronologically.
Hint: You want to order by year first, and then month.

SELECT month(o.order_date) m, year(o.order_date) a, sum(p.price - p.cost) profit
FROM analyst.products p
JOIN analyst.order_details od ON (p.prod_id=od.prod_id)
JOIN analyst.orders o ON (o.order_id=od.order_id)
GROUP BY month(o.order_date), year(o.order_date)
ORDER BY a, m
;

----------------------------------------
solucion del manual: monthly_profits.sql
----------------------------------------

SELECT month(order_date) AS m, year(order_date) as y, sum(price-cost) AS profit
  FROM analyst.orders o
    JOIN analyst.order_details d ON o.order_id = d.order_id
    JOIN analyst.products p ON d.prod_id = p.prod_id
    GROUP BY year(order_date),month(order_date) ORDER BY y,m;

;

• Find the four most profitable months and the four least profitable months.

SELECT month(o.order_date) m, year(o.order_date) a, sum(p.price - p.cost) profit
FROM analyst.products p
JOIN analyst.order_details od ON (p.prod_id=od.prod_id)
JOIN analyst.orders o ON (o.order_id=od.order_id)
GROUP BY month(o.order_date), year(o.order_date)
ORDER BY profit
LIMIT 4
;

SELECT month(o.order_date) m, year(o.order_date) a, sum(p.price - p.cost) profit
FROM analyst.products p
JOIN analyst.order_details od ON (p.prod_id=od.prod_id)
JOIN analyst.orders o ON (o.order_id=od.order_id)
GROUP BY month(o.order_date), year(o.order_date)
ORDER BY profit DESC
LIMIT 4
;

-----------------------------------------------
solucion del manual: most_profitable_months.sql
-----------------------------------------------
-- Most profitable months
SELECT month(order_date) AS m, year(order_date) as y, sum(price-cost) AS profit
  FROM analyst.orders o
    JOIN analyst.order_details d ON o.order_id = d.order_id
    JOIN analyst.products p ON d.prod_id = p.prod_id
    GROUP BY month(order_date),year(order_date) ORDER BY profit DESC LIMIT 4;

-- Least profitable months
SELECT month(order_date) AS m, year(order_date) as y,
 sum(price-cost) AS profit
  FROM analyst.orders o
    JOIN analyst.order_details d ON o.order_id = d.order_id
    JOIN analyst.products p ON d.prod_id = p.prod_id
    GROUP BY month(order_date),year(order_date) ORDER BY profit ASC LIMIT 4;

------------------
Hands-On Exercise: Analytic Functions
------------------
1. Review the ads table to familiarize yourself with its contents.

2. How many ads were displayed per site per day?
Hint: You don’t need to use analytic functions or windowing for this one.

SELECT display_site, display_date, count(campaign_id) num
FROM analyst.ads
GROUP BY display_site, display_date
;

| megasource.example.com        | 05/04/2013   | 297 |
| bitpress.example.com          | 05/10/2013   | 191 |
| bitspan.example.com           | 05/29/2013   | 191 |
| lawnmaster.example.com        | 05/18/2013   | 185 |
| movienet.example.com          | 05/02/2013   | 261 |
| jokeport.example.com          | 05/12/2013   | 89  |
| sysadmin.example.com          | 05/09/2013   | 203 |
| grillingtips.example.com      | 05/20/2013   | 91  |
| techpack.example.com          | 05/12/2013   | 248 |
| budgetravel.example.com       | 05/29/2013   | 240 |
| burritofinder.example.com     | 05/12/2013   | 83  |
| dealreviewer.example.com      | 05/26/2013   | 188 |
| dvdreview.example.com         | 05/31/2013   | 279 |
| supportwiz.example.com        | 05/23/2013   | 259 |
| codemonkeys.example.com       | 05/05/2013   | 187 |
| salesmaster.example.com       | 05/31/2013   | 176 |
| gigaspan.example.com          | 05/22/2013   | 189 |
| tabletblog.example.com        | 05/08/2013   | 773 |
| videophile.example.com        | 05/08/2013   | 516 |
| techtips.example.com          | 05/25/2013   | 259 |
| masterbaker.example.com       | 05/22/2013   | 78  |
| mobilenews.example.com        | 05/30/2013   | 242 |
| lawblog.example.com           | 05/16/2013   | 164 |
| techlink.example.com          | 05/09/2013   | 277 |
| dealgamer.example.com         | 05/02/2013   | 66  |
| homemechanic.example.com      | 05/26/2013   | 72  |
| bitport.example.com           | 05/10/2013   | 167 |
| techsizzle.example.com        | 05/29/2013   | 372 |
| 1337hax4u.example.com         | 05/03/2013   | 233 |
| techflash.example.com         | 05/13/2013   | 392 |
| techfocus.example.com         | 05/14/2013   | 250 |
| techreviews.example.com       | 05/29/2013   | 517 |
| techwire.example.com          | 05/06/2013   | 266 |
| marketingfocus.example.com    | 05/05/2013   | 188 |
| vbscriptcentral.example.com   | 05/08/2013   | 262 |
| ultradeals.example.com        | 05/19/2013   | 251 |
| footwear.example.com          | 05/14/2013   | 74  |
| siliconcenter.example.com     | 05/12/2013   | 242 |
| backupwiz.example.com         | 05/31/2013   | 160 |
| mobiletips.example.com        | 05/08/2013   | 239 |
| techbuzz.example.com          | 05/05/2013   | 468 |
| procoder.example.com          | 05/23/2013   | 193 |
| sysadmin.example.com          | 05/15/2013   | 242 |
| datawire.example.com          | 05/29/2013   | 188 |
| cellcore.example.com          | 05/24/2013   | 382 |
| plumbing101.example.com       | 05/06/2013   | 174 |
| dealmonkey.example.com        | 05/17/2013   | 179 |
| techspace.example.com         | 05/30/2013   | 259 |
| lawnmaster.example.com        | 05/04/2013   | 169 |
| techblog.example.com          | 05/31/2013   | 390 |
| firesale.example.com          | 05/28/2013   | 185 |
| megasource.example.com        | 05/18/2013   | 270 |
| pcexpert.example.com          | 05/15/2013   | 178 |
| datascientist.example.com     | 05/13/2013   | 181 |
| lawnphotos.example.com        | 05/14/2013   | 72  |
| megatips.example.com          | 05/12/2013   | 83  |
| xmlhome.example.com           | 05/18/2013   | 130 |
| digicams.example.com          | 05/02/2013   | 185 |
| megaflash.example.com         | 05/02/2013   | 106 |
| photosite.example.com         | 05/16/2013   | 274 |
| butterworld.example.com       | 05/09/2013   | 75  |
| bitfancy.example.com          | 05/18/2013   | 96  |
| bitsizzler.example.com        | 05/23/2013   | 94  |
| tabletreviews.example.com     | 05/03/2013   | 864 |
| megawave.example.com          | 05/14/2013   | 83  |
| linuxlife.example.com         | 05/23/2013   | 171 |
| superdeals.example.com        | 05/13/2013   | 393 |
| techline.example.com          | 05/06/2013   | 270 |
| mobiletips.example.com        | 05/14/2013   | 251 |
| filmbuffs.example.com         | 05/02/2013   | 252 |
| techbuzz.example.com          | 05/19/2013   | 526 |
| audioexpert.example.com       | 05/02/2013   | 345 |
| ultradeals.example.com        | 05/05/2013   | 268 |
| footwear.example.com          | 05/08/2013   | 85  |
| technews.example.com          | 05/01/2013   | 370 |
| marketingfocus.example.com    | 05/19/2013   | 192 |
| vbscriptcentral.example.com   | 05/14/2013   | 257 |
| techfire.example.com          | 05/10/2013   | 393 |
| granolaworld.example.com      | 05/26/2013   | 99  |
| techfocus.example.com         | 05/08/2013   | 239 |
| filmport.example.com          | 05/20/2013   | 187 |
| ccpstudygroup.example.com     | 05/23/2013   | 79  |
| siliconwire.example.com       | 05/03/2013   | 254 |
| printoperator.example.com     | 05/27/2013   | 89  |
| techlink.example.com          | 05/15/2013   | 246 |
| videophile.example.com        | 05/14/2013   | 488 |
| gamersite.example.com         | 05/16/2013   | 278 |
| tabletblog.example.com        | 05/14/2013   | 805 |
| audiophile.example.com        | 05/31/2013   | 541 |
| codemonkeys.example.com       | 05/19/2013   | 174 |
| fashionforward.example.com    | 05/30/2013   | 175 |
| necktieworld.example.com      | 05/12/2013   | 243 |
| trumpeter.example.com         | 05/10/2013   | 250 |
| homevideo.example.com         | 05/20/2013   | 258 |
| mobilesource.example.com      | 05/24/2013   | 260 |
+-------------------------------+--------------+-----+

3. For each day, how did each site rank in number of displays?
   Hint: Use your previous query as a subquery.
   Hint: To make it easy to see if you have done this correctly, first sort by day, and then by rank within each day.

SELECT display_site, display_date, num, RANK() OVER (PARTITION BY display_date ORDER BY num DESC) rnk
  FROM (SELECT display_site, display_date, count(campaign_id) num
          FROM analyst.ads
        GROUP BY display_site, display_date) sq
ORDER BY display_date, rnk
;

| tabletreviews.example.com     | 05/31/2013   | 883 | 1   |
| salestiger.example.com        | 05/31/2013   | 785 | 2   |
| tabletblog.example.com        | 05/31/2013   | 776 | 3   |
| audiophile.example.com        | 05/31/2013   | 541 | 4   |
| techreviews.example.com       | 05/31/2013   | 517 | 5   |
| techbuzz.example.com          | 05/31/2013   | 509 | 6   |
| videophile.example.com        | 05/31/2013   | 479 | 7   |
| techbits.example.com          | 05/31/2013   | 411 | 8   |
| technews.example.com          | 05/31/2013   | 403 | 9   |
| techsizzle.example.com        | 05/31/2013   | 403 | 9   |
| superdeals.example.com        | 05/31/2013   | 398 | 11  |
| techfire.example.com          | 05/31/2013   | 395 | 12  |
| techblog.example.com          | 05/31/2013   | 390 | 13  |
| audioexpert.example.com       | 05/31/2013   | 367 | 14  |
| techflash.example.com         | 05/31/2013   | 360 | 15  |
| albumreview.example.com       | 05/31/2013   | 358 | 16  |
| cellnews.example.com          | 05/31/2013   | 352 | 17  |
| dealfinder.example.com        | 05/31/2013   | 343 | 18  |
| cellcore.example.com          | 05/31/2013   | 309 | 19  |
| mobilesource.example.com      | 05/31/2013   | 296 | 20  |
| mobiletips.example.com        | 05/31/2013   | 290 | 21  |
| siliconcenter.example.com     | 05/31/2013   | 285 | 22  |
| dvdreview.example.com         | 05/31/2013   | 279 | 23  |
| ultranet.example.com          | 05/31/2013   | 279 | 23  |
| racecars.example.com          | 05/31/2013   | 278 | 25  |
| necktieworld.example.com      | 05/31/2013   | 276 | 26  |
| movienet.example.com          | 05/31/2013   | 271 | 27  |
| photosite.example.com         | 05/31/2013   | 267 | 28  |
| mobilenews.example.com        | 05/31/2013   | 265 | 29  |
| siliconbuzz.example.com       | 05/31/2013   | 264 | 30  |
| vbscriptcentral.example.com   | 05/31/2013   | 256 | 31  |
| megasource.example.com        | 05/31/2013   | 256 | 31  |
| sixminuteabs.example.com      | 05/31/2013   | 255 | 33  |
| 1337hax4u.example.com         | 05/31/2013   | 254 | 34  |
| techpack.example.com          | 05/31/2013   | 254 | 34  |
| homevideo.example.com         | 05/31/2013   | 252 | 36  |
| supportwiz.example.com        | 05/31/2013   | 251 | 37  |
| toastrecipes.example.com      | 05/31/2013   | 251 | 37  |
| techwiz.example.com           | 05/31/2013   | 249 | 39  |
| budgetravel.example.com       | 05/31/2013   | 249 | 39  |
| ultradeals.example.com        | 05/31/2013   | 248 | 41  |
| datasnap.example.com          | 05/31/2013   | 246 | 42  |
| trumpeter.example.com         | 05/31/2013   | 245 | 43  |
| siliconwire.example.com       | 05/31/2013   | 245 | 43  |
| techfocus.example.com         | 05/31/2013   | 244 | 45  |
| techwire.example.com          | 05/31/2013   | 244 | 45  |
| techspace.example.com         | 05/31/2013   | 240 | 47  |
| gamersite.example.com         | 05/31/2013   | 237 | 48  |
| siliconshore.example.com      | 05/31/2013   | 236 | 49  |
| techlink.example.com          | 05/31/2013   | 235 | 50  |
| techline.example.com          | 05/31/2013   | 232 | 51  |
| sysadmin.example.com          | 05/31/2013   | 231 | 52  |
| techtips.example.com          | 05/31/2013   | 230 | 53  |
| filmbuffs.example.com         | 05/31/2013   | 224 | 54  |
| bitslingers.example.com       | 05/31/2013   | 207 | 55  |
| filmport.example.com          | 05/31/2013   | 196 | 56  |
| datawire.example.com          | 05/31/2013   | 191 | 57  |
| bitpress.example.com          | 05/31/2013   | 190 | 58  |
| lawnmaster.example.com        | 05/31/2013   | 186 | 59  |
| plumbing101.example.com       | 05/31/2013   | 181 | 60  |
| gamernews.example.com         | 05/31/2013   | 180 | 61  |
| dealreviewer.example.com      | 05/31/2013   | 178 | 62  |
| diskcentral.example.com       | 05/31/2013   | 177 | 63  |
| salesmaster.example.com       | 05/31/2013   | 176 | 64  |
| gigaspan.example.com          | 05/31/2013   | 174 | 65  |
| procoder.example.com          | 05/31/2013   | 174 | 65  |
| gardengrower.example.com      | 05/31/2013   | 173 | 67  |
| datascientist.example.com     | 05/31/2013   | 173 | 67  |
| dealmonkey.example.com        | 05/31/2013   | 173 | 67  |
| digicams.example.com          | 05/31/2013   | 169 | 70  |
| codemonkeys.example.com       | 05/31/2013   | 168 | 71  |
| marketingfocus.example.com    | 05/31/2013   | 167 | 72  |
| bitspan.example.com           | 05/31/2013   | 167 | 72  |
| fashionforward.example.com    | 05/31/2013   | 167 | 72  |
| datanews.example.com          | 05/31/2013   | 166 | 75  |
| lawblog.example.com           | 05/31/2013   | 164 | 76  |
| expertlink.example.com        | 05/31/2013   | 163 | 77  |
| pcexpert.example.com          | 05/31/2013   | 163 | 77  |
| firesale.example.com          | 05/31/2013   | 162 | 79  |
| linuxlife.example.com         | 05/31/2013   | 162 | 79  |
| cpusite.example.com           | 05/31/2013   | 161 | 81  |
| backupwiz.example.com         | 05/31/2013   | 160 | 82  |
| bytewiz.example.com           | 05/31/2013   | 157 | 83  |
| xmlhome.example.com           | 05/31/2013   | 146 | 84  |
| bitport.example.com           | 05/31/2013   | 142 | 85  |
| betamaxholdouts.example.com   | 05/31/2013   | 104 | 86  |
| megaflash.example.com         | 05/31/2013   | 101 | 87  |
| megatips.example.com          | 05/31/2013   | 100 | 88  |
| printoperator.example.com     | 05/31/2013   | 96  | 89  |
| dbacentral.example.com        | 05/31/2013   | 94  | 90  |
| burritofinder.example.com     | 05/31/2013   | 93  | 91  |
| masterbaker.example.com       | 05/31/2013   | 91  | 92  |
| hadoopusers.example.com       | 05/31/2013   | 90  | 93  |
| bitsizzler.example.com        | 05/31/2013   | 90  | 93  |
| bassoonenthusiast.example.com | 05/31/2013   | 89  | 95  |
| bitfancy.example.com          | 05/31/2013   | 88  | 96  |
| megawave.example.com          | 05/31/2013   | 87  | 97  |
| amateurcoder.example.com      | 05/31/2013   | 87  | 97  |
| coffeenews.example.com        | 05/31/2013   | 86  | 99  |
| ccpstudygroup.example.com     | 05/31/2013   | 85  | 100 |
| granolaworld.example.com      | 05/31/2013   | 85  | 100 |
| homemechanic.example.com      | 05/31/2013   | 83  | 102 |
| dealgamer.example.com         | 05/31/2013   | 82  | 103 |
| architectworld.example.com    | 05/31/2013   | 81  | 104 |
| lawnphotos.example.com        | 05/31/2013   | 79  | 105 |
| footwear.example.com          | 05/31/2013   | 79  | 105 |
| grillingtips.example.com      | 05/31/2013   | 77  | 107 |
| jokeport.example.com          | 05/31/2013   | 76  | 108 |
| butterworld.example.com       | 05/31/2013   | 76  | 108 |
+-------------------------------+--------------+-----+-----+

4. For each day, how did each site’s daily count compare to the previous day?
   The results should display the site’s daily count and the previous day’s count in the same row.
   Put the sites together in alphanumerical order, and then within each site, put the daily counts in order.

SELECT display_site, display_date, num, LAG(num) OVER (PARTITION BY display_site ORDER BY display_date) AS lag
  FROM (
    SELECT display_site, display_date, COUNT(display_date) AS num
      FROM analyst.ads 
    GROUP BY display_site, display_date) sq
  ORDER BY display_site, display_date;

  xmlhome.example.com         | 05/01/2013   | 119 | NULL |
| xmlhome.example.com         | 05/02/2013   | 141 | 119  |
| xmlhome.example.com         | 05/03/2013   | 112 | 141  |
| xmlhome.example.com         | 05/04/2013   | 116 | 112  |
| xmlhome.example.com         | 05/05/2013   | 129 | 116  |
| xmlhome.example.com         | 05/06/2013   | 122 | 129  |
| xmlhome.example.com         | 05/07/2013   | 125 | 122  |
| xmlhome.example.com         | 05/08/2013   | 142 | 125  |
| xmlhome.example.com         | 05/09/2013   | 112 | 142  |
| xmlhome.example.com         | 05/10/2013   | 132 | 112  |
| xmlhome.example.com         | 05/11/2013   | 133 | 132  |
| xmlhome.example.com         | 05/12/2013   | 134 | 133  |
| xmlhome.example.com         | 05/13/2013   | 139 | 134  |
| xmlhome.example.com         | 05/14/2013   | 124 | 139  |
| xmlhome.example.com         | 05/15/2013   | 122 | 124  |
| xmlhome.example.com         | 05/16/2013   | 105 | 122  |
| xmlhome.example.com         | 05/17/2013   | 112 | 105  |
| xmlhome.example.com         | 05/18/2013   | 130 | 112  |
| xmlhome.example.com         | 05/19/2013   | 119 | 130  |
| xmlhome.example.com         | 05/20/2013   | 126 | 119  |
| xmlhome.example.com         | 05/21/2013   | 148 | 126  |
| xmlhome.example.com         | 05/22/2013   | 105 | 148  |
| xmlhome.example.com         | 05/23/2013   | 124 | 105  |
| xmlhome.example.com         | 05/24/2013   | 136 | 124  |
| xmlhome.example.com         | 05/25/2013   | 124 | 136  |
| xmlhome.example.com         | 05/26/2013   | 133 | 124  |
| xmlhome.example.com         | 05/27/2013   | 127 | 133  |
| xmlhome.example.com         | 05/28/2013   | 126 | 127  |
| xmlhome.example.com         | 05/29/2013   | 124 | 126  |
| xmlhome.example.com         | 05/30/2013   | 140 | 124  |
| xmlhome.example.com         | 05/31/2013   | 146 | 140

--5. What is the per-site average count for a seven-day period?
-- Use a sliding window (with ROWS BETWEEN ...) 
    
SELECT display_site, display_date, num, AVG(num) OVER (PARTITION BY display_site ORDER BY display_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW ) seven_day
FROM( SELECT display_site, display_date, count(campaign_id) num
        FROM analyst.ads
      GROUP BY display_site, display_date) sq
--WHERE (display_site = '1337hax4u.example.com')
ORDER BY display_site, display_date
;
+-----------------------+--------------+-----+-------------------+
| display_site          | display_date | num | seven_day         |
+-----------------------+--------------+-----+-------------------+
| 1337hax4u.example.com | 05/01/2013   | 264 | 264               |
| 1337hax4u.example.com | 05/02/2013   | 252 | 258               |
| 1337hax4u.example.com | 05/03/2013   | 233 | 249.6666666666667 |
| 1337hax4u.example.com | 05/04/2013   | 262 | 252.75            |
| 1337hax4u.example.com | 05/05/2013   | 255 | 253.2             |
| 1337hax4u.example.com | 05/06/2013   | 244 | 251.6666666666667 |
| 1337hax4u.example.com | 05/07/2013   | 260 | 252.8571428571429 |
| 1337hax4u.example.com | 05/08/2013   | 255 | 251.5714285714286 |
| 1337hax4u.example.com | 05/09/2013   | 256 | 252.1428571428571 |
| 1337hax4u.example.com | 05/10/2013   | 260 | 256               |
| 1337hax4u.example.com | 05/11/2013   | 265 | 256.4285714285714 |
| 1337hax4u.example.com | 05/12/2013   | 272 | 258.8571428571428 |
| 1337hax4u.example.com | 05/13/2013   | 247 | 259.2857142857143 |
| 1337hax4u.example.com | 05/14/2013   | 246 | 257.2857142857143 |
| 1337hax4u.example.com | 05/15/2013   | 242 | 255.4285714285714 |
| 1337hax4u.example.com | 05/16/2013   | 263 | 256.4285714285714 |
| 1337hax4u.example.com | 05/17/2013   | 281 | 259.4285714285714 |
| 1337hax4u.example.com | 05/18/2013   | 279 | 261.4285714285714 |
| 1337hax4u.example.com | 05/19/2013   | 242 | 257.1428571428572 |
| 1337hax4u.example.com | 05/20/2013   | 263 | 259.4285714285714 |
| 1337hax4u.example.com | 05/21/2013   | 277 | 263.8571428571428 |
| 1337hax4u.example.com | 05/22/2013   | 259 | 266.2857142857143 |
| 1337hax4u.example.com | 05/23/2013   | 243 | 263.4285714285714 |
| 1337hax4u.example.com | 05/24/2013   | 252 | 259.2857142857143 |
| 1337hax4u.example.com | 05/25/2013   | 260 | 256.5714285714286 |
| 1337hax4u.example.com | 05/26/2013   | 259 | 259               |
| 1337hax4u.example.com | 05/27/2013   | 281 | 261.5714285714286 |
| 1337hax4u.example.com | 05/28/2013   | 267 | 260.1428571428572 |
| 1337hax4u.example.com | 05/29/2013   | 258 | 260               |
| 1337hax4u.example.com | 05/30/2013   | 266 | 263.2857142857143 |
| 1337hax4u.example.com | 05/31/2013   | 254 | 263.5714285714286 |
+-----------------------+--------------+-----+-------------------+


-- Otros ejemplo para ver lo de Window Clause
SELECT display_site, display_date, num,
       AVG(num) OVER (PARTITION BY display_site ORDER BY display_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW ) seven_day,
       SUM(num) OVER (PARTITION BY display_site ORDER BY display_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS row_sum_unbound_current,
       SUM(num) OVER (PARTITION BY display_site ORDER BY display_date RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS range_sum_unbound_current_
FROM( SELECT display_site, display_date, count(campaign_id) num
        FROM analyst.ads
      GROUP BY display_site, display_date) sq
--WHERE (display_site = '1337hax4u.example.com')
ORDER BY display_site, display_date
;

+-------------------------------+--------------+-----+-------------------+-------------------------+----------------------------+
| display_site                  | display_date | num | seven_day         | row_sum_unbound_current | range_sum_unbound_current_ |
+-------------------------------+--------------+-----+-------------------+-------------------------+----------------------------+
| 1337hax4u.example.com         | 05/01/2013   | 264 | 264               | 264                     | 264                        |
| 1337hax4u.example.com         | 05/02/2013   | 252 | 258               | 516                     | 516                        |
| 1337hax4u.example.com         | 05/03/2013   | 233 | 249.6666666666667 | 749                     | 749                        |
| 1337hax4u.example.com         | 05/04/2013   | 262 | 252.75            | 1011                    | 1011                       |
| 1337hax4u.example.com         | 05/05/2013   | 255 | 253.2             | 1266                    | 1266                       |
| 1337hax4u.example.com         | 05/06/2013   | 244 | 251.6666666666667 | 1510                    | 1510                       |
| 1337hax4u.example.com         | 05/07/2013   | 260 | 252.8571428571429 | 1770                    | 1770                       |
| 1337hax4u.example.com         | 05/08/2013   | 255 | 251.5714285714286 | 2025                    | 2025                       |
| 1337hax4u.example.com         | 05/09/2013   | 256 | 252.1428571428571 | 2281                    | 2281                       |
| 1337hax4u.example.com         | 05/10/2013   | 260 | 256               | 2541                    | 2541                       |
| 1337hax4u.example.com         | 05/11/2013   | 265 | 256.4285714285714 | 2806                    | 2806                       |
| 1337hax4u.example.com         | 05/12/2013   | 272 | 258.8571428571428 | 3078                    | 3078                       |
| 1337hax4u.example.com         | 05/13/2013   | 247 | 259.2857142857143 | 3325                    | 3325                       |
| 1337hax4u.example.com         | 05/14/2013   | 246 | 257.2857142857143 | 3571                    | 3571                       |
| 1337hax4u.example.com         | 05/15/2013   | 242 | 255.4285714285714 | 3813                    | 3813                       |
| 1337hax4u.example.com         | 05/16/2013   | 263 | 256.4285714285714 | 4076                    | 4076                       |
| 1337hax4u.example.com         | 05/17/2013   | 281 | 259.4285714285714 | 4357                    | 4357                       |
| 1337hax4u.example.com         | 05/18/2013   | 279 | 261.4285714285714 | 4636                    | 4636                       |
| 1337hax4u.example.com         | 05/19/2013   | 242 | 257.1428571428572 | 4878                    | 4878                       |
| 1337hax4u.example.com         | 05/20/2013   | 263 | 259.4285714285714 | 5141                    | 5141                       |
| 1337hax4u.example.com         | 05/21/2013   | 277 | 263.8571428571428 | 5418                    | 5418                       |
| 1337hax4u.example.com         | 05/22/2013   | 259 | 266.2857142857143 | 5677                    | 5677                       |
| 1337hax4u.example.com         | 05/23/2013   | 243 | 263.4285714285714 | 5920                    | 5920                       |
| 1337hax4u.example.com         | 05/24/2013   | 252 | 259.2857142857143 | 6172                    | 6172                       |
| 1337hax4u.example.com         | 05/25/2013   | 260 | 256.5714285714286 | 6432                    | 6432                       |
| 1337hax4u.example.com         | 05/26/2013   | 259 | 259               | 6691                    | 6691                       |
| 1337hax4u.example.com         | 05/27/2013   | 281 | 261.5714285714286 | 6972                    | 6972                       |
| 1337hax4u.example.com         | 05/28/2013   | 267 | 260.1428571428572 | 7239                    | 7239                       |
| 1337hax4u.example.com         | 05/29/2013   | 258 | 260               | 7497                    | 7497                       |
| 1337hax4u.example.com         | 05/30/2013   | 266 | 263.2857142857143 | 7763                    | 7763                       |
| 1337hax4u.example.com         | 05/31/2013   | 254 | 263.5714285714286 | 8017                    | 8017                       |
| albumreview.example.com       | 05/01/2013   | 360 | 360               | 360                     | 360                        |
| albumreview.example.com       | 05/02/2013   | 386 | 373               | 746                     | 746                        |
| albumreview.example.com       | 05/03/2013   | 401 | 382.3333333333333 | 1147                    | 1147                       |
| albumreview.example.com       | 05/04/2013   | 382 | 382.25            | 1529                    | 1529                       |
| albumreview.example.com       | 05/05/2013   | 339 | 373.6             | 1868                    | 1868                       |
| albumreview.example.com       | 05/06/2013   | 362 | 371.6666666666667 | 2230                    | 2230                       |
| albumreview.example.com       | 05/07/2013   | 383 | 373.2857142857143 | 2613                    | 2613                       |
| albumreview.example.com       | 05/08/2013   | 326 | 368.4285714285714 | 2939                    | 2939                       |
| albumreview.example.com       | 05/09/2013   | 339 | 361.7142857142857 | 3278                    | 3278                       |
| albumreview.example.com       | 05/10/2013   | 351 | 354.5714285714286 | 3629                    | 3629                       |
| albumreview.example.com       | 05/11/2013   | 372 | 353.1428571428572 | 4001                    | 4001                       |
| albumreview.example.com       | 05/12/2013   | 365 | 356.8571428571428 | 4366                    | 4366                       |
| albumreview.example.com       | 05/13/2013   | 351 | 355.2857142857143 | 4717                    | 4717                       |
| albumreview.example.com       | 05/14/2013   | 359 | 351.8571428571428 | 5076                    | 5076                       |
| albumreview.example.com       | 05/15/2013   | 347 | 354.8571428571428 | 5423                    | 5423                       |
| albumreview.example.com       | 05/16/2013   | 365 | 358.5714285714286 | 5788                    | 5788                       |
| albumreview.example.com       | 05/17/2013   | 347 | 358               | 6135                    | 6135                       |
| albumreview.example.com       | 05/18/2013   | 343 | 353.8571428571428 | 6478                    | 6478                       |
| albumreview.example.com       | 05/19/2013   | 351 | 351.8571428571428 | 6829                    | 6829                       |
| albumreview.example.com       | 05/20/2013   | 338 | 350               | 7167                    | 7167                       |
| albumreview.example.com       | 05/21/2013   | 353 | 349.1428571428572 | 7520                    | 7520                       |
| albumreview.example.com       | 05/22/2013   | 389 | 355.1428571428572 | 7909                    | 7909                       |
| albumreview.example.com       | 05/23/2013   | 345 | 352.2857142857143 | 8254                    | 8254                       |
| albumreview.example.com       | 05/24/2013   | 318 | 348.1428571428572 | 8572                    | 8572                       |
| albumreview.example.com       | 05/25/2013   | 364 | 351.1428571428572 | 8936                    | 8936                       |
| albumreview.example.com       | 05/26/2013   | 355 | 351.7142857142857 | 9291                    | 9291                       |
| albumreview.example.com       | 05/27/2013   | 317 | 348.7142857142857 | 9608                    | 9608                       |
| albumreview.example.com       | 05/28/2013   | 358 | 349.4285714285714 | 9966                    | 9966                       |
| albumreview.example.com       | 05/29/2013   | 337 | 342               | 10303                   | 10303                      |
| albumreview.example.com       | 05/30/2013   | 352 | 343               | 10655                   | 10655                      |
| albumreview.example.com       | 05/31/2013   | 358 | 348.7142857142857 | 11013                   | 11013                      |
  ........
  ........
  ........
+-------------------------------+--------------+-----+-------------------+-------------------------+----------------------------+

SELECT prod_id, brand, name, previous, price, next, row_sum, range_sum
  FROM ( SELECT prod_id, brand, name,
                LAG (price, 1) OVER (PARTITION BY brand ORDER BY prod_id) previous,
                price,
                LEAD (price, 1) OVER (PARTITION BY brand ORDER BY prod_id) next,
                SUM(price) OVER (PARTITION BY brand ORDER BY prod_id ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS row_sum,
                SUM(price) OVER (PARTITION BY brand ORDER BY prod_id RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS range_sum
         FROM test.products2 ) sq
;

+---------+----------+----------+----------+-------+------+---------+-----------+
| prod_id | brand    | name     | previous | price | next | row_sum | range_sum |
+---------+----------+----------+----------+-------+------+---------+-----------+
| 1       | Dualcore | mesa     | NULL     | 500   | 600  | 500     | 500       |
| 2       | Dualcore | silla    | 500      | 600   | 700  | 1100    | 1100      |
| 3       | Dualcore | cama     | 600      | 700   | 500  | 1800    | 1800      |
| 4       | Dualcore | nevera   | 700      | 500   | 700  | 2300    | 2300      |
| 5       | Dualcore | cuadro   | 500      | 700   | 700  | 3000    | 3000      |
| 6       | Dualcore | manta    | 700      | 700   | NULL | 3700    | 3700      |
| 7       | Gigabux  | almohada | NULL     | 400   | 600  | 400     | 400       |
| 8       | Gigabux  | alfombra | 400      | 600   | 600  | 1000    | 1000      |
| 9       | Gigabux  | CD       | 600      | 600   | 1600 | 1600    | 1600      |
| 10      | Gigabux  | mesilla  | 600      | 1600  | 400  | 3200    | 3200      |
| 11      | Gigabux  | radio    | 1600     | 400   | NULL | 3600    | 3600      |
+---------+----------+----------+----------+-------+------+---------+-----------+


Bonus Exercise: Finding Medians

Recall that the median of a dataset is the value which half the data is above, and half is below. 
For a dataset with an odd number of points, the median is the point in the exact middle. 
In this set of seven points, the median is the first 4; there are three points below it and three points above it:
1, 2, 2, 4, 4, 8, 12
For a dataset with an even number of points, the median is typically the average of the two middle points. In this set of eight points, the last 2 and fist 4 are the two middle points, so the median is (2 + 4)/2, or 3:
1, 2, 2, 2, 4, 4, 8, 12
In Apache Hive, you can calculate percentiles for integers; percentile(column, 0.5) will produce the median. 
However, neither Hive nor Apache Impala have a function that calculates a precise median for non-integers. 
For large datasets, it’s a complex operation, so approximate medians are typically accurate enough.

Both Hive and Impala have functions to approximate the median for non-integers. 
For Hive, you can use percentile_approx(column, 0.5).
For Impala, you can use APPX_MEDIAN(column).

You can use workarounds with the analytic functions to find your own approximation of the median, but the resulting queries are a bit complex.

1. Use the CUME_DIST function in a subquery to find the median price of all the products in the products table. 
Your query should output only one row, with only one column.

Hint: This can sometimes provide the exact median, but it’s not guaranteed. 
      The issue is finding a point where the cumulative distribution is exactly 0.5. 
      If you get zero rows returned, you might try relaxing comparison expressions.

SELECT price, cumedist
  FROM(SELECT price, CUME_DIST () OVER (ORDER BY price) cumedist
         FROM analyst.products) sq
WHERE cumedist >= 0.5
ORDER BY price
LIMIT 1
;
+-------+--------------------+
| price | cumedist           |
+-------+--------------------+
| 5909  | 0.5017953321364452 |
+-------+--------------------+
Utilizando la función de IMPALA: APPX_MEDIAN

SELECT APPX_MEDIAN(price) 
FROM analyst.products
;
+--------------------+
| appx_median(price) |
+--------------------+
| 5909               |
+--------------------+

Utilizando la función de HIVE: PERCENTILE_APPROX

SELECT percentile_approx(price, 0.5) 
FROM analyst.products
;
+--------------------+--+
|        _c0         |
+--------------------+--+
| 5902.333333333333  |
+--------------------+--+

2. Modify your query to find the median price for each brand in the products table.
Your query should return one row for each brand. You might want to review the
analytic functions.

-- Esto no esta mal... pero sólo me da los de 0.5 
SELECT brand, price, cumedist
  FROM(SELECT brand, price, CUME_DIST () OVER (PARTITION BY brand ORDER BY price) cumedist
         FROM analyst.products) sq 
WHERE cumedist = 0.5
ORDER BY brand, price;
+----------------+-------+----------+
| brand          | price | cumedist |
+----------------+-------+----------+
| ACME           | 6049  | 0.5      |
| ARCAM          | 38769 | 0.5      |
| Artie          | 27829 | 0.5      |
| BDT            | 5859  | 0.5      |
| Bigdeal        | 2359  | 0.5      |
| BuckLogix      | 7919  | 0.5      |
| Byteweasel     | 9169  | 0.5      |
| Chestnut       | 2389  | 0.5      |
| Homertech      | 7109  | 0.5      |
| Krustybitz     | 10749 | 0.5      |
| Olde-Gray      | 32589 | 0.5      |
| Orion          | 3179  | 0.5      |
| Orion          | 3179  | 0.5      |
| Spindown       | 3859  | 0.5      |
| Sprite         | 25799 | 0.5      |
| TPS            | 2619  | 0.5      |
| Terrapin Sands | 8839  | 0.5      |
| Tortoise       | 14289 | 0.5      |
| Weisenheimer   | 13429 | 0.5      |
| Wernham        | 18759 | 0.5      |
| Whiteacre      | 8469  | 0.5      |
| Wolfpack       | 10639 | 0.5      |
| XYZ            | 3019  | 0.5      |
| Yoyodyne       | 91889 | 0.5      |
+----------------+-------+----------+
SELECT DISTINCT brand, first_price
  FROM 
       (SELECT brand, price, FIRST_VALUE(price) OVER(PARTITION BY brand ORDER BY cumedist) first_price
          FROM
               (SELECT brand, price, cumedist
                  FROM 
                       (SELECT brand, price, CUME_DIST () OVER (PARTITION BY brand ORDER BY price) cumedist
                          FROM analyst.products) sq
                  WHERE cumedist >= 0.5) sq2) sq3
ORDER BY brand;

-------------------------------------------------
solucion del manual: solucion median_by_brand.sql
-------------------------------------------------
SELECT DISTINCT brand, med
  FROM
    (SELECT brand,
      FIRST_VALUE(price) OVER (PARTITION BY brand ORDER BY cd) AS med
      FROM
        (SELECT brand, price,
          CUME_DIST() OVER (PARTITION BY brand ORDER BY price) AS cd
          FROM analyst.products
        )x
      WHERE cd >=0.5
    ) y;
+------------------+--------+
| brand            | med    |
+------------------+--------+
| BuckLogix        | 7919   |
| Krustybitz       | 10749  |
| Tyrell           | 3269   |
| Megachango       | 3019   |
| BDT              | 5859   |
| Sparky           | 9769   |
| Dorx             | 5209   |
| Orion            | 3179   |
| Duff             | 1969   |
| ARCAM            | 38769  |
| Chatter Audio    | 12619  |
| Whiteacre        | 8469   |
| Bitmonkey        | 188579 |
| Yoyodyne         | 91889  |
| Terrapin Sands   | 8839   |
| SuperGamer       | 13199  |
| Argo             | 29439  |
| Tortoise         | 14289  |
| Olde-Gray        | 32589  |
| Bitbucket        | 12889  |
| DevNull          | 2109   |
| Weebits          | 4629   |
| Homertech        | 7109   |
| Chestnut         | 2389   |
| XYZ              | 3019   |
| TPS              | 2619   |
| Dualcore         | 3949   |
| McDowell         | 17839  |
| Texi             | 6779   |
| Ultramegaco      | 2799   |
| Byteweasel       | 9169   |
| United Digistuff | 7939   |
| Lemmon           | 16509  |
| Gigabux          | 2419   |
| Wolfpack         | 10639  |
| Bargain Barn     | 4289   |
| ACME             | 6049   |
| Sprite           | 25799  |
| Electrosaurus    | 8979   |
| Wernham          | 18759  |
| Overtop          | 1509   |
| Artie            | 27829  |
| Spindown         | 3859   |
| Foocorp          | 8669   |
| Bigdeal          | 2359   |
| Weisenheimer     | 13429  |
| Bytefortress     | 23259  |
+------------------+--------+

------------------
Hands-On Exercise: Complex Data
------------------
Creating, Loading, and Querying Complex Data with Hive
Dualcore recently started a loyalty program to reward our best customers. 
A colleague has already provided us with a sample of the data that contains information about customers who have signed up for the program, 
including their phone numbers (as a MAP), 
a list of past order IDs (as an ARRAY), and 
a STRUCT that summarizes the minimum, maximum, average, and total value of past orders (in cents). 
You will use Hive to create the table, populate it with the provided data, and then run a few queries to practice referencing these types of fields. 
Finally, you will create a Parquet table containing the same data, to allow you to use Impala to query the data.
You may use either the Beeline shell or Hue’s Hive Query Editor for these exercises.

1. In the analyst database, create a text file-based table with the following characteristics:

Name: loyalty_program
Type: EXTERNAL
Field Terminator: | (pipe)
Collection Item Terminator: , (comma)
Map Key Terminator: : (colon)
Location: /analyst/dualcore/loyalty_program

Columns: Field Name Field Type
cust_id INT
fname STRING
lname STRING
email STRING
level STRING
phone MAP<STRING,STRING>
order_ids ARRAY<INT>
order_value STRUCT<min:INT, max:INT, avg:INT, total:INT>


DROP TABLE IF EXISTS analyst.loyalty_program;
CREATE EXTERNAL TABLE IF NOT EXISTS analyst.loyalty_program(
       cust_id INT,
       fname STRING,
       lname STRING,
       email STRING,
       level STRING,
       phone MAP<STRING,STRING>,
       order_ids ARRAY<INT>,
       order_value STRUCT<min:INT, 
                          max:INT, 
                          avg:INT, 
                          total:INT>
)
ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|'
    COLLECTION ITEMS TERMINATED BY ','
    MAP KEYS TERMINATED BY ':'
LOCATION '/analyst/dualcore/loyalty_program'
;

solucion del manual: loyalty_program_table.hql
----------------------------------------------

CREATE EXTERNAL TABLE analyst.loyalty_program
   (cust_id INT,
    fname STRING,
    lname STRING,
    email STRING,
    level STRING,
    phone MAP<STRING, STRING>,
    order_ids ARRAY<INT>,
    order_value STRUCT<min:INT, 
                       max:INT, 
                       avg:INT, 
                       total:INT>)       
   ROW FORMAT DELIMITED 
      FIELDS TERMINATED BY '|'
      COLLECTION ITEMS TERMINATED BY ',' 
      MAP KEYS TERMINATED BY ':'
      LOCATION '/analyst/dualcore/loyalty_program';

2. Using either a terminal window editor, examine the data loyalty_data.txt to see how it corresponds to the fields in the table.
   cat loyalty_data.txt

3. Load the data file by placing it into the HDFS warehouse directory for the new table.
You can use either the Hue File Browser, or the hdfs command:

hdfs dfs -put loyalty_data.txt /analyst/dualcore/loyalty_program
hdfs dfs -ls /analyst/dualcore/loyalty_program
-rw-r--r--   1 cloudera supergroup       1636 2021-04-09 09:43 /analyst/dualcore/loyalty_program/loyalty_data.txt

4. Using either the Beeline shell or Hue’s Hive Query Editor, run a query to select the HOME phone number for customer ID 1000238. 
(Hint: MAP keys are case-sensitive.)
You should see 408-555-4914 as the result.

SELECT cust_id, fname, lname, email, level, ph_name, ph_number
  FROM analyst.loyalty_program
       LATERAL VIEW explode(phone) myTable as ph_name, ph_number
WHERE cust_id = '1000238'
;
SELECT cust_id, fname, lname, email, level, ph_name, ph_number
  FROM analyst.loyalty_program
       LATERAL VIEW explode(phone) myTable as ph_name, ph_number
WHERE cust_id = '1001661'
;


5. Select the third element from the order_ids field (of ARRAY type) for customer ID 1000238
(Hint: elements are indexed from zero.) - 5517663

SELECT cust_id, fname, lname, email, level, ph_name, ph_number, order_ids [2] order
  FROM analyst.loyalty_program
       LATERAL VIEW explode(phone) myTable as ph_name, ph_number
WHERE cust_id = '1000238'
;
SELECT cust_id, fname, lname, email, level, ph_name, ph_number, order_ids [2] order
  FROM analyst.loyalty_program
       LATERAL VIEW explode(phone) myTable as ph_name, ph_number
WHERE cust_id = '1001661'
;
+----------+--------+---------+--------------------------+---------+----------+---------------+----------+--+
| cust_id  | fname  |  lname  |          email           |  level  | ph_name  |   ph_number   |  order   |
+----------+--------+---------+--------------------------+---------+----------+---------------+----------+--+
| 1001661  | Jody   | Culver  | jody.culver@example.com  | SILVER  | MOBILE   | 515-555-8686  | 5459665  |
| 1001661  | Jody   | Culver  | jody.culver@example.com  | SILVER  | HOME     | 515-555-2233  | 5459665  |
+----------+--------+---------+--------------------------+---------+----------+---------------+----------+--+

6. Select the total attribute from the order_value field (of STRUCT type) for customer ID 1000238.

SELECT cust_id, fname, lname, email, level, ph_name, ph_number, order_ids [2] order, order_value.total total
  FROM analyst.loyalty_program
       LATERAL VIEW explode(phone) myTable as ph_name, ph_number
WHERE cust_id = '1000238'
;

SELECT cust_id, fname, lname, email, level, ph_name, ph_number, order_ids [2] order, order_ids [2] order, order_value.total total
  FROM analyst.loyalty_program
       LATERAL VIEW explode(phone) myTable as ph_name, ph_number
WHERE cust_id = '1001661'
;
+----------+--------+---------+--------------------------+---------+----------+---------------+----------+----------+---------+--+
| cust_id  | fname  |  lname  |          email           |  level  | ph_name  |   ph_number   |  order   |  order   |  total  |
+----------+--------+---------+--------------------------+---------+----------+---------------+----------+----------+---------+--+
| 1001661  | Jody   | Culver  | jody.culver@example.com  | SILVER  | MOBILE   | 515-555-8686  | 5459665  | 5459665  | 557817  |
| 1001661  | Jody   | Culver  | jody.culver@example.com  | SILVER  | HOME     | 515-555-2233  | 5459665  | 5459665  | 557817  |
+----------+--------+---------+--------------------------+---------+----------+---------------+----------+----------+---------+--+

solucion del manual: loyalty_program_queries.hql 
------------------------------------------------
USE analyst;

--  Returns the HOME phone number for customer ID 1000238
SELECT phone['HOME'] FROM loyalty_program WHERE cust_id=1000238;

-- Returns the third element from the orders array for customer ID 1000238
SELECT order_ids[2] FROM loyalty_program WHERE cust_id=1000238;

-- Returns the value of the total attribute from the order_value struct
SELECT order_value.total FROM loyalty_program WHERE cust_id=1000238;


7. Create a Parquet-based table named loyalty_program_parquet with the same characteristics as the loyalty_program table, 
and use Hive to copy all the data from the loyalty_program table into this new loyalty_program_parquet table. 
(Hint: You can use CREATE TABLE AS SELECT (CTAS) to do this with a single HiveQL statement.)

-- Create in Hive
DROP TABLE IF EXISTS analyst.loyalty_program_parquet
CREATE TABLE IF NOT EXISTS analyst.loyalty_program_parquet
  STORED AS PARQUET
AS 
  SELECT * 
    FROM analyst.loyalty_program

solucion del manual: loyalty_program_parquet_table.hql
------------------------------------------------------
CREATE TABLE analyst.loyalty_program_parquet
  STORED AS PARQUET
AS
  SELECT * from analyst.loyalty_program;

Querying Complex Data in a Parquet Table with Impala

Now that you have used Hive to create, load, and query data with complex columns,
continue by using Impala to query complex data in a Parquet-based table. 
Remember that the Impala SQL syntax for queries involving ARRAY and MAP types differs from the HiveQL syntax.
You may use either the Impala shell or Hue’s Impala Query Editor to complete these exercises.

8. Invalidate Impala’s metadata for the loyalty_program_parquet table, which you created and populated in the previous step, so it can be accessed with Impala.

INVALIDATE METADATA analyst.loyalty_program_parquet;
DESCRIBE FORMATTED analyst.loyalty_program_parquet;

9. Run a query to return the distinct key values in the phone column. 
(Hint: You can query a MAP column in Impala as if it were a separate table with columns key and value.)

SELECT DISTINCT key, value
  FROM analyst.loyalty_program_parquet.phone;

+--------+--------------+
| key    | value        |
+--------+--------------+
| HOME   | 515-555-2233 |
| WORK   | 303-555-3538 |
| WORK   | 415-555-4950 |
| MOBILE | 918-555-1162 |
| WORK   | 915-555-7945 |
| WORK   | 916-555-2791 |
| MOBILE | 515-555-8686 |
+--------+--------------+


10. These distinct key values represent types of phone numbers. 
Now run a query to count how many of each type of phone number there are in the phone column.

SELECT key, COUNT(value)
  FROM analyst.loyalty_program_parquet.phone
GROUP BY key
;

+--------+--------------+
| key    | count(value) |
+--------+--------------+
| WORK   | 4            |
| HOME   | 1            |
| MOBILE | 2            |
+--------+--------------+


11. Run a query to count the total number of order IDs listed in the order_ids column. 
(Hint: You can query an ARRAY column in Impala as if it were a separate table with columns item and pos.)

SELECT COUNT(item)
  FROM analyst.loyalty_program_parquet.order_ids
;
+-------------+
| count(item) |
+-------------+
| 123         |
+-------------+

12. Select the first element in the order_ids field (of ARRAY type) for each customer.
(Hint: The pos column represents the position of each element in an ARRAY, indexed from zero.)

SELECT cust_id, order_ids.item
  FROM analyst.loyalty_program_parquet, analyst.loyalty_program_parquet.order_ids 
WHERE order_ids.pos = 0
;

+---------+---------+
| cust_id | item    |
+---------+---------+
| 1000238 | 5179798 |
| 1000279 | 5262426 |
| 1000810 | 5006384 |
| 1001219 | 5104660 |
| 1001661 | 5002071 |
| 1003097 | 5085927 |
+---------+---------+


13. Run a query to count how many customers have 30 or more orders.  

SELECT cust_id, COUNT(order_ids.item) as num
  FROM analyst.loyalty_program_parquet, analyst.loyalty_program_parquet.order_ids 
GROUP BY cust_id
HAVING num > 20
;
+---------+-----+
| cust_id | num |
+---------+-----+
| 1001219 | 25  |
| 1000238 | 29  |
| 1001661 | 21  |
| 1000279 | 22  |
+---------+-----+

SELECT cust_id, order_ids.item
  FROM analyst.loyalty_program_parquet, analyst.loyalty_program_parquet.order_ids
WHERE order_ids.pos = 28;
+---------+---------+
| cust_id | item    |
+---------+---------+
| 1000238 | 6612924 |
+---------+---------+

-- Returns the number of customers having 30 or more orders
SELECT COUNT(order_ids.item) FROM loyalty_program_parquet.order_ids WHERE order_ids.pos = 29;

solucion del manual: loyalty_program_parquet_queries.sql
--------------------------------------------------------
USE analyst;

--  Returns the unique key values in the MAP column phone
SELECT DISTINCT(phone.key) FROM loyalty_program_parquet.phone;

-- Returns counts of phone numbers by type
SELECT phone.key, COUNT(phone.value) FROM loyalty_program_parquet.phone GROUP BY phone.key;

-- Returns the number of orders in the ARRAY column order_ids
SELECT COUNT(order_ids.item) FROM loyalty_program_parquet.order_ids;

-- Returns the first order ID for each customer
SELECT order_ids.item FROM loyalty_program_parquet.order_ids WHERE order_ids.pos = 0;

-- Returns the number of customers having 30 or more orders
SELECT COUNT(order_ids.item) FROM loyalty_program_parquet.order_ids WHERE order_ids.pos = 29;

Bonus Exercise 1: Hive Table-Generating Functions
1. Run a Hive query to return the order IDs for customer ID 1000238, one per row.
(Hint: Use the explode function.)

-- Hive
SELECT cust_id, col
  FROM analyst.loyalty_program_parquet
       LATERAL VIEW explode(order_ids) myTable AS col
 WHERE cust_id = 1000238 
 GROUP BY cust_id
;
+----------+------+--+
| cust_id  | _c1  |
+----------+------+--+
| 1000238  | 29   |
+----------+------+--+

-- Impala
SELECT cust_id, COUNT(item)
  FROM analyst.loyalty_program_parquet, analyst.loyalty_program_parquet.order_ids                 
 WHERE cust_id = 1000238 
 GROUP BY cust_id;
;
+---------+-------------+
| cust_id | count(item) |
+---------+-------------+
| 1000238 | 29          |
+---------+-------------+

solucion del manual: order_id_one_per_row.hql
---------------------------------------------
SELECT explode(order_ids)
  FROM analyst.loyalty_program
 WHERE cust_id=1000238
;

2. Run a Hive query to return the customer IDs and order IDs for all customers who have an average order total of greater than $900 (90,000 cents). 
The query should return one order ID per row. 
(Hint: Use LATERAL VIEW.)

-- Hive
SELECT cust_id, order_id, order_value.avg avg, order_value.total tot
  FROM analyst.loyalty_program_parquet
       LATERAL VIEW explode(order_ids) myTable as order_id
GROUP BY cust_id
HAVING order_value.avg > 90000
;

-- Impala
SELECT cust_id, avg(order_ids.item) as num
  FROM analyst.loyalty_program_parquet, analyst.loyalty_program_parquet.order_ids 
GROUP BY cust_id
HAVING avg > 90000
;

solucion del manual: large_orders.hql  
-------------------------------------
SELECT cust_id, order_id
  FROM analyst.loyalty_program
    LATERAL VIEW explode(order_ids) o AS order_id
 WHERE order_value.avg > 90000;

Bonus Exercise 2: Impala Complex Column Join Notation
1. Run an Impala query to return the same result set as the previous Hive query: 
the customer IDs and order IDs for all customers who have an average order total of greater than $900. 
(Hint: Use join notation, and remember to query the Parquet-based table.)

solucion del manual: large_orders.sql
-------------------------------------
SELECT cust_id, order_ids.item
  FROM loyalty_program_parquet, loyalty_program_parquet.order_ids
 WHERE order_value.avg > 90000;

2. Now run an Impala query that returns the customer IDs, first names, last names, phone numbers, and phone number types for these customers who have an average order of greater than $900

solucion del manual: large_orders.sql
-------------------------------------
SELECT cust_id, order_ids.item
  FROM loyalty_program_parquet, loyalty_program_parquet.order_ids
 WHERE order_value.avg > 90000;

solucion del manual: large_order_customer_info.sql 
--------------------------------------------------
SELECT cust_id, fname, lname, phone.value, phone.key
  FROM loyalty_program_parquet, loyalty_program_parquet.phone
 WHERE order_value.avg > 90000;

------------------
Hands-On Exercise: 
------------------
------------------
Hands-On Exercise: 
------------------

========================================================================================================================================================================
Exercices CCA 159 - sueltos
========================================================================================================================================================================
+---+
|1-.|
+---+
-----
SQOOP
-----
Utilizando sqoop, importar la tabla customers de mysql con las siguientes condiciones:
a. Solo hay que importar las columnas cust_id, fname, lname.
b. En los ficheros finales, las columnas tienen que estar separadas por el carácter ':'.
c. El resultado se debe grabar en el directorio /customers. Si el directorio ya existiera, hay que borralo antes.
d. Tiene que haber solo dos ficheros con el resultado de la importación.

otras consideraciones a tener en cuenta:
- customers tiene clave primaria por defecto sqoop utilizara cuatro hilos
- creará un directorio con el nombre de la tabla con las columnas seleccionadas

sqoop import \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root \
--password cloudera \
--table customers \
--columns "cust_id, fname, lname" \
--fields-terminated-by ":" \
--target-dir /customers  \
--delete-target-dir \
--num-mappers 2 	ó -m 2

hdfs dfs -ls /customers

+---+
|2-.|
+---+
-----
SQOOP
-----
Mediante el comando sqoop-import-all-tables importar todas las tablas de la BBDD analyst_dualcore a hdfs.
Cada una de las tablas tiene que ir a un directorio hdfs con nombre igual al de la tabla y que esté situado bajo el directorio /user/hive/warehouse/exam.db. 
Por ejemplo, los ficheros correspondientes a la tabla customers tiene que estar situados bajo /user/hive/warehouse/exam.db/customers.

otras consideraciones a tener en cuenta:
- order_details no tiene clave primaria 
- warehouse-dir indica que cada una de las tablas debe ir en un subdirectorio identificado con el nombre de la tabla
- en una de las tablas no tengo clave primaria: debo utilizar auto-reset-to-mapper ó use one mapper, No se puede usar con split-by
- Se utiliza cuando la primary key es string or si quiero splitting una columna de texto -Dorg.apache.sqoop.splitter.allow_text_splitter=true 

sqoop import-all-tables \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root \
--password cloudera \
--warehouse-dir /user/hive/warehouse/exam.db \
--autoreset-to-one-mapper

ó

If a table does not have a primary key defined and the --split-by in not provides
--autoreset-to-one-mapper or --num-mappers 1

sqoop import-all-tables \
-Dorg.apache.sqoop.splitter.allow_text_splitter=true \
-m 1 \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root \
--password cloudera \
--warehouse-dir /user/hive/warehouse/exam.db

ó

sqoop import-all-tables \
-Dorg.apache.sqoop.splitter.allow_text_splitter=true \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root \
--password cloudera \
--warehouse-dir /user/hive/warehouse/exam.db \
--autoreset-to-one-mapper

+---+
|3-.|
+---+
-------------
MYSQL - SQOOP
-------------
Crear en mysql la tabla analyst_dualcore.suppliers_bis de la siguiente manera:

mysql> CREATE TABLE suppliers_bis as 
SELECT supp_id, contact 
FROM suppliers 
WHERE supp_id < 1005 ;

mysql> SELECT * FROM suppliers_bis;
+---------+------------------+
| supp_id | contact          |
+---------+------------------+
|    1000 | Albert Whittaker |
|    1001 | Dorothy Ruiz     |
|    1002 | Sam Esposito     |
|    1003 | Cristina Daniels |
|    1004 | Jim Calhoun      |
+---------+------------------+

mysql> ALTER TABLE suppliers_bis ADD PRIMARY KEY (supp_id) ;

updated_suppliers.csv:
1000,John Doe
1001,Liza Whitaker
1010,Peter Sokolowsky
1011,Maria Cruz
1012,Alber Callaghan
1013,Suzanne Greco

updated_suppliers2.csv:
1002,Joan Martin
1003,Vega Zancada
2000,Alfonso Calamaro

subir los ficheros a hdfs: hdfs dfs -put CCA 159/2019-09-18/updated_suppliers*.csv .
(.)=> /user/cloudera/

Actualizar la tabla suppliers_bis con el contenido de updated_suppliers.csv

sqoop-export \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root \
--password cloudera \
--table suppliers_bis \
--export-dir /user/cloudera/updated_suppliers.csv \
--update-key supp_id \
--update-mode allowinsert


mysql> SELECT * FROM suppliers_bis -> ;
---------+------------------+
| supp_id | contact          |
+---------+------------------+
|    1000 | John Doe         |
|    1001 | Liza Whitaker    |
|    1002 | Sam Esposito     |
|    1003 | Cristina Daniels |
|    1004 | Jim Calhoun      |
|    1010 | Peter Sokolowsky |
|    1011 | Maria Cruz       |
|    1012 | Alber Callaghan  |
|    1013 | Suzanne Greco    |
+---------+------------------+

Actualizar la tabla suppliers_bis con el contenido de updated_suppliers2.csv - No insertar nuevos sólo actualizar existentes

sqoop-export \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root \
--password cloudera \
--table suppliers_bis \
--export-dir /user/cloudera/updated_suppliers2.csv \
--update-key supp_id \
--update-mode updateonly

mysql> SELECT * FROM suppliers_bis; 
+---------+------------------+
| supp_id | contact          |
+---------+------------------+
|    1000 | John Doe         |
|    1001 | Liza Whitaker    |
|    1002 | Joan Martin      |
|    1003 | Vega Zancada     |
|    1004 | Jim Calhoun      |
|    1010 | Peter Sokolowsky |
|    1011 | Maria Cruz       |
|    1012 | Alber Callaghan  |
|    1013 | Suzanne Greco    |
+---------+------------------+

+---+
|4-.|
+---+
-----
SQOOP
-----
Importar nuevamente la tabla customers en hdfs. - ojo - adicionar el contenido a la que ya existe:

sqoop import \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root \
--password cloudera \
--table customers \
--target-dir /customers  \
--delete-target-dir \
--num-mappers 2

Retrieved 201375 records.
hdfs dfs -ls /customers
-rw-r--r--   1 cloudera supergroup          0 2021-03-09 13:42 /customers/_SUCCESS
-rw-r--r--   1 cloudera supergroup    2220014 2021-03-09 13:42 /customers/part-m-00000
-rw-r--r--   1 cloudera supergroup    2219922 2021-03-09 13:42 /customers/part-m-00001

sqoop import \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root \
--password cloudera \
--table customers \
--append \
--target-dir /customers  \
--num-mappers 2

hdfs dfs -ls /customers
-rw-r--r--   1 cloudera supergroup          0 2021-03-09 13:42 /customers/_SUCCESS
-rw-r--r--   1 cloudera supergroup    2220014 2021-03-09 13:42 /customers/part-m-00000
-rw-r--r--   1 cloudera supergroup    2219922 2021-03-09 13:42 /customers/part-m-00001
-rw-r--r--   1 cloudera cloudera      2220014 2021-03-09 13:54 /customers/part-m-00002
-rw-r--r--   1 cloudera cloudera      2219922 2021-03-09 13:54 /customers/part-m-00003

+---+
|5-.|
+---+
Mostrar las fechas de mayo de 2013 en las que el cliente con cust_id 1139477 hizo compras, ordenada de menor a mayor.

SELECT c.cust_id, to_date(o.order_date) as fecha
FROM customers c
INNER JOIN orders o ON(o.cust_id=c.cust_id)
WHERE c.cust_id = 1139477
ORDER BY fecha

SELECT to_date(order_date) as fecha
FROM orders
WHERE cust_id = 1139477
ORDER BY fecha

mysql> SELECT order_date 
FROM orders  
WHERE cust_id = 1139477 and order_date >= '2013-05-01' and order_date <= '2013-05-31' 
ORDER BY order_date;

-- Fechas SELECT now(), to_date(now()), to_date(cast(now() AS STRING)), unix_timestamp(now()), from_unixtime(cast(now() as BIGINT));
[quickstart.cloudera:21000] > SELECT to_date(cast(order_date AS STRING)) 
FROM orders  
WHERE cust_id = 1139477 and to_date(cast(order_date AS STRING)) >= '2013-05-01' and to_date(cast(order_date AS STRING)) <= '2013-05-31' 
ORDER BY to_date(cast(order_date AS STRING)); -- esto no lo pude comprobar con mis tablas 

0: jdbc:hive2://localhost:10000> SELECT to_date(order_date) as fecha 
FROM orders 
WHERE cust_id = 1139477 
  AND to_date(order_date) >= '2013-05-01' 
  AND to_date(order_date) <= '2013-05-31' 
ORDER BY fecha; -- esto no lo pude comprobar con mis tablas 

+---+
|8-.|
+---+
1. Averiguar cuántas órdenes se hicieron en julio independientemente del año. Hay que hacerlo de dos maneras: 
a) usando la función month 

mysql> SELECT COUNT(*) 
FROM orders
WHERE MONTH(order_date)= 7;

0: jdbc:hive2://localhost:10000> SELECT count(*) 
FROM orders 
WHERE month(cast(order_date AS STRING)) = 7;

[quickstart.cloudera:21000] > SELECT count(*) 
FROM orders 
WHERE extract(month from cast(order_date AS string)) = 7; 

b) usando la función substr.

mysql> SELECT COUNT(*) 
FROM orders 
WHERE SUBSTRING(cast(order_date AS CHAR), 6, 2) = '07';

0: jdbc:hive2://localhost:10000> SELECT count(*)
FROM orders
WHERE SUBSTRING(unix_timestamp(to_date(cast(order_date AS STRING))),6,2) = '07'; -- esto no lo pude comprobar con mis tablas 

[quickstart.cloudera:21000] > SELECT count(*) 
FROM orders 
WHERE SUBSTRING((to_date(cast(order_date AS STRING))),6,2) = '07'; -- esto no lo pude comprobar con mis tablas 

Averiguar cuántas órdenes se hicieron en julio de cada año.

mysql> SELECT YEAR(order_date) year, MONTH(order_date) month, count(*) count 
FROM orders 
WHERE MONTH (order_date) = 07 
GROUP BY YEAR(order_date), MONTH (order_date);

mysql> SELECT YEAR(order_date) year, MONTH(order_date) month, count(*) count 
FROM orders 
GROUP BY YEAR(order_date), MONTH (order_date) 
HAVING month = 7;

+---+
|9-.|
+---+
Crear una tabla, fechas, que conste de una sóla columna, fecha_americana, de tipo string.  
CREATE TABLE fechas (fecha_americana STRING);
A continuación, mediante insert into fechas values insertar en la tabla fechas en formato americano; es decir MM/dd/yyyy.  
Por ejemplo '07/15/2019', '08/13/2020'.
INSERT INTO fechas values ('07/15/2019'), ('08/13/2020');
Ejecutar un SELECT sobre fechas que muestre su contenido en formato 

1) dia, mes (en letra), y año y 
SELECT fecha_americana, unix_timestamp(fecha_americana, 'MM/dd/yyyy'), from_unixtime((unix_timestamp(fecha_americana, 'MM/dd/yyyy')), 'dd/MMM/yyyy')
FROM fechas;
+-----------------+-----------------------------------------------+-------------------------------------------------------------------------------+
| fecha_americana | unix_timestamp(fecha_americana, 'mm/dd/yyyy') | from_unixtime((unix_timestamp(fecha_americana, 'mm/dd/yyyy')), 'dd/mmm/yyyy') |
+-----------------+-----------------------------------------------+-------------------------------------------------------------------------------+
| 07/15/2019      | 1563148800                                    | 15/Jul/2019                                                                   |
| 08/13/2020      | 1597276800                                    | 13/Aug/2020                                                                   |
+-----------------+-----------------------------------------------+-------------------------------------------------------------------------------+

2) timestamp.
SELECT fecha_americana, to_timestamp(fecha_americana, 'MM/dd/yyyy')
FROM fechas;
+-----------------+---------------------------------------------+
| fecha_americana | to_timestamp(fecha_americana, 'mm/dd/yyyy') |
+-----------------+---------------------------------------------+
| 07/15/2019      | 2019-07-15 00:00:00                         |
| 08/13/2020      | 2020-08-13 00:00:00                         |
+-----------------+---------------------------------------------+

+---+
|6-.|
+---+
Se pide crear una tabla orders2 que contenga todas las filas de la tabla order que contengan un valor de order_id múltiplo de 100.000.
A continuación averiguar cuantas órdenes se efectuaron entre las 0 y las 9 horas o entre las 14 y las 23.

mysql> 
(CTAS)
CREATE TABLE orders2 AS 
SELECT order_id, cust_id, order_date 
FROM orders 
WHERE order_id%100000 = 0;

SELECT count(o.order_id) cantidad, hour((o.order_date)) hours 
FROM orders2 o 
GROUP BY hours 
HAVING hours BETWEEN 0 AND 9 OR hours BETWEEN 14 AND 23
ORDER BY hours;

SELECT *
FROM orders2
WHERE hour(order_date) BETWEEN 0 AND 9 
   OR hour(order_date) BETWEEN 14 AND 23;
+----------+---------+---------------------+
| order_id | cust_id | order_date          |
+----------+---------+---------------------+
|  5200000 | 1140040 | 2010-05-29 22:00:33 |
|  5300000 | 1105155 | 2010-09-26 14:30:08 |
|  5400000 | 1057932 | 2011-01-19 21:53:24 |
|  5500000 | 1041089 | 2011-05-04 23:03:09 |
|  5600000 | 1082054 | 2011-08-04 14:48:54 |
|  5700000 | 1001320 | 2011-10-30 16:23:23 |
|  5900000 | 1037466 | 2012-03-18 00:49:22 |
|  6000000 | 1102161 | 2012-05-12 22:54:32 |
|  6100000 | 1032751 | 2012-08-19 23:27:39 |
|  6200000 | 1016355 | 2012-12-05 23:53:40 |
|  6300000 | 1170914 | 2013-02-01 08:23:50 |
|  6400000 | 1069907 | 2013-03-10 19:37:34 |
|  6500000 | 1123838 | 2013-04-14 16:32:54 |
|  6600000 | 1120581 | 2013-05-15 08:58:28 |
+----------+---------+---------------------+
Profe:
SELECT * 
FROM orders2 
WHERE hour(order_date) NOT BETWEEN 10 AND 14;
+----------+---------+---------------------+
| order_id | cust_id | order_date          |
+----------+---------+---------------------+
|  5200000 | 1140040 | 2010-05-29 22:00:33 |
|  5400000 | 1057932 | 2011-01-19 21:53:24 |
|  5500000 | 1041089 | 2011-05-04 23:03:09 |
|  5700000 | 1001320 | 2011-10-30 16:23:23 |
|  5900000 | 1037466 | 2012-03-18 00:49:22 |
|  6000000 | 1102161 | 2012-05-12 22:54:32 |
|  6100000 | 1032751 | 2012-08-19 23:27:39 |
|  6200000 | 1016355 | 2012-12-05 23:53:40 |
|  6300000 | 1170914 | 2013-02-01 08:23:50 |
|  6400000 | 1069907 | 2013-03-10 19:37:34 |
|  6500000 | 1123838 | 2013-04-14 16:32:54 |
|  6600000 | 1120581 | 2013-05-15 08:58:28 |
+----------+---------+---------------------+

+---+
|7-.|
+---+
En este ejercicio hay que usar la función GROUP_CONCAT de Impala
Se da lo siguiente:
DROP TABLE IF EXISTS numbers;
CREATE TABLE numbers (n int, property string) ;
INSERT INTO numbers VALUES
(2, "par"),
(4, "par"),
(6, "par"),
(8, "par"),
(10, "par"),
(1, "impar"),
(3, "impar"),
(5, "impar"),
(7, "impar"),
(9, "impar") ;
Se pide:

1. Hacer un SELECT que produzca una sóla línea con la concatenación de los números separados por '|'.
SELECT GROUP_CONCAT(cast(n AS STRING),"|") 
FROM numbers;
+--------------------------------------+
| group_concat(cast(n as string), '|') |
+--------------------------------------+
| 2|4|6|8|10|1|3|5|7|9                 |
+--------------------------------------+

2. Igual que el punto 1 pero los números tienen que aparecer ordenados de menor a mayor.
SELECT GROUP_CONCAT(cast(n AS STRING),"|") 
FROM 
(SELECT n FROM numbers ORDER BY n) nums;

SELECT GROUP_CONCAT(cast(n AS STRING),"|") 
FROM 
(SELECT n FROM numbers ORDER BY n LIMIT 10) nums;

SELECT GROUP_CONCAT(cast(n AS STRING),"|") 
FROM 
(SELECT n FROM numbers ORDER BY n DESC LIMIT 10) nums;

3. Hacer un SELECT que produzca una sóla línea con la concatenación de los valores de property sin repetición. 
Los valores tienen que estar separados por ', '; Es decir, una coma y un espacio en blanco.

-- por defecto (', ')
SELECT GROUP_CONCAT(DISTINCT property)
FROM numbers;

-- pero se puede concatenar con otra cosa('| ')
SELECT GROUP_CONCAT(DISTINCT property, "| ") 
FROM numbers;

+----+
|10-.|
+----+
Crear una tabla, fechas2, que conste de 2 columnas de tipo string: nombre y fecha.
El separador de campos tiene que ser '@' y los datos tienen que estar en /user/cloudera/dates.

CREATE TABLE fechas2 (nombre STRING, fecha STRING) 
 ROW FORMAT DELIMITED 
  FIELD TERMINATED BY '@'
LOCATION '/user/cloudera/dates';

Meter en el directorio /user/cloudera/dates estos 3 ficheros:
fichero1
maria@12:28:2000
juana@01:31:2001
fichero2
pedro@12-18-2000
manuel@01-01-2001
fichero3
marcos@07_07_1999
azucena@03_09_2011

hdfs dfs -put fichero1 /user/cloudera/dates
hdfs dfs -put fichero2 /user/cloudera/dates
hdfs dfs -put fichero3 /user/cloudera/dates

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/dates/
Found 3 items
-rw-r--r--   1 cloudera cloudera         34 2021-03-20 06:31 /user/cloudera/dates/fichero1
-rw-r--r--   1 cloudera cloudera         35 2021-03-20 06:31 /user/cloudera/dates/fichero2
-rw-r--r--   1 cloudera cloudera         37 2021-03-20 06:31 /user/cloudera/dates/fichero3

[quickstart.cloudera:21000] > SELECT * FROM fechas2;
+---------+------------+
| nombre  | fecha      |
+---------+------------+
| maria   | 12:28:2000 |
| juana   | 01:31:2001 |
| pedro   | 12-18-2000 |
| manuel  | 01-01-2001 |
| marcos  | 07_07_1999 |
| azucena | 03_09_2011 |
+---------+------------+

El formato de la fecha es el formato americano; es decir, mes, día, y año.
Ejecutar un SELECT sobre la tabla que muestre los nombres y las personas en formato día/mes/año:
Sugerencia: Ver la documentación de la función translate.

SELECT nombre, translate(fecha, ':-_', '///' ) 
FROM fechas2;
+---------+--------------------------------+
| nombre  | translate(fecha, ':-_', '///') |
+---------+--------------------------------+
| marcos  | 07/07/1999                     |
| azucena | 03/09/2011                     |
| pedro   | 12/18/2000                     |
| manuel  | 01/01/2001                     |
| maria   | 12/28/2000                     |
| juana   | 01/31/2001                     |
+---------+--------------------------------+
SELECT nombre, from_unixtime(unix_timestamp(translate(fecha, ':-_','///'), 'MM/dd/yyyy'), 'dd/MM/yyyy')
FROM fechas2;
+---------+-------------------------------------------------------------------------------------------+
| nombre  | from_unixtime(unix_timestamp(translate(fecha, ':-_', '///'), 'mm/dd/yyyy'), 'dd/mm/yyyy') |
+---------+-------------------------------------------------------------------------------------------+
| marcos  | 07/07/1999                                                                                |
| azucena | 09/03/2011                                                                                |
| maria   | 28/12/2000                                                                                |
| juana   | 31/01/2001                                                                                |
| pedro   | 18/12/2000                                                                                |
| manuel  | 01/01/2001                                                                                |
+---------+-------------------------------------------------------------------------------------------+

+----+
|11-.|
+----+
Crear una tabla, clientes, en la base de datos analyst con los siguientes campos:
id INT
nombre STRING
apellido STRING

La sentencia CREATE TABLE ... tiene que ser mínima:
CREATE DABATABES test
CREATE TABLE IF NOT EXISTS test.clientes (id INT, nombre STRING, apellido STRING);

Mediante el comando sqoop, importar los datos necesarios para la tabla clientes a partir de las columnas pertinentes de la tabla customers de mysql.
sqoop import \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root \
--password cloudera \
--table customers \
--columns "cust_id, fname, lname" \
--fields-terminated-by '\001' \
--target-dir /user/hive/warehouse/test.db/clientes 

Comprobar que clientes contiene los datos adecuados.
use test;
show tables;
SELECT * 
FROM test.clientes 
LIMIT 10;

SELECT COUNT(*)
FROM test.clientes;

SELECT COUNT(*)
FROM default.customers;

+----+
|12-.|
+----+
1. Importar todas las tablas de analyst_dualcore y crear las correspondientes tablas en la BBDD exam. 
Las tablas tienen que cumplir las siguientes condiciones:

a. Su nombre tiene que ser ex_nombre_tabla_mysql. Por ejemplo, en mysql hay una tabla llamada customers. 
En hive la tabla correspondiente tiene que llamarse ex_customers.
b. Ser EXTERNAL.
c. La ubicación de sus directorios tiene que ser /user/hive/warehouse/exam.db. El nombre del directorio no debe contener ex_. 
Por ejemplo la tabla ex_customers tiene que tener sus ficheros en el directorio /user/hive/warehouse/exam.db/customers.
d. El separador entre campos de una misma fila tiene que ser '\t'.
e. Para hacer todo lo anterior, el procedimiento tiene que ser: 
   1) Importar el contenido de las tablas mysql a hdfs
   2) Crear las tablas hive con el comando create external table

1) Importar el contenido de las tablas mysql a hdfs 

CREATE DATABASE exam;

sqoop import-all-tables 
--connect jdbc:mysql://quickstart:3306/analyst_dualcore 
--username root 
--password cloudera 
--warehouse-dir /user/hive/warehouse/exam.db 
--fields-terminated-by '\t' 
--autoreset-to-one-mapper;

sqoop import-all-tables 
-m 1 
--connect jdbc:mysql://quickstart:3306/analyst_dualcore 
--username root 
--password cloudera 
--warehouse-dir /user/hive/warehouse/exam.db 
--fields-terminated-by '\t' 
--autoreset-to-one-mapper;

sqoop-import-all-tables 
-Dorg.apache.sqoop.splitter.allow_text_splitter=true 
--connect jdbc:mysql://quickstart:3306/analyst_dualcore 
--username root 
--password cloudera 
--warehouse-dir /user/hive/warehouse/exam.db 
--fields-terminated-by '\t' 
--autoreset-to-one-mapper


CREATE EXTERNAL TABLE IF NOT EXISTS exam.ex_customers
	(cust_id  int,
	 fname    STRING,
	 lname    STRING,
	 address  STRING,
	 city     STRING,
	 state    STRING,
	 zipcode  STRING) 
COMMENT 'tabla externa para mysql.analyst_dualcore.customers' 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/user/hive/warehouse/exam.db/customers/';

CREATE EXTERNAL TABLE IF NOT EXISTS exam.ex_employees
( emp_id     STRING,
  fname      STRING,
  lname      STRING,
  address    STRING,
  city       STRING,
  state      STRING,
  zipcode    STRING,
  job_title  STRING,
  email      STRING,
  active     STRING,
  salary     int)
COMMENT 'tabla externa para mysql.analyst_dualcore.employees'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY \'t'
LOCATION '/user/hive/warehouse/exam.db/employees';

CREATE EXTERNAL TABLE IF NOT EXISTS exam.ex_order_details
( order_id INT, 
  prod_id INT )
COMMENT 'tabla externa para mysql.analyst_dualcore.order_details'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/user/hive/warehouse/exam.db/order_details';

CREATE EXTERNAL TABLE exam.ex_order (
order_id INT,
cust_id INT,
order_date TIMESTAMP)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LOCATION '/user/hive/warehouse/exam.db/orders';

CREATE EXTERNAL TABLE exam.ex_orders2(
order_id INT,
cust_id INT,
order_date TIMESTAMP)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/user/hive/warehouse/exam.db/orders2';

CREATE EXTERNAL TABLE exam.ex_products(
prod_id     INT,
brand       STRING,
name        STRING,
price       INT,
cost        INT,
shipping_wt SMALLINT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/user/hive/warehouse/exam.db/products';

CREATE EXTERNAL TABLE exam.ex_suppliers(
supp_id int,
company STRING,
contact STRING,
address STRING,
city    STRING,
state   STRING,
zipcode STRING,
phone   STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/user/hive/warehouse/exam.db/suppliers';

+----+
|13-.|
+----+
1. Importar todas las tablas de analyst_dualcore y crear las correspondientes tablas en la BBDD exam2. 
Las tablas tienen que cumplir las siguientes condiciones:

a. Su nombre tiene que ser ex_nombre_tabla_mysql. Por ejemplo, en mysql hay una tabla llamada customers. 
En hive la tabla correspondiente tiene que llamarse ex_customers.
b. Ser EXTERNAL.
c. La ubicación de sus directorios tiene que ser /user/hive/warehouse/exam2.db. 
El nombre del directorio no debe contener ex_. 
Por ejemplo la tabla ex_customers tiene que tener sus ficheros en el directorio /user/hive/warehouse/exam2.db/customers.
d. El separador entre campos de una misma fila tiene que ser '\t'.

Hay que seguir los siguientes pasos:
1. En el comando sqoop se importan también los datos y se crean las tablas.
2. Luego, hay que cambiar el nombre de las tablas y hacerlas externas. Para esto, utilizar el comando ALTER TABLE.

CREATE DATABASE exam2;
sqoop-import-all-tables \
-Dorg.apache.sqoop.splitter.allow_text_splitter=true \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root \
--password cloudera \
--warehouse-dir /user/hive/warehouse/exam2.db \
--fields-terminated-by '\t' \
--autoreset-to-one-mapper \
--hive-import \
--hive-database exam2;

solucion del manual: change_table.iql:
--------------------------------------
ALTER TABLE exam2.${var:tablename} SET TBLPROPERTIES ('EXTERNAL'='TRUE');
ALTER TABLE exam2.${var:tablename} RENAME TO exam2.ex_${var:tablename} ;

desde la cmdline
impala-shell -f change_table.iql --car tablename=customers
impala-shell -f change_table.iql --car tablename=employees
impala-shell -f change_table.iql --car tablename=order_details
impala-shell -f change_table.iql --car tablename=orders
impala-shell -f change_table.iql --car tablename=orders2
impala-shell -f change_table.iql --car tablename=products
impala-shell -f change_table.iql --car tablename=suppliers 

+----+
|14-.|
+----+
A partir de la tabla customers, generar un SELECT de una sóla columna, name, que represente el nombre y el apellido de los clientes. 
El nombre y el apellido deben estar separados por un espacio en blanco y los elementos de la columna name deben estar ordenados por el
apellido.

[quickstart.cloudera:21000] > DESCRIBE analyst.customers;
Query: describe analyst.customers
+---------+--------+---------+
| name    | type   | comment |
+---------+--------+---------+
| cust_id | int    |         |
| fname   | string |         |
| lname   | string |         |
| address | string |         |
| city    | string |         |
| state   | string |         |
| zipcode | string |         |
+---------+--------+---------+


[quickstart.cloudera:21000] > SELECT concat_ws(' ', fname, lname ) name
FROM (SELECT fname, lname
      FROM analyst.customers
      ORDER BY lname, fname LIMIT 10) odesc;
Query: select concat_ws(' ', fname, lname ) name
FROM (SELECT fname, lname
      FROM analyst.customers
      ORDER BY lname, fname LIMIT 10) odesc
Query submitted at: 2021-03-29 05:48:24 (Coordinator: http://quickstart.cloudera:25000)
Query progress can be monitored at: http://quickstart.cloudera:25000/query_plan?query_id=8f4b011507654b25:225b01a800000000
+-------------------+
| name              |
+-------------------+
| Barbara Aaron     |
| Christopher Aaron |
| Darrel Aaron      |
| Diane Aaron       |
| Elliott Aaron     |
| Ernie Aaron       |
| Etta Aaron        |
| Eugene Aaron      |
| Irving Aaron      |
| Jacquelyn Aaron   |
+-------------------+

-- Esto funciona bien en Impala NO en hive
[quickstart.cloudera:21000] > SELECT concat_ws(' ', fname, lname ) name
      FROM analyst.customers
      ORDER BY lname, fname LIMIT 10;

+----+
|15-.|
+----+
Tenemos el fichero /user/training/employees.dat con el siguiente contenido:
(Nombre,Apellido,id,FechaLogin,FechaAlta,Departamento)
Sandra,Torres,101,20131206,20131207,ventas
Marta,Cordero,102,20131206,20110607,contabilidad
Sandra,Torres,101,20131209,20131207,ventas
Sandra,Torres,101,20131211,20131207,ventas
Sandra,Torres,101,20131212,20131207,ventas
Sandra,Torres,101,20131213,20131207,ventas
Sandra,Torres,101,20131216,20131207,ventas
Sandra,Torres,101,20131217,20131207,ventas
Marta,Cordero,102,20131206,20110607,contabilidad
Marta,Cordero,102,20131209,20110607,contabilidad
Marta,Cordero,102,20131210,20110607,contabilidad
Marta,Cordero,102,20131211,20110607,contabilidad
Marta,Cordero,102,20131212,20110607,contabilidad
Lucas,Barral,103,20131213,20110607,recursos humanos
Lucas,Barral,103,20131214,20110607,recursos humanos

[cloudera@quickstart data]$ hdfs dfs -put employees.dat /user/cloudera/employees.dat

1. Crear una tabla, info, que tenga una sóla columna, line, de tipo STRING cuyo contenido sea cada una de las líneas del fichero. 
La LOCATION de la tabla tiene que ser la de por omisión.

[quickstart.cloudera:21000] > CREATE TABLE IF NOT EXISTS analyst.info ( line STRING) ;
Query: create TABLE IF NOT EXISTS analyst.info ( line STRING)
Fetched 0 row(s) in 0.60s

0: jdbc:hive2://localhost:10000> LOAD DATA INPATH '/user/cloudera/employees.dat' INTO TABLE analyst.info ;
0: jdbc:hive2://localhost:10000> SELECT * FROM analyst.info;

-- en impala no me deja cargar los datos por not have WRITE permissions
[quickstart.cloudera:21000] > invalidate metadata analyst.info;
[quickstart.cloudera:21000] > LOAD DATA INPATH '/user/cloudera/employees.dat' INTO TABLE analyst.info ;
Query: load DATA INPATH '/user/cloudera/employees.dat' INTO TABLE analyst.info
ERROR: AnalysisException: Unable to LOAD DATA from hdfs://quickstart.cloudera:8020/user/cloudera/employees.dat because Impala does not have WRITE permissions on its parent directory hdfs://quickstart.cloudera:8020/user/cloudera

2. Usando CTAS, crear la tabla temp1 que contenga 6 columnas; una columna por cada uno de los campos que hay en cada fila de info.
En hive hay que usar la función split y en impala split_part. 
Las columnas tienen que ser de tipo STRING.

-- Impala: split_part(string source, string delimiter, bigint n)
---------
[quickstart.cloudera:21000] > CREATE TABLE IF NOT EXISTS analyst.temp1
       AS (SELECT split_part(line,',',1) Nombre, 
                  split_part(line,',',2) Apellido, 
                  split_part(line,',',3) id, 
                  split_part(line,',',4) FechaLogin, 
                  split_part(line,',',5) FechaAlta,
                  split_part(line,',',6) Departamento
             FROM analyst.info)

[quickstart.cloudera:21000] > SELECT * FROM analyst.temp1;
Query: select * FROM analyst.temp1
Query submitted at: 2021-03-29 11:18:38 (Coordinator: http://quickstart.cloudera:25000)
Query progress can be monitored at: http://quickstart.cloudera:25000/query_plan?query_id=f42ce8f0d5576af:669acbeb00000000
+--------+----------+-----+------------+-----------+------------------+
| nombre | apellido | id  | fechalogin | fechaalta | departamento     |
+--------+----------+-----+------------+-----------+------------------+
| Sandra | Torres   | 101 | 20131206   | 20131207  | ventas           |
| Marta  | Cordero  | 102 | 20131206   | 20110607  | contabilidad     |
| Sandra | Torres   | 101 | 20131209   | 20131207  | ventas           |
| Sandra | Torres   | 101 | 20131211   | 20131207  | ventas           |
| Sandra | Torres   | 101 | 20131212   | 20131207  | ventas           |
| Sandra | Torres   | 101 | 20131213   | 20131207  | ventas           |
| Sandra | Torres   | 101 | 20131216   | 20131207  | ventas           |
| Sandra | Torres   | 101 | 20131217   | 20131207  | ventas           |
| Marta  | Cordero  | 102 | 20131206   | 20110607  | contabilidad     |
| Marta  | Cordero  | 102 | 20131209   | 20110607  | contabilidad     |
| Marta  | Cordero  | 102 | 20131210   | 20110607  | contabilidad     |
| Marta  | Cordero  | 102 | 20131211   | 20110607  | contabilidad     |
| Marta  | Cordero  | 102 | 20131212   | 20110607  | contabilidad     |
| Lucas  | Barral   | 103 | 20131213   | 20110607  | recursos humanos |
| Lucas  | Barral   | 103 | 20131214   | 20110607  | recursos humanos |
+--------+----------+-----+------------+-----------+------------------+

-- Hive: split(string str, string pat)
--------
CREATE TABLE analyst.temp1 
AS SELECT split( line, ',')[0] Nombre, 
          split(line, ',')[1] Apellido, 
          split (line, ',')[2] id, 
          split (line, ',')[4] FechaLogin, 
          split (line, ',')[4] FechaAlta, 
          split (line, ',')[5] Departamento 
     FROM analyst.info;

0: jdbc:hive2://localhost:10000> CREATE TABLE analyst.temp1 AS SELECT split( line, ',')[0] Nombre, split(line, ',')[1] Apellido, split (line, ',')[2] id, split (line, ',')[4] FechaLogin, split (line, ',')[4] FechaAlta, split (line, ',')[5] Departamento FROM analyst.info;
0: jdbc:hive2://localhost:10000> SELECT * FROM analyst.temp1;

3. A partir de temp1, crear la tabla temp2 con las mismas columnas.  Pero hay que quitar filas duplicadas. 
La columna fecha_login no se considera para los duplicados. 
La única variación permitida en el contenido es el campo fecha_login. 
Por cada empleado, tiene que ser uno cualquiera de los que había anteriormente.

-- Hive: 
--------
CREATE TABLE analyst.temp2
AS SELECT Nombre, Apellido, id, min(FechaLogin)FechaLogin, FechaAlta, Departamento 
FROM analyst.temp1
GROUP BY Nombre, Apellido, id, FechaLogin, FechaAlta, Departamento 

0: jdbc:hive2://localhost:10000> CREATE TABLE analyst.temp2 AS SELECT Nombre, Apellido, id, min(FechaLogin)FechaLogin, FechaAlta, Departamento FROM analyst.temp1 GROUP BY Nombre, Apellido, id, FechaLogin, FechaAlta, Departamento;
0: jdbc:hive2://localhost:10000> SELECT * FROM analyst.temp2;

-- Impala: 
----------
CREATE TABLE analyst.temp2 AS SELECT Nombre, Apellido, id, FechaAlta, Departamento, min(FechaLogin)FechaLogin FROM analyst.temp1 GROUP BY Nombre, Apellido, id, FechaAlta, Departamento;


4. Volcar las filas de la tabla temp2 en el directorio hdfs /user/hive/datos. 
Cada línea de los ficheros de /user/hive/datos debe contener un solo campo con las columnas de la correspondiente fila de la tabla unidas por el carácter '-'. 
Por ejemplo: Sandra-Torres-101-20131206-20131207-ventas

-- Only Hive:
-------------
0: jdbc:hive2://localhost:10000>  INSERT OVERWRITE DIRECTORY '/user/hive/datos' SELECT concat_ws('-', Nombre, Apellido, id, FechaLogin, FechaAlta, Departamento) FROM analyst.temp2;

[cloudera@quickstart data]$ hdfs dfs -ls /user/hive/datos
Found 1 items
-rwxrwxrwx   1 cloudera supergroup        144 2021-03-29 12:52 /user/hive/datos/000000_0
[cloudera@quickstart data]$ hdfs dfs -cat /user/hive/datos/000000_0;
Lucas-Barral-103-20110607-20110607-recursos humanos
Marta-Cordero-102-20110607-20110607-contabilidad
Sandra-Torres-101-20131207-20131207-ventas

+----+
|16-.|
+----+
1. Mediante sqoop-import crear una tabla hive, exam.customers, a partir de la tabla customers de mysql.

sqoop import \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root \
--password cloudera \
--fields-terminated-by '\t' \
--table customers \
--hive-import \
--hive-database exam

2. Volcar en los directorios hdfs /user/cloudera/customers/CA, /user/cloudera/customers/NY, y /user/cloudera/customers/OR 
el contenido de las filas de la tabla customers que pertenezcan a cada uno de estos 3 estados respectivamente.

Los campos de los customers tienen que estar separados por el carácter ':'.

0: jdbc:hive2://localhost:10000>  
FROM exam.customers exc
INSERT OVERWRITE DIRECTORY '/user/cloudera/customers/CA'
  ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ':'
  SELECT cust_id, fname, lname, address, city, zipcode
  WHERE state = 'CA'
INSERT OVERWRITE DIRECTORY '/user/cloudera/customers/NY'
  ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ':'
  SELECT cust_id, fname, lname, address, city, zipcode
  WHERE state = 'NY'
INSERT OVERWRITE DIRECTORY '/user/cloudera/customers/OR'
  ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ':'
  SELECT cust_id, fname, lname, address, city, zipcode
  WHERE state = 'OR';

[cloudera@quickstart data]$ hdfs dfs -ls /user/cloudera/customers
Found 3 items
drwxr-xr-x   - cloudera cloudera          0 2021-03-29 15:48 /user/cloudera/customers/CA
drwxr-xr-x   - cloudera cloudera          0 2021-03-29 15:48 /user/cloudera/customers/NY
drwxr-xr-x   - cloudera cloudera          0 2021-03-29 15:48 /user/cloudera/customers/OR

+----+
|18-.|
+----+
Se dan los ficheros au (Australia), ca (Canadá) y us (Estados Unidos) tienen el siguiente formato.
[cloudera@quickstart users]$ hdfs dfs -ls /user/cloudera/data_de_hdfs/
-rw-r--r--   1 cloudera cloudera        528 2021-04-01 07:05 /user/cloudera/data_de_hdfs/au
-rw-r--r--   1 cloudera cloudera        972 2021-04-01 07:05 /user/cloudera/data_de_hdfs/ca
-rw-r--r--   1 cloudera cloudera        538 2021-04-01 07:05 /user/cloudera/data_de_hdfs/us

Formato1:
firstname
lastname
address
country
city
state
post
phone1
phone2
email
web

Por otra parte, los ficheros us_ak (Estados Unidos/Alaska) y us_az (Estados Unidos/Arizona) tienen el mismo formato excepto que no contienen ni country ni state. 
[cloudera@quickstart solution]$ hdfs dfs -ls /user/cloudera/data_de_hdfs
-rw-r--r--   1 cloudera cloudera        752 2021-04-01 07:18 /user/cloudera/data_de_hdfs/us_ak
-rw-r--r--   1 cloudera cloudera       1198 2021-04-01 07:19 /user/cloudera/data_de_hdfs/us_az

Formato2:
firstname
lastname
address
country
city
state
post
phone1
phone2
email
web

1
Crear una tabla, users_partitioned, particionada con todos los campos especificados en Formato1 y particionada por country y state. 
Hay que poner todos los parámetros por omisión excepto el separador de campos.

[cloudera@quickstart solution]$ hdfs dfs -cat /user/cloudera/data_de_hdfs/au |more
Soledad,Mockus,75 Elm Rd #1190,AU,Barton,AC,2600,02-1291-8182,0444-126-746,soledad_mockus@yahoo.com,http://www.sinclairmachineproductsinc.com.au
Dana,Vock,49 Walnut St,AU,Yarralumla,AC,2600,02-6689-1150,0411-398-917,dana_vock@yahoo.com,http://www.friedmonteesq.com.au
Wenona,Carmel,44 Bush St,AU,Grosvenor Place,NS,1220,02-2832-1545,0439-849-209,wenona@gmail.com,http://www.maierkristinem.com.au
Deane,Haag,9 Hamilton Blvd #299,AU,Sydney South,NS,1235,02-9718-2944,0453-828-758,dhaag@hotmail.com,http://www.malsbarymfgco.com.au

DROP TABLE IF EXISTS test.users_partitioned ;
CREATE EXTERNAL TABLE IF NOT EXISTS test.users_partitioned
  (
   firstname STRING,
   lastname STRING,
   address STRING,
   city STRING,
   post INT,
   phone1 STRING,
   phone2 STRING,
   email STRING,
   web STRING
  )
  PARTITIONED BY (country STRING COMMENT 'particion por country', state STRING COMMENT 'particion por state') 
  ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

2
Crear una partición en users_partitioned con country='US' y state='AK'.  Esta partición tiene que estar en la ubicación por omisión.
ALTER TABLE test.users_partitioned ADD IF NOT EXISTS PARTITION (country='US', state='AK');

Ejecutar también el comando Hive/Impala correspondiente para comprobar que hay constancia de la partición en el metastore.
0: jdbc:hive2://localhost:10000> describe formatted test.users_partitioned;

| # Partition Information       | NULL                                               | NULL                   |
| # col_name                    | data_type                                          | comment                |
|                               | NULL                                               | NULL                   |
| country                       | string                                             | particion por country  |
| state                         | string                                             | particion por state    |

| Table Parameters:             | NULL                                               | NULL                   |
|                               | numPartitions                                      | 1                      |

Una vez creada la partición comprobar que el fichero hdfs que se tiene que crear se crea. 
[cloudera@quickstart solution]$ hdfs dfs -ls /user/hive/warehouse/test.db
drwxrwxrwx   - impala supergroup          0 2021-04-01 07:44 /user/hive/warehouse/test.db/users_partitioned
[cloudera@quickstart solution]$ hdfs dfs -ls /user/hive/warehouse/test.db/users_partitioned
drwxrwxrwx   - impala supergroup          0 2021-04-01 07:44 /user/hive/warehouse/test.db/users_partitioned/country=US
[cloudera@quickstart solution]$ hdfs dfs -ls /user/hive/warehouse/test.db/users_partitioned/country=US
drwxrwxrwx   - impala supergroup          0 2021-04-01 07:44 /user/hive/warehouse/test.db/users_partitioned/country=US/state=AK
[cloudera@quickstart solution]$ hdfs dfs -ls /user/hive/warehouse/test.db/users_partitioned/country=US/state=AK


3
Mediante el comando LOAD DATA... meter los datos del fichero us_ak en la partición creada en 2.
LOAD DATA INPATH '/user/cloudera/data_de_hdfs/us_ak' 
INTO TABLE test.users_partitioned
  PARTITION (country='US', state='AK');

Comprobar que efectivamente los datos están en la tabla. 
SELECT COUNT(*) 
FROM test.users_partitioned;

SELECT COUNT(*) 
FROM test.users_partitioned 
WHERE country='US' AND state='AK';

[quickstart.cloudera:21000] > SELECT COUNT(*) FROM test.users_partitioned;
+----------+
| count(*) |
+----------+
| 6        |
+----------+
Fetched 1 row(s) in 3.06s

[quickstart.cloudera:21000] > SELECT COUNT(*) FROM test.users_partitioned WHERE COUNTRY='US' AND STATE='AK' ;
+----------+
| count(*) |
+----------+
| 6        |
+----------+
Fetched 1 row(s) in 1.04s

Comprobar también que se ha borrado el fichero us_ak del directorio data de hdfs.
[cloudera@quickstart solution]$ hdfs dfs -ls /user/cloudera/data_de_hdfs
-rw-r--r--   1 cloudera cloudera        528 2021-04-01 07:05 /user/cloudera/data_de_hdfs/au
-rw-r--r--   1 cloudera cloudera        972 2021-04-01 07:05 /user/cloudera/data_de_hdfs/ca
-rw-r--r--   1 cloudera cloudera        538 2021-04-01 07:05 /user/cloudera/data_de_hdfs/us
-rw-r--r--   1 cloudera cloudera       1198 2021-04-01 07:19 /user/cloudera/data_de_hdfs/us_az

4
Crear el directorio /users_us_az en hdfs y mover el fichero us_az del directorio data de hdfs al directorio /users_us_az.

hdfs dfs -mkdir /users_us_az 
hdfs dfs -mv /user/cloudera/data_de_hdfs/us_az /users_us_az

[cloudera@quickstart solution]$ hdfs dfs -mkdir /users_us_az
[cloudera@quickstart solution]$ hdfs dfs -mv /user/cloudera/data_de_hdfs/us_az /users_us_az
[cloudera@quickstart solution]$ hdfs dfs -ls /users_us_az
-rw-r--r--   1 cloudera cloudera       1198 2021-04-01 07:19 /users_us_az/us_az

Una vez hecho esto, crear otra partición en la tabla users_partitioned que tenga como country 'US' y como state 'AZ'. 
Obviamente, en el metastore tiene que haber una indicación de que los datos de esta partición están en /users_us_az.
ALTER TABLE test.users_partitioned ADD IF NOT EXISTS PARTITION (country='US', state='AZ')
LOCATION '/users_us_az'

0: jdbc:hive2://localhost:10000> describe formatted test.users_partitioned;

| # Partition Information       | NULL                                               | NULL                   |
| # col_name                    | data_type                                          | comment                |
|                               | NULL                                               | NULL                   |
| country                       | string                                             | particion por country  |
| state                         | string                                             | particion por state    |
|                               | NULL                                               | NULL                   |

|                               | numPartitions                                      | 2                      |


[quickstart.cloudera:21000] > SELECT COUNT(*) 
FROM test.users_partitioned;
+----------+
| count(*) |
+----------+
| 15       |
+----------+
Fetched 1 row(s) in 18.98s

0: jdbc:hive2://localhost:10000> SELECT COUNT(*) 
FROM test.users_partitioned 
WHERE country='US' 
 AND state='AZ';
+------+--+
| _c0  |
+------+--+
| 9    |
+------+--+

[quickstart.cloudera:21000] > invalidate metadata test.users_partitioned;
[quickstart.cloudera:21000] > SELECT COUNT(*) 
FROM test.users_partitioned;
+----------+
| count(*) |
+----------+
| 15       |
+----------+
Fetched 1 row(s) in 0.74s

[quickstart.cloudera:21000] > SELECT COUNT(*) 
FROM test.users_partitioned 
WHERE country='US' 
  AND state='AZ';
+----------+
| count(*) |
+----------+
| 9        |
+----------+
Fetched 1 row(s) in 0.66s

[cloudera@quickstart solution]$ hdfs dfs -ls /user/cloudera/data_de_hdfs/
-rw-r--r--   1 cloudera cloudera        528 2021-04-01 07:05 /user/cloudera/data_de_hdfs/au
-rw-r--r--   1 cloudera cloudera        972 2021-04-01 07:05 /user/cloudera/data_de_hdfs/ca
-rw-r--r--   1 cloudera cloudera        538 2021-04-01 07:05 /user/cloudera/data_de_hdfs/us

5
Crear una tabla users_aux con los campos indicados en el Formato1.
Hay que poner todos los parámetros por omisión excepto el separador de campos.
DROP TABLE IF EXISTS test.users_aux ;
CREATE EXTERNAL TABLE IF NOT EXISTS test.users_aux
  (
   firstname STRING,
   lastname STRING,
   address STRING,
   country STRING,
   city STRING,
   state STRING,
   post INT,
   phone1 STRING,
   phone2 STRING,
   email STRING,
   web STRING
  )
  ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

ALTER TABLE test.users_aux CHANGE post post STRING;

[cloudera@quickstart solution]$ hdfs dfs -ls /user/cloudera/data_de_hdfs/
drwxrwxrwx   - impala supergroup          0 2021-04-01 08:50 /user/hive/warehouse/test.db/users_aux

Una vez hecho lo anterior, copiar los ficheros au, ca, y us al directorio correspondiente de la tabla recién creada. 
[cloudera@quickstart]$ hdfs dfs -cp /user/cloudera/data_de_hdfs/* /user/hive/warehouse/test.db/users_aux/

[cloudera@quickstart solution]$ hdfs dfs -ls /user/hive/warehouse/test.db/users_aux/
-rw-r--r--   1 cloudera supergroup        528 2021-04-01 08:55 /user/hive/warehouse/test.db/users_aux/au
-rw-r--r--   1 cloudera supergroup        972 2021-04-01 08:55 /user/hive/warehouse/test.db/users_aux/ca
-rw-r--r--   1 cloudera supergroup        538 2021-04-01 08:55 /user/hive/warehouse/test.db/users_aux/us

Comprobar que la tabla efectivamente, contiene los datos.
SELECT COUNT(*) 
FROM test.users_aux;

[quickstart.cloudera:21000] > INVALIDATE METADATA test.users_aux ;
Fetched 0 row(s) in 0.33s
[quickstart.cloudera:21000] > SELECT COUNT(*) 
FROM test.users_aux ;
+----------+
| count(*) |
+----------+
| 15       |
+----------+
Fetched 1 row(s) in 3.83s

6
Usar particionamiento dinámico para meter en users_partitioned todos los registros correspondientes a CA y AU que se encuentre en users_aux. 
Por lo tanto, no se deben usar los registros de 'US'. Comprobar los datos.

INSERT INTO TABLE test.users_partitioned
PARTITION(country, state)
SELECT firstname, lastname, address, city, post, phone1, phone2, email, web, country, state
FROM test.users_aux
WHERE country = 'CA' OR country = 'AU';

[quickstart.cloudera:21000] > ALTER TABLE test.users_partitioned 
CHANGE post post STRING;

[quickstart.cloudera:21000] > INSERT OVERWRITE TABLE test.users_partitioned
PARTITION(country, state)
SELECT firstname, lastname, address, city, post, phone1, phone2, email, web, country, state
FROM test.users_aux
WHERE country = 'CA' OR country = 'AU';

7
Usar simultáneamente particionamiento estático y dinámico para crear las particiones correspondientes a 'US'. 
Es decir, en la sentencia correspondiente hay que especificar como country 'US' y no indicar ningún state específico.

INSERT INTO TABLE test.users_partitioned
PARTITION(country='US', state)
SELECT firstname, lastname, address, city, post, phone1, phone2, email, web, state
FROM test.users_aux
WHERE country='US';

0: jdbc:hive2://localhost:10000> describe formatted test.users_partitioned;
| # Partition Information       | NULL                                               | NULL                   |
| # col_name                    | data_type                                          | comment                |
|                               | NULL                                               | NULL                   |
| country                       | string                                             | particion por country  |
| state                         | string                                             | particion por state    |
|                               | NULL                                               | NULL                   |

|                               | numPartitions                                      | 8                      |

[quickstart.cloudera:21000] > SELECT COUNT(*) 
FROM test.users_partitioned 
WHERE country='US';
+----------+
| count(*) |
+----------+
| 19       |
+----------+
Fetched 1 row(s) in 0.79s

+----+
|17-.|
+----+
Se dan los ficheros siguientes
employee.csv
1001,Carlos,hombre,35
1002,Juan,hombre,36
1003,Marcos,hombre,28
1004,Ananda,mujer,30
1005,Perla,mujer,42

salary.csv
1001,120000
1002,99000
1003,106000
1004,79000
1005,89000

Hay que hacer lo siguiente:

1. Cargar los ficheros en hdfs.
hdfs dfs -put employee.csv /analyst/dualcore/
hdfs dfs -put salary.csv /analyst/dualcore/

2. Crear dos tablas en mysql: employee y salary
DROP TABLE IF EXISTS test.employee;
CREATE TABLE if not exists test.employee (
id int primary key,
name varchar(10),
sex varchar(6),
age int
) ;

DROP TABLE IF EXISTS test.salary;
CREATE TABLE if not exists test.salary (
id int primary key,
salary int
) ;

3. Mediante sqoop-export, volcar los ficheros de hdfs a las tablas respectivas
-- los ficheros .avsc que se crean con sqoop se generan donde se ejecuta el comando

sqoop-export \
--connect jdbc:mysql://quickstart:3306/test \
--username root \
--password cloudera \
--table employee \
--export-dir '/analyst/dualcore/employee.csv' \
--input-fields-terminated-by ','

sqoop-export \
--connect jdbc:mysql://quickstart:3306/test \
--username root \
--password cloudera \
--table salary \
--export-dir '/analyst/dualcore/salary.csv' \
--input-fields-terminated-by ','

mysql> select * from test.employee;
+------+--------+--------+------+
| id   | name   | sex    | age  |
+------+--------+--------+------+
| 1001 | Carlos | hombre |   35 |
| 1002 | Juan   | hombre |   36 |
| 1003 | Marcos | hombre |   28 |
| 1004 | Ananda | mujer  |   30 |
| 1005 | Perla  | mujer  |   42 |
+------+--------+--------+------+
5 rows in set (0.00 sec)

mysql> select * from test.salary;
+------+--------+
| id   | salary |
+------+--------+
| 1001 | 120000 |
| 1002 |  99000 |
| 1003 | 106000 |
| 1004 |  79000 |
| 1005 |  89000 |
+------+--------+
5 rows in set (0.00 sec)

4. Crear en hdfs ficheros avro con los datos de las tablas de mysql creadas en el punto 2. 
   Los directorios hdfs tienen que ser:
- /retail/employee (tabla employee) y 
- /retail/salary (tabla salary).

─ Los ficheros de schema *.avsc que se crean con sqoop se generan en el lugar donde se ejecuta el comando
sqoop-import \
--connect jdbc:mysql://quickstart:3306/test \
--username root \
--password cloudera \
--table employee \
--target-dir '/retail/employee' \
--as-avrodatafile

─ Los ficheros de schema *.avsc que se crean con sqoop se generan en el lugar donde se ejecuta el comando
sqoop-import \
--connect jdbc:mysql://quickstart:3306/test \
--username root \
--password cloudera \
--table salary \
--target-dir '/retail/salary' \
--compress \
--as-avrodatafile

[cloudera@quickstart ~]$ hdfs dfs -ls /retail/employee
Found 6 items
-rw-r--r--   1 cloudera supergroup          0 2021-04-02 17:29 /retail/employee/_SUCCESS
-rw-r--r--   1 cloudera supergroup        525 2021-04-02 17:29 /retail/employee/part-m-00000.avro
-rw-r--r--   1 cloudera supergroup        523 2021-04-02 17:29 /retail/employee/part-m-00001.avro
-rw-r--r--   1 cloudera supergroup        525 2021-04-02 17:29 /retail/employee/part-m-00002.avro
-rw-r--r--   1 cloudera supergroup        524 2021-04-02 17:29 /retail/employee/part-m-00003.avro
-rw-r--r--   1 cloudera supergroup        523 2021-04-02 17:29 /retail/employee/part-m-00004.avro
[cloudera@quickstart ~]$ hdfs dfs -ls /retail/salary
Found 6 items
-rw-r--r--   1 cloudera supergroup          0 2021-04-02 17:31 /retail/salary/_SUCCESS
-rw-r--r--   1 cloudera supergroup        352 2021-04-02 17:31 /retail/salary/part-m-00000.avro
-rw-r--r--   1 cloudera supergroup        352 2021-04-02 17:31 /retail/salary/part-m-00001.avro
-rw-r--r--   1 cloudera supergroup        352 2021-04-02 17:31 /retail/salary/part-m-00002.avro
-rw-r--r--   1 cloudera supergroup        352 2021-04-02 17:31 /retail/salary/part-m-00003.avro
-rw-r--r--   1 cloudera supergroup        352 2021-04-02 17:31 /retail/salary/part-m-00004.avro

5. Crear tablas avro en hive apuntando a los datos de los ficheros del punto anterior. 
Hay que crear ambas tablas usando TBLPROPERTIES ( 'avro.schema.url', ...
Para ello, por cada tabla, necesitamos un fichero avsc con el esquema correpondiente. 
(-- los ficheros .avsc que se crean con sqoop se generan donde se ejecuta el comando)
Para la tabla employee se puede usar el fichero que crea automáticamene sqoop cuando importa los datos.
Para la tabla salary, hay que obtener el fichero avsc con el esquema usando la herramienta avro-tools aplicada sobre uno de los ficheros que se ha descargado sqoop..


[cloudera@quickstart ~]$ avro-tools getschema hdfs://quickstart/retail/employee/part-m-00000.avro > employee.avsc
[cloudera@quickstart ~]$ avro-tools getschema hdfs://quickstart/retail/salary/part-m-00000.avro > salary.avsc
[cloudera@quickstart ~]$ hdfs dfs -put *.avsc /user/cloudera/

DROP DATABASE IF EXISTS employee;
[quickstart.cloudera:21000] > CREATE TABLE IF NOT EXISTS employee
STORED AS AVRO
LOCATION '/retail/employee'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera:8020/user/cloudera/employee.avsc');

DROP DATABASE IF EXISTS salary;
CREATE TABLE IF NOT EXISTS salary
STORED AS AVRO
LOCATION '/retail/salary'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera:8020/user/cloudera/salary.avsc');

[quickstart.cloudera:21000] > select * from test.employee;
+------+--------+--------+-----+
| id   | name   | sex    | age |
+------+--------+--------+-----+
| 1003 | Marcos | hombre | 28  |
| 1001 | Carlos | hombre | 35  |
| 1005 | Perla  | mujer  | 42  |
| 1004 | Ananda | mujer  | 30  |
| 1002 | Juan   | hombre | 36  |
+------+--------+--------+-----+
[quickstart.cloudera:21000] > select * from test.salary;
+------+--------+
| id   | salary |
+------+--------+
| 1001 | 120000 |
| 1004 | 79000  |
| 1005 | 89000  |
| 1002 | 99000  |
| 1003 | 106000 |
+------+--------+

+----+
|19-.|
+----+
Sea el fichero hdfs /user/cloudera/ads/ads.log

hdfs dfs -put ads.log /user/cloudera/ads/ads.log

1,http://cloudera.com/path1/p.php?keyword=hadoop&country=france#Ref1,30/JUN/2016,45
2,http://hive.com/path1/p.php?keyword=hive&country=italy#Ref1,30/JUN/2016,55
3,http://impala.com/path1/p.php?keyword=spark&country=france#Ref1,30/JUN/2016,90
4,http://cloudera.com/path1/p.php?keyword=pig&country=italy#Ref1,30/JUN/2016,31
5,http://hive.com/path1/p.php?keyword=datascience&country=france#Ref1,30/JUN/2016,88
6,http://impala.com/path1/p.php?keyword=java&country=italy#Ref1,30/JUN/2016,40
7,http://cloudera.com/path1/p.php?keyword=jee&country=france#Ref1,01/JUL/2016,5
8,http://hive.com/path1/p.php?keyword=apache&country=italy#Ref1,01/JUL/2016,85
9,http://impala.com/path1/p.php?keyword=hadoopexam&country=france#Ref1,01/JUL/2016,29
10,http://cloudera.com/path1/p.php?keyword=hadooptraining&country=italy#Ref1,01/JUL/2016,25
11,http://hive.com/path1/p.php?keyword=de575&country=france#Ref1,01/JUL/2016,49
12,http://impala.com/path1/p.php?keyword=cca175&country=italy#Ref1,01/JUL/2016,95
1,http://cloudera.com/path1/p.php?keyword=hadoop&country=france#Ref1,30/JUN/2016,83
2,http://hive.com/path1/p.php?keyword=hive&country=italy#Ref1,30/JUN/2016,30
3,http://impala.com/path1/p.php?keyword=spark&country=france#Ref1,30/JUN/2016,40
4,http://cloudera.com/path1/p.php?keyword=pig&country=italy#Ref1,30/JUN/2016,40
5,http://hive.com/path1/p.php?keyword=datascience&country=france#Ref1,30/JUN/2016,66
6,http://impala.com/path1/p.php?keyword=java&country=italy#Ref1,30/JUN/2016,69
7,http://cloudera.com/path1/p.php?keyword=jee&country=france#Ref1,01/JUL/2016,71
8,http://hive.com/path1/p.php?keyword=apache&country=italy#Ref1,01/JUL/2016,73
9,http://impala.com/path1/p.php?keyword=hadoopexam&country=france#Ref1,01/JUL/2016,77
10,http://cloudera.com/path1/p.php?keyword=hadooptraining&country=italy#Ref1,01/JUL/2016,35
11,http://hive.com/path1/p.php?keyword=de575&country=france#Ref1,01/JUL/2016,76
12,http://impala.com/path1/p.php?keyword=cca175&country=italy#Ref1,01/JUL/2016,40

En el que los campos representan: id, url, date, cpc (cost per click)
Se pide:
1. Crear una tabla, ads, cuyos datos sean el contenido de este fichero.  
hdfs dfs -mkdir /user/cloudera/ads
hdfs dfs -put ads.log /user/cloudera/ads
[cloudera@quickstart]$ hdfs dfs -ls /user/cloudera/ads
-rw-r--r--   1 cloudera cloudera       1970 2021-04-03 15:52 /user/cloudera/ads/ads.log

El tipo de date tiene que ser STRING y su formato el mismo que el del fichero.

DROP TABLE IF EXISTS test.ads;
CREATE TABLE IF NOT EXISTS test.ads (
       id INT, 
       url STRING, 
       sdate STRING, 
       cpc INT COMMENT 'cost per click'
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ;

-- Carga los datos de la tabla con el fichero subido a hdfs 
LOAD DATA INPATH '/user/cloudera/ads'
 INTO TABLE test.ads;

2. Usando Impala y la función parse_url, crear una tabla, ads_partitioned, particionada por domain y country con los datos de la tabla ads. 
La tabla tiene que tener 6 columnas: 1) id, 2) url, 3) ddate, 4) cpc, 5) domain, y 6) country. 
La columna domain es la primera parte del URL y la columna country el valor del parámetro country.

-- Este select muestra los datos de la tabla test.ads, e incluye dos campos más para extractar dominio y country de la url:
SELECT id, url, sdate, cpc, parse_url(url, 'HOST') domain, parse_url(url, 'QUERY', 'country') country 
FROM test.ads;
+----+---------------------------------------------------------------------------+-------------+-----+--------------+---------+
| id | url                                                                       | sdate       | cpc | domain       | country |
+----+---------------------------------------------------------------------------+-------------+-----+--------------+---------+
| 1  | http://cloudera.com/path1/p.php?keyword=hadoop&country=france#Ref1        | 30/JUN/2016 | 45  | cloudera.com | france  |

DROP TABLE IF EXISTS test.ads_partitioned;
CREATE TABLE IF NOT EXISTS test.ads_partitioned
  PARTITIONED BY (domain, country) 
   COMMENT 'table partitioned by columns domain and country'
AS
  SELECT id, url, sdate, cpc, parse_url(url, 'HOST') domain, parse_url(url, 'QUERY', 'country') country 
   FROM test.ads;

+--------------------+
| summary            |
+--------------------+
| Inserted 24 row(s) |
+--------------------+

3. Crear una tabla, ads_standard_date: a) usando CTAS, b) en formato PARQUET, c) con columnas id, url, ddate, cpc provenientes de
ads_partitioned, d) con la fecha (ddate) en formato estándar; por ejemplo, en vez de '01/JUL/2016' tiene que tener '2016-07-01'.

-- Only Impala
DROP TABLE IF EXISTS test.ads_standard_date;
CREATE TABLE IF NOT EXISTS test.ads_standard_date
   COMMENT 'table parquet ads_standard_date'
  STORED AS PARQUET
AS
  SELECT id, url, to_date(to_timestamp(sdate, 'dd/MMM/yyyy')) ddate, cpc
    FROM test.ads_partitioned;
+--------------------+
| summary            |
+--------------------+
| Inserted 24 row(s) |
+--------------------+

-- Hive/Impala
DROP TABLE IF EXISTS test.ads_standard_date;
CREATE TABLE IF NOT EXISTS test.ads_standard_date
   COMMENT 'table parquet ads_standard_date'
  STORED AS PARQUET
AS
  SELECT id, url, to_date(from_unixtime(unix_timestamp( sdate, 'dd/MMM/yyyy'))) ddate, cpc 
    FROM test.ads_partitioned;

+----+
|20-.|
+----+

Nos dan dos ficheros parquet, file1.parq y file2.parq, situados en el directorio hdfs /user/cloudera/sales. 
[cloudera@quickstart CCAl159]$ hdfs dfs -ls /user/cloudera/sales
-rw-r--r--   1 cloudera cloudera        653 2021-04-05 05:19 /user/cloudera/sales/file1.parq
-rw-r--r--   1 cloudera cloudera        645 2021-04-05 05:19 /user/cloudera/sales/file2.parq

Tienen el siguiene esquema:
id TINYINT,
name STRING,
amount INT
Se pide:
1
Crear una tabla externa usando Hive, sales_hive, con las siguientes condiciones:
a. El esquema tiene que ser igual al de los ficheros.
b. El formato parquet.
c. Su LOCATION /user/cloudera/sales.

DROP TABLE IF EXISTS sales.sales_hive;
CREATE EXTERNAL TABLE IF NOT EXISTS sales.sales_hive(
       id TINYINT,
       name STRING,
       amount INT
)
  STORED AS PARQUET
  LOCATION '/user/cloudera/sales';

2
Crear una tabla externa usando Impala, sales_impala, con las siguientes condiciones:
a. El esquema tiene que ser igual al de los ficheros parquet del punto 1.
b. El formato avro.
c. Su LOCATION /user/cloudera/sales_avro.
d. En la sentencia CREATE TABLE no se pueden especificar explícitamente las columnas.

-- para generar el schema
[cloudera@quickstart CCAl159]$ avro-tools getschema hdfs://quickstart/user/cloudera/sales/file1.parq > file1.avsc
[cloudera@quickstart CCAl159]$ hdfs dfs -put file1* /user/cloudera/schemas

DROP TABLE IF EXISTS sales.sales_impala;
CREATE EXTERNAL TABLE IF NOT EXISTS sales.sales_impala
LIKE PARQUET '/user/cloudera/schemas/file1.parq'
STORED AS AVRO
LOCATION '/user/cloudera/sales_avro/';

CREATE EXTERNAL TABLE IF NOT EXISTS sales.sales_impala(
       id TINYINT,
       name STRING,
       amount INT
)
  STORED AS AVRO 
  LOCATION '/user/cloudera/sales_avro';

3
Meter en sales_impala el contenido de sales_hive
-- Only Hive - Impala no permite insertar datos en una tabla avro
INSERT INTO sales.sales_hive
SELECT * FROM sales.sales_hive;

+----+
|21-.|
+----+

Tenemos el siguiente esquema avro:
{
"type" : "record",
"name" : "products",
"doc" : "Sqoop import of products",
"fields" : [ {
"name" : "prod_id",
"type" : [ "null", "int" ],
"default" : null,
"columnName" : "prod_id",
"sqlType" : "4"
}, {
"name" : "brand",
"type" : [ "null", "string" ],
"default" : null,
"columnName" : "brand",
"sqlType" : "12"
}, {
"name" : "name",
"type" : [ "null", "string" ],
"default" : null,
"columnName" : "name",
"sqlType" : "12"
}, {
"name" : "price",
"type" : [ "null", "int" ],
"default" : null,
"columnName" : "price",
"sqlType" : "4"
}, {
"name" : "cost",
"type" : [ "null", "int" ],
"default" : null,
"columnName" : "cost",
"sqlType" : "4"
}, {
"name" : "shipping_wt",
"type" : [ "null", "int" ],
"default" : null,
"columnName" : "shipping_wt",
"sqlType" : "5"
} ],
"tableName" : "products"}

1
Se pide crear una tabla products_avro en la BBDD default con las siguientes condiciones:
1. Formato avro
2. Particionada por brand
3. Hay que utilizar TBLPROPERTIES para indicar dónde está el esquema

DROP TABLE IF EXISTS default.products_avro;
CREATE TABLE IF NOT EXISTS default.products_avro
PARTITIONED BY (brand)
STORED AS AVRO
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera:8020/user/cloudera/schemas/products.avsc');

2
Una vez creada, hay que usar particionamiento dinámico para pasar los valores de la tabla Hive analyst.products a products_avro


SELECT count(dbrand.brand)
       FROM (SELECT DISTINCT brand
               FROM analyst.products) dbrand;
+---------------------+
| count(dbrand.brand) |
+---------------------+
| 47                  |
+---------------------+

set hive.exec.dynamic.partition.mode=nonstrict ;
INSERT OVERWRITE TABLE default.products_avro
PARTITION(brand)
SELECT prod_id, brand, name, price, cost, shipping_wt, brand
 FROM analyst.products;

[cloudera@quickstart CCAl159]$ hdfs dfs -ls /user/hive/warehouse/products_avro/
Found 47 items
drwxrwxrwx   - username supergroup          0 2021-04-05 07:42 /user/hive/warehouse/products_avro/brand=ACME
drwxrwxrwx   - username supergroup          0 2021-04-05 07:42 /user/hive/warehouse/products_avro/brand=ARCAM
drwxrwxrwx   - username supergroup          0 2021-04-05 07:42 /user/hive/warehouse/products_avro/brand=Argo
.....

+----+
|24-.|
+----+

Se ejecutan los siguientes comandos en mysql:
DROP TABLE IF EXISTS analyst_dualcore.departments ;

CREATE TABLE IF NOT EXISTS analyst_dualcore.departments (
id int(11) PRIMARY KEY,
name VARCHAR(20)
) ;

INSERT INTO analyst_dualcore.departments VALUES
(1, 'Marketing'),
(2, 'Casimiro') ;

SELECT * 
FROM analyst_dualcore.departments ;

1
Importar mediante sqoop las filas de la tabla departments al directorio hdfs /user/hive/warehouse/departments. 
Los ficheros en hdfs deben tener el formato avro.

─ Los ficheros de schema *.avsc que se crean con sqoop se generan en el lugar donde se ejecuta el comando
sqoop-import --connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root --password cloudera \   
--table departments \ 
--target-dir '/user/hive/warehouse/departments' \
--delete-target-dir \
-m 1 \
--as-avrodatafile

departments.avsc
{
  "type" : "record",
  "name" : "departments",
  "doc" : "Sqoop import of departments",
  "fields" : [ {
    "name" : "id",
    "type" : [ "null", "int" ],
    "default" : null,
    "columnName" : "id",
    "sqlType" : "4"
  }, {
    "name" : "name",
    "type" : [ "null", "string" ],
    "default" : null,
    "columnName" : "name",
    "sqlType" : "12"
  } ],
  "tableName" : "departments"
}

[cloudera@quickstart ~]$ hdfs dfs -put departments.avsc /user/cloudera/schemas
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/schemas
-rw-r--r--   1 cloudera cloudera        396 2021-04-05 09:49 /user/cloudera/schemas/departments.avsc



2
En el paso 1, sqoop crea un fichero, departments.avsc, con el esquema avro de la tabla. Este fichero hay que utilizarlo para crear la tabla hive departments.  
Se debe crear con las siguientes condiciones:
1. Hay que indicar que el formato de los ficheros es avro.
2. No hay que especificar las columnas.
3. Para indicar el esquema hay que usar la propiedad avro.schema.url de TBLPROPERTIES.
Una vez creada la tabla, ejecutar un SELECT * FROM departments para comprobar los datos.

DROP TABLE IF EXISTS departments;
CREATE TABLE IF NOT EXISTS departments
STORED AS AVRO
LOCATION '/user/hive/warehouse/departments'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera:8020/user/cloudera/schemas/departments.avsc');



3
Modificar el fichero departments.asvc para incluir 
1) una columna, mgr_id, entre las columnas existentes id y name y 
2) otra columna,dept_loc, después de la columna name. 
Es decir, el esquema avro tiene que indicar que hay 4 columnas en este order: id, mgr_id, name, y dept_loc.
La columna mgr_id debe ser de tipo entero pero no debe admitir nulos.
Es decir la parte type del esquema tiene que poner simplemente int en vez de [ "null", "int" ]. 
El valor por omisión tiene que ser -1.
La columna dept_loc debe ser de tipo string pero no debe admitir nulos. 
Es decir la parte type del esquema tiene que poner simplemente string en vez de [ "null", "string" ]. 
El valor por omisión tiene que ser unknown.
En la definición de las columnas, se pueden quitar los elementos columnName y sqlType.

departments.avsc

{
  "type" : "record",
  "name" : "departments",
  "doc" : "Sqoop import of departments",
  "fields" : [ {
    "name" : "id",
    "type" : [ "null", "int" ],
    "default" : null,
    "columnName" : "id",
    "sqlType" : "4"
  },{
    "name" : "mgr_id",
    "type" : "int",
    "default" : -1,
    "columnName" : "mgr_id",
    "sqlType" : "4"
  },{
    "name" : "name",
    "type" : [ "null", "string" ],
    "default" : null,
    "columnName" : "name",
    "sqlType" : "12"
  },{
    "name" : "dept_loc",
    "type" : "string",
    "default" : "unknown",
    "columnName" : "dept_loc",
    "sqlType" : "12"
  } ],
  "tableName" : "departments"
}

[quickstart.cloudera:21000] > invalidate metadata analyst.departments;
[quickstart.cloudera:21000] > select * from analyst.departments;
+----+--------+-----------+----------+
| id | mgr_id | name      | dept_loc |
+----+--------+-----------+----------+
| 1  | -1     | Marketing | unknown  |
| 2  | -1     | Casimiro  | unknown  |
+----+--------+-----------+----------+


4
Mediante INSERT INTO ... meter datos en la tabla departments. A continuación, hacer un SELECT * FROM departments. 
Se tienen que ver los datos nuevos y los antiguos. En estos últimos, los valores correpondientes a mgr_id y dept_loc tienen que ser -1 y unknown respectivamente.

INSERT INTO analyst.departments VALUES
(3, 20, 'R&D', 'first floor'),
(4, 90, 'Cloudera', 'third floor') ;

0: jdbc:hive2://localhost:10000> select * from departments;
+-----------------+---------------------+-------------------+-----------------------+--+
| departments.id  | departments.mgr_id  | departments.name  | departments.dept_loc  |
+-----------------+---------------------+-------------------+-----------------------+--+
| 3               | 20                  | R&D               | first floor           |
| 4               | 90                  | Cloudera          | third floor           |
| 1               | -1                  | Marketing         | unknown               |
| 2               | -1                  | Casimiro          | unknown               |
+-----------------+---------------------+-------------------+-----------------------+--+

+----+
|22-.|
+----+

Se crean las tablas custA y custB del siguiente modo:
DROP TABLE IF EXISTS custA ;
CREATE TABLE IF NOT EXISTS custA (
name STRING,
state STRING
) ;
INSERT INTO custA VALUES ('Pete', 'CA'), ('Luigi', 'PA'), ('Manuel', 'CA'), ('Mark', 'PA'), ('Francis', 'OR'), ('John', 'OR') ;
DROP TABLE IF EXISTS custB ;
CREATE TABLE IF NOT EXISTS custB (
name STRING,
state STRING
) ;
INSERT INTO custB VALUES ('Pete', 'CA'), ('Luigi', 'PA'), ('Sandra', 'CA'), ('Lorna', 'PA'), ('Suzanne', 'OR'), ('Margaret', 'OR') ;

Se pide:
1. Crear una tabla cust utilizando CTAS que:
a. contenga todas las filas de custA cuyo state sea 'CA' or 'PA'
b. contenga también todas las filas de custB cuyo state sea 'CA' or 'OR'
c. Si hay filas duplicadas, éstas deben estar presentes en cust.

DROP TABLE IF EXISTS test.cust;
CREATE TABLE IF NOT EXISTS test.cust
AS
SELECT a.name, a.state
FROM test.custA a 
WHERE a.state IN ('CA', 'PA')
UNION ALL
SELECT b.name, b.state
FROM test.custB b 
WHERE b.state IN ('CA', 'OR') ; 

SELECT * FROM test.cust;

+----+
|23-.|
+----+

Sean las siguientes dos tablas con sus correspondientes filas.

DROP TABLE IF EXISTS test.studentsA;
CREATE TABLE IF NOT EXISTS test.studentsA (
courseId INT,
year INT,
userId INT,
isActive STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
STORED AS TEXTFILE;

INSERT INTO test.studentsA VALUES
(1000, date_part('year', current_timestamp()), 101, 'Y'),
(1001, date_part('year', current_timestamp()), 102, 'Y'),
(1000, date_part('year', current_timestamp()), 102, 'Y'),
(1000, date_part('year', current_timestamp()), 103, 'Y'),
(1003, date_part('year', current_timestamp()), 104, 'Y');

DROP TABLE IF EXISTS test.studentsB;
CREATE TABLE IF NOT EXISTS test.studentsB (
courseId INT,
year INT,
userId INT,
isActive STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
STORED AS TEXTFILE;

INSERT INTO test.studentsB VALUES
(1000, date_part('year', current_timestamp()), 101, 'Y'),
(1001, date_part('year', current_timestamp()), 102, 'Y'),
(1001, date_part('year', current_timestamp()), 102, 'Y'),
(1000, date_part('year', current_timestamp()), 103, 'Y'),
(1004, date_part('year', current_timestamp()), 104, 'Y');

1. Imprimir las filas que están en alguna de las 2 tablas pero sin duplicados. - Impala Only

SELECT * 
FROM test.studentsA
UNION DISTINCT 
SELECT *
FROM test.studentsB ;

2. Imprimir las filas que están en alguna de las 2 tablas pero con duplicados.

SELECT * 
FROM test.studentsA
UNION ALL
SELECT *
FROM test.studentsB ;

3. Imprimir las filas de studentsA cuyo courseId sea superior a 102 y las de studentsB cuyo courseId sea inferior a 103 pero no puede haber duplicados.
   Hay que usar una cláusula WITH ... SELECT.

WITH
t1 AS
(SELECT * 
 FROM studentsA 
 WHERE userId > 102),
t2 AS
(SELECT * 
 FROM studentsB 
 WHERE userId < 103)

SELECT * 
FROM t1 
UNION 
SELECT * 
FROM t2 ;

+----+
|25-.|
+----+

DROP TABLE IF EXISTS test.employees 
CREATE TABLE IF NOT EXISTS test.employees (
  empno    int,  
  ename    string,  
  job      string,  
  mgr      int,  
  hiredate timestamp,  
  sal      double,  
  comm     double,  
  deptno   int
) ;

INSERT INTO test.employees VALUES
  (7698, 'BLAKE',  'MANAGER',  7839, '1981-05-01', 2850, NULL, 30 ),
  (7782, 'CLARK',  'MANAGER',  7839, '1981-06-09', 2450, NULL, 10 ),
  (7566, 'JONES',  'MANAGER',  7839, '1981-04-02', 2975, NULL, 20 ),
  (7788, 'SCOTT',  'ANALYST',  7566, '1987-07-13', 3000, NULL, 20 ),
  (7902, 'FORD',   'ANALYST',  7566, '1981-12-03', 3000, NULL, 20 ),
  (7369, 'SMITH',  'CLERK',    7902, '1980-12-17', 800,  NULL, 20 ),
  (7499, 'ALLEN',  'SALESMAN', 7698, '1981-02-20', 1600, 300,  30 ),
  (7521, 'WARD',   'SALESMAN', 7698, '1981-02-22', 1250, 500,  30 ),
  (7654, 'MARTIN', 'SALESMAN', 7698, '1981-09-28', 1250, 1400, 30 ),
  (7844, 'TURNER', 'SALESMAN', 7698, '1981-09-08', 1500, 0,    30 ),
  (7876, 'ADAMS',  'CLERK',    7788, '1987-07-13', 1100, NULL, 20 ),
  (7900, 'JAMES',  'CLERK',    7698, '1981-12-03', 950,  NULL, 30 ),
  (7934, 'MILLER', 'CLERK',    7782, '1982-01-23', 1300, NULL, 10 );


DROP TABLE IF EXISTS test.deps (
CREATE TABLE IF NOT EXISTS test.deps (
  deptno INT,
  dname  STRING,
  loc    STRING
) ;

INSERT INTO test.deps VALUES
  (10, 'ACCOUNTING', 'NEW YORK'),
  (20, 'RESEARCH',   'DALLAS'),
  (30, 'SALES',      'SAN FRANCISCO'),
  (40, 'OPERATIONS', 'PITTSBURGH');

1. Ejecutar un query que desnormalice las tablas:

SELECT e.empno, e.ename, e.job, e.mgr, e.hiredate,  e.sal, e.comm, d.dname, d.loc
FROM test.employees e
INNER JOIN test.deps d ON(e.deptno = d.deptno);

2. Imprimir los departamentos que no tengan empleados

SELECT d.deptno, d.dname, count(e.empno) tot
FROM test.employees e
RIGHT JOIN test.deps d ON(e.deptno = d.deptno)
GROUP BY d.deptno, d.dname
HAVING tot = 0
;

SELECT dname
FROM employees e
RIGHT OUTER JOIN deps d ON (e.deptno = d.deptno )
WHERE e.deptno is NULL ;

+----+
|26-.|
+----+

Tenemos la siguiente tabla
DROP TABLE IF EXISTS test.courses ;
CREATE TABLE test.courses (
subject STRING,
course STRING,
price
DECIMAL(9, 3)
) ;

INSERT INTO test.courses VALUES
('Hadoop', 'Hive', 4000), ('Hadoop', 'Impala', 2400),
('Hadoop', 'Pig', 2400), ('Hadoop', 'OOzie', 1500),
('Hadoop', 'Zookeeper', 1550),
('Hadoop', 'Mahout', 2600), ('Spark', 'SQL', 1450),
('Spark', 'Spark', 2400),
('Spark', 'Machine Learning', 5000) ;

1. Mostar la tabla ordenando las filas por precio de más bajo a más alto. 
   Cada fila debe mostrar el orden. Si dos cursos tienen el mismo precio, el número asociado al orden debe ser igual

SELECT RANK() OVER (ORDER BY price) orden, subject, course, price
FROM test.courses
+-------+---------+------------------+----------+
| orden | subject | course           | price    |
+-------+---------+------------------+----------+
| 1     | Spark   | SQL              | 1450.000 |
| 2     | Hadoop  | OOzie            | 1500.000 |
| 3     | Hadoop  | Zookeeper        | 1550.000 |
| 4     | Hadoop  | Impala           | 2400.000 |
| 4     | Hadoop  | Pig              | 2400.000 |
| 4     | Spark   | Spark            | 2400.000 |
| 7     | Hadoop  | Mahout           | 2600.000 |
| 8     | Hadoop  | Hive             | 4000.000 |
| 9     | Spark   | Machine Learning | 5000.000 |
+-------+---------+------------------+-------

-- Con este group by sobre la tabla no sabemos cual es el ranking
SELECT subject, course, price 
FROM test.courses
GROUP BY subject, course, price
ORDER BY price DESC
+---------+------------------+----------+
| subject | course           | price    |
+---------+------------------+----------+
| Spark   | Machine Learning | 5000.000 |
| Hadoop  | Hive             | 4000.000 |
| Hadoop  | Mahout           | 2600.000 |
| Hadoop  | Pig              | 2400.000 |
| Spark   | Spark            | 2400.000 |
| Hadoop  | Impala           | 2400.000 |
| Hadoop  | Zookeeper        | 1550.000 |
| Hadoop  | OOzie            | 1500.000 |
| Spark   | SQL              | 1450.000 |
+---------+------------------+----------+

2. Mostrar los cursos cuyo precio está entre los 4 más caros.

SELECT *
  FROM (SELECT RANK() OVER (ORDER BY price DESC) orden, subject, course, price
          FROM test.courses) rnk
WHERE rnk.orden <= 4
;
+-------+---------+------------------+----------+
| orden | subject | course           | price    |
+-------+---------+------------------+----------+
| 1     | Spark   | Machine Learning | 5000.000 |
| 2     | Hadoop  | Hive             | 4000.000 |
| 3     | Hadoop  | Mahout           | 2600.000 |
| 4     | Hadoop  | Impala           | 2400.000 |
| 4     | Hadoop  | Pig              | 2400.000 |
| 4     | Spark   | Spark            | 2400.000 |
+-------+---------+------------------+----------+

-O-J-O- 2. OBSERVAR EL ORDEN DEL RANK no es lo que buscamos:

SELECT rnk.orden, rnk.subject, rnk.course, rnk.price
FROM (SELECT RANK() OVER (ORDER BY price) orden, subject, course, price
      FROM test.courses) rnk
WHERE rnk.orden >= 4
ORDER BY rnk.orden DESC

+-------+---------+------------------+----------+
| orden | subject | course           | price    |
+-------+---------+------------------+----------+
| 9     | Spark   | Machine Learning | 5000.000 |
| 8     | Hadoop  | Hive             | 4000.000 |
| 7     | Hadoop  | Mahout           | 2600.000 |
| 4     | Hadoop  | Impala           | 2400.000 |
| 4     | Hadoop  | Pig              | 2400.000 |
| 4     | Spark   | Spark            | 2400.000 |
+-------+---------+------------------+----------+

3. Mostrar los cursos cuyo precio está entre los 4 más baratos.  
   En la salida, los precios más caros tienen que salir al principio. (primero hacer el rank y luego el order del rank para mostrar la salida)

SELECT *
FROM (SELECT RANK() OVER (ORDER BY price) orden, subject, course, price
      FROM test.courses) rnk
WHERE rnk.orden <= 4
ORDER BY price DESC
;

+-------+---------+-----------+----------+
| orden | subject | course    | price    |
+-------+---------+-----------+----------+
| 4     | Hadoop  | Impala    | 2400.000 |
| 4     | Hadoop  | Pig       | 2400.000 |
| 4     | Spark   | Spark     | 2400.000 |
| 3     | Hadoop  | Zookeeper | 1550.000 |
| 2     | Hadoop  | OOzie     | 1500.000 |
| 1     | Spark   | SQL       | 1450.000 |
+-------+---------+-----------+----------+

4. Por cada materia (subject), mostrar los dos cursos más baratos.
SELECT * 
FROM (SELECT RANK() OVER (PARTITION by subject ORDER BY price) orden, subject, course, price
      FROM test.courses) rnksubj
WHERE rnksubj.orden <= 2
;

+-------+---------+-----------+----------+
| orden | subject | course    | price    |
+-------+---------+-----------+----------+
| 1     | Hadoop  | OOzie     | 1500.000 |
| 2     | Hadoop  | Zookeeper | 1550.000 |
| 1     | Spark   | SQL       | 1450.000 |
| 2     | Spark   | Spark     | 2400.000 |
+-------+---------+-----------+----------+

5. Por cada materia (subject), mostrar los dos cursos más caros.
SELECT *
FROM ( SELECT subject, course, price, RANK() OVER(PARTITION by subject ORDER BY price DESC) rank
       FROM test.courses ) rnk
WHERE rank <= 2
+---------+------------------+----------+------+
| subject | course           | price    | rank |
+---------+------------------+----------+------+
| Hadoop  | Hive             | 4000.000 | 1    |
| Hadoop  | Mahout           | 2600.000 | 2    |
| Spark   | Machine Learning | 5000.000 | 1    |
| Spark   | Spark            | 2400.000 | 2    |
+---------+------------------+----------+------+

+----+
|27-.| 
+----+

DROP TABLE IF EXISTS test.courses ;
CREATE TABLE IF NOT EXISTS test.courses (
subject STRING,
course STRING,
price DECIMAL(9, 3)
) ;
INSERT INTO test.courses VALUES
('Hadoop', 'Hive', 4000), ('Hadoop', 'Impala', 2400),
('Hadoop', 'Pig', 2400), ('Hadoop', 'OOzie', 1500),
('Hadoop', 'Zookeeper', 1550),
('Hadoop', 'Mahout', 2600), ('Spark', 'SQL', 1450),
('Spark', 'Spark', 2400),
('Spark', 'Machine Learning', 5000) ;

1. Por cada curso imprimir su precio junto con el precio más barato dentro de su categoría (subject). 
   La salida tiene que estar ordenada por categoría (subject) y curso (course).

SELECT subject, course, price, FIRST_VALUE(price) OVER(PARTITION BY subject ORDER BY price) cheap
FROM test.courses
ORDER BY subject, course
;
+---------+------------------+----------+----------+
| subject | course           | price    | cheap    |
+---------+------------------+----------+----------+
| Hadoop  | Hive             | 4000.000 | 1500.000 |
| Hadoop  | Impala           | 2400.000 | 1500.000 |
| Hadoop  | Mahout           | 2600.000 | 1500.000 |
| Hadoop  | OOzie            | 1500.000 | 1500.000 |
| Hadoop  | Pig              | 2400.000 | 1500.000 |
| Hadoop  | Zookeeper        | 1550.000 | 1500.000 |
| Spark   | Machine Learning | 5000.000 | 1450.000 |
| Spark   | SQL              | 1450.000 | 1450.000 |
| Spark   | Spark            | 2400.000 | 1450.000 |
+---------+------------------+----------+----------+

-- Genera el mismo resultado
SELECT subject, course, price, MIN(price) OVER(PARTITION BY subject ORDER BY price) expensive
FROM test.courses
ORDER BY subject, course
;

+---------+------------------+----------+-----------+
| subject | course           | price    | expensive |
+---------+------------------+----------+-----------+
| Hadoop  | Hive             | 4000.000 | 1500.000  |
| Hadoop  | Impala           | 2400.000 | 1500.000  |
| Hadoop  | Mahout           | 2600.000 | 1500.000  |
| Hadoop  | OOzie            | 1500.000 | 1500.000  |
| Hadoop  | Pig              | 2400.000 | 1500.000  |
| Hadoop  | Zookeeper        | 1550.000 | 1500.000  |
| Spark   | Machine Learning | 5000.000 | 1450.000  |
| Spark   | SQL              | 1450.000 | 1450.000  |
| Spark   | Spark            | 2400.000 | 1450.000  |
+---------+------------------+----------+-----------+

2. Por cada curso imprimir su precio junto con el precio más caro dentro de su categoría (subject). 

-OJO- ESTO NO LO VEO...- 
SELECT subject, course, price, LAST_VALUE(price) OVER(PARTITION BY subject ORDER BY price DESC) expensive
FROM test.courses
;

SELECT subject, course, price, MAX(price) OVER(PARTITION BY subject ORDER BY price DESC) expensive
FROM test.courses;
;

+---------+------------------+----------+-----------+
| subject | course           | price    | expensive |
+---------+------------------+----------+-----------+
| Hadoop  | Hive             | 4000.000 | 4000.000  |
| Hadoop  | Mahout           | 2600.000 | 4000.000  |
| Hadoop  | Impala           | 2400.000 | 4000.000  |
| Hadoop  | Pig              | 2400.000 | 4000.000  |
| Hadoop  | Zookeeper        | 1550.000 | 4000.000  |
| Hadoop  | OOzie            | 1500.000 | 4000.000  |
| Spark   | Machine Learning | 5000.000 | 5000.000  |
| Spark   | Spark            | 2400.000 | 5000.000  |
| Spark   | SQL              | 1450.000 | 5000.000  |
+---------+------------------+----------+-----------+


+----+
|28-.| 
+----+

DROP TABLE IF EXISTS test.courses ;
CREATE TABLE IF EXISTS test.courses (
subject STRING,
course STRING,
price DECIMAL(9, 3)
) ;
INSERT INTO test.courses VALUES
('Hadoop', 'Hive', 4000), ('Hadoop', 'Impala', 2400),
('Hadoop', 'Pig', 2400), ('Hadoop', 'OOzie', 1500),
('Hadoop', 'Zookeeper', 1550),
('Hadoop', 'Mahout', 2600), ('Spark', 'SQL', 1450),
('Spark', 'Spark', 2400),
('Spark', 'Machine Learning', 5000) ;

1. Escribir un query que produzca el 33% de los cursos basandose en el precio más alto.

SELECT subject, course, price
  FROM
       (SELECT subject, course, price, NTILE (3) OVER (ORDER BY price DESC) ntile3
          FROM test.courses) sq
  WHERE ntile3=1
;
+---------+------------------+----------+
| subject | course           | price    |
+---------+------------------+----------+
| Spark   | Machine Learning | 5000.000 |
| Hadoop  | Hive             | 4000.000 |
| Hadoop  | Mahout           | 2600.000 |
+---------+------------------+----------+

Con vistas
----------
DROP VIEW IF EXISTS test.ntile33 ;
CREATE VIEW IF NOT EXISTS test.ntile33 AS
SELECT *, ntile(3) OVER( ORDER BY price DESC) ntile
FROM test.courses ;
SELECT subject, course, price 
FROM test.ntile33
WHERE ntile = 1 ;
DROP VIEW IF EXISTS test.ntile33 ;


2. Escribir un query que produzca el 33% de los cursos basandose en el precio más bajo.

SELECT subject, course, price
  FROM
       (SELECT subject, course, price, NTILE (3) OVER (ORDER BY price ) ntile3
          FROM test.courses) sq
  WHERE ntile3=1
;
+---------+-----------+----------+
| subject | course    | price    |
+---------+-----------+----------+
| Spark   | SQL       | 1450.000 |
| Hadoop  | OOzie     | 1500.000 |
| Hadoop  | Zookeeper | 1550.000 |
+---------+-----------+----------+

3. Escribir un query que produzca el 33% de los cursos basandose en el precio más bajo por materia (subject).

SELECT subject, course, price, ntile3
  FROM
       (SELECT subject, course, price, NTILE (3) OVER (PARTITION BY subject ORDER BY price ) ntile3
          FROM test.courses) sq
  WHERE ntile3=1
;
+---------+-----------+----------+--------+
| subject | course    | price    | ntile3 |
+---------+-----------+----------+--------+
| Hadoop  | OOzie     | 1500.000 | 1      |
| Hadoop  | Zookeeper | 1550.000 | 1      |
| Spark   | SQL       | 1450.000 | 1      |
+---------+-----------+----------+--------+

+----+
|29-.|
+----+

Se crea la tabla stock de la siguiente manera:
DROP TABLE IF EXISTS test.stock;
CREATE TABLE IF NOT EXISTS test.stock (
eexchange string,
ssymbol string,
ddate timestamp,
price decimal(8, 2)
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t' ;

INSERT INTO test.stock VALUES
('NSE','TCS','2009-08-09', 2200.1 ),
('NSE','TCS','2009-08-10', 2100.34 ),
('NSE','TCS','2009-08-11', 2110.1 ),
('NSE','TCS','2009-08-12', 2144.1 ),
('NSE','TCS','2009-08-13', 2232.1 ),
('NSE','TCS','2009-08-14', 2221.1 ),
('NSE','TCS','2009-08-15', 2212.1 ),
('NSE','TCS','2009-08-16', 2256.1 ),
('NSE','TCS','2009-08-17', 2267.1 ),
('NSE','TCS','2009-08-18', 2220.1 ),
('NSE','TCS','2009-08-19', 2220.1 ),
('NSE','TCS','2009-08-20', 2211.1 ),
('NSE','TCS','2009-08-21', 2200.1 ),
('NSE','INFY','2009-08-09', 1200.1 ),
('NSE','INFY','2009-08-10', 1100.34 ),
('NSE','INFY','2009-08-11', 1110.1 ),
('NSE','INFY','2009-08-12', 1144.1 ),
('NSE','INFY','2009-08-13', 1232.1 ),
('NSE','INFY','2009-08-14', 1221.1 ),
('NSE','INFY','2009-08-15', 1212.1 ),
('NSE','INFY','2009-08-16', 1256.1 ),
('NSE','INFY','2009-08-17', 1267.1 ),
('NSE','INFY','2009-08-18', 1220.1 ),
('NSE','INFY','2009-08-19', 1255.1 ),
('NSE','INFY','2009-08-20', 1211.1 ),
('NSE','INFY','2009-08-21', 1200.1 ) ;

1. Generar un informe con las siguientes columnas:

+---------+------------+---------+-----------+--------+
| ssymbol | sdate      | today   | yesterday | diff   |
+---------+------------+---------+-----------+--------+

today es el precio de hoy,
yesterday el precio de ayer, y 
diff la diferencia entre el precio de ayer y hoy

SELECT *, ( yesterday - today ) diff
  FROM 
       ( SELECT *, LAG (today, 1) OVER(PARTITION BY ssymbol ORDER BY sdate) yesterday
           FROM 
                ( SELECT ssymbol, to_date(ddate) sdate, price as today
                    FROM test.stock ) s1 ) s2
;

SELECT ssymbol `symbol`, to_date(ddate) `date`, price today,
       LAG (price) OVER(PARTITION BY ssymbol ORDER BY ddate) yesterday,
       LAG (price) OVER(PARTITION BY ssymbol ORDER BY ddate) - price diff
  FROM test.stock
;

+--------+------------+---------+-----------+--------+
| symbol | date       | today   | yesterday | diff   |
+--------+------------+---------+-----------+--------+
| INFY   | 2009-08-09 | 1200.10 | NULL      | NULL   |
| INFY   | 2009-08-10 | 1100.34 | 1200.10   | 99.76  |
| INFY   | 2009-08-11 | 1110.10 | 1100.34   | -9.76  |
| INFY   | 2009-08-12 | 1144.10 | 1110.10   | -34.00 |
| INFY   | 2009-08-13 | 1232.10 | 1144.10   | -88.00 |
| INFY   | 2009-08-14 | 1221.10 | 1232.10   | 11.00  |
| INFY   | 2009-08-15 | 1212.10 | 1221.10   | 9.00   |
| INFY   | 2009-08-16 | 1256.10 | 1212.10   | -44.00 |
| INFY   | 2009-08-17 | 1267.10 | 1256.10   | -11.00 |
| INFY   | 2009-08-18 | 1220.10 | 1267.10   | 47.00  |
| INFY   | 2009-08-19 | 1255.10 | 1220.10   | -35.00 |
| INFY   | 2009-08-20 | 1211.10 | 1255.10   | 44.00  |
| INFY   | 2009-08-21 | 1200.10 | 1211.10   | 11.00  |
| TCS    | 2009-08-09 | 2200.10 | NULL      | NULL   |
| TCS    | 2009-08-10 | 2100.34 | 2200.10   | 99.76  |
| TCS    | 2009-08-11 | 2110.10 | 2100.34   | -9.76  |
| TCS    | 2009-08-12 | 2144.10 | 2110.10   | -34.00 |
| TCS    | 2009-08-13 | 2232.10 | 2144.10   | -88.00 |
| TCS    | 2009-08-14 | 2221.10 | 2232.10   | 11.00  |
| TCS    | 2009-08-15 | 2212.10 | 2221.10   | 9.00   |
| TCS    | 2009-08-16 | 2256.10 | 2212.10   | -44.00 |
| TCS    | 2009-08-17 | 2267.10 | 2256.10   | -11.00 |
| TCS    | 2009-08-18 | 2220.10 | 2267.10   | 47.00  |
| TCS    | 2009-08-19 | 2220.10 | 2220.10   | 0.00   |
| TCS    | 2009-08-20 | 2211.10 | 2220.10   | 9.00   |
| TCS    | 2009-08-21 | 2200.10 | 2211.10   | 11.00  |
+--------+------------+---------+-----------+--------+

SELECT ssymbol `symbol`, to_date(ddate) `date`, price today,
       LEAD (price, 1) OVER(PARTITION BY ssymbol ORDER BY ddate) tomorrow,
       CASE
            WHEN ( price > LEAD (price, 1) OVER(PARTITION BY ssymbol ORDER BY ddate) ) THEN 'greater'
            WHEN ( price = LEAD (price, 1) OVER(PARTITION BY ssymbol ORDER BY ddate) ) THEN 'equal'
            WHEN ( price < LEAD (price, 1) OVER(PARTITION BY ssymbol ORDER BY ddate) ) THEN 'less'
            ELSE ('N/A')
       END comparison
FROM test.stock
;
+--------+------------+---------+----------+------------+
| symbol | date       | today   | tomorrow | comparison |
+--------+------------+---------+----------+------------+
| INFY   | 2009-08-09 | 1200.10 | 1100.34  | greater    |
| INFY   | 2009-08-10 | 1100.34 | 1110.10  | less       |
| INFY   | 2009-08-11 | 1110.10 | 1144.10  | less       |
| INFY   | 2009-08-12 | 1144.10 | 1232.10  | less       |
| INFY   | 2009-08-13 | 1232.10 | 1221.10  | greater    |
| INFY   | 2009-08-14 | 1221.10 | 1212.10  | greater    |
| INFY   | 2009-08-15 | 1212.10 | 1256.10  | less       |
| INFY   | 2009-08-16 | 1256.10 | 1267.10  | less       |
| INFY   | 2009-08-17 | 1267.10 | 1220.10  | greater    |
| INFY   | 2009-08-18 | 1220.10 | 1255.10  | less       |
| INFY   | 2009-08-19 | 1255.10 | 1211.10  | greater    |
| INFY   | 2009-08-20 | 1211.10 | 1200.10  | greater    |
| INFY   | 2009-08-21 | 1200.10 | NULL     | N/A        |
| TCS    | 2009-08-09 | 2200.10 | 2100.34  | greater    |
| TCS    | 2009-08-10 | 2100.34 | 2110.10  | less       |
| TCS    | 2009-08-11 | 2110.10 | 2144.10  | less       |
| TCS    | 2009-08-12 | 2144.10 | 2232.10  | less       |
| TCS    | 2009-08-13 | 2232.10 | 2221.10  | greater    |
| TCS    | 2009-08-14 | 2221.10 | 2212.10  | greater    |
| TCS    | 2009-08-15 | 2212.10 | 2256.10  | less       |
| TCS    | 2009-08-16 | 2256.10 | 2267.10  | less       |
| TCS    | 2009-08-17 | 2267.10 | 2220.10  | greater    |
| TCS    | 2009-08-18 | 2220.10 | 2220.10  | equal      |
| TCS    | 2009-08-19 | 2220.10 | 2211.10  | greater    |
| TCS    | 2009-08-20 | 2211.10 | 2200.10  | greater    |
| TCS    | 2009-08-21 | 2200.10 | NULL     | N/A        |
+--------+------------+---------+----------+------------+

+----+
|30-.|
+----+

Creamos la tabla product2 e insertamos datos en ella datos:
DROP TABLE IF EXISTS test.products2;
CREATE TABLE IF NOT EXISTS test.products2 (
prod_id int,
brand STRING,
name STRING,
price int
) ;
INSERT INTO test.products2 VALUES
(1,'Dualcore', 'mesa',500),
(2,'Dualcore', 'silla',600),
(3,'Dualcore', 'cama',700),
(4,'Dualcore', 'nevera',500),
(5,'Dualcore', 'cuadro',700),
(6,'Dualcore', 'manta',700),
(7,'Gigabux', 'almohada',400),
(8,'Gigabux', 'alfombra',600),
(9,'Gigabux', 'CD',600),
(10,'Gigabux', 'mesilla',1600),
(11,'Gigabux', 'radio',400) ;

1. Ordenar la filas por marca (brand) y precio (price) y mostrar el rango y el número de fila.

SELECT prod_id, brand, name, price,
       RANK() OVER(PARTITION BY brand ORDER BY price) rank,
       ROW_NUMBER() OVER(PARTITION BY brand ORDER BY price) n
  FROM
test.products2
;
+---------+----------+----------+-------+------+---+
| prod_id | brand    | name     | price | rank | n |
+---------+----------+----------+-------+------+---+
| 1       | Dualcore | mesa     | 500   | 1    | 1 |
| 4       | Dualcore | nevera   | 500   | 1    | 2 |
| 2       | Dualcore | silla    | 600   | 3    | 3 |
| 3       | Dualcore | cama     | 700   | 4    | 4 |
| 5       | Dualcore | cuadro   | 700   | 4    | 5 |
| 6       | Dualcore | manta    | 700   | 4    | 6 |
| 7       | Gigabux  | almohada | 400   | 1    | 1 |
| 11      | Gigabux  | radio    | 400   | 1    | 2 |
| 8       | Gigabux  | alfombra | 600   | 3    | 3 |
| 9       | Gigabux  | CD       | 600   | 3    | 4 |
| 10      | Gigabux  | mesilla  | 1600  | 5    | 5 |
+---------+----------+----------+-------+------+---+

-- Para asegurar el orden de la salida - Recordad que en SQL no hay orden de filas, 
SELECT * FROM
              ( SELECT prod_id, brand, name, price,
                       RANK() OVER(PARTITION BY brand ORDER BY price) rank,
                       ROW_NUMBER() OVER(PARTITION BY brand ORDER BY price) n
                  FROM test.products2) sq
ORDER BY brand, price
Query submitted at: 2021-04-09 03:03:24 (Coordinator: http://quickstart.cloudera:25000)
Query progress can be monitored at: http://quickstart.cloudera:25000/query_plan?query_id=ea443f0352d09024:e059ed9000000000
+---------+----------+----------+-------+------+---+
| prod_id | brand    | name     | price | rank | n |
+---------+----------+----------+-------+------+---+
| 1       | Dualcore | mesa     | 500   | 1    | 1 |
| 4       | Dualcore | nevera   | 500   | 1    | 2 |
| 2       | Dualcore | silla    | 600   | 3    | 3 |
| 3       | Dualcore | cama     | 700   | 4    | 4 |
| 5       | Dualcore | cuadro   | 700   | 4    | 5 |
| 6       | Dualcore | manta    | 700   | 4    | 6 |
| 7       | Gigabux  | almohada | 400   | 1    | 1 |
| 11      | Gigabux  | radio    | 400   | 1    | 2 |
| 8       | Gigabux  | alfombra | 600   | 3    | 3 |
| 9       | Gigabux  | CD       | 600   | 3    | 4 |
| 10      | Gigabux  | mesilla  | 1600  | 5    | 5 |
+---------+----------+----------+-------+------+---+

2. Mostrar todos los productos que tienen un precio repetido; o lo que es lo mismo, cuyo precio sea igual al precio del anterior o del siguiente.

SELECT prod_id, brand, name, price
       FROM (SELECT prod_id, brand, name, price,
                    LAG(price, 1) OVER(PARTITION BY brand ORDER BY price) AS lag,
                    LEAD(price, 1) OVER(PARTITION BY brand ORDER BY price) AS lead
               FROM test.products2 ) sq
WHERE price = lag OR price = lead
;
+---------+----------+----------+-------+
| prod_id | brand    | name     | price |
+---------+----------+----------+-------+
| 1       | Dualcore | mesa     | 500   |
| 4       | Dualcore | nevera   | 500   |
| 3       | Dualcore | cama     | 700   |
| 5       | Dualcore | cuadro   | 700   |
| 6       | Dualcore | manta    | 700   |
| 7       | Gigabux  | almohada | 400   |
| 11      | Gigabux  | radio    | 400   |
| 8       | Gigabux  | alfombra | 600   |
| 9       | Gigabux  | CD       | 600   |
+---------+----------+----------+-------+

3. Mostrar los productos que tienen el precio más caro dentro de cada marca.

SELECT *
  FROM ( SELECT prod_id, brand, name, price, 
         RANK() OVER(PARTITION BY brand ORDER BY price DESC) rank,
         ROW_NUMBER() OVER(PARTITION BY brand ORDER BY price DESC) n
        FROM test.products2 ) s1
WHERE rank = 1
;
+---------+----------+---------+-------+------+---+
| prod_id | brand    | name    | price | rank | n |
+---------+----------+---------+-------+------+---+
| 3       | Dualcore | cama    | 700   | 1    | 1 |
| 5       | Dualcore | cuadro  | 700   | 1    | 2 |
| 6       | Dualcore | manta   | 700   | 1    | 3 |
| 10      | Gigabux  | mesilla | 1600  | 1    | 1 |
+---------+----------+---------+-------+------+---+

4. Dentro de cada marca, mostrar los productos que están entre los 2 más baratos.

SELECT prod_id, brand, name, price
       FROM (SELECT prod_id, brand, name, price,
               RANK() OVER(PARTITION BY brand ORDER BY price) rnk
               FROM test.products2 ) sq
WHERE rnk <= 3;
;

+---------+----------+----------+-------+
| prod_id | brand    | name     | price |
+---------+----------+----------+-------+
| 1       | Dualcore | mesa     | 500   |
| 4       | Dualcore | nevera   | 500   |
| 2       | Dualcore | silla    | 600   |
| 7       | Gigabux  | almohada | 400   |
| 11      | Gigabux  | radio    | 400   |
| 8       | Gigabux  | alfombra | 600   |
| 9       | Gigabux  | CD       | 600   |
+---------+----------+----------+-------+


5. Por cada producto, mostrar los siguientes precios 
a) el del producto con el prod_id anterior, 
b) el del propio producto, y 
c) el del producto con el prod_id siguiente.

SELECT prod_id, brand, name, previous, next
  FROM ( SELECT prod_id, brand, name,
                LAG (price, 1) OVER (ORDER BY prod_id) previous,
                LEAD (price, 1) OVER (ORDER BY prod_id) next
         FROM test.products2 ) sq
;

+---------+----------+----------+----------+------+
| prod_id | brand    | name     | previous | next |
+---------+----------+----------+----------+------+
| 1       | Dualcore | mesa     | NULL     | 600  |
| 2       | Dualcore | silla    | 500      | 700  |
| 3       | Dualcore | cama     | 600      | 500  |
| 4       | Dualcore | nevera   | 700      | 700  |
| 5       | Dualcore | cuadro   | 500      | 700  |
| 6       | Dualcore | manta    | 700      | 400  |
| 7       | Gigabux  | almohada | 700      | 600  |
| 8       | Gigabux  | alfombra | 400      | 600  |
| 9       | Gigabux  | CD       | 600      | 1600 |
| 10      | Gigabux  | mesilla  | 600      | 400  |
| 11      | Gigabux  | radio    | 1600     | NULL |
+---------+----------+----------+----------+------+

6. Por cada marca y producto, mostrar los siguientes precios 
a) el del producto con el prod_id anterior, 
b) el del propio producto, y 
c) el del producto con el prod_id siguiente.

SELECT prod_id, brand, name, previous, next
  FROM ( SELECT prod_id, brand, name,
                LAG (price, 1) OVER (PARTITION BY brand ORDER BY prod_id) previous,
                LEAD (price, 1) OVER (PARTITION BY brand ORDER BY prod_id) next
         FROM test.products2 ) sq
;

+---------+----------+----------+----------+------+
| prod_id | brand    | name     | previous | next |
+---------+----------+----------+----------+------+
| 1       | Dualcore | mesa     | NULL     | 600  |
| 2       | Dualcore | silla    | 500      | 700  |
| 3       | Dualcore | cama     | 600      | 500  |
| 4       | Dualcore | nevera   | 700      | 700  |
| 5       | Dualcore | cuadro   | 500      | 700  |
| 6       | Dualcore | manta    | 700      | NULL |
| 7       | Gigabux  | almohada | NULL     | 600  |
| 8       | Gigabux  | alfombra | 400      | 600  |
| 9       | Gigabux  | CD       | 600      | 1600 |
| 10      | Gigabux  | mesilla  | 600      | 400  |
| 11      | Gigabux  | radio    | 1600     | NULL |
+---------+----------+----------+----------+------+

+----+
|40-.|
+----+

-- Hacer con Impala.
Nos dan el siguiente fichero csv:
name,salary,sex,age
juan,10000,male,40
vega,15000,female,40
carmen,19000,female,35
marta,20000,female,35
pedro,10400,male,40
alfonso,30000,male,35
noelia,10555,female,35

1. Pasar este fichero a hdfs incluyendo la primera línea.

hdfs dfs -mkdir /user/cloudera/people
hdfs dfs -put people.csv /user/cloudera/people

2. Crear una tabla externa, people, de tipo texto asociada a este fichero.
USE test;
DROP TABLE IF EXISTS test.people;
CREATE EXTERNAL TABLE IF NOT EXISTS test.people(
       name STRING,
       salary INT,
       sex STRING,
       age TINYINT
  )
  ROW FORMAT DELIMITED
      FIELDS TERMINATED BY ','
  LOCATION '/user/cloudera/people'
  TBLPROPERTIES ('skip.header.line.count' = '1')
;

Se pueden modificar las propiedades de la tabla:
ALTER TABLE test.people SET tblproperties('skip.header.line.count'='1');

3. Crear una tabla de tipo parquet, people_parquet, con los mismos datos que people.
DROP TABLE IF EXISTS test.people_parquet;
CREATE EXTERNAL TABLE IF NOT EXISTS test.people_parquet
  STORED AS PARQUET
AS
  SELECT * FROM test.people
;

4. Crear una tabla de tipo parquet, people_parquet2, 
basándose en un fichero de datos de la tabla people_parquet y que esté particionada por una columna year de tipo int.
DROP TABLE IF EXISTS test.people_parquet2;
CREATE EXTERNAL TABLE IF NOT EXISTS test.people_parquet2
  LIKE PARQUET '/user/hive/warehouse/test.db/people_parquet/f44a549099bb182d-4bef584b00000000_1113473584_data.0.parq'
  PARTITIONED BY (year INT)
  STORED AS PARQUET
;

5. Meter todos los datos de people_parquet en la partición year=2019 de la tabla people_parquet.

Una forma de hacerlo:

ALTER TABLE test.people_parquet2 ADD partition (year=2021);
LOAD DATA INPATH '/user/hive/warehouse/test.db/people_parquet/'
     INTO TABLE test.people_parquet2
     PARTITION (year=2021);
+----------------------------------------------------------+
| summary                                                  |
+----------------------------------------------------------+
| Loaded 1 file(s). Total files in destination location: 1 |
+----------------------------------------------------------+

- Otra forma de hacerlo:

-- Estoy poniendo un año concreto, es decir, voy a tomar todos los datos de la tabla people_parquet y los voy a meter en una partición especifica year=2019
INSERT INTO test.people_parquet2
     PARTITION (year=2019)
SELECT * FROM test.people_parquet;

-- Esto es lo mismo que lo anterior, sólo que especificando por campo, esta table debe recibir campos con los tipos que ha sido creada en este caso: STRING, INT, STRING, TINYINT sea de donde sea, pero en este orden
INSERT INTO test.people_parquet2
     PARTITION (year=2019)
SELECT name, salary, sex, age
FROM test.people_parquet;

-- Esto es lo mismo que lo anterior, sólo que especificando por campo, esta table debe recibir campos con los tipos que ha sido creada en este caso: STRING, INT, STRING, TINYINT pero además debo pasarle los datos de la partición en el formato adecuado
INSERT INTO test.people_parquet2
     PARTITION (year)
SELECT name, salary, sex, age, 2020
FROM test.people_parquet;

SELECT name, salary, sex, age 
  FROM test.people_parquet;
+---------+--------+--------+-----+
| name    | salary | sex    | age |
+---------+--------+--------+-----+
| juan    | 10000  | male   | 40  |
| vega    | 15000  | female | 40  |
| carmen  | 19000  | female | 35  |
| marta   | 20000  | female | 35  |
| pedro   | 10400  | male   | 40  |
| alfonso | 30000  | male   | 35  |
| noelia  | 10555  | female | 35  |
+---------+--------+--------+-----+
SELECT name, salary, sex, age, year 
  FROM test.people_parquet2;
+---------+--------+--------+-----+------+
| name    | salary | sex    | age | year |
+---------+--------+--------+-----+------+
| juan    | 10000  | male   | 40  | 2019 |
| vega    | 15000  | female | 40  | 2019 |
| carmen  | 19000  | female | 35  | 2019 |
| marta   | 20000  | female | 35  | 2019 |
| pedro   | 10400  | male   | 40  | 2019 |
| alfonso | 30000  | male   | 35  | 2019 |
| noelia  | 10555  | female | 35  | 2019 |
+---------+--------+--------+-----+------+
SELECT name, salary, sex, age 
  FROM test.people_parquet2;
+---------+--------+--------+-----+
| name    | salary | sex    | age |
+---------+--------+--------+-----+
| juan    | 10000  | male   | 40  |
| vega    | 15000  | female | 40  |
| carmen  | 19000  | female | 35  |
| marta   | 20000  | female | 35  |
| pedro   | 10400  | male   | 40  |
| alfonso | 30000  | male   | 35  |
| noelia  | 10555  | female | 35  |
+---------+--------+--------+-----+
SELECT name, salary, sex, age 
  FROM test.people_parquet2 
WHERE year = 2019;
+---------+--------+--------+-----+
| name    | salary | sex    | age |
+---------+--------+--------+-----+
| juan    | 10000  | male   | 40  |
| vega    | 15000  | female | 40  |
| carmen  | 19000  | female | 35  |
| marta   | 20000  | female | 35  |
| pedro   | 10400  | male   | 40  |
| alfonso | 30000  | male   | 35  |
| noelia  | 10555  | female | 35  |
+---------+--------+--------+-----+

INSERT INTO test.people_parquet2
     PARTITION (year)
SELECT name, salary, sex, age, 2020
  FROM test.people_parquet;

hdfs dfs -ls /user/hive/warehouse/test.db/people_parquet2/
drwxrwxrwx   - impala supergroup          0 2021-04-10 05:16 /user/hive/warehouse/test.db/people_parquet2/_impala_insert_staging
drwxr-xr-x   - impala supergroup          0 2021-04-10 04:59 /user/hive/warehouse/test.db/people_parquet2/year=2019
drwxr-xr-x   - impala supergroup          0 2021-04-10 05:16 /user/hive/warehouse/test.db/people_parquet2/year=2020

SELECT name, salary, sex, age
  FROM test.people_parquet2 
 WHERE year=2020;
+---------+--------+--------+-----+
| name    | salary | sex    | age |
+---------+--------+--------+-----+
| juan    | 10000  | male   | 40  |
| vega    | 15000  | female | 40  |
| carmen  | 19000  | female | 35  |
| marta   | 20000  | female | 35  |
| pedro   | 10400  | male   | 40  |
| alfonso | 30000  | male   | 35  |
| noelia  | 10555  | female | 35  |
+---------+--------+--------+-----+

SELECT name, salary, sex, age
  FROM test.people_parquet2 
WHERE year=2019;
+---------+--------+--------+-----+
| name    | salary | sex    | age |
+---------+--------+--------+-----+
| juan    | 10000  | male   | 40  |
| vega    | 15000  | female | 40  |
| carmen  | 19000  | female | 35  |
| marta   | 20000  | female | 35  |
| pedro   | 10400  | male   | 40  |
| alfonso | 30000  | male   | 35  |
| noelia  | 10555  | female | 35  |
+---------+--------+--------+-----+
SELECT name, salary, sex, age, year
  FROM test.people_parquet2 
 WHERE year=2019;
+---------+--------+--------+-----+------+
| name    | salary | sex    | age | year |
+---------+--------+--------+-----+------+
| juan    | 10000  | male   | 40  | 2019 |
| vega    | 15000  | female | 40  | 2019 |
| carmen  | 19000  | female | 35  | 2019 |
| marta   | 20000  | female | 35  | 2019 |
| pedro   | 10400  | male   | 40  | 2019 |
| alfonso | 30000  | male   | 35  | 2019 |
| noelia  | 10555  | female | 35  | 2019 |
+---------+--------+--------+-----+------+
SELECT name, salary, sex, age, year
  FROM test.people_parquet2;
+---------+--------+--------+-----+------+
| name    | salary | sex    | age | year |
+---------+--------+--------+-----+------+
| juan    | 10000  | male   | 40  | 2019 |
| vega    | 15000  | female | 40  | 2019 |
| carmen  | 19000  | female | 35  | 2019 |
| marta   | 20000  | female | 35  | 2019 |
| pedro   | 10400  | male   | 40  | 2019 |
| alfonso | 30000  | male   | 35  | 2019 |
| noelia  | 10555  | female | 35  | 2019 |
| juan    | 10000  | male   | 40  | 2020 |
| vega    | 15000  | female | 40  | 2020 |
| carmen  | 19000  | female | 35  | 2020 |
| marta   | 20000  | female | 35  | 2020 |
| pedro   | 10400  | male   | 40  | 2020 |
| alfonso | 30000  | male   | 35  | 2020 |
| noelia  | 10555  | female | 35  | 2020 |
+---------+--------+--------+-----+------+

+-----------------------+
|Ejercicios de Examen -.|
+-----------------------+
mysql -u root -p
Enter password:cloudera

beeline -u jdbc:hive2://localhost:10000 -n root -p cloudera --silent=true 
beeline -u jdbc:hive2://localhost:10000 -n root -p cloudera --silent=true -o fileOut
beeline -u jdbc:hive2://localhost:10000 -n root -p cloudera --silent=true -f sql_hive.hql

impala-shell 
impala-shell -o fileOut
impala-shell -f sql_impala.hql

ejecutar este fichero en mysql: create_table_empleados.sql
----------------------------------------------------------
DROP TABLE IF EXISTS examen.empleados ;
CREATE TABLE IF NOT EXISTS examen.empleados(
       id INT PRIMARY KEY,
       name VARCHAR(10),
       position VARCHAR(12),
       salary INT,
       dept VARCHAR(30)
) ;
INSERT INTO examen.empleados VALUES
(201, 'Pedro', 'supervisor', 45000, 'investigacion y desarrollo'),
(202, 'Luis', 'gerente', 40000, 'recursos humanos'),
(203, 'Azucena', 'programadora', 30000, 'marketing'),
(204, 'Luisa', 'directiva', 60000, 'ventas'),
(205, 'Paloma', 'supervisor', 50000, 'contabilidad')
;

ejecutar este fichero en hive: create_table_clientes.sql
--------------------------------------------------------
CREATE DATABASE IF NOT EXISTS examen;
USE examen;
DROP TABLE IF EXISTS examen.clientes ;
CREATE TABLE IF NOT EXISTS examen.clientes (
  nombre STRING,
  fecha STRING
) ;
INSERT INTO examen.clientes VALUES
  ('pepe', '05/01/95'),
  ('pepa', '05-01-95'),
  ('juana', '10/28/93'),
  ('juan', '12/31/94'),
  ('patricia', '01-28-99'),
  ('patricio', '09/28/90') ;

ejecutar este fichero en hive: create_table_courses.sql
-------------------------------------------------------
USE EXAMEN ;
DROP TABLE IF EXISTS examen.courses ;
CREATE TABLE IF NOT EXISTS examen.courses (
  subject STRING,
  course STRING,
  price DECIMAL(9, 3)
) ;
INSERT INTO examen.courses VALUES
  ('Hadoop', 'Hive', 4000), ('Hadoop', 'Impala', 2400),
  ('Hadoop', 'Pig', 2400), ('Hadoop', 'OOzie', 1500),
  ('Hadoop', 'Zookeeper', 1550),
  ('Hadoop', 'Mahout', 2600), ('Spark', 'SQL', 1450),
  ('Spark', 'Spark', 2400),
  ('Spark', 'Machine Learning', 5000) ;

SELECT * FROM examen.courses ;

ejecutar este fichero en hive: create_table_family_head.sql
-----------------------------------------------------------
USE examen ;
DROP TABLE IF EXISTS examen.family_head ;
CREATE TABLE IF NOT EXISTS examen.family_head (
    name string,
    places ARRAY<string>,
    sex_age STRUCT<sex:string,age:int>,
    children MAP<string,int>
)
ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '|'
    collection items terminated by ':'
    map keys terminated by ',' ;

datos para esta tabla: puede ser ejecutando este fichero: sh passFamily.sh
--------------------------------------------------------------------------
hdfs dfs -put family.txt /user/hive/warehouse/examen.db/family_head

ó directamente escribiendo desde la línea de comandos: hdfs dfs -put family.txt /user/hive/warehouse/examen.db/family_head

family.txt
----------
Roberta|Barcelona:Londres:Roma|female:52|Andrea,2:Vega,5:Hugo,7
Pedro|Budapest:Delhi|male:36|Bernardo,2

+------+
|Exe 5.|
+------+

1
Insertar los datos de la tabla en un directorio hdfs con las siguientes condiciones.
a. Sólo se deben insertar las filas que tengan supervisor como valor de la columna position.  
a. Los ficheros tienen que situarse debajo del directorio '/user/cloudera/empleados'
b. Los campos en los ficheros tienen que estar separados por el carácter ':'
c. Hay que considerar la posibilidad de que ya exista el directorio '/user/cloudera/empleados'. 
   En este caso, hay que borrar su contenido antes de hacer la importación.

sqoop-import \
--connect jdbc:mysql://quickstart:3306/examen \
--username root --password cloudera \
--table empleados \
--where "position = 'supervisor'" \
--outdir /tmp/ \
--delete-target-dir \
--fields-terminated-by ':'

-- sqoop crea el fichero empleados.java
outdir /tmp/ 
[cloudera@quickstart ~]$ ls -ltsha /tmp/emp*
20K -rw-rw-r-- 1 cloudera cloudera 17K Apr 10 07:10 /tmp/empleados.java

-- si no le damos el target-dir sqoop crea el fichero por defecto en el home del usuario hdfs, es decir: /user/cloudera/

hdfs dfs -ls /user/cloudera/empleados
-rw-r--r--   1 cloudera cloudera          0 2021-04-10 07:25 /user/cloudera/empleados/_SUCCESS
-rw-r--r--   1 cloudera cloudera         54 2021-04-10 07:25 /user/cloudera/empleados/part-m-00000
-rw-r--r--   1 cloudera cloudera          0 2021-04-10 07:25 /user/cloudera/empleados/part-m-00001
-rw-r--r--   1 cloudera cloudera          0 2021-04-10 07:25 /user/cloudera/empleados/part-m-00002
-rw-r--r--   1 cloudera cloudera          0 2021-04-10 07:25 /user/cloudera/empleados/part-m-00003
-rw-r--r--   1 cloudera cloudera         41 2021-04-10 07:25 /user/cloudera/empleados/part-m-00004

2.
Una vez insertados los registros correspondientes, hay que añadir dos filas más a la tabla de MariaDB mediante:
INSERT INTO examen.empleados VALUES
(206, 'Vega', 'supervisor', 50000, 'contabilidad'),
(207, 'Juan', 'gerente', 45000, 'investigacion y desarrollo') ;

A continuación, hacer un import incremental de tal manera que se importen todas las filas nuevas de la base de datos desde la última importación pero que cumplan además con la condición de que tengan en la columna position un valor de supervisor. 
Resumiendo, las condiciones del comando sqoop que hay que utilizar son:

a. Hay que usar una importación incremental.
d. Aparte de esto, al igual que en el punto 1, sólo se deben insertar las filas que tengan supervisor como valor de la columna position.

sqoop-import \
--connect jdbc:mysql://quickstart:3306/examen \
--username root --password cloudera \
--table empleados \
--check-column id \
--incremental append \
--last-value 205 \
--outdir /tmp/ \
--where "position = 'supervisor'" \
--fields-terminated-by ':'

-- check-column indica a sqoop cual es la columna que debe revisar para hacer la importación incremental dandole el valor del último registro importado
check-column id 
incremental append \
last-value 205 \

+------+
|Exe 6.|
+------+
Tenemos la siguiente tabla:

DROP TABLE examen.courses;
CREATE TABLE examen.courses(
       subject STRING,
       course STRING,
       price DECIMAL(9,3)
)

INSERT INTO examen.courses VALUES
  ('Hadoop', 'Hive', 4000), ('Hadoop', 'Impala', 2400),
  ('Hadoop', 'Pig', 2400), ('Hadoop', 'OOzie', 1500),
  ('Hadoop', 'Zookeeper', 1550),
  ('Hadoop', 'Mahout', 2600), ('Spark', 'SQL', 1450),
  ('Spark', 'Spark', 2400),
  ('Spark', 'Machine Learning', 5000) ;

Query: describe courses
+---------+--------------+---------+
| name    | type         | comment |
+---------+--------------+---------+
| subject | string       |         |
| course  | string       |         |
| price   | decimal(9,3) |         |
+---------+--------------+---------+
SELECT * FROM examen.courses;
+---------+------------------+----------+
| subject | course           | price    |
+---------+------------------+----------+
| Hadoop  | Hive             | 4000.000 |
| Hadoop  | Impala           | 2400.000 |
| Hadoop  | Pig              | 2400.000 |
| Hadoop  | OOzie            | 1500.000 |
| Hadoop  | Zookeeper        | 1550.000 |
| Hadoop  | Mahout           | 2600.000 |
| Spark   | SQL              | 1450.000 |
| Spark   | Spark            | 2400.000 |
| Spark   | Machine Learning | 5000.000 |
+---------+------------------+----------

1. Calcular la acumulación distributiva, CUME_DIST, del precio (price) entre todas las materias (subject) y cursos (course). 
La salida tiene que estar ordenada de mayor a menor por cume_dist().

SELECT *, CUME_DIST () OVER (ORDER BY price) cume_dist
  FROM examen.courses 
ORDER BY cume_dist DESC;
+---------+------------------+----------+--------------------+
| subject | course           | price    | cume_dist          |
+---------+------------------+----------+--------------------+
| Spark   | Machine Learning | 5000.000 | 1                  |
| Hadoop  | Hive             | 4000.000 | 0.8888888888888888 |
| Hadoop  | Mahout           | 2600.000 | 0.7777777777777778 |
| Hadoop  | Impala           | 2400.000 | 0.6666666666666666 |
| Hadoop  | Pig              | 2400.000 | 0.6666666666666666 |
| Spark   | Spark            | 2400.000 | 0.6666666666666666 |
| Hadoop  | Zookeeper        | 1550.000 | 0.3333333333333333 |
| Hadoop  | OOzie            | 1500.000 | 0.2222222222222222 |
| Spark   | SQL              | 1450.000 | 0.1111111111111111 |
+---------+------------------+----------+--------------------+


2. Calcular la acumulación distributiva, CUME_DIST, del precio (price) por materia (subject). 
La salida tiene que estar ordenada por subject y de mayor a menor por cume_dist().

SELECT *, CUME_DIST () OVER (PARTITION BY subject ORDER BY price) cume_dist
  FROM examen.courses 
;
+---------+------------------+----------+--------------------+
| subject | course           | price    | cume_dist          |
+---------+------------------+----------+--------------------+
| Hadoop  | Hive             | 4000.000 | 1                  |
| Hadoop  | Mahout           | 2600.000 | 0.8333333333333334 |
| Hadoop  | Impala           | 2400.000 | 0.6666666666666666 |
| Hadoop  | Pig              | 2400.000 | 0.6666666666666666 |
| Hadoop  | Zookeeper        | 1550.000 | 0.3333333333333333 |
| Hadoop  | OOzie            | 1500.000 | 0.1666666666666667 |
| Spark   | Machine Learning | 5000.000 | 1                  |
| Spark   | Spark            | 2400.000 | 0.6666666666666666 |
| Spark   | SQL              | 1450.000 | 0.3333333333333333 |
+---------+------------------+----------+--------------------+

3. Calcular la acumulación distributiva, CUME_DIST, del precio (price) por materia (subject) en order decreciente. 
La salida tiene que estar ordenada por subject y de mayor a menor por cume_dist().

SELECT *, CUME_DIST () OVER (PARTITION BY subject ORDER BY price DESC) cume_dist
  FROM examen.courses 
;
+---------+------------------+----------+--------------------+
| subject | course           | price    | cume_dist          |
+---------+------------------+----------+--------------------+
| Hadoop  | OOzie            | 1500.000 | 1                  |
| Hadoop  | Zookeeper        | 1550.000 | 0.8333333333333334 |
| Hadoop  | Impala           | 2400.000 | 0.6666666666666666 |
| Hadoop  | Pig              | 2400.000 | 0.6666666666666666 |
| Hadoop  | Mahout           | 2600.000 | 0.3333333333333333 |
| Hadoop  | Hive             | 4000.000 | 0.1666666666666667 |
| Spark   | SQL              | 1450.000 | 1                  |
| Spark   | Spark            | 2400.000 | 0.6666666666666666 |
| Spark   | Machine Learning | 5000.000 | 0.3333333333333333 |
+---------+------------------+----------+--------------------+

+------+
|Exe 7.|
+------+

Hacer en En Impala:

Creamos una tabla de texto en hive de la siguiente manera: 
DROP TABLE if exists examen.family_head (
CREATE TABLE if not exists examen.family_head (
       name string,
       places ARRAY<string>,
       sex_age STRUCT<sex:string,age:int>,
       children MAP<string,int>
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
collection items terminated by ':'
map keys terminated by ',' ;

Tenemos el siguiente fichero de texto, family.txt:
Roberta|Barcelona:Londres:Roma|female:52|Andrea,2:Vega,5:Hugo,7
Pedro|Budapest:Delhi|male:36|Bernardo,2

Pasamos este fichero de texto a /user/hive/warehouse/examen.db/family_head: 
hdfs dfs -put family.txt /user/hive/warehouse/examen.db/family_head

1
Usando hive, crear una tabla parquet, family_head_parquet, a partir de la tabla family_head creada anteriormente. 
En la misma sentencia de creación de la tabla, hay que meter los datos.

CREATE TABLE examen.family_head_parquet
STORED as PARQUET
AS
SELECT * 
FROM examen.family_head ;

2
Imprimir los nombres de las personas junto con sus hijos, ordenando por name, child, y age
SELECT name, key child, value age
  FROM examen.family_head_parquet, examen.family_head_parquet.children ;

+---------+----------+-----+
| name    | child    | age |
+---------+----------+-----+
| Roberta | Andrea   | 2   |
| Roberta | Vega     | 5   |
| Roberta | Hugo     | 7   |
| Pedro   | Bernardo | 2   |
+---------+----------+-----+

3
Imprimir los nombres de las personas junto con las ciudades en las que trabaja. Hay que ordenar por name y city.
SELECT name, item
  FROM examen.family_head_parquet , examen.family_head_parquet.places 
;

+---------+-----------+
| name    | item      |
+---------+-----------+
| Roberta | Barcelona |
| Roberta | Londres   |
| Roberta | Roma      |
| Pedro   | Budapest  |
| Pedro   | Delhi     |
+---------+-----------+

4. Imprimir los nombres de las personas junto con su sexo y edad
SELECT name, sex_age.sex sex, sex_age.age
  FROM examen.family_head_parquet , examen.family_head_parquet.places 
;
+---------+--------+-------------+
| name    | sex    | sex_age.age |
+---------+--------+-------------+
| Roberta | female | 52          |
| Roberta | female | 52          |
| Roberta | female | 52          |
| Pedro   | male   | 36          |
| Pedro   | male   | 36          |
+---------+--------+-------------+

+------+
|Exe 8.|
+------+
Tenemos el fichero: provincias.tsv:

01003	Alava	Vitoria
02004	Albacete	Albacete
03001	Alicante	Alicante
03010	Alicante	Benidorm
06009	Badajoz	Badajoz
06006	Badajoz	Almendralejo
08003	Barcelona	Barcelona
08233	Barcelona	Sabadell
08234	Barcelona	Badalona
28001	Madrid	Madrid
28240	Madrid	Majadahona
28281	Madrid	Alcobendas

Los campos dentro de provincias.tsv están separados por tabulaciones. 
Se supone que el fichero está en /user/training/provincias.tsv.

hdfs dfs -put provincias.tsv /user/cloudera/
hdfs dfs -ls /user/cloudera/provincias*
-rw-r--r--   1 cloudera cloudera        285 2021-04-10 08:24 /user/cloudera/provincias.tsv

1
Crear un tabla, provincias, con estas condiciones:
a. Borrar la tabla si ya existe.
e. Tiene que tener tres campos, todos de tipo STRING: codigo_postal, provincia, población
b. Su formato tiene que ser texto
f. Su LOCATION /provincias
g. Los separadores adecuados al contenido del fichero provincias.tsv
h. Una vez creada la tabla, pasar el fichero provincias.tsv a la ubicación correspondiente usando el comando LOAD ...

DROP TABLE IF EXISTS examen.provincias;
CREATE TABLE IF NOT EXISTS examen.provincias(
       codigo_postal STRING,
       provincia STRING,
       poblacion STRING
)
   ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
   LOCATION '/provincias'
;

0: jdbc:hive2://localhost:10000> 
LOAD DATA INPATH '/user/cloudera/provincias.tsv' 
INTO TABLE examen.provincias
;

SELECT * FROM provincias;
+---------------------------+-----------------------+-----------------------+--+
| provincias.codigo_postal  | provincias.provincia  | provincias.poblacion  |
+---------------------------+-----------------------+-----------------------+--+
| 01003                     | Alava                 | Vitoria               |
| 02004                     | Albacete              | Albacete              |
| 03001                     | Alicante              | Alicante              |
| 03010                     | Alicante              | Benidorm              |
| 06009                     | Badajoz               | Badajoz               |
| 06006                     | Badajoz               | Almendralejo          |
| 08003                     | Barcelona             | Barcelona             |
| 08233                     | Barcelona             | Sabadell              |
| 08234                     | Barcelona             | Badalona              |
| 28001                     | Madrid                | Madrid                |
| 28240                     | Madrid                | Majadahona            |
| 28281                     | Madrid                | Alcobendas            |
+---------------------------+-----------------------+-----------------------+--+

2. Crear una tabla, provincias_partitioned, que tenga los campos código_postal, población y provincia. 
Tiene que estar particionada por provincia. 
El separador de los campos tiene que ser una tabulación.
Insertar mediante INSERT OVERWRITE TABLE las filas de provincias en provincias_partitioned

DROP TABLE IF EXISTS examen.provincias_partitioned;
CREATE TABLE IF NOT EXISTS examen.provincias_partitioned(
       codigo_postal STRING,
       poblacion STRING
)
   PARTITIONED BY (provincia STRING)
   ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
   LOCATION '/provincias'
;

0: jdbc:hive2://localhost:10000> 
set hive.exec.dynamic.partition.mode=nonstric
INSERT OVERWRITE TABLE examen.provincias_partitioned
       PARTITION(provincia)
       SELECT codigo_postal, poblacion, provincia
         FROM examen.provincias
;

SELECT * 
FROM examen.provincias_partitioned
;

---/borrar/---estas eran pruebas del ejemplo 8 pero ya no las revise.. hay algo que no funciona en este ejemplo

hdfs dfs -put customers.txt /user/cloudera/examen/custom/customers.txt
hdfs dfs -ls /user/cloudera/examen/custom/customers.txt
-rw-r--r--   1 cloudera cloudera   12577346 2021-04-10 12:46 /user/cloudera/examen/custom/customers.txt

DROP TABLE IF EXISTS examen.custom;
CREATE TABLE IF NOT EXISTS examen.custom
        (cust_id  int,
         fname    STRING,
         lname    STRING,
         address  STRING,
         city     STRING,
         state    STRING,
         zipcode  STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/user/cloudera/examen/custom'
;
DROP TABLE IF EXISTS examen.custom_p;
CREATE TABLE examen.custom_p
AS
SELECT * FROM examen.custom
;
set hive.exec.dynamic.partition.mode=nonstric
ALTER TABLE examen.custom_p ADD PARTITION (state='NY');
INSERT OVERWRITE TABLE examen.custom_p
PARTITION(state)
SELECT cust_id, fname, lname, address, city, zipcode, state
FROM examen.custom
;
---/borrar/---

+------+
|Exe 9.|
+------+

Tenemos la tabla clientes creada de esta manera:
USE examen;
DROP TABLE IF EXISTS examen.clientes;
CREATE TABLE IF NOT EXISTS examen.clientes (
       nombre STRING,
       fecha STRING
) ;
INSERT INTO examen.clientes VALUES
('pepe', '05/01/95'),
('pepa', '05-01-95'),
('juana', '10/28/93'),
('juan', '12/31/94'),
('patricia', '01-28-99'),
('patricio', '09/28/90') ;

En donde la fecha está en uno de estos dos formatos: mm/dd/yy o mm-dd-yy.  Se pide lo siguiente:
Mediante una sóla sentencia INSERT OVERWRITE TABLE ... modificar la tabla clientes para que la columna fecha contenga la fecha en el formato dd-mm-yy. 

INSERT OVERWRITE TABLE examen.clientes
SELECT nombre, regexp_replace(fecha, )
  FROM examen.clientes
;

+-------+
|Exe 10.|
+-------+

El punto 1 hay que hacerlo en impala y el 2 en hive..
Se pide lo siguiente:
1
Crear una tabla examen.customers con una sentencia CTAS a partir de la tabla analyst.customers con las siguientes condiciones:
Sólo tiene que tener 2 columnas fname y lname.
Sólo tiene que contener las 20 primeras filas de analyst.customers ordenadas por cust_id.

Una vez creada la tabla, añadir dos filas más de la siguiente manera:

insert into examen.customers values
('Ann', 'Shepard'),
('Joseph', 'Shepard') ;

2
Hacer un SELECT de examen.customers de tal manera que haya una sóla columna, name, que sea la concatenación de fname y lname. 
Las filas tienen que estar ordenadas por lname y luego por fname. 
Es obligatorio usar la función regexp_extract.

========================================================================================================================================================================
Exercices CCA 175 - Hands-On Exercises - Developer Training for Apache Spark and Hadoop
========================================================================================================================================================================
Just Enough Python
==================
------------------
Hands-On Exercise: Using IPython
-----------------
To run a program, from the command line, type python filename.py
Python is the default shell. - just open a terminal and type python - it has limited interaction with the host environment.

IPython provides additional features. This is an REPL (Read-Evaluate-Print-Loop)
There are three variations of IPython, console, qt (which provides inline graphics), and web-notebook.
You can edit a file without leaving IPython using the command %ed, is to launch the vi editor.
when you save and exit (<ESC>:wq) the IPython shell will automatically load the file that was edited
and run the program.

You can load and run a program without leaving IPython by typing run filename.py
magic command is a command preceded by the percent sign (%), IPython also has native commands that do not require a percent sign.

help()
help> int
# to quit
help> q
(base) hadoop@sc-ubuntu-20-04-2-lts:~$ ipython
Python 3.8.8 (default, Apr 13 2021, 19:58:26) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.22.0 -- An enhanced Interactive Python. Type '?' for help.

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/CCA175/training_materials/jep$ ipython
Python 3.8.8 (default, Apr 13 2021, 19:58:26) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.22.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: %ed hellopython.py
Editing... done. Executing edited code...
Hola: Sonia
Sonia

In [2]: mystring?
Type:        str
String form: Sonia
Length:      5
Docstring:  
str(object='') -> str
str(bytes_or_buffer[, encoding[, errors]]) -> str

Create a new string object from the given object. If encoding or
errors is specified, then the object must expose a data buffer
that will be decoded using the given encoding and error handler.
Otherwise, returns the result of object.__str__() (if defined)
or repr(object).
encoding defaults to sys.getdefaultencoding().
errors defaults to 'strict'.

solucion del manual: /training_materials/jep/solutions/Hello-solved.py
--------------------------------------------
------------------
Hands-On Exercise: Loudacre Mobile
-----------------
In [5]: ls
data/  hellopython.py  manifest.txt*  solutions/

In [6]: ls data
loudacre.log*  sample_one_record.txt*

In [7]: cat data/loudacre.log |more
2014-03-15:10:10:20,iFruit 1,6474caf1-7bbf-4594-a526-9ba8ea82e151,0,15,71,77,0,40,TRUE,enabled,connected,37.90310537,-121.5614513
2014-03-15:10:10:20,iFruit 2,5c2d40d8-b1e0-4c2a-b050-06ca6c590741,1,28,66,67,40,49,TRUE,disabled,connected,34.12789546,-108.9681595

In [12]: file=open('data/loudacre.log', 'rt')

In [13]: file?
Type:        TextIOWrapper
String form: <_io.TextIOWrapper name='data/loudacre.log' mode='rt' encoding='UTF-8'>
Docstring:  
Character and line based layer over a BufferedIOBase object, buffer.

encoding gives the name of the encoding that the stream will be
decoded or encoded with. It defaults to locale.getpreferredencoding(False).

errors determines the strictness of encoding and decoding (see
help(codecs.Codec) or the documentation for codecs.register) and
defaults to "strict".

newline controls how line endings are handled. It can be None, '',
'\n', '\r', and '\r\n'.  It works as follows:

* On input, if newline is None, universal newlines mode is
  enabled. Lines in the input can end in '\n', '\r', or '\r\n', and
  these are translated into '\n' before being returned to the
  caller. If it is '', universal newline mode is enabled, but line
  endings are returned to the caller untranslated. If it has any of
  the other legal values, input lines are only terminated by the given
  string, and the line ending is returned to the caller untranslated.

* On output, if newline is None, any '\n' characters written are
  translated to the system default line separator, os.linesep. If
  newline is '' or '\n', no translation takes place. If newline is any
  of the other legal values, any '\n' characters written are translated
  to the given string.

If line_buffering is True, a call to flush is implied when a call to
write contains a newline character.

In [14]: file
Out[14]: <_io.TextIOWrapper name='data/loudacre.log' mode='rt' encoding='UTF-8'>

In [25]: line = ' '

In [26]: file=open('data/loudacre.log', 'rt')

In [27]: while True:
    ...:     line = file.readline()
    ...:     if not line:
    ...:         break
    ...:     print (line)
    ...: file.close()

file description
record description
0 - Date Time:
1 - Model name and number:
2 - Unique device ID:
3 - Device temperature (Celsius):
4 - Ambient temperature (Celsius):
5 - Battery available (percent):
6 - Signal strength (percent):
7 - CPU utilization (percent):
8 - RAM memory usage (percent):
9 - GPS status (enabled=TRUE, disabled=FALSE):
10 - Bluetooth status (enabled, disabled, or connected):
11 - WiFi status (enabled, disabled, or connected):
12 - Latitude:
13 - Longitude:

2014-03-15:10:10:20,iFruit 1,6474caf1-7bbf-4594-a526-9ba8ea82e151,0,15,71,77,0,40,TRUE,enabled,connected,37.90310537,-121.5614513

2014-03-15:10:10:20,iFruit 2,5c2d40d8-b1e0-4c2a-b050-06ca6c590741,1,28,66,67,40,49,TRUE,disabled,connected,34.12789546,-108.9681595

2014-03-15:10:10:20,iFruit 3,27178d24-3a61-42f7-a784-e3263f25cc6f,1,30,91,89,41,17,TRUE,enabled,enabled,37.92489617,-122.2068682

solucion del manual: /training_materials/jep/solutions/Readlog-solved.py
--------------------------------------------
------------------
Hands-On Exercise: Variables
-----------------
# Numerical variable basics
# tempC = temp Celsius
# tempF = temp Fahrenheit
# C_to_F = 9/5

In [28]: tempC = 22

In [29]: C_to_F = 9/5

In [30]: tempF = tempC * C_to_F

In [32]: print(tempC, tempF)
22 39.6

In [33]: type(tempC)
Out[33]: int

In [34]: type(tempF)
Out[34]: float

In [35]: type(C_to_F)
Out[35]: float

# multiple assignment and dynamic typing
In [37]: ambientTemp, deviceTemp = 19, 47.0
    ...: print (type(ambientTemp), type(deviceTemp))
    ...: print (ambientTemp, deviceTemp)
<class 'int'> <class 'float'>
19 47.0

In [39]: ambientTemp, deviceTemp = 19.0, 47
    ...: print (type(ambientTemp), type(deviceTemp))
    ...: print (ambientTemp, deviceTemp)
<class 'float'> <class 'int'>
19.0 47

Operators and Reflexive Operators
# operators and reflexive operators
#
# cpuT1 = sample CPU utilization at time 1
# cpuT2 = sample CPU utilization at time 2

In [40]: cpuT1 = 17
    ...: cpuT2 = 38
    ...: averageCPU = (cpuT1 + cpuT2) / 2.0
    ...: print("Average CPU: ", averageCPU)
Average CPU:  27.5

# New software could reduce cpu utilization by 12%
In [41]: reduction = 12.0/100
    ...: print("Expected CPU: ", cpuT2-(cpuT2*reduction))
Expected CPU:  33.44

# Increment cpu utilization at time 1 by 1%
In [42]: cpuT1 += 1
    ...: print (cpuT1)
18

# Find total of time 1 and time 2 cpu samples
In [43]: cpuT1 += cpuT2
    ...: print (cpuT1)
56

# Increment cpuT2
In [44]: cpuT2++
  File "<ipython-input-44-1aae61a3dba3>", line 1
    cpuT2++
           ^
SyntaxError: invalid syntax

In [45]: cpuT2 = cpuT2 + 1
    ...: print (cpuT2)
39

# Python provides 'floor division' and modulus
In [46]: print (37.0 / 5.0)
    ...: print (37.0 // 5.0) # Gives the whole divisor
    ...: print (37.0 % 5.0) # Gives the remainder
7.4
7.0
2.0

# Explicit casting between integer and float
In [47]: cpuT1, cpuT2 = 50, 30.1
    ...: print (cpuT1, cpuT2)
    ...: cpuT1 = float(cpuT1)
    ...: cpuT2 = int(cpuT2)
    ...: print (cpuT1, cpuT2)
50 30.1
50.0 30

solucion del manual: /training_materials/jep/solutions/Operators-solved.py.
--------------------------------------------
Binary, Octal, and Hexadecimal

# -- Unique ID --
# PartC = 0x13f4

PartC = 0x13f4
print ("ID: ", PartC, type(hexID))

# Even though you entered PartC as a hex value,
# Python converted it to decimal
# Now try casting as you did with int() and float()

hexID = hex(PartC)
binID = bin(PartC)
octID = oct(PartC)
print ("hex ID: ", hexID, type(hexID))
print ("bin ID: ", binID, type(binID))
print ("oct ID: ", octID, type(octID))

# Prefixes are 0x for hex, leading zero for octal, and 0b for binary

In [3]: %ed
IPython will make a temporary file named: /tmp/ipython_edit_xzj1fcai/ipython_edit_z2n_mrbc.py
Editing... done. Executing edited code...
ID:  5108 <class 'str'>
hex ID:  0x13f4 <class 'str'>
bin ID:  0b1001111110100 <class 'str'>
oct ID:  0o11764 <class 'str'>
Out[3]: '# -- Unique ID --\n# PartC = 0x13f4\n\nPartC = 0x13f4\nprint ("ID: ", PartC, type(hexID))\n\n# Even though you entered PartC as a hex value,\n# Python converted it to decimal\n# Now try casting as you did with int() and float()\n\nhexID = hex(PartC)\nbinID = bin(PartC)\noctID = oct(PartC)\nprint ("hex ID: ", hexID, type(hexID))\nprint ("bin ID: ", binID, type(binID))\nprint ("oct ID: ", octID, type(octID))\n'

solucion del manual: /training_materials/jep/solutions/BinOctHEx-solved.py.
--------------------------------------------

Advanced Mathematical Functions

In [4]: sqrt(192)
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-4-6ac2351074cf> in <module>
----> 1 sqrt(192)
NameError: name 'sqrt' is not defined

In [5]: from math import sqrt
   ...: print (sqrt(192))
13.856406460551018

In [6]: sqrt(192)
Out[6]: 13.856406460551018

solucion del manual: /training_materials/jep/solutions/AdvMath-solved.py
------------------
Strings
record = "2014-03-15:10:10:31,Titanic 4000,\
1882b564-c7e0-4315-aa24-228c0155ee1b,\
58,36,39,31,15,0,\
TRUE,enabled,enabled,\
40.69206648,-119.4216429"

In [7]: record = "2014-03-15:10:10:31,Titanic 4000,\
   ...: 1882b564-c7e0-4315-aa24-228c0155ee1b,\
   ...: 58,36,39,31,15,0,\
   ...: TRUE,enabled,enabled,\
   ...: 40.69206648,-119.4216429"

In [8]: print("record: ", record, type(record))
record:  2014-03-15:10:10:31,Titanic 4000,1882b564-c7e0-4315-aa24-228c0155ee1b,58,36,39,31,15,0,TRUE,enabled,enabled,40.69206648,-119.4216429 <class 'str'>

In [10]: len(record)
Out[10]: 132

7. Use the find() string method to locate the start of the model name (Titanic). 
Then use slice ranges to extract the model name and model number into separate variables. 
Remember that the model number comes immediately after the model name in the data.

find return int where start "Titanic"
print(record[20])
20
T

In [3]: end=len(record)
In [4]: start=pos=0
In [6]: pos = record.find('Titanic')
In [7]: pos
Out[7]: 20
In [8]: pos = record[pos:end].find(',')
In [9]: pos
Out[9]: 12
In [10]: record[20:32]
Out[10]: 'Titanic 4000'

In [11]: s_mod_num = record[20:32]
    ...: s_mod_num
Out[11]: 'Titanic 4000'

In [12]: len(s_mod_num)
Out[12]: 12

In [14]: model = s_mod_num[0:len('Titanic')]
In [15]: model
Out[15]: 'Titanic'

In [16]: number = s_mod_num[len('Titanic'):]
In [17]: number
Out[17]: ' 4000'

In [18]: model.upper()
Out[18]: 'TITANIC'
In [19]: model += " " + number
In [20]: model
Out[20]: 'Titanic  4000'

# devuelve una lista con las posiciones de las comas en record
end = len(record)
list = []
pos = start = 0
while(pos != -1):
    pos = record[start:end].find(',')
    list.append(start+pos)
    start = start + pos + 1
    print(list)

[19]
[19, 32]
[19, 32, 69]
[19, 32, 69, 72]
[19, 32, 69, 72, 75]
[19, 32, 69, 72, 75, 78]
[19, 32, 69, 72, 75, 78, 81]
[19, 32, 69, 72, 75, 78, 81, 84]
[19, 32, 69, 72, 75, 78, 81, 84, 86]
[19, 32, 69, 72, 75, 78, 81, 84, 86, 91]
[19, 32, 69, 72, 75, 78, 81, 84, 86, 91, 99]
[19, 32, 69, 72, 75, 78, 81, 84, 86, 91, 99, 107]
[19, 32, 69, 72, 75, 78, 81, 84, 86, 91, 99, 107, 119]
[19, 32, 69, 72, 75, 78, 81, 84, 86, 91, 99, 107, 119, 119]

1 - Model name and number:
In [112]: record_list = record.split(',')
     ...: model_number = record_list[1]

In [113]: model_number
Out[113]: 'Titanic 4000'

In [114]: model_number_list = model_number.split(' ')

In [115]: model_number_list
Out[115]: ['Titanic', '4000']

In [116]: model = model_number_list[0]

In [117]: number = model_number_list[1]

In [118]: print("model: ", model, " number: ", number)
model:  Titanic  number:  4000

In [125]: model = model_number_list[0].upper()

In [126]: print("model: ", model, " number: ", number)
model:  TITANIC  number:  4000

Use a reflexive operator to concatenate the model number onto the new string
In [132]: model += " " + number
     ...: print('model += " " + number: ', model)
model += " " + number:  TITANIC 4000

solucion del manual: /training_materials/jep/solutions/Strings-solved.py.
--------------------------------------------

Boolean
Extract the Bluetooth and WiFi fields from the record string.
Each field can contain three values: enabled, disabled, and connected

10 - Bluetooth status (enabled, disabled, or connected):
11 - WiFi status (enabled, disabled, or connected):

record_list = record.split(',')
print("Byuetooth status: ", record_list[10]
print("WiFi status: ", record_list[11]

Bluetooth and WiFi status are:  enabled
Byuetooth status:  enabled

11. Use == to determine if Bluetooth and WiFi are ON and print the results.


In [7]: if (record_list[10] == record_list[11]):
   ...:     print("Bluetooth and WiFi status are: ", record_list[10])
   ...: 
   ...: print("Byuetooth status: ", record_list[10])
   ...: print("WiFi status: ", record_list[11])
   ...: 
   ...: print ("Bluetooth is ON :", record_list[10] == 'enabled')
   ...: print ("Wifi is ON      :", record_list[11] == 'enabled')
   ...: print ("Wifi is ONLINE  :", record_list[11] == 'connected')
Bluetooth and WiFi status are:  enabled
Byuetooth status:  enabled
WiFi status:  enabled
Bluetooth is ON : True
Wifi is ON      : True
Wifi is ONLINE  : False


12. Use == to determine if WiFi is connected to an access point.
Hint: use string.find() in REPL mode to locate the start value of a slice.

Bluetooth = record[92:99]
WiFi = record[100:107]

In [18]: if record[100:107].find('conected') != 0:
    ...:     print("NO esta conectada, esta: ", record[100:107])
    ...: 
NO esta conectada, esta:  enabled

13. Bonus exercise: Print out the device temperature field for records greater than 50 degrees.
The completed program is in the solutions/Boolean-solved.py file.

3 - Device temperature (Celsius)

In [21]: if int(record_list[3]) > 50:
    ...:     print("La temperatura del disposivo es mayor de 50 grados Celsius: " , record_list[3])
    ...: 
La temperatura del disposivo es mayor de 50 grados Celsius:  58

solucion del manual: /training_materials/jep/solutions/Boolean-solved.py
--------------------------------------------

------------------
Hands-on Exercise: Collections 
-----------------

Reading the Data
Use the line at a time readline() technique to read the loudacre.log file.  Each record is read as a string. 
In this exercise you will process each string and break out the fields into a separate list. 
Finally, you will store the list of fields into a list of records to create an information base. 
You will use a simple while()loop. 
Each string returned from readline() is a record composed of comma-delimited fields. 
Use the split() method to separate each record into a list of fields. 
Store the list of fields into a list of records using the list.append() method.

In [73]: 
    ...: line_lst = []
    ...: lines = []
    ...: filename = ('data/loudacre.log')
    ...: file = open(filename,'rt')
    ...: while True:
    ...:     line = file.readline()
    ...:     if not line:
    ...:         break
    ...:     line_lst = line.split(',')
    ...:     lines.append(line_lst)
    ...: file.close()

1. Print out the number of records in the list.
In [78]: len(lines)
Out[78]: 500

2. Print out the number of fields in each record.
Note: This is a nested list. The first list contains elements that are lists.

In [76]: len(line_lst)
Out[76]: 14

3. Print out field 2 (the unique ID field) for records 375, 410, and 435.
2 - Unique device ID:

In [83]: print("\nlines[375][2]: ", lines[375][2], "\nlines[410][2]: ", lines[410][2], "\nlines[435][2]: ", lines[435][2])

lines[375][2]:  8d431088-c262-4f26-8d2f-9fb558cc6242 
lines[410][2]:  99a93ac5-19a6-4123-91f6-08c695d7b6e0 
lines[435][2]:  077ea017-3de3-4f14-8964-56ab9c987bb5

solucion del manual: /training_materials/jep/solutions/Split-solved.py
--------------------------------------------
Part 2
In this exercise you will read the loudacre.log file into memory twice using two different file-reading techniques. 
Then you will compare the buffers to see if the two methods produce the same results.

5. Create an empty list, loglist1.

loglist1 = []

6. Read the loudacre.log file using the line-at-a-time technique using the readline() method. Append each line to the list loglist1.

filename = ('data/loudacre.log')
file = open(filename,'rt')
while True:
    line = file.readline()
    if not line:
        break
    loglist1.append(line) 

7. Close the file with file.close()
file.close()

8. Create an empty list, loglist2.
loglist2 = []

9. Read the loudacre.log file using the list-at-a-time technique with readlines() – note the “s” in the method name – which returns an entire list object instead of individual lines.

filename = ('data/loudacre.log')
file = open(filename,'rt')
while True:
    line = file.readlines()
    if not line:
        break
    loglist2.append(line) 

10. Close the file with file.close()
file.close()

11. Print out the length of loglist1 and loglist2. Are they the same length?

print('len(loglist1): ', len(loglist1))
print('len(loglist2): ', len(loglist2))

12. Compare loglist1 and loglist2. Do they have the same value? 
Hint: you can compare them with ==. Python knows they are lists and will compare all elements in each list in sequence to determine if they are identical.

if loglist1 == loglist2:
    print("EQUALS")
print("NOT EQUALS")


13. Append one more of the last line read from loglist1 onto to loglist2.  
Compare the lists again. Do they have the same value?

loglist2.append(line)
print('len(loglist1): ', len(loglist1))
print('len(loglist2): ', len(loglist2))
if loglist1 == loglist2:
    print("EQUALS")
print("NOT EQUALS")


14. Concatenate loglist2 onto loglist1. 
Hint: use the reflexive operator. How many records does loglist1 have now?

loglist1 += loglist2
print('len(loglist1): ', len(loglist1))
print('len(loglist2): ', len(loglist2))
if loglist1 == loglist2:
    print("EQUALS")
print("NOT EQUALS")

In [84]: loglist1 = []
In [85]: filename = ('data/loudacre.log')
    ...: file = open(filename,'rt')
    ...: while True:
    ...:     line = file.readline()
    ...:     if not line:
    ...:         break
    ...:     loglist1.append(line)
    ...: 
In [86]: file.close()
    ...: 
In [87]: loglist2 = []
    ...: 
In [88]: filename = ('data/loudacre.log')
    ...: file = open(filename,'rt')
    ...: while True:
    ...:     line = file.readlines()
    ...:     if not line:
    ...:         break
    ...:     loglist2.append(line)
    ...: 
In [89]: file.close()
    ...: 
In [90]: print('len(loglist1): ', len(loglist1))
    ...: print('len(loglist2): ', len(loglist2))
    ...: 
len(loglist1):  500
len(loglist2):  1
In [91]: 
    ...: if loglist1 == loglist2:
    ...:     print("EQUALS")
    ...: print("NOT EQUALS")
    ...: 
NOT EQUALS
In [92]: loglist2.append(line)
    ...: print('len(loglist1): ', len(loglist1))
    ...: print('len(loglist2): ', len(loglist2))
    ...: if loglist1 == loglist2:
    ...:     print("EQUALS")
    ...: print("NOT EQUALS")
    ...: 
len(loglist1):  500
len(loglist2):  2
NOT EQUALS
In [93]: loglist1 += loglist2
    ...: print('len(loglist1): ', len(loglist1))
    ...: print('len(loglist2): ', len(loglist2))
    ...: if loglist1 == loglist2:
    ...:     print("EQUALS")
    ...: print("NOT EQUALS")
    ...: 
len(loglist1):  502
len(loglist2):  2
NOT EQUALS


Note: The process of reading all of the lines of a file into a list is fast and efficient.
However, the activity is atomic. That means you CAN'T process each line using split() as in the last exercise. 

solucion del manual: /training_materials/jep/solutions/List-solved.py
--------------------------------------------
Generators - yield - rendimiento

Problem #1

- Loudacre marketing wants to send an advertising text message to every telephone in the 555 area code.
- First challenge is to write a program to generate all the phone numbers
– 555-eee-ssss = 3 + 1 + 3 + 1 + 4 = Each phone is a 12 character string
– Skip zeroes in the exchange ﬁeld
– aaa-123-4567 = 10,000,000 = about 10 million numbers
– 12 x 10m = 120 MB of data


def phonegen():
    d1=d2=d3=d4=d5=d6=d7 = 0
    phonelist = []
    for d1 in range(1,10):
        for d2 in range(1,10):
            for d3 in range(1,10):
                for d4 in range(0,10):                          # – Skip zeroes in the exchange ﬁeld
                    for d5 in range(0,10):
                        for d6 in range(0,10):
                            for d7 in range(0,10):
                                phone = '555' + '-' + str(d1) + str(d2) + str(d3) 
                                phone = phone + '-' + str(d4) + str(d5) + str(d6) + str(d7)
                                phonelist.append(phone)
    return phonelist

# -- test program
list_of_phones = phonegen()
for each_phone in list_of_phones:
    # send_message(each_phone)
    print(each_phone)

...
...
555-138-7510
555-138-7511
555-138-7512
555-138-7513
555-138-7514
555-138-7515
555-138-7516
555-138-7517
...
...

Problem #2
- The advertising program was a success! Marketing wants to expand to cover 250 area codes in the United States.
- Concerns - Preocupación
– 250 area codes x 120 MB each = 30 GB of data
    – My computer only has 16 GB of RAM!
– The previous program took about 30 seconds to generate the list
– The new program will take about two hours to generate the list!
- What's needed is a way to iterate over each phone number in sequence
- But how do you iterate with a function?
    – Local variables are lost when the function returns
    – How would you preserve the current state so you could continue at the next number in sequence each time you call phonegen()?
- A Python generator is an iterable function
    – Also known as a cofunction in computer science
- yield - Rendimiento
    – Preserves the functional context (local variables and state)
    – Returns control to the caller
    – Passes an iterator object back

Yield is a keyword in Python that is used to return from a function without destroying the states of its local variable and when the function is called, 
the execution starts from the last yield statement. Any function that contains a yield keyword is termed (denominada) a generator. 
Hence(por lo tanto), yield is what makes a generator. 

§ Yield devolverá el contexto funcional. Así que se necesita una nueva forma de pasar el número de teléfono al código de llamada.

#-- Global phone number
phonenum = "- "

# --- Generate all phone numbers in the target area codes
def phonegen():
    target_area_codes = [555, 205, 251, 256, 334, 907, 480, 520,
    623, 928, 501, 870, 209, 213, 310, 323, 408, 415, 510, 530, 559,
    562, 619, 626, 650, 661, 707, 714, 760, 805, 818, 831, 858, 909,
    916, 925, 949, 303, 719, 720, 970, 203, 860, 302, 305, 321, 352,
    386, 407, 561, 727, 754, 772, . . .  . . . . . .  . . . . . 704,
    828, 910, 919, 980, 701, 216, 234, 330, 419, 440, 513, 614, 740,
    717, 724, 814, 878, 401, 803, 843, 864, 210, 214, 254, 281, 361,
    409, 469, 512, 682, 713, 806, 817, 830, 832, 903, 915, 936, 940,
    956, 972, 979, 435, 801, 802, 276, 434, 540, 571, 703, 757, 804,
    206, 253, 360, 425, 509, 715, 920, 307]
    global phonenum
    d1=d2=d3=d4=d5=d6=d7 = 0
    for d1 in range(1,10):
        for d2 in range(1,10):
            for d3 in range(1,10):
                for d4 in range(0,10):                          # – Skip zeroes in the exchange ﬁeld
                    for d5 in range(0,10):
                        for d6 in range(0,10):
                            for d7 in range(0,10):
                                phone = '555' + '-' + str(d1) + str(d2) + str(d3) 
                                phone = phone + '-' + str(d4) + str(d5) + str(d6) + str(d7)
                                phonenum = phone
    yield phonenum

it = 0
for phonenum in phonegen():
    if it > 50:
        break
    print phonenum
    it += 1

------------------
Hands-On Exercise: Flow Control
-----------------
Part 1
In this exercise you will be preparing a brand index and a model index using sets.
The final sets are to be ordered and unique, displaying the brands and the models mentioned in the loudacre.log.

1. Read the loudacre.log file into memory using readlines()

 
In [32]: lines = [ ]
    ...: filename = 'files/loudacre.log'
    ...: file = open(filename,'rt')
    ...: lines = file.readlines()
    ...: file.close()

2. Separate each record into a list using split()
3. Locate the model name and number field (Example: “Titanic 4000”). Collect the model field into a set().
Note that there is no literal for the empty set. If you initialize a variable like allmodels = {}, 
you will get a “dict” type instead of a “set” type. So you must initialize like so: allmodels = set()
4. Use split() to separate the brand name (Example: “Titanic”) from the model number. 
Hint: The brand name is separated from the model number by a space.
5. Collect the brand names into a set().

In [32]: brandindex = set()
    ...: modelindex = set()

In [30]: record = []
    ...: for line in lines:
    ...:     # los campos de cada line estan delimitados por coma
    ...:     record = line.split(',')
    ...:     # el modelo es el campo 1 y esta separado por espacio
    ...:     model = record[1].split(' ')
    ...:     brandindex.add(model[0])
    ...:     modelindex.add(model[1])
    ...: 
    ...: 

In [31]: print("brandindex: ", brandindex)
    ...: print("modelindex: ", modelindex)
brandindex:  {'Sorrento', 'MeeToo', 'Ronin', 'iFruit', 'Titanic'}
modelindex:  {'1.0', 'F32L', '4.1', 'F22L', '2500', '3', '2000', 'S1', '3.1', 'F31L', '2400', '2200', 'Novelty', 'F21L', 'F33L', 'S2', '2300', '4.0', '3A', 'F20L', 'F23L', '1', '3.0', 'F00L', 'F40L', 'F10L', '5.1', '2.0', 'F30L', '1100', '5', 'S4', '4A', 'S3', '2', 'F24L', '5.0', 'F01L', '1000', '4', 'DeckChairs', '3000', '4000', 'F11L', '2100', 'F41L'}

6. Sort both indexes. There is no sort() method for sets. Use sorted()
In [34]: brandindex = sorted(brandindex)
    ...: modelindex = sorted(modelindex)
    ...: 
7. Print the brand list and the total number.
8. Print the model list and the total number.
In [35]: print("brandindex: ", brandindex)
    ...: print("modelindex: ", modelindex)
brandindex:  ['MeeToo', 'Ronin', 'Sorrento', 'Titanic', 'iFruit']
modelindex:  ['1', '1.0', '1000', '1100', '2', '2.0', '2000', '2100', '2200', '2300', '2400', '2500', '3', '3.0', '3.1', '3000', '3A', '4', '4.0', '4.1', '4000', '4A', '5', '5.0', '5.1', 'DeckChairs', 'F00L', 'F01L', 'F10L', 'F11L', 'F20L', 'F21L', 'F22L', 'F23L', 'F24L', 'F30L', 'F31L', 'F32L', 'F33L', 'F40L', 'F41L', 'Novelty', 'S1', 'S2', 'S3', 'S4']

How many brands are mentioned in the loudacre.log file?
How many unique models of device are there?

In [36]: print("len(brandindex): ", len(brandindex))
    ...: print("len(modelindex): ", len(modelindex))
len(brandindex):  5
len(modelindex):  46

In [37]: for brand in brandindex:
    ...:     print("brand: ", brand)
    ...: 
brand:  MeeToo
brand:  Ronin
brand:  Sorrento
brand:  Titanic
brand:  iFruit

In [38]: for model in modelindex:
    ...:     print("model: ", model)
    ...: 
model:  1
model:  1.0
model:  1000
model:  1100
model:  2
model:  2.0
model:  2000
model:  2100
model:  2200
model:  2300
model:  2400
model:  2500
model:  3
model:  3.0
model:  3.1
model:  3000
model:  3A
model:  4
model:  4.0
model:  4.1
model:  4000
model:  4A
model:  5
model:  5.0
model:  5.1
model:  DeckChairs
model:  F00L
model:  F01L
model:  F10L
model:  F11L
model:  F20L
model:  F21L
model:  F22L
model:  F23L
model:  F24L
model:  F30L
model:  F31L
model:  F32L
model:  F33L
model:  F40L
model:  F41L
model:  Novelty
model:  S1
model:  S2
model:  S3
model:  S4

Part 2
In this exercise you will be processing and preparing an information base from the device log file. 
The fields in the log file are stored as strings. 
You will be converting these to int, float, and bool types to make it easier to use the data later. 
Also, the final information base may be used by concurrent programs, so it needs to be made immutable by making it into a tuple. 
Because the tuple cannot be changed after it is initialized, you will have to construct the information base in a list first, and then cast it to a tuple.

9. Read the loudacre.log file into memory using readlines()


10. Separate each record into fields using split()

11. Convert the following fields from strings to integer values: Device Temperature, Ambient Temperature, Battery Available, Signal Strength, CPU Utilization, and RAM usage. 
Hint: the fields are in sequence.

            #5 - Battery available (percent):
            #6 - Signal strength (percent):
            #7 - CPU utilization (percent):
            #8 - RAM memory usage (percent):
            
    for i in range(5,9):
        record[i] = int(record[i])

12. Convert the Device Temperature and Ambient Temperature from Celsius to Fahrenheit. Hint: Fahrenheit = 9/5 * Celsius +32
             record[3] = 9/5 * int(record[3]) + 32
             record[4] = 9/5 * int(record[4]) + 32  # Ambient temp

13. Convert the GPS status to a bool Boolean value. Hint: the TRUE or FALSE in the file is a string.
  if record[9]  == "TRUE":
     record[9]  = True
  else:
     record[9]  = False

14. Convert the latitude and longitude from strings to floating point values.
            #12 - Latitude - 13 - Longitude
            record[12] = float(record[12])
            record[13] = float(record[13])

15. Isolate the brand name from the model name and number field, and insert this after the date and time stamp field using the insert() method. 
Hint: use split(). The brand name and model number are separated by a space.

             model_number = record[1].split('')
             record.insert(1, model_number[0])

16. Append the fully processed record to a temporary device base list.

             line_lst.append(record)

17. After the temporary base is constructed, cast it to a tuple to “freeze” it. 
Print out the number of records in the tuple and the type to verify it's a tuple.
             line_tuple = tuple(line_lst)
             pritn("len(line_lst): ", len(line_lst))
             print("len(line_tuple): ", len(line_tuple))
             print("type(line_tuple): ", type(line_tuple))

line_lst = [ ]
record  = [ ]
line_tuple = ( )

lines = [ ]
filename = 'files/loudacre.log'
file = open(filename, 'rt')
lines = file.readlines()
file.close()

for line in lines:
  record  = line.split(',')
# -- leave these fields unchanged --
#
#               dts = record[0]
#             model = record[1]
#          uniqueID = record[2]
#

# -- convert temperature to F
  record[3] = 9/5 * int(record[3]) + 32  # Device temp
  record[4] = 9/5 * int(record[4]) + 32  # Ambient temp

# -- convert numerical strings to integers
#          batt    = int(record[5])
#          signal  = int(record[6])
#          cpu     = int(record[7])
#          ram     = int(record[8])
  for i in range(5,9):
    record[i] = int(record[i])

# -- convert gps status to bool
#  gps    = record[9]
  if record[9]  == "TRUE":
     record[9]  = True
  else:
     record[9]  = False

# -- leave these fields unchanged --
#
#         btooth   = record[10]
#           wifi   = record[11]
#
# -- convert location to float
  record[13]  = float(record[12])                # Latitude
  record[13]  = float(record[13])                # Longitude

# --  split out the brand name and add it in a separate field
#
#        Note that the indexes for most fields just shifted right by one
#
  model = record[1].split(' ')
  record.insert(1,model[0])

# -- field processing is complete
# -- collect the modified record in line_lst
#
  line_lst.append(record)

# -- processing is done -- cast the tuple
line_tuple = tuple(line_lst)

print("Longitud in line_lst: len(line_lst): ", len(line_lst))
print("line_lst is a type(line_lst): ", type(line_lst))
print("Number of records in line_tuple: len(line_tuple): ", len(line_tuple))
print("line_tuple is a type(line_tuple): ", type(line_tuple))

Longitud in line_lst: len(line_lst):  500
line_lst is a type(line_lst):  <class 'list'>
Number of records in line_tuple: len(line_tuple):  500
line_tuple is a type(line_tuple):  <class 'tuple'>

18. You should now be able to write code that uses the tuple to answer the following questions: 
How many Sorrento brand devices are found in the loudacre.log file, and how many of them had GPS turned on?

1 - Model
10 - GPS status (enabled=TRUE, disabled=FALSE):

count_model = 0
count_GPS = 0
for line in line_tuple:
    if line[1] == 'Sorrento':
       count_model += 1
       if line[10]:
          count_GPS += 1
    
print("There are ", count_model , " Sorrento brand devices and ", count_GPS, " of them had GPS turned on.")

There are  224  Sorrento brand devices and  216  of them had GPS turned on.

record description

0 - Date Time:
1 - Model
2 - Model name and number:
3 - Unique device ID:
4 - Device temperature (Celsius):
5 - Ambient temperature (Celsius):
6 - Battery available (percent):
7 - Signal strength (percent):
8 - CPU utilization (percent):
9 - RAM memory usage (percent):
10 - GPS status (enabled=TRUE, disabled=FALSE):
11 - Bluetooth status (enabled, disabled, or connected):
12 - WiFi status (enabled, disabled, or connected):
13 - Latitude:
14 - Longitude:

sorrentos = [ line[10] for line in line_tuple if line[1] == "Sorrento" ]        
print("len(sorrentos): ", len(sorrentos))
print("sorrentos.count(True): ", sorrentos.count(True))

solucion del manual: /training_materials/jep/solutions/Devicebase-solved.py
--------------------------------------------

------------------
Hands-On Exercise: Program Structure
-----------------

Pass by Reference
In this exercise you will be creating a filtered version of the information base from the last exercise using the built-in filter()function, and passing by reference a function that will perform the filtering.
The field we want to use in the filter is nested in element[1] – the brand name of the device that you inserted in step 6 of the previous exercise. 
You will create a function that returns True if the length of this field is greater than 7.
You will first create the function as a standard named function. Next, you will comment that out and recreate the function as a single-line named function. 
In the next exercise, you will convert the function to an inline anonymous function using the lambda keyword.

named function:
def mayorque(s):
    return len(s) > 7

s = "1234567"
print(mayorque(s))
False

s = "12345678"
print(mayorque(s))
True

s = ("Sorrento", "Madrid")

print(mayorque(s))
False

print(mayorque(s[1]))
False

print(mayorque(s[0]))
True

print(mayorque(line_tuple[1]))
True

named function:
def mayorque(s):
    result = len(s[1]) > 7
    return result

s = line_tuple
result = mayorque(s)

#print("s: ", s)
print("type(s): ", type(s))         #   type(s):  <class 'tuple'>

result = mayorque(s)
type(result)
bool
print(result)
True

single-line named function:
def mayorque(s): return len(s[1]) > 7
result = mayorque(s)
type(result)
bool
print(result)
True
count = 0
for i in result:
    count = count + 1
    print(i)
    
print(count)
2

print(len(result))
2

solucion del manual: /training_materials/jep/solutions/Functions-solved.py
--------------------------------------------

Lambda Functions
In this exercise you will replace the named functions from the previous exercise with an anonymous function.
5. Comment out the second version of biggerthan and the call to filter() that uses a named function.
6. Rewrite the call to filter() using an inline anonymous (lambda) function.  
Demonstrate that the results are the same by printing the length (number of elements) in the result.

lambda function:
filter(lambda s: len(s[1]) > 7, s)
<filter at 0x7f54aac88cd0>

result = filter(lambda s: len(s[1]) > 7, s)
type(result)
filter

print(result)
<filter object at 0x7f54a9789ee0>

for i in result:
    print(i)

imprimirá todos los s[1] de la tupla, (la posicion 1 de la tupla) que cumplan la condición de la funcion
['2014-03-15:10:10:20', 'Sorrento', 'Sorrento F01L', 'ab508fbf-f5ca-4ee6-92f7-c5e090752d2d', 32.0, 59.0, 92, 36, 62, 53, True, 'enabled', 'connected', '34.12599932', 34.12599932]
....

result = filter(mayorque, s)

print(result)
<filter object at 0x7f54a9c600d0>

type(result)
filter

for i in result:
    print(i)

print(len(result))
2

solucion del manual: /training_materials/jep/solutions/Lambda-solved.py
--------------------------------------------

Python Map and Reduce
In this exercise, you will prepare a list of data using Python's map() function and then find the largest result with Python's reduce() function.
7. Extract all the device temperature data into a tuple.
8. Convert the temperatures to Fahrenheit using the Python map() function with an anonymous function to perform the conversion.
9. Use reduce() to find the average device temperature, with an anonymous function to perform the comparison. 
You will need to use reduce to generate the sum of the values, and then divide that by the total number of values.
10. What is the hottest device temperature?

list= [ ]
temp_tuple= ( )
record  = [ ]

lines = [ ]
filename = 'files/loudacre.log'
file = open(filename, 'rt')
lines = file.readlines()
file.close()

list_temp = [ ]
for line in lines:
    record  = line.split(',')
    list_temp.append(int(record[3]))
    #list_temp.append(record[4])


print("Number of records: ", len(list_temp))
print("Type is a ", type(list_temp))
print(list_temp)

Number of records:  500
Type is a  <class 'list'>
[0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 7, 8, 3, 9, 6, 5, 2, 7, 5, 7, 43, 21, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 7, 8, 4, 7, 2, 9, 6, 81, 32, 65, 80, 40, 22, 58, 67, 81, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 4, 7, 3, 3, 5, 8, 32, 96, 77, 74, 29, 52, 36, 61, 52, 20, 40, 43, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 6, 5, 4, 8, 13, 6, 3, 3, 5, 9, 4, 62, 70, 99, 61, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 3, 10, 5, 4, 10, 5, 4, 7, 5, 73, 73, 37, 22, 74, 49, 94, 37, 87, 40, 66, 86, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 12, 10, 8, 5, 11, 7, 8, 7, 5, 7, 52, 47, 98, 54, 80, 57, 76, 51, 20, 50, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 5, 7, 10, 4, 6, 10, 7, 6, 4, 6, 5, 9, 56, 67, 60, 95, 63, 61, 82, 93, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 8, 10, 7, 3, 8, 6, 3, 3, 3, 5, 41, 38, 58, 83, 59, 52, 62, 94, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 12, 10, 4, 4, 5, 4, 4, 7, 10, 42, 56, 95, 80, 96, 74, 96, 36, 96, 87, 35, 30, 1, 0, 0, 0, 1, 1, 2, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 5, 4, 12, 2, 9, 3, 3, 38, 28, 50, 47, 50, 88, 27, 83, 99, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 7, 5, 4, 8, 7, 11, 6, 5, 55, 75, 85, 42, 77, 98, 26, 96, 48, 31, 62, 81, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 6, 6, 7, 11, 4, 11, 2, 3, 10, 70, 35, 78, 22, 71, 36, 55, 58]

s = tuple(list_temp)

len(s)
500

temp2 = map(lambda s: (9/5 * (s) + 32), s)
type(temp2)
#from functools import reduce            # 
mayor = reduce(lambda x,y: x if(x>y) else y, temp2)
print("reduce: ", mayor)

mi propuesta: no funciona en principio porque esta mal escrita.. hahah
# reduce(lambda temp2: sum(temp2)/len(temp2), temp2)

# pero la propuesta del manual tampoco
# -- Part 2: use reduce() to find the average temp
average = reduce(lambda x,y: x+y, temp2)/float(len(temp2))
print "average:",average

solucion del manual: /training_materials/jep/solutions/MapReduce-solved.py.
--------------------------------------------

Generator Part 1
Security has decided to use a little-known mathematical series to generate part of the unique device IDs. 
The Stern series is defined as the ratio of the whole number to the sum of its digits.
Here is an example:
Whole number in sequence: 951
Sum of the digits: 9 + 5 + 1 = 15

Ratio of the whole to the sum of the digits: 951/15 = 63.4

11. Define a function that generates all the numbers in the series up to maxnum.

The numbers should be stored as pairs of tuples with the first element in the tuple being the whole number (int) and the second number being the ratio (float). 
Store each tuple in a list. Then have the function return the list to the calling program.

12. The calling program should iterate over the entire list, printing out each pair of tuples in the series.
13. Generate the first 500 numbers in the series.  What is the ratio for number 481?

solucion del manual: /training_materials/jep/solutions/Series-solved.py.
--------------------------------------------

/* idea inicial */
definir un for desde 0 hasta el maxnum y dentro de el generar la lista
maxnum = 501
list_num = []
c=d=u=0
for c in range(0,5):
    for d in range(0,10):
        for u in range(0,10):
            tuple_num = set ()
            x = str(c) + str(d) + str(u)
            print(str(c) + str(d) + str(u))
            y = (c+d+u)
            print(c+d+u)
            if y == 0:
               ratio = 0
            else:
               ratio = int(x)/float(y)
            tuple_num.add(x)
            tuple_num.add(ratio)
            list_num.append(tuple_num)

ini = 1
fin = 500
def crea_serie(ini,fin):
   lista_num = []
   tupla_num = ()
   numero = 0
   for num in range (ini, fin):
       # convierte el numero en string
       str_num = str(num)
       # print('str_num: ', str_num)
       sum = 0
       # print("len(str_num): ", len(str_num))
       # suma los numeros entre sí 
       for i in range(0, len(str_num)):
           sum += int(str_num[i])
       # calcula el ratio
       ratio = float(num) / float(sum)
       # crea la tupla
       tupla_num = (num, ratio)
       # print("type(tupla_num): ", type(tupla_num))
       # print("tupla_num: ", tupla_num)
       lista_num.append(tupla_num)
   return lista_num 


lista_num = crea_serie(1,500)
for i in lista_num:
   print("lista_num: ", lista_num) 

Generator Part 2
Security needs the first 32 billion numbers in the series. The problem is, there is not enough memory on the platform to hold the list of values. 
Turn the previous program into a Python Generator using yield.
14. You will need to remove the “return” from the function and add yield. The yield must be inside the loop.
15. The yield will return the iterator.
16. Because you can no longer return the pair of the whole number and the ratio, you will need a global tuple to make sure the calling program receives both results.
17. Generate the first 500 numbers this way.


def crea_serie(ini,fin):
   global tupla_num
   lista_num = []
   numero = 0
   for num in range (ini, fin):
       # convierte el numero en string
       str_num = str(num)
       # print('str_num: ', str_num)
       sum = 0
       # print("len(str_num): ", len(str_num))
       # suma los numeros entre sí 
       for i in range(0, len(str_num)):
           sum += int(str_num[i])
       # calcula el ratio
       ratio = float(num) / float(sum)
       # crea la tupla
       tupla_num = (num, ratio)
       # print("type(tupla_num): ", type(tupla_num))
       # print("tupla_num: ", tupla_num)
       lista_num.append(tupla_num)
       yield
       
ini = 1
fin = 500
tupla_num = set()
generador_num = crea_serie(ini, fin)
print("type(generador_num): ", type(generador_num))

for num in range(ini, fin):
    print("num: ", num, " tupla_num: ", tupla_num)
    next(generador_num)

solucion del manual: /training_materials/jep/solutions/Generator-solved.py.
--------------------------------------------

------------------
Hands-On Exercise: Libraries
------------------
Using an Import Statement
solucion del manual: /training_materials/jep/solutions/Import-solved.py, while
--------------------------------------------
In this exercise, you will create a simple library and use it in a main program.
1. Create a library with two functions, loadfile() and userinput().
2. Use readlines() in loadfile to read a log file and return a list containing the contents of the file.
3. The userinput() function should read text from the console using input() and print it.

/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Python/PycharmProjects/pythonPUE/MyLibrary.py

def loadfile(filename):
    lines=[ ]
    file = open(filename,'rt')
    lines = file.readlines();
    file.close()
    return lines

def userinput(prompt):
   buffer = input(prompt)
   return buffer

4. Finally, write a main program that loads the library and calls the two functions.
the completed library program is in solutions/Library06AB.py.

/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Python/PycharmProjects/pythonPUE/myLibrary.py

import MyLibrary

filename='files/loudacre.log'
lines = MyLibrary.loadfile(filename)
for line in lines:
    print("line: ", line)

myinput = MyLibrary.userinput('Dime Algo: \n')
print("myInput: ", myinput)

solucion del manual: /training_materials/jep/solutions/Hello-solved.py
--------------------------------------------

Using a “from” Import Statement
In this exercise, you will adapt the previous program to use the "from" import method and its calling semantics.
5. Copy the main program and adapt it to use from libraryname import functions instead of import libraryname

/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Python/PycharmProjects/pythonPUE/myLibrary2.py

from MyLibrary import loadfile

filename='files/loudacre.log'
lines = loadfile(filename)
for line in lines:
    print("line: ", line)

from MyLibrary import userinput
myinput = userinput('Dime Algo: \n')
print("myInput: ", myinput)

solucion del manual: /training_materials/jep/solutions/From-solved.py.
--------------------------------------------

Path
In this exercise, you will use one method to expand the locations where libraries can be staged.
6. Relocate the library you created in the previous steps to a subdirectory and give it a new name. 
(In the example, the Library06AB.py module was copied into a subdirectory under /solutions called /library and renamed to pathlibdemo.py).
7. Use sys.path.append() within the main program to update the search path so that Python can find the library file again.

/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Python/PycharmProjects/pythonPUE/library/MyLibrary.py

def loadfile(filename):
    lines=[ ]
    file = open(filename,'rt')
    lines = file.readlines();
    file.close()
    return lines

def userinput(prompt):
   buffer = input(prompt)
   return buffer


/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Python/PycharmProjects/pythonPUE/myLibrary3.py

import sys
sys.path.append('/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Python/PycharmProjects/pythonPUE/library')

from MyLibrary2 import loadfile

filename='files/loudacre.log'
lines = loadfile(filename)
for line in lines:
    print("line: ", line)

from MyLibrary2 import userinput
myinput = userinput('Dime Algo: \n')
print("myInput: ", myinput)


solucion del manual: /training_materials/jep/solutions/Path-solved.py file, 
while the library program is in solutions/library/pathlibdemo.py.
--------------------------------------------

Arguments
In this exercise, you will process arguments from the command line. For example, you might enter:
$ python myprogram.py Titanic 4400 connected
8. Load any libraries you need to access the system environment.
9. Discover how many command line arguments were passed in.
10. Iterate over the arguments and print them.

/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Python/PycharmProjects/pythonPUE/myProgram.py

import sys
arguments = len(sys.argv)

for i in range(0,arguments):
    print("argv[",i,"] ", sys.argv[i])

for i in range(0,arguments):
    print("argv[",i,"] ", type(sys.argv[i]), sys.argv[i])

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Python/PycharmProjects/pythonPUE$ python3 myProgram.py Titanic 4400 connected 
argv[ 0 ]  myProgram.py
argv[ 1 ]  Titanic
argv[ 2 ]  4400
argv[ 3 ]  connected
argv[ 0 ]  <class 'str'> myProgram.py
argv[ 1 ]  <class 'str'> Titanic
argv[ 2 ]  <class 'str'> 4400
argv[ 3 ]  <class 'str'> connected

solucion del manual: /training_materials/jep/solutions/Args-solved.py file.
--------------------------------------------

RegEx
Below is a Return Authorization form that arrived water-damaged. 
As you can see, the only bits of information that are still readable are the "4" and the "b" in the Device Unique ID. 
We know that the device has been having issues and likely logged a restart record in the data/loudacre.log file. 

Device Unique ID: xxxxxxxx-4xxx-xbxx-xxxx-xxxxxxxxxxxx

Your challenge is to locate any records based on only these two pieces of information, and hopefully recover the unique Device ID so that the RA record can be found. 
To accomplish this, you will use the Python re (Regular Expression) library. 
Refer to the completed program as necessary for help with using Regular Expressions in Python.
11. The program will read the loudacre.log file, then use a regular expression to search the list of Unique IDs for a match.

12. The phone we are looking for has a “4” in the first column of the second part of the unique ID, 
and a “b” in the second column of the third part of the unique ID.
xxxxxxxx-4xxx-xbxx-xxxx-xxxxxxxxxxxx

13. Read loudacre.log and separate out the Unique Device ID field.
14. Import the re library, and craft a regular expression to find records matching the criteria.
15. Iterate over the records. Print the entire record that matches.

'       0-   1-   2-   3-           4'
'12345678-1234-1234-1234-123456789012'
'xxxxxxxx-4xxx-xbxx-xxxx-xxxxxxxxxxxx'

'6aaeca81-f279-4322-b23e-dbb2d7f4095c', 

xxxxxxxx-4xxx-xb
[0-9abcdef]*8-4[0-9abcdef][0-9abcdef][0-9abcdef]-[0-9abcdef]b

import re

lines = [ ]
filename = 'files/loudacre.log'
file = open(filename,'rt')
lines = file.readlines()
file.close()

record = [ ]
lst_device = [ ]
for line in lines:
    record=line.split(',')
#2 - Unique device ID:
    lst_device.append(record[2])

#print("lst_device: ", lst_device)
print("len(lst_device):", len(lst_device))

#pattern = re.compile("[0-9abcdef]*8-4[0-9abcdef][0-9abcdef][0-9abcdef]-[0-9abcdef]b[0-9abcdef][0-9abcdef]-[0-9abcdef]*4-[0-9abcdef]*12")

pattern = re.compile("[0-9abcdef]*8-4[0-9abcdef][0-9abcdef][0-9abcdef]-[0-9abcdef]b")
for i in lst_device:
    results = pattern.match(i)
    if results != None:
        print('xxxxxxxx-4xxx-xbxx-xxxx-xxxxxxxxxxxx')
        print(i)
   
len(lst_device): 500
xxxxxxxx-4xxx-xbxx-xxxx-xxxxxxxxxxxx
10c2d6b8-4a7d-4b22-8aaa-01037f388e4a

solucion del manual: /training_materials/jep/solutions/RegEx-solved.py.
--------------------------------------------
Just Enough Scala
==================
(base) hadoop@sc-ubuntu-20-04-2-lts:~$ which scala
/usr/bin/scala
(base) hadoop@sc-ubuntu-20-04-2-lts:~$ scala
Welcome to Scala 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_231).
Type in expressions for evaluation. Or try :help.

scala> (base) hadoop@sc-ubuntu-20-04-2-lts:~$ help
GNU bash, version 5.0.17(1)-release (x86_64-pc-linux-gnu)
These shell commands are defined internally.  Type `help' to see this list.
Type `help name' to find out more about the function `name'.
Use `info bash' to find out more about the shell in general.
Use `man -k' or `info' to find out more about commands not in this list.

A star (*) next to a name means that the command is disabled.

 job_spec [&]                                                               history [-c] [-d offset] [n] or history -anrw [filename] or history -ps>
 (( expression ))                                                           if COMMANDS; then COMMANDS; [ elif COMMANDS; then COMMANDS; ]... [ else>
 . filename [arguments]                                                     jobs [-lnprs] [jobspec ...] or jobs -x command [args]
 :                                                                          kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [>
 [ arg... ]                                                                 let arg [arg ...]
 [[ expression ]]                                                           local [option] name[=value] ...
 alias [-p] [name[=value] ... ]                                             logout [n]
 bg [job_spec ...]                                                          mapfile [-d delim] [-n count] [-O origin] [-s count] [-t] [-u fd] [-C c>
 bind [-lpsvPSVX] [-m keymap] [-f filename] [-q name] [-u name] [-r keyse>  popd [-n] [+N | -N]
 break [n]                                                                  printf [-v var] format [arguments]
 builtin [shell-builtin [arg ...]]                                          pushd [-n] [+N | -N | dir]
 caller [expr]                                                              pwd [-LP]
 case WORD in [PATTERN [| PATTERN]...) COMMANDS ;;]... esac                 read [-ers] [-a array] [-d delim] [-i text] [-n nchars] [-N nchars] [-p>
 cd [-L|[-P [-e]] [-@]] [dir]                                               readarray [-d delim] [-n count] [-O origin] [-s count] [-t] [-u fd] [-C>
 command [-pVv] command [arg ...]                                           readonly [-aAf] [name[=value] ...] or readonly -p
 compgen [-abcdefgjksuv] [-o option] [-A action] [-G globpat] [-W wordlis>  return [n]
 complete [-abcdefgjksuv] [-pr] [-DEI] [-o option] [-A action] [-G globpa>  select NAME [in WORDS ... ;] do COMMANDS; done
 compopt [-o|+o option] [-DEI] [name ...]                                   set [-abefhkmnptuvxBCHP] [-o option-name] [--] [arg ...]
 continue [n]                                                               shift [n]
 coproc [NAME] command [redirections]                                       shopt [-pqsu] [-o] [optname ...]
 declare [-aAfFgilnrtux] [-p] [name[=value] ...]                            source filename [arguments]
 dirs [-clpv] [+N] [-N]                                                     suspend [-f]
 disown [-h] [-ar] [jobspec ... | pid ...]                                  test [expr]
 echo [-neE] [arg ...]                                                      time [-p] pipeline
 enable [-a] [-dnps] [-f filename] [name ...]                               times
 eval [arg ...]                                                             trap [-lp] [[arg] signal_spec ...]
 exec [-cl] [-a name] [command [arguments ...]] [redirection ...]           true
 exit [n]                                                                   type [-afptP] name [name ...]
 export [-fn] [name[=value] ...] or export -p                               typeset [-aAfFgilnrtux] [-p] name[=value] ...
 false                                                                      ulimit [-SHabcdefiklmnpqrstuvxPT] [limit]
 fc [-e ename] [-lnr] [first] [last] or fc -s [pat=rep] [command]           umask [-p] [-S] [mode]
 fg [job_spec]                                                              unalias [-a] name [name ...]
 for NAME [in WORDS ... ] ; do COMMANDS; done                               unset [-f] [-v] [-n] [name ...]
 for (( exp1; exp2; exp3 )); do COMMANDS; done                              until COMMANDS; do COMMANDS; done
 function name { COMMANDS ; } or name () { COMMANDS ; }                     variables - Names and meanings of some shell variables
 getopts optstring name [arg]                                               wait [-fn] [id ...]
 hash [-lr] [-p pathname] [-dt] [name ...]                                  while COMMANDS; do COMMANDS; done
 help [-dms] [pattern ...]                                                  { COMMANDS ; }

- Scala creates result variables automatically
  – res0, res1, res2,… resN

scala> val myInt = 123
myInt: Int = 123

scala> myInt
res0: Int = 123

scala> println(res0)
123

scala> var str: String = scala.io.StdIn.readLine("Enter:\n")
Enter:
str: String = Hi

scala> str
res1: String = Hi

scala> print(res1)
Hi

scala> print(str)
Hi

scala> val formatStr = "Temperature range is %d to %f celsius"
formatStr: String = Temperature range is %d to %f celsius

scala> print(formatStr.format(24, 31.24))
Temperature range is 24 to 31.240000 celsius
scala> import scala.io.Source
import scala.io.Source

scala> val filename: String="files/loudacre.log"
filename: String = files/loudacre.log

scala> val buffer = Source.fromFile(filename)
buffer: scala.io.BufferedSource = non-empty iterator

scala> buffer.foreach(print)
2014-03-15:10:10:20,iFruit 1,6474caf1-7bbf-4594-a526-9ba8ea82e151,0,15,71,77,0,40,TRUE,enabled,connected,37.90310537,-121.5614513
.....
.....

otra forma equivalente:

scala> val filename: String="files/loudacre.log"
filename: String = files/loudacre.log

scala> Source.fromFile(filename).foreach(print)
2014-03-15:10:10:20,iFruit 1,6474caf1-7bbf-4594-a526-9ba8ea82e151,0,15,71,77,0,40,TRUE,enabled,connected,37.90310537,-121.5614513
.....
.....


scala> Source.fromFile(filename)
res0: scala.io.BufferedSource = non-empty iterator

base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ vi ListPhones.scala
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ cat ListPhones.scala 
object ListPhones {
def main(args: Array[String]) {
    println("MeToo")
    println("Titanic")
    println("Ronin")
}
}

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scalac ListPhones.scala 
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ ls
/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src
total 24K
4,0K drwxrwxr-x 2 hadoop hadoop 4,0K oct  7 16:10  .
4,0K -rw-rw-r-- 1 hadoop hadoop  677 oct  7 16:10 'ListPhones$.class'
4,0K -rw-rw-r-- 1 hadoop hadoop  586 oct  7 16:10  ListPhones.class
4,0K -rw-rw-r-- 1 hadoop hadoop  121 oct  7 16:09  ListPhones.scala
4,0K drwxrwxr-x 5 hadoop hadoop 4,0K oct  7 15:33  ..
4,0K -rw-rw-r-- 1 hadoop hadoop   19 oct  7 15:28  myClass.scala
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala ListPhones
MeToo
Titanic
Ronin

import scala.io.Source
Source.fromFile(datadir + "files/loudacre.log").foreach(print)

user training 
password training
root password training
eclipse workspace: /home/training/workspace

echo $SCA
/home/training/training_materials/jes

echo $SCADATA
/home/training/training_materials/jes/data

------------------
Hands-On Exercise: Working with the VM and Scala Shell
------------------
Working in the Scala Shell
3. Scala ships with an interactive interpreter called the Scala shell. 
It is a REPL, or (Read-Evaluate-Print Loop) interpreter.

Many of the Scala exercises can be completed using the Scala shell. A nice feature of
Scala is that code written in the shell can be used without modification outside the shell by pasting it into a file and compiling.
4. Start the Scala shell from the command line by typing the following command into the terminal window, and then pressing the [ENTER] key: $ scala

5. You will enter Scala code into the shell, but the shell also supports directive commands; 
they generally begin with a colon, for example, :directive. View the list of directive commands:
scala> :help

6. The Scala shell provides completion and hints when the [TAB] key is pressed.
Try it:

<<TAB>>

7. Quit the shell:
scala> :quit

Working with Scala Program Files
You can use the Scala shell for simpler solutions, other problems will require writing programs that need to be compiled and run. 
Use the .scala extension when naming files that contain Scala programs.
Compile a .scala file using the scalac command:

/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE

$ scalac src/ListPhones.scala

Al compilar crea los siguientes ficheros:
(los crea en el sitio donde se compile)
$ ls 
4,0K -rw-rw-r-- 1 hadoop hadoop  677 oct  7 21:09 'ListPhones$.class'
4,0K -rw-rw-r-- 1 hadoop hadoop  586 oct  7 21:09  ListPhones.class

Run the program from the command line using the scala command:
$ scala ListaPhones

Alternately, you can run the program by using the :load command from within the
Scala shell:

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala
Welcome to Scala 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_231).
Type in expressions for evaluation. Or try :help.

scala> :load ListPhones.scala
Loading ListPhones.scala...
defined object ListPhones

It is useful to keep two terminal windows open: one for the Scala shell and the
other for working with the operating system, editing files, and compiling code. This
is helpful because you cannot easily edit a file from within the Scala shell. Rather
than repeatedly entering and exiting the shell, just keep two terminal windows open

------------------
Hands-On Exercise: Performing Basic Input/Output
------------------
Exploring the Course Materials
1. In your terminal window, change to the main course directory and use the Linux
ls command to examine the contents of this directory and its subdirectories.

$ cd $SCA
$ ls
$ ls data

2. The exercises directory contains one subdirectory for each exercise. 
For this exercise, you will find the exercise solutions in the .scala.txt and .scala
files of the using_scala directory. 
Change to the solution (or hints) subdirectory for this exercise:

$ cd $SCA/exercises/using_scala/solution
$ ls

Writing and Running Code in the Scala Shell
3. Open a new terminal window and enter:
$ scala

The Scala shell enables you to have an interactive session with Scala. 
You will use this to explore some of the basic principles of the Scala language.

4. Create an immutable variable called ab and assign the integer value 25 to it:
scala> val ab = 25
ab: Int = 25

The name, type, and value of the variable are displayed as confirmation.

5. Enter the variable as a command:
scala> ab
res0: Int = 25

Note two things: first, that a variable entered without any command returns its value, 
because all commands in Scala are expressions that return a value; 
and that the return value is assigned to an automatically created result variable, res0.

6. Use the built-in print function to display the value of ab:
scala> print(ab)
25

7. Try entering a string:
scala> "Hello, Scala"
res2: String = Hello, Scala

Entering a literal like this also returns a value, and the value is assigned to a result variable. 
(The number assigned to the result variable, for example res2, 
will depend on how many result variables Scala has created in this session.)

8. Try entering an invalid command; enter the print function without the final parenthesis:

scala> print(ab
     |

9. Note the indented vertical bar prompt. This means Scala detected that the
command is not yet complete, and you can continue typing. Or, if you want to
cancel entry of the command, hit [ENTER] twice:

scala> print(ab
     | 
     | 
You typed two blank lines.  Starting a new command.

scala> type ab
     | 
     | ;
defined type alias ab

10. Finally, try referencing an invalid variable:
scala> print(xy)
<console>:12: error: not found: value xy
       print(xy)
             ^

Note that the caret (^) indicates the point in the entered code where the error was encountered. 
This can help you determine which part of a complex line of code is causing the syntax error.

Writing, Compiling, and Running a Scala Program without Eclipse

16. Return to the terminal window where you were using the Linux command line earlier. 
Use an editor of your choice to edit the file HelloScala.scala in exercise directory: 
$SCA/exercises/using_scala/solution (or hints).

17. Enter and save the following code into the file HelloScala.scala 
(you can copy and paste the code below, if you prefer):

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ cat HelloScala.scala 
object HelloScala {
  def main(args: Array[String]) {
      println("Hello, Scala!")
  }
}

18. Compile the program using the scalac compiler:
$ scalac HelloScala.scala
Compilation of the source file produces a class file, HelloScala.class. (It
also creates a companion class named HelloScala$.class.)

19. Run the program by specifying the main class name:
scala HelloScala
Hello, Scala!

Introducing Loudacre
Loudacre is a (fictional) mobile phone carrier. 
In many of the exercises in this course you will use Loudacre device status data from mobile phones.
Whenever a phone has to do a soft or hard restart, it gathers error information and
sends it back to Loudacre's central facility where the data is collected for analysis.
Start by examining the log file at $SCADATA/loudacre.log using an editor of your choice.

file description
record description
0 - Date Time:
1 - Model name and number:
2 - Unique device ID:
3 - Device temperature (Celsius):
4 - Ambient temperature (Celsius):
5 - Battery available (percent):
6 - Signal strength (percent):
7 - CPU utilization (percent):
8 - RAM memory usage (percent):
9 - GPS status (enabled=TRUE, disabled=FALSE):
10 - Bluetooth status (enabled, disabled, or connected):
11 - WiFi status (enabled, disabled, or connected):
12 - Latitude:
13 - Longitude:

2014-03-15:10:10:20,iFruit 1,6474caf1-7bbf-4594-a526-9ba8ea82e151,0,15,71,77,0,40,TRUE,enabled,connected,37.90310537,-121.5614513
2014-03-15:10:10:20,iFruit 2,5c2d40d8-b1e0-4c2a-b050-06ca6c590741,1,28,66,67,40,49,TRUE,disabled,connected,34.12789546,-108.9681595

Reading and Displaying a Data File

20. Enter the following code in the Scala shell to print out each line of the loudacre.log file.

scala> import scala.io.Source
import scala.io.Source

scala> Source.fromFile( "../files/loudacre.log").foreach(print)
2014-03-15:10:10:20,iFruit 1,6474caf1-7bbf-4594-a526-9ba8ea82e151,0,15,71,77,0,40,TRUE,enabled,connected,37.90310537,-121.5614513
2014-03-15:10:10:20,iFruit 2,5c2d40d8-b1e0-4c2a-b050-06ca6c590741,1,28,66,67,40,49,TRUE,disabled,connected,34.12789546,-108.9681595

------------------
Hands-On Exercise: Exploring Scala Variables and Typing
------------------


In this exercise, you will interactively explore variables in the Scala shell and write a program to extract fields from a single data record
Exploring Mutability, Reassignment, and Redefinition of Variables

1. Create an immutable variable and reassign a value to it. What happens? No puedo reasignar una VAL porque es IMMUTABLE

scala> val ambientTemp = 19
ambientTemp: Int = 19

scala> ambientTemp = 47
<console>:12: error: reassignment to val
       ambientTemp = 47
                   ^
2. Redefine the variable and assign a new value having an inferred type of Double:

scala> val ambientTemp = 27.0
ambientTemp: Double = 27.0

3. Create a mutable variable with an inferred type of Int. 

scala> val ambientTemp = 19
ambientTemp: Int = 19

scala> ambientTemp = 47
<console>:12: error: reassignment to val
       ambientTemp = 47
                   ^

scala> val ambientTemp = 27.0
ambientTemp: Double = 27.0

scala> var deviceTemp = 21
deviceTemp: Int = 21

What happens when you assign a value of a different type, for example, a Double? What happens: Puedo reasignar una VAR porque es MUTABLE pero da un error por tipo

scala> ambientTemp.getClass
res0: Class[Double] = double

scala> deviceTemp.getClass
res1: Class[Int] = int

scala> deviceTemp = ambientTemp
<console>:13: error: type mismatch;
 found   : Double
 required: Int
       deviceTemp = ambientTemp
                    ^

when you try to change the value to a new value of the same type? NO PROBLEM

scala> deviceTemp = 45
deviceTemp: Int = 45


4. It is possible to redefine the mutable variable with a different inferred type: NO PROBLEM

scala> var deviceTemp = 33.1 
deviceTemp: Double = 33.1

5. You can define variables of explicit types:

scala> var deviceTemp = 33.1
deviceTemp: Double = 33.1

scala> val phoneModel: String = "Sorrento"
phoneModel: String = Sorrento

scala> val temp: Int = 30
temp: Int = 30

scala> val location: Double = 33.1765
location: Double = 33.1765

solucion del manual: /training_materials/jes/solutions/Mutability.scala.txt
--------------------------------------------

Working with Numeric Variables

6. In this exercise, you will use the Scala shell to explore integer and floating point types by converting Celsius temperatures to Fahrenheit.

Note that the variables and final result are all integers. c_to_f has the value 1,
causing the conversion to be incorrect with tempF = 59 degrees Fahrenheit.


scala> val tempC = 27
tempC: Int = 27

scala> val c_to_f = 9/5
c_to_f: Int = 1

scala> val tempF = tempC * c_to_f + 32
tempF: Int = 59

7. Repeat the calculation using floating point numbers to correctly convert 27 degrees Celsius to 80.6 degrees Fahrenheit.

scala> val tempC = 27.0
tempC: Double = 27.0

scala> val c_to_f =  9.0/5.0
c_to_f: Double = 1.8

scala> val tempF = tempC * c_to_f + 32
tempF: Double = 80.6

8. You can find the type of the resulting object using getClass: 

scala> println(tempC.getClass, tempF.getClass)
(double,double)

solucion del manual: /training_materials/jes/solutions/Numeric.scala.txt.
--------------------------------------------

Using Scala Operators
Loudacre reports CPU utilization in percentages. Use Scala interactively to explore Scala’s operators.
9. Calculate the average of two CPU utilization percentages:

scala> val cpuT1 = 17
cpuT1: Int = 17

scala> val cpuT2 = 38
cpuT2: Int = 38

scala> val averageCPU = (cpuT1 + cpuT2) / 2 
averageCPU: Int = 27

10. The calculation returned an integer result of 27. Try assigning the variable type explicitly and note the implicit conversion

scala> val averageCPU: Double = (cpuT1 + cpuT2) / 2
averageCPU: Double = 27.0

11. Although the result type above has been converted to a Double, 27.0, the answer is not quite right - it should be 27.5. 
The result of dividing two integers will be an integer. 
Had either the dividend or the divisor been a Double, floating point division would have been performed, yielding 27.5 as the result.

scala> val averageCPU: Double = (cpuT1 + cpuT2) / 2.0
averageCPU: Double = 27.5

12. Print the result of the calculation. Note the implicit conversion of Double to String using the String + concatenation operator

scala> println("Average CPU: " + averageCPU)
Average CPU: 27.5

13. New and improved phone software can reduce CPU utilization by 12%. 
Using in- line operations, display the expected new CPU results after the improvement has been implemented.

scala> println("Expected CPU utilization: " + (cpuT2 - (cpuT2 * reduction) + "%"))
Expected CPU utilization: 33.44%

14. Increment CPU utilization at time 1 by 1% using the increment operator +=.  What happens?

scala> cpuT1 += 1
<console>:13: error: value += is not a member of Int
  Expression does not convert to assignment because receiver is not assignable.
       cpuT1 += 1
  

scala> cpuT1.getClass
res8: Class[Int] = int

15. How would you fix this problem? (If necessary, review the solution.)

scala> var cpuT1 = 17
cpuT1: Int = 17

scala> cpuT1 += 1

scala> println( cpuT1 )
18

16. Explore casting between different variable types using conversion methods.

scala> val cpuT1: Int = 35
cpuT1: Int = 35

scala> val cpuT2: Double = 37.23
cpuT2: Double = 37.23

scala> cpuT1.toDouble
res12: Double = 35.0

scala> cpuT2.toInt
res13: Int = 37

scala> cpuT1.getClass
res14: Class[Int] = int

scala> cpuT2.getClass
res15: Class[Double] = double

scala> cpuT2.toString
res16: String = 37.23

scala> cpuT2.getClass
res17: Class[Double] = double

scala> "cpuT1: " + cpuT1 + " cpuT2: " + cpuT2
res19: String = cpuT1: 35 cpuT2: 37.23

scala> val cpuT1: Int = 35
cpuT1: Int = 35

scala> cpuT1.toDouble
res20: Double = 35.0

scala> cpuT1.getClass
res21: Class[Int] = int

scala> val cpuT2: Double = 37.23
cpuT2: Double = 37.23

scala> cpuT2.toInt
res22: Int = 37

scala> cpuT2.getClass
res23: Class[Double] = double

scala> "cpuT1: " + cpuT1 + " cpuT2: " + cpuT2
res24: String = cpuT1: 35 cpuT2: 37.23

scala> cpuT2.toString
res25: String = 37.23

scala> cpuT2.getClass
res26: Class[Double] = double

scala> "cpuT1: " + cpuT1 + " cpuT2: " + cpuT2
res27: String = cpuT1: 35 cpuT2: 37.23

scala> cpuT1.getClass
res28: Class[Int] = int

scala> cpuT2.getClass
res29: Class[Double] = double

17. What other methods are available on the Int type? Enter cpuT1, followed by a dot, and then press [TAB] to see a list

scala> cpuT1.
!=   /    >=          ceil          getClass        isPosInfinity   isWhole     shortValue       toDegrees     toOctalString   underlying   
%    <    >>          compare       intValue        isValidByte     longValue   signum           toDouble      toRadians       until        
&    <<   >>>         compareTo     isInfinite      isValidChar     max         to               toFloat       toShort         |            
*    <=   ^           doubleValue   isInfinity      isValidInt      min         toBinaryString   toHexString   unary_+                      
+    ==   abs         floatValue    isNaN           isValidLong     round       toByte           toInt         unary_-                      
-    >    byteValue   floor         isNegInfinity   isValidShort    self        toChar           toLong        unary_~                      

solucion del manual: /training_materials/jes/solutions/Operators.scala.txt.
--------------------------------------------

Importing and Using Mathematical Functions
18. Math functions in Scala are provided through a built-in library called math. 

Try using the sqrt function.

scala> math.sqrt(192)
res30: Double = 13.856406460551018

19. If you plan to call the sqrt function often, it is useful to import the function to enable you to invoke it more simply as sqrt:

scala> import scala.math.sqrt
import scala.math.sqrt

scala> sqrt(192)
res31: Double = 13.856406460551018

solucion del manual: /training_materials/jes/solutions/MathFunctions.scala.txt.
--------------------------------------------

Working with Strings
20. Use the following single string to represent one record in the loudacre.log file. 
The record string is also available in the file

/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/files
vi sample_one_record.txt

val record = "2014-03-15:10:10:31, Titanic 4000, 1882b564-c7e0-4315-aa24-228c0155ee1b, 58, 36, 39, 31, 15, 0, TRUE, enabled, enabled, 37.819722,-122.478611"

21. Print out the record and the length of the record. The record should be 141 characters long.

scala> println(record)
2014-03-15:10:10:31, Titanic 4000, 1882b564-c7e0-4315-aa24-228c0155ee1b, 58, 36, 39, 31, 15, 0, TRUE, enabled, enabled, 37.819722,-122.478611

scala> record.length
res35: Int = 141

scala> println(record)
2014-03-15:10:10:31, Titanic 4000, 1882b564-c7e0-4315-aa24-228c0155ee1b, 58, 36, 39, 31, 15, 0, TRUE, enabled, enabled, 37.819722,-122.478611

22. Use the contains method to determine whether the model name Titanic is in the record.

scala> record.contains("Titanic")
res37: Boolean = true

23. Use indexOf to locate the index of the first character of Titanic.
scala> record.indexOf("Titanic")
res38: Int = 21

24. Use the substring method to output the model name in upper case using the toUpperCase method. 
Use function calls to compute the starting and ending indexes rather than hardcoding the values.

scala> record.substring(21,28).toUpperCase
res41: String = TITANIC

scala> val model_name = record.substring(21,28)
model_name: String = Titanic

scala> model_name.toUpperCase
res42: String = TITANIC

25. Get the four-digit model number that follows the model name and remember to skip the blank that separates the model name from the model number.

scala> val model = record.substring(21,33)
model: String = Titanic 4000

scala> val model_split = model.split(' ')
model_split: Array[String] = Array(Titanic, 4000)

scala> model_split
res50: Array[String] = Array(Titanic, 4000)

scala> model_split(0)
res51: String = Titanic

scala> model_split(1)
res52: String = 4000

scala> model_split(1).toInt
res53: Int = 4000

scala> model_split(1).getClass
res54: Class[_ <: String] = class java.lang.String

scala> model_split.getClass
res55: Class[_ <: Array[String]] = class [Ljava.lang.String;

scala> model_split(0).getClass
res56: Class[_ <: String] = class java.lang.String

scala> val model_number= model_split(1).toInt
model_number: Int = 4000

scala> model_number.getClass
res57: Class[Int] = int

solucion del manual: /training_materials/jes/solutions/Strings.scala.txt.
--------------------------------------------
Booleans
26. In this exercise you will extract the Bluetooth and WiFi fields from the record string and use boolean operators to test them.
The Bluetooth and WiFi status is in the fourth and third elements from the end of the record, respectively. 
Each field can contain one of three values: enabled, disabled, or connected.

27. Create a string as follows
scala> val record2 = "2014-03-15:10:10:31, Titanic 4000, 1882b564-c7e0-4315-aa24-228c0155ee1b, 58, 36, 39, 31, 15, 0, TRUE, disabled, enabled, 37.819722,- 122.478611"
record2: String = 2014-03-15:10:10:31, Titanic 4000, 1882b564-c7e0-4315-aa24-228c0155ee1b, 58, 36, 39, 31, 15, 0, TRUE, disabled, enabled, 37.819722,- 122.478611

28. You can use containsSlice to test whether a string is contained within another string

scala> record2.containsSlice("connected")
res60: Boolean = false

scala> record2.containsSlice("enabled")
res61: Boolean = true

scala> record2.containsSlice("disabled")
res62: Boolean = true

30. Use substring together with indexOfSlice to extract the Bluetooth state and store it in val bluetooth.

               # posición 10 - Bluetooth status (enabled, disabled, or connected):

scala> val bluetooth = record2.indexOfSlice("disabled")
bluetooth: Int = 102

scala> "disabled".length
res64: Int = 8

# extrae el substring("empieza la palabra disabled", (empieza la palabra disabled + longitud de "disabled"))
scala> val bluetooth = record2.substring(record2.indexOfSlice("disabled"), record2.indexOfSlice("disabled") + "disabled".length)
bluetooth: String = disabled

scala> println("Bluetooth: ", bluetooth);
(Bluetooth: ,disabled)

31. Perform similar steps to find the WiFi state and store it in val wifi.

               # posición 11 - WiFi status (enabled, disabled, or connected):
# extrae el substring("empieza la palabra enabled", (empieza la palabra enabled + longitud de "enabled"))
scala> val wifi = record2.substring(record2.indexOfSlice("enabled"), record2.indexOfSlice("enabled") + "enabled".length)
wifi: String = enabled

scala> println("WiFi: ", wifi)
(WiFi: ,enabled)

32. Use == to determine if Bluetooth and WiFi both have the same status.

scala> println("Bluetooth and WiFi have the same status: " + (bluetooth == wifi))
Bluetooth and WiFi have the same status: false

solucion del manual: /training_materials/jes/solutions/Boolean.scala.txt.
--------------------------------------------
Optional: Viewing the Scala API Documentation

33. Start Firefox using the icon in your virtual machine’s top menu.
34. Click the Scala API bookmark, or open local file URI: 

     https://www.scala-lang.org/api/current/

35. On the left is a list of packages and the classes they contain 

The first package displayed is the main scala package, containing the default set of types. 
Select one or more of the types covered in this chapter, such as Double or Boolean, and review the API.

------------------
Hands-On Exercise: Exploring Tuples, Lists, and Maps
------------------

Working with Tuples
1. Create the following tuple in the Scala shell. Verify its type with getClass.

scala> val phoneTuple = ("Titanic", "3000", "disabled", "connected")
phoneTuple: (String, String, String, String) = (Titanic,3000,disabled,connected)

scala> phoneTuple.getClass
res72: Class[_ <: (String, String, String, String)] = class scala.Tuple4

2. You can access elements of a tuple using the _n selector. Display the first and third elements of phoneTuple.

scala> phoneTuple._1
res73: String = Titanic

scala> phoneTuple._3
res74: String = disabled

3. You can also access elements of the tuple using phoneTuple.productElement(n), 
but keep in mind that n is zero-based when using this technique. 
Display the first and third elements of phoneTuple using this technique.

scala> phoneTuple.productElement(0)
res76: Any = Titanic

scala> phoneTuple.productElement(2)
res77: Any = disabled

4. Check the arity of the tuple.

scala> phoneTuple.productArity
res78: Int = 4

5. You can assign names to the elements in the tuple using the following technique.
Note that each of the four variables is assigned the value of the corresponding tuple element.

scala> def getPhoneInfo = ("Titanic", "3000", "disabled", "connected")
getPhoneInfo: (String, String, String, String)

scala> getPhoneInfo.getClass
res79: Class[_ <: (String, String, String, String)] = class scala.Tuple4


scala> val (modName, modNum, blueStat, wifiStat) = getPhoneInfo
modName: String = Titanic
modNum: String = 3000
blueStat: String = disabled
wifiStat: String = connected

scala> getPhoneInfo
res84: (String, String, String, String) = (Titanic,3000,disabled,connected)

scala> getPhoneInfo.getClass
res85: Class[_ <: (String, String, String, String)] = class scala.Tuple4

scala> print(
     | "Name: " + modName + "\n" +
     | "Number: " + modNum + "\n" +
     | "Bluetooth: " + blueStat + "\n" +
     | "WiFi: " + wifiStat + "\n")
Name: Titanic
Number: 3000
Bluetooth: disabled
WiFi: connected

Working with Lists
6. Start by converting the phoneList tuple to a list:

scala> val phoneList = phoneTuple.productIterator.toList
phoneList: List[Any] = List(Titanic, 3000, disabled, connected)

scala> phoneTuple.getClass
res82: Class[_ <: (String, String, String, String)] = class scala.Tuple4

scala> phoneList.getClass
res83: Class[_ <: List[Any]] = class scala.collection.immutable.$colon$colon

7. Using the [TAB] key, determine which type – the tuple or the list – has more methods?

scala> phoneTuple.
_1   _2   _3   _4   canEqual   copy   equals   hashCode   productArity   productElement   productIterator   productPrefix   toString

scala> phoneList.
++             compose         flatten           intersect            mkString           reduceRightOption   span           toSet           
++:            contains        fold              isDefinedAt          nonEmpty           repr                splitAt        toStream        
+:             containsSlice   foldLeft          isEmpty              orElse             reverse             startsWith     toString        
/:             copyToArray     foldRight         isTraversableAgain   padTo              reverseIterator     stringPrefix   toTraversable   
:+             copyToBuffer    forall            iterator             par                reverseMap          sum            toVector        
::             corresponds     foreach           last                 partition          reverse_:::         tail           transpose       
:::            count           genericBuilder    lastIndexOf          patch              runWith             tails          union           
:\             diff            groupBy           lastIndexOfSlice     permutations       sameElements        take           unzip           
WithFilter     distinct        grouped           lastIndexWhere       prefixLength       scan                takeRight      unzip3          
addString      drop            hasDefiniteSize   lastOption           product            scanLeft            takeWhile      updated         
aggregate      dropRight       hashCode          length               productArity       scanRight           to             view            
andThen        dropWhile       head              lengthCompare        productElement     segmentLength       toArray        withFilter      
apply          endsWith        headOption        lift                 productIterator    seq                 toBuffer       zip             
applyOrElse    equals          indexOf           map                  productPrefix      size                toIndexedSeq   zipAll          
canEqual       exists          indexOfSlice      mapConserve          reduce             slice               toIterable     zipWithIndex    
collect        filter          indexWhere        max                  reduceLeft         sliding             toIterator                     
collectFirst   filterNot       indices           maxBy                reduceLeftOption   sortBy              toList                         
combinations   find            init              min                  reduceOption       sortWith            toMap                          
companion      flatMap         inits             minBy                reduceRight        sorted              toSeq                          

8. Let’s convert our list to a comma-separated list of values, which we could then import into a spreadsheet. 
You can use the map method to apply an operation to each member of the list.

scala> val csvStr1 = phoneList.map("\"" + _ + "\"").toString
csvStr1: String = List("Titanic", "3000", "disabled", "connected")

scala> csvStr1.getClass
res86: Class[_ <: String] = class java.lang.String

However, did you notice that the println shows the collection type, List?
scala> println(csvStr1)
List("Titanic", "3000", "disabled", "connected")

9. You can use mkString to convert the list to a string and leave off the collection type.

scala> val csvStr2 = phoneList.map("\"" + _ + "\"").mkString
csvStr2: String = "Titanic""3000""disabled""connected"

scala> println(csvStr2)
"Titanic""3000""disabled""connected"

scala> csvStr2.getClass
res89: Class[_ <: String] = class java.lang.String

10. This is closer yet, but the values run together. Provide a comma separator as an argument to mkString to separate each value by a comma.

scala> val csvStr3 = phoneList.map("\"" + _ + "\"").mkString(", ")
csvStr3: String = "Titanic", "3000", "disabled", "connected"

scala> println(csvStr3)
"Titanic", "3000", "disabled", "connected"

scala> csvStr3.getClass
res91: Class[_ <: String] = class java.lang.String

solucion del manual: /training_materials/jes/solutions/TuplesToLists.scala.txt.
--------------------------------------------

Working with Maps
Loudacre support has received a report containing the model name of the phone in error, but the brand name has been omitted. 
They requested a command line tool to enable reverse lookup of the brand name based on the model name.
In this section you will create a command line tool that will use a Map to look up phone model numbers and return their corresponding manufacturer brand names.

11. Return to the terminal window where you were using the Linux command line earlier. 
Use an editor of your choice to edit the file ModelToBrand.scala in exercise directory: $SCA/exercises/collections/hints (or solution). 
If using Eclipse, you’ll work on either the ModelToBrandHints or ModelToBrand project (the latter represents the solution).

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ vi ModelToBrand.scala
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scalac ModelToBrand.scala
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala ModelToBrand
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ cat ModelToBrand.scala
object ModelToBrand {
  def main(args: Array[String]) {
      if (args.length > 0) {
         println("Model name entered: " + args(0))
      }
  }
}
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala ModelToBrand 3A
Model name entered: 3A

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala ModelToBrand 4kk
Model name entered: 4kk

12. Expand the program to define a Map that captures all of the following relationships:

iFruit brand: Models 1, 2, 3, 3A, 4, 4A, 5
Ronin brand: Models S1, S2, S3
Sorrento brand: Models F01L, F11L, F21L, F23L, F33LL, F41L

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ cat Model2ToBrand.scala

object Model2ToBrand {
  def main(args: Array[String]) {
      val mod2b = Map( ("1","iFruit"),("2","iFruit"),("3","iFruit"),
                       ("3A","iFruit"),("4","iFruit"),("4A","iFruit"),
                       ("5","iFruit"), ("S1","Ronin"), ("S2","Ronin"),
                       ("S3", "Ronin"), ("F01L","Sorrento"), ("F11L","Sorrento"),
                       ("F21L","Sorrento"), ("F23L","Sorrento"), ("F33LL","Sorrento"), 
                       ("F41L","Sorrento")  )
 
  }
}
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scalac Model2ToBrand.scala
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala Model2ToBrand 3A
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala Model2ToBrand 3
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala Model2ToBrand

13. If the model is not recognized, print out “Record not found.”

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ cat Model3ToBrand.scala

object Model3ToBrand {
  def main(args: Array[String]) {
      val mod2b = Map( ("1","iFruit"),("2","iFruit"),("3","iFruit"),
                       ("3A","iFruit"),("4","iFruit"),("4A","iFruit"),
                       ("5","iFruit"), ("S1","Ronin"), ("S2","Ronin"),
                       ("S3", "Ronin"), ("F01L","Sorrento"), ("F11L","Sorrento"),
                       ("F21L","Sorrento"), ("F23L","Sorrento"), ("F33LL","Sorrento"), 
                       ("F41L","Sorrento")  )
 
      if(mod2b.contains(args(0) )) {  
         println(" Brand is: " + mod2b(args(0)) ) 
      } else { 
         println(" Record not found ") 
      }
  }
}

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala Model3ToBrand 3A
 Brand is: iFruit

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala Model3ToBrand 4kk
 Record not found 

14. If the program is called with no input arguments (that is, if the length of the args array is 0), then print out a list of keys (models) and a list of values (brands).

• Hint: Generate a list of the Map’s values (brands) and convert it to a Set. 
Because the Set collection ensures unique values, it will automatically remove the duplicate brands.

object Model4ToBrand {
  def main(args: Array[String]) {
      val mod2b = Map( ("1","iFruit"),("2","iFruit"),("3","iFruit"),
                       ("3A","iFruit"),("4","iFruit"),("4A","iFruit"),
                       ("5","iFruit"), ("S1","Ronin"), ("S2","Ronin"),
                       ("S3", "Ronin"), ("F01L","Sorrento"), ("F11L","Sorrento"),
                       ("F21L","Sorrento"), ("F23L","Sorrento"), ("F33LL","Sorrento"), 
                       ("F41L","Sorrento")  )
 
      if(mod2b.contains(args(0) )) {  
         println(" Brand is mod2b.contains(args(0)): " + mod2b(args(0)) ) 
      } else { 
         println(" Record not found ") 
      }

     println("mod2b.keys: " + mod2b.keys)
     val mySet = mod2b.values.toSet
     println("mod2b.values.toSet: " + mySet)
  }
}

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala Model4ToBrand 3A
 Brand is mod2b.contains(args(0)): iFruit
mod2b.keys: Set(S1, F01L, 4, 4A, F23L, F41L, F11L, 5, 1, S3, F21L, 2, 3A, F33LL, S2, 3)
mod2b.values.toSet: Set(Ronin, Sorrento, iFruit)

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala Model4ToBrand 3kk
 Record not found 
mod2b.keys: Set(S1, F01L, 4, 4A, F23L, F41L, F11L, 5, 1, S3, F21L, 2, 3A, F33LL, S2, 3)
mod2b.values.toSet: Set(Ronin, Sorrento, iFruit)

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ cat Model4ToBrand.scala 

object Model4ToBrand {
  def main(args: Array[String]) {
      val mod2b = Map( ("1","iFruit"),("2","iFruit"),("3","iFruit"),
                       ("3A","iFruit"),("4","iFruit"),("4A","iFruit"),
                       ("5","iFruit"), ("S1","Ronin"), ("S2","Ronin"),
                       ("S3", "Ronin"), ("F01L","Sorrento"), ("F11L","Sorrento"),
                       ("F21L","Sorrento"), ("F23L","Sorrento"), ("F33LL","Sorrento"), 
                       ("F41L","Sorrento")  )
 
      if(mod2b.contains(args(0) )) {  
         println(" Brand is mod2b.contains(args(0)): " + mod2b(args(0)) ) 
      } else { 
         println(" Record not found ") 
      }

     println("mod2b.keys: " + mod2b.keys)
     val mySet = mod2b.values.toSet
     println("mod2b.values.toSet: " + mySet)
  }
}

Optional: Viewing Collection Types in the Scala API Documentation

15. In Firefox, click the Scala API bookmark, or open the local file URI:
https://www.scala-lang.org/api/current/scala/index.html

16. On the left, scroll down to the scala.collection.immutable package.

(Hint: click display packages only to view just the packages; 
       click display all entities to return to a view of all packages and their contents.)
       
https://www.scala-lang.org/api/current/scala/collection/immutable/index.html

17. Review the API for some of the collection types discussed in this chapter, such as List or Map.

https://www.scala-lang.org/api/current/scala/collection/immutable/List.html
https://www.scala-lang.org/api/current/scala/collection/immutable/Map.html

solucion del manual: /training_materials/jes/solutions/ModelToBrand.scala.
--------------------------------------------
------------------
Hands-On Exercise: Iterating through Data Efficiently
------------------
In this exercise, you will explore flow control and program structure in Scala.
Comparing Approaches to Iteration
In this section, you will process a collection of floating point numbers. 
The numbers represent phone charge percentages, and you will convert them to milliamps per hour (mAh).
You will repeat the same task using different approaches in order to compare them.

1. Start by defining a function to convert percentage to mAh:
2. Define a list of phone battery charge percentages:
3. Classic iterative approach: use a for loop with a range to iterate over the list and calculate and print the results.

scala> def mAh(percent: Double) = (percent/100) * 5400
mAh: (percent: Double)Double

scala> val phoneBattery = List(82.3, 31.6, 72.5, 64.7)
phoneBattery: List[Double] = List(82.3, 31.6, 72.5, 64.7)

scala> for (i <- 0 until phoneBattery.length)
     |     println("Battery[" + phoneBattery(i) + "]" + "% = " + mAh(phoneBattery(i)) + " mAh")
Battery[82.3]% = 4444.2 mAh
Battery[31.6]% = 1706.4 mAh
Battery[72.5]% = 3915.0 mAh
Battery[64.7]% = 3493.8 mAh

4. Iteration with a generator: instead of using a counting variable with a range, use the List with the for generator.
scala> for (percent <- phoneBattery) println(percent + "% = " + mAh(percent) + " mAh")
82.3% = 4444.2 mAh
31.6% = 1706.4 mAh
72.5% = 3915.0 mAh
64.7% = 3493.8 mAh

5. List comprehension: instead of having the for loop print out each value, have it return a new List containing the converted values by using for and yield.

scala> val phoneBattery = List(82.3, 31.6, 72.5, 64.7)
phoneBattery: List[Double] = List(82.3, 31.6, 72.5, 64.7)

scala> val myList = for (percent <- phoneBattery) yield mAh(percent)
myList: List[Double] = List(4444.2, 1706.4, 3915.0, 3493.8)

scala> myList.getClass
res0: Class[_ <: List[Double]] = class scala.collection.immutable.$colon$colon

scala> println(myList)
List(4444.2, 1706.4, 3915.0, 3493.8)

6. Now you can loop through the resulting list to display the values any number of ways, such as:
scala> myList.foreach(println)
4444.2
1706.4
3915.0
3493.8

7. The map method can be used as a convenience method for for and yield:

scala> val myListMap = phoneBattery.map(percent => mAh(percent))
myListMap: List[Double] = List(4444.2, 1706.4, 3915.0, 3493.8)

scala> myListMap.getClass
res4: Class[_ <: List[Double]] = class scala.collection.immutable.$colon$colon

scala> myListMap.foreach(println)
4444.2
1706.4
3915.0
3493.8

8. Try the same code as in the preceding step, substituting a different name for percent. 
This works because the parameter name need not match the parameter name given when mAh was defined.

scala> val myListMapAnyName = phoneBattery.map(anyName => mAh(anyName))
myListMapAnyName: List[Double] = List(4444.2, 1706.4, 3915.0, 3493.8)

scala> myListMapAnyName.foreach(println)
4444.2
1706.4
3915.0
3493.8

9. Try using a placeholder parameter in the above map call as a shortcut:
scala> val myListMapPlaceHolder = phoneBattery.map(mAh(_))
myListMapPlaceHolder: List[Double] = List(4444.2, 1706.4, 3915.0, 3493.8)

scala> myListMapPlaceHolder.getClass
res0: Class[_ <: List[Double]] = class scala.collection.immutable.$colon$colon

scala> myListMapPlaceHolder.foreach(println)
4444.2
1706.4
3915.0
3493.8

10. Or as an even more succinct shortcut:

scala>  val myListShortCut = phoneBattery.map(mAh)
myListShortCut: List[Double] = List(4444.2, 1706.4, 3915.0, 3493.8)

scala> myListShortCut.getClass
res2: Class[_ <: List[Double]] = class scala.collection.immutable.$colon$colon

scala> myListShortCut.foreach(println)
4444.2
1706.4
3915.0
3493.8

11. Anonymous function: another option, rather than defining and using the named function mAh, 
is to pass the conversion function as an anonymous function.

scala> val myListAnonymousFunc = phoneBattery.map(percent => percent / 100 * 5400)
myListAnonymousFunc: List[Double] = List(4444.2, 1706.4, 3915.0, 3493.8)

scala> myListAnonymousFunc.foreach(println)
4444.2
1706.4
3915.0
3493.8

scala> myListAnonymousFunc.getClass
res1: Class[_ <: List[Double]] = class scala.collection.immutable.$colon$colon

12. You can also use a placeholder parameter in an anonymous function:

scala> val myListAnonymousFuncPlaceHolder = phoneBattery.map(_ / 100 * 5400)
myListAnonymousFuncPlaceHolder: List[Double] = List(4444.2, 1706.4, 3915.0, 3493.8)

scala> myListAnonymousFuncPlaceHolder.getClass
res2: Class[_ <: List[Double]] = class scala.collection.immutable.$colon$colon

scala> myListAnonymousFuncPlaceHolder.foreach(println)
4444.2
1706.4
3915.0
3493.8

Using Iterators
In this section, you will explore a List and an Iterator in the Scala shell.

13. Create a list called myBrands with the following values: iFruit, MeToo, Titanic, Ronin, Sorrento:
14. Now create an iterator myIt that refers to myBrands. Does it have items left in the iterator?
15. Use while code to step through the iterator twice. What do you think will happen? 
Will the second time work the same as the first?

scala> val myBrands = List("iFruit", "MeToo", "Titanic", "Ronin", "Sorrento")
myBrands: List[String] = List(iFruit, MeToo, Titanic, Ronin, Sorrento)

scala> var myIt = myBrands.toIterator
myIt: Iterator[String] = non-empty iterator

scala> myIt.hasNext
res5: Boolean = true

scala> while(myIt.hasNext) println(myIt.next)
iFruit
MeToo
Titanic
Ronin
Sorrento

scala> while(myIt.hasNext) println(myIt.next)

scala> myIt.hasNext
res8: Boolean = false

scala> while(myIt.hasNext) println(myIt.next)

16. Create a new iterator then check how many items are in the iterator using size.  
Do you predict this method will consume the iterator or not? Check by calling hasNext.

scala> val myIt = myBrands.toIterator
myIt: Iterator[String] = non-empty iterator

scala> myIt.hasNext
res13: Boolean = true

scala> while( myIt.hasNext ){ println(myIt.next) }
iFruit
MeToo
Titanic
Ronin
Sorrento

scala> while( myIt.hasNext ){ println(myIt.next) }

scala> myIt.size
res16: Int = 0

scala> myIt.hasNext
res17: Boolean = false

solucion del manual: /training_materials/jes/solutions/ExploreIterator.scala.txt
--------------------------------------------
Using for with a File Source Iterator
17. In an earlier exercise, you saw how to read and display the contents of a file using code like this:
     Source.fromFile(filename).foreach(print)

In this exercise you will continue working with the same data file, but in different ways.

18. Enter this code:
19. What type is fsource? Use getClass to find out.
20. Use a for loop to display the first 500 characters of the file:

scala> import scala.io.Source
import scala.io.Source

scala> val fname = "files/loudacre.log"
fname: String = files/loudacre.log

scala> var fsource = Source.fromFile(fname)
fsource: scala.io.BufferedSource = non-empty iterator

scala> fsource.getClass
res0: Class[_ <: scala.io.BufferedSource] = class scala.io.BufferedSource

scala> for (x <- 1 to 500) print(fsource.next)
2014-03-15:10:10:20,iFruit 1,6474caf1-7bbf-4594-a526-9ba8ea82e151,0,15,71,77,0,40,TRUE,enabled,connected,37.90310537,-121.5614513
2014-03-15:10:10:20,iFruit 2,5c2d40d8-b1e0-4c2a-b050-06ca6c590741,1,28,66,67,40,49,TRUE,disabled,connected,34.12789546,-108.9681595
2014-03-15:10:10:20,iFruit 3,27178d24-3a61-42f7-a784-e3263f25cc6f,1,30,91,89,41,17,TRUE,enabled,enabled,37.92489617,-122.2068682
2014-03-15:10:10:20,iFruit 3A,fe4674a7-0632-494a-aaf3-f5865a724e1c,0,21,95,73,19,62,TRUE,disabled,enabled,

21. Try repeating the 500 count for loop again. Is the output the same as the first time?
fsource is a character-based iterator over the file. Subsequent calls read from the last cursor location.

scala> for (x <- 1 to 500) print(fsource.next)
38.0180756,-120.0678602
2014-03-15:10:10:20,iFruit 5,86e4bd60-5c72-4558-a019-caf1de9d14f1,0,29,68,32,63,62,TRUE,enabled,connected,39.47088617,-119.6599261
2014-03-15:10:10:20,MeeToo 1.0,ef8c7564-0a1a-4650-a655-c8bbd5f8f943,0,31,63,70,39,27,TRUE,enabled,enabled,37.43210889,-121.4850296
2014-03-15:10:10:20,MeeToo 1.0,23eba027-b95a-4729-9a4b-a3cca51c5548,0,20,21,86,54,34,TRUE,enabled,enabled,39.43789083,-120.9389785
2014-03-15:10:10:20,MeeToo 1.0,16085fbf-cda5-4489-84b9-0fad888f9e7a,0,29,26,97,

scala> for (x <- 1 to 500) print(fsource.next)
8,11,TRUE,enabled,enabled,34.048762,-111.9288717
2014-03-15:10:10:20,MeeToo 1.0,e205ee4a-e7f8-44ba-bfb0-2f4a7a604e09,0,21,54,53,53,20,TRUE,enabled,enabled,39.53705419,-114.7519081
2014-03-15:10:10:20,MeeToo 1.0,ff375011-34f0-4758-bade-e68cea787115,1,29,87,98,37,24,TRUE,disabled,enabled,33.42232383,-111.3701289
2014-03-15:10:10:20,MeeToo 3.0,8316b507-7620-47aa-b56b-cae5cb2cd819,0,19,69,31,51,44,TRUE,enabled,disabled,33.4467594,-111.3653269
2014-03-15:10:10:20,MeeToo 4.1,673f7e4b-d52b-44fc-882

22. Use the getLines method to return a line-based iterator:
scala> var flines = Source.fromFile(fname).getLines
flines: Iterator[String] = non-empty iterator

scala> println(flines)
non-empty iterator

scala> flines.getClass
res5: Class[_ <: Iterator[String]] = class scala.io.BufferedSource$BufferedLineIterator

23. Print out the first line of the file: 
scala> println(flines.next)
2014-03-15:10:10:20,iFruit 1,6474caf1-7bbf-4594-a526-9ba8ea82e151,0,15,71,77,0,40,TRUE,enabled,connected,37.90310537,-121.5614513

scala> println(flines.next)
2014-03-15:10:10:20,iFruit 2,5c2d40d8-b1e0-4c2a-b050-06ca6c590741,1,28,66,67,40,49,TRUE,disabled,connected,34.12789546,-108.9681595

scala> println(flines.next)
2014-03-15:10:10:20,iFruit 3,27178d24-3a61-42f7-a784-e3263f25cc6f,1,30,91,89,41,17,TRUE,enabled,enabled,37.92489617,-122.2068682

24. Use flines to display the remaining lines in the file.
• Hint: There are two ways to do this, both equally valid.

scala> for(line <- flines) {println(line)}

scala> while( flines.hasNext ){ println(flines.next) }

25. What happens if you try to call flines.next now that you have already iterated through the whole file?

scala> println(flines.next)
java.util.NoSuchElementException: next on empty iterator
  at scala.collection.Iterator$$anon$2.next(Iterator.scala:39)
  at scala.collection.Iterator$$anon$2.next(Iterator.scala:37)
  at scala.io.BufferedSource$BufferedLineIterator.next(BufferedSource.scala:81)
  at scala.io.BufferedSource$BufferedLineIterator.next(BufferedSource.scala:66)
  ... 32 elided

solucion del manual: /training_materials/jes/solutions/FileIterator.scala.txt
--------------------------------------------

Parsing String Input Using split
The next task is to go through each line in the file and parse it to extract the Device ID field.
26. Start by exploring a single line of data in the Scala shell. 
Use the following single string to represent one record in the loudacre.log file.
• Note: The record string is also available to copy-and-paste in $SCADATA/sample_one_record.txt

scala> val record = "2014-03-15:10:10:31, Titanic 4000, 1882b564-c7e0-4315-aa24-228c0155ee1b, 58, 36, 39, 31, 15, 0, TRUE, enabled, enabled, 37.819722, -122.478611"
record: String = 2014-03-15:10:10:31, Titanic 4000, 1882b564-c7e0-4315-aa24-228c0155ee1b, 58, 36, 39, 31, 15, 0, TRUE, enabled, enabled, 37.819722, -122.478611

scala> record.getClass
res14: Class[_ <: String] = class java.lang.String

scala> record.size
res15: Int = 142

scala> record.length
res16: Int = 142

27. Now split the string into its respective fields, using the comma (,) as the delimiter character. 
The split method creates an array of values.

scala> val fields = record.split(',')
fields: Array[String] = Array(2014-03-15:10:10:31, " Titanic 4000", " 1882b564-c7e0-4315-aa24-228c0155ee1b", " 58", " 36", " 39", " 31", " 15", " 0", " TRUE", " enabled", " enabled", " 37.819722", " -122.478611")

28. What data type is returned by split? How many fields are there in fields?

scala> fields.getClass
res17: Class[_ <: Array[String]] = class [Ljava.lang.String;

scala> fields.length
res18: Int = 14

scala> fields.size
res19: Int = 14

29. Now write, compile, and run a program that displays the Device ID from each line of data in the loudacre.log data file.

Split.scala:

import scala.io.Source
object Split{
  def main(args: Array[String]) {
      val fname = "../files/loudacre.log"
      var flines = Source.fromFile(fname).getLines
      var device_ID = " "
      for(line <- flines) {
          var line_string: String = line                                // Use line to populate a String
          var line_split = line_string.split(",")                       // Buffer the results of split
          device_ID = (line_split(2))                                   // Print out field 2, the Unique ID
          println("Device ID: " + device_ID)                            // Device ID: 1882b564-c7e0-4315-aa24-228c0155ee1b
     }
  }
}

import scala.io.Source
object Split{
  def main(args: Array[String]) {
      val fname = "../files/loudacre.log"
      var flines = Source.fromFile(fname).getLines
      var device_ID = " "
      for(line <- flines) {
          // println("flines is:" + flines.getClass)                       // flines is:class scala.io.BufferedSource$BufferedLineIterator
          // println("line is:" + line.getClass)                           // line is:class java.lang.String
          var line_string: String = line                                // Use line to populate a String
          // println("line_string is: " + line_string.getClass)            // line_string is: class java.lang.String
          var line_split = line_string.split(",")                       // Buffer the results of split
          // println("line_split is: " + line_split.getClass)              // line_split is: class [Ljava.lang.String;
          device_ID = (line_split(2))                                   // Print out field 2, the Unique ID
          // println("device_ID is: " + device_ID.getClass)                // device_ID is: class java.lang.String 
          println("Device ID: " + device_ID)                            // Device ID: 1882b564-c7e0-4315-aa24-228c0155ee1b
     }
  }
}

Using filter to Limit Processing
30. There are six instances of “Titanic 4000” model phones in the Loudacre log file.
print out the device IDs for these six records only.
Use the Iterator.filter method to limit processing to only those lines containing Titanic 4000 phone records.

import scala.io.Source
object Filter{
  def main(args: Array[String]) {
      val fname = "../files/loudacre.log"
      var flines = Source.fromFile(fname).getLines.filter(_ contains "4000")
      var device_ID = " "
      var Model = " "
      for(line <- flines) {
          var line_string: String = line      				// Use line to populate a String
          var line_split = line_string.split(",")      			// Buffer the results of split
          device_ID = (line_split(2))             			// Print out field 2, the Unique ID
          println("Device ID: " + device_ID)                            // Device ID: 1882b564-c7e0-4315-aa24-228c0155ee1b
          Model = (line_split(1))             				// Print out field 1, Model name and number
          println("Model: " + Model)                                   
     }
  }
}

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala Filter
Device ID: 27a418dc-9f20-4cb9-9c69-92d16c596bad
Model: Titanic 4000
Device ID: 3d88c332-61fa-46f0-a171-84a8b02dd52a
Model: Titanic 4000
Device ID: c9b5d5d4-19a2-4259-bff7-e915d967d3f4
Model: Titanic 4000
Device ID: 54994404-ab72-4e5f-a6bc-0d485b3806f2
Model: Titanic 4000
Device ID: 57d50d31-ce40-440d-a2b3-a041eb9a29f5
Model: Titanic 4000
Device ID: 1882b564-c7e0-4315-aa24-228c0155ee1b
Model: Titanic 4000

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ cat Filter2.scala

import scala.io.Source
object Filter2{
  def main(args: Array[String]) {
      val fname = "../files/loudacre.log"
      var flines = Source.fromFile(fname).getLines.filter(_ contains "4000")
      var device_ID = " "
      var Model = " "
      for(line <- flines) {
          // println("flines is:" + flines.getClass)                       // flines is:class scala.io.BufferedSource$BufferedLineIterator
          // println("line is:" + line.getClass)                           // line is:class java.lang.String
          var line_string: String = line      				// Use line to populate a String
          if( line_string.contains("4000") ) { 
	  // println("line_string is: " + line_string.getClass)            // line_string is: class java.lang.String
              var line_split = line_string.split(",")      			// Buffer the results of split
	  // println("line_split is: " + line_split.getClass)              // line_split is: class [Ljava.lang.String;
              device_ID = (line_split(2))             			// Print out field 2, the Unique ID
	  // println("device_ID is: " + device_ID.getClass)                // device_ID is: class java.lang.String 
              println("Device ID: " + device_ID)                            // Device ID: 1882b564-c7e0-4315-aa24-228c0155ee1b
              Model = (line_split(1))             				// Print out field 1, Model name and number
              println("Model: " + Model)         
          }                          
     }
  }
}

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala Filter2
Device ID: 27a418dc-9f20-4cb9-9c69-92d16c596bad
Model: Titanic 4000
Device ID: 3d88c332-61fa-46f0-a171-84a8b02dd52a
Model: Titanic 4000
Device ID: c9b5d5d4-19a2-4259-bff7-e915d967d3f4
Model: Titanic 4000
Device ID: 54994404-ab72-4e5f-a6bc-0d485b3806f2
Model: Titanic 4000
Device ID: 57d50d31-ce40-440d-a2b3-a041eb9a29f5
Model: Titanic 4000
Device ID: 1882b564-c7e0-4315-aa24-228c0155ee1b
Model: Titanic 4000

solucion del manual: /training_materials/jes/solutions/FilterLines.scala.
--------------------------------------------
------------------
-O-J-O- sin hacer - revisar -Hands-On Exercise: Working with Libraries in Scala
------------------
------------------
No estan las soluciones con lo que no lo he confirmado
He creado estas clases:

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ ls *Log*
/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src

-- Estas es para probar en el mismo directorio 
4,0K -rw-rw-r-- 1 hadoop hadoop 1,3K oct 12 20:29 'TestLog2Utils$.class'
4,0K -rw-rw-r-- 1 hadoop hadoop  605 oct 12 20:29  TestLog2Utils.class
4,0K -rw-rw-r-- 1 hadoop hadoop 1,6K oct 12 20:29  Log2Utils.class
4,0K -rw-rw-r-- 1 hadoop hadoop  543 oct 12 20:29  TestLog2Utils.scala
4,0K -rw-rw-r-- 1 hadoop hadoop  858 oct 12 20:26  Log2Utils.scala

-- Estas es para probar lo de los paquetes.. 
4,0K -rw-rw-r-- 1 hadoop hadoop  420 oct 12 19:01  TestLogUtils.scala
4,0K -rw-rw-r-- 1 hadoop hadoop  753 oct 12 18:39  LogUtils.scala

------------------


In this exercise, you will write a program to process a set of device logs and display the device ID and model.
In order to do this you will create a class to represent a phone device, and a utility object for parsing log lines.
This exercise makes use of various techniques you have previously worked with in the course. 
1. In your exercise working directory (or Eclipse), create a singleton object called LogUtils with the following characteristics:
• A function getDevId that, given a line from a device log file, returns the Device ID string
• A function getModel that, given a line from a device log file, returns the Model name

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ cat LogUtils.scala 

package com.loudacre.utilslib

class LogUtils(val line: String) {
// A function getDevId that, given a line from a device log file, returns the Device ID string
      def getDevId ( line: String ) {
          val line_split = line.split (",")                       // Buffer the results of split
          val device_ID = ( line_split (2) )                       // Print out field 2, the Unique ID
      }

// A function getModel that, given a line from a device log file, returns the Model name
      def getModel ( line: String ) {
          val line_split = line.split(",")                       // Buffer the results of split
          val Model = ( line_split (1) )                                       // Print out field 1, the Model
      }
}

2. Compile the LogUtils object.

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scalac LogUtils.scala
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ ls com/loudacre/utilslib/
/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src
total 12K
4,0K drwxrwxr-x 2 hadoop hadoop 4,0K oct 12 18:39 .
4,0K drwxrwxr-x 5 hadoop hadoop 4,0K oct 12 18:39 ..
4,0K -rw-rw-r-- 1 hadoop hadoop 1,3K oct 12 18:39 LogUtils.class

3. Start a Scala shell instance in the exercise working directory.

val record = "2014-03-15:10:10:31, Titanic 4000, 1882b564-c7e0-4315-aa24-228c0155ee1b, 58, 36, 39, 31, 15, 0, TRUE, enabled, enabled, 37.819722, -122.478611"


4. Test the LogUtils class:

import com.loudacre.LogUtils
val record = "2014-03-15:10:10:31, Titanic 4000, 1882b564-c7e0-4315-aa24-228c0155ee1b, 58, 36, 39, 31, 15, 0, TRUE, enabled, enabled, 37.819722, -122.478611" 
LogUtils.getDevId(record)
LogUtils.getDevModel(record)

5. Once you have confirmed that the methods work correctly, return to your exercise working directory and create a class called Phone with the following characteristics:
• Two String member variables: devId and model
• The constructor takes devId and model as parameters and sets the member variables
• Overrides the toString method to display devId:model as the object identifier
   A sample solution is in este archivo no esta - Phone.scala.
6. Compile the Phone.scala, PhoneDriver.scala
7. Restart the Scala shell, and use the shell to test instantiation of a Phone object and the Phone.toString method:

import com.loudacre.LogUtils
val record = "2014-03-15:10:10:31, Titanic 4000, 1882b564-c7e0-4315-aa24-228c0155ee1b, 58, 36, 39, 31, 15, 0, TRUE, enabled, enabled, 37.819722, -122.478611"
import com.loudacre.Phone
val p = new Phone("my-id", "my-device-model")
p
val p = new Phone(LogUtils.getDevId(record),
LogUtils.getDevModel(record))
p

8. Now write a main program class called PhoneInfoFormatter with the following characteristics:
• The program takes a command line parameter with the path name of a log file
• The program prints out an error and usage message if the correct number of parameters is not provided
• The program creates a List of Phone instances, one for each line in the log file, 
  using the LogUtils methods you wrote above to find the device ID and model name
• The program displays the contents of the List

A sample solution is in este programa no está PhoneInfoFormatter.scala.
9. Compile the PhoneInfoFormatter class.
(In Eclipse, copy your LogUtils.scala and Phone.scala solutions into the PhoneInfoFormatter project, and then clean and build the project.)

10. Test your program by using the loudacre.log file as input.

$ scala PhoneInfoFormatter
/home/training/training_materials/jes/data/loudacre.log

(To run in Eclipse, right-click on the file PhoneInfoFormatter.scala and
select Run As -> Run Configurations. Specify PhoneInfoFormatter for the
Name, Project, and Main class prompts. Click the Arguments tab, enter
/home/training/training_materials/jes/data/loudacre.log
into the Program arguments box, and then click Run.)


solucion del manual: /training_materials/jes/solutions/npi.scala.
--------------------------------------------

============================================
Definición de un Scala Objeto
object nameObject{
  def main(args: Array[String]) {

  }
}

--------------------------------------------
Singleton Objects - Definición de un Scala Class
- Singleton objects are created without an explicit class
- Scala generates a class automatically, the object is a companion of the class
class nameClass(argumentos) {

}
------------------
Developer Training for Apache Spark and Hadoop
==================
------------------
Hands-On Exercise: Starting the Exercise Environment - aqui no hay ejercicios, todo esto es tema del entorno training-materials
------------------
------------------
Hands-On Exercise: Working with HDFS
------------------
1. Open a terminal window using the shortcut on the remote desktop menu bar.
2. In the new terminal session, use the HDFS command line to list the content of the
HDFS root directory using the following command:

scelisdev02@cca175-m:~$ hdfs dfs -ls /
Found 3 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-16 19:00 /devsh_loudacre
drwxrwxrwt   - hdfs        hadoop          0 2021-09-17 09:28 /tmp
drwxrwxrwt   - hdfs        hadoop          0 2021-09-21 11:59 /user

scelisdev02@cca175-m:~$ hdfs dfs -ls /user
Found 10 items
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/hbase
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/hdfs
drwxrwxrwt   - hdfs        hadoop          0 2021-09-16 13:13 /user/hive
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/mapred
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/pig
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-21 11:59 /user/scelisdev02      <-- ok en GCP
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/spark
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-17 09:30 /user/training         <-- ok en Cloudera
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/yarn
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/zookeeper

scelisdev02@cca175-m:~$ hdfs dfs -mkdir /user/scelisdev02

scelisdev02@cca175-m:~$ hdfs dfs -ls /user/scelisdev02
Found 2 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 10:43 /user/scelisdev02/.sparkStaging
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset

4. List the contents of your home directory by running:
scelisdev02@cca175-m:~$ hdfs dfs -ls /user/training         -->   en el cluster hadoop este user
scelisdev02@cca175-m:~$ hdfs dfs -ls /user/scelisdev02      -->   en GCP este user
Found 1 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-21 11:59 /user/scelisdev02/.sparkStaging

5. Create a new directory called devsh_loudacre in the HDFS root directory. 
The new directory will be the top-level directory for the remaining exercises in the course.

scelisdev02@cca175-m:~$ hdfs dfs -mkdir /devsh_loudacre

6. Change directories to the Linux local filesystem directory containing sample data you will be using in the course.

scelisdev02@cca175-m:~/training_materials/devsh/data$ cd $DEVDATA
scelisdev02@cca175-m:~/training_materials/devsh/data$ pwd
/home/scelisdev02/training_materials/devsh/data
scelisdev02@cca175-m:~/training_materials/devsh/data$

6. Insert the activations directory into HDFS:

scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -put activations /devsh_loudacre
scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -ls /devsh_loudacre/activations
Found 66 items
-rw-r--r--   2 scelisdev02 hadoop      19044 2021-09-16 19:00 /devsh_loudacre/activations/2008-10.xml
-rw-r--r--   2 scelisdev02 hadoop      73748 2021-09-16 19:00 /devsh_loudacre/activations/2008-11.xml
-rw-r--r--   2 scelisdev02 hadoop      68764 2021-09-16 19:00 /devsh_loudacre/activations/2008-12.xml
-rw-r--r--   2 scelisdev02 hadoop      90337 2021-09-16 19:00 /devsh_loudacre/activations/2009-01.xml
-rw-r--r--   2 scelisdev02 hadoop      67655 2021-09-16 19:00 /devsh_loudacre/activations/2009-02.xml
-rw-r--r--   2 scelisdev02 hadoop      86243 2021-09-16 19:00 /devsh_loudacre/activations/2009-03.xml
-rw-r--r--   2 scelisdev02 hadoop      82718 2021-09-16 19:00 /devsh_loudacre/activations/2009-04.xml
-rw-r--r--   2 scelisdev02 hadoop      90148 2021-09-16 19:00 /devsh_loudacre/activations/2009-05.xml
-rw-r--r--   2 scelisdev02 hadoop      88713 2021-09-16 19:00 /devsh_loudacre/activations/2009-06.xml
-rw-r--r--   2 scelisdev02 hadoop      86835 2021-09-16 19:00 /devsh_loudacre/activations/2009-07.xml
-rw-r--r--   2 scelisdev02 hadoop      79114 2021-09-16 19:00 /devsh_loudacre/activations/2009-08.xml
-rw-r--r--   2 scelisdev02 hadoop      83012 2021-09-16 19:00 /devsh_loudacre/activations/2009-09.xml
-rw-r--r--   2 scelisdev02 hadoop      91007 2021-09-16 19:00 /devsh_loudacre/activations/2009-10.xml
-rw-r--r--   2 scelisdev02 hadoop      93132 2021-09-16 19:00 /devsh_loudacre/activations/2009-11.xml
-rw-r--r--   2 scelisdev02 hadoop      96328 2021-09-16 19:00 /devsh_loudacre/activations/2009-12.xml
-rw-r--r--   2 scelisdev02 hadoop     192394 2021-09-16 19:00 /devsh_loudacre/activations/2010-01.xml
-rw-r--r--   2 scelisdev02 hadoop     191631 2021-09-16 19:00 /devsh_loudacre/activations/2010-02.xml
-rw-r--r--   2 scelisdev02 hadoop     200690 2021-09-16 19:00 /devsh_loudacre/activations/2010-03.xml
-rw-r--r--   2 scelisdev02 hadoop     200510 2021-09-16 19:00 /devsh_loudacre/activations/2010-04.xml
-rw-r--r--   2 scelisdev02 hadoop     227033 2021-09-16 19:00 /devsh_loudacre/activations/2010-05.xml
-rw-r--r--   2 scelisdev02 hadoop     235350 2021-09-16 19:00 /devsh_loudacre/activations/2010-06.xml
-rw-r--r--   2 scelisdev02 hadoop     230267 2021-09-16 19:00 /devsh_loudacre/activations/2010-07.xml
-rw-r--r--   2 scelisdev02 hadoop     241761 2021-09-16 19:00 /devsh_loudacre/activations/2010-08.xml
-rw-r--r--   2 scelisdev02 hadoop     234168 2021-09-16 19:00 /devsh_loudacre/activations/2010-09.xml
-rw-r--r--   2 scelisdev02 hadoop     237083 2021-09-16 19:00 /devsh_loudacre/activations/2010-10.xml
-rw-r--r--   2 scelisdev02 hadoop     209493 2021-09-16 19:00 /devsh_loudacre/activations/2010-11.xml
-rw-r--r--   2 scelisdev02 hadoop     235910 2021-09-16 19:00 /devsh_loudacre/activations/2010-12.xml
-rw-r--r--   2 scelisdev02 hadoop     441580 2021-09-16 19:00 /devsh_loudacre/activations/2011-01.xml
-rw-r--r--   2 scelisdev02 hadoop     421089 2021-09-16 19:00 /devsh_loudacre/activations/2011-02.xml
-rw-r--r--   2 scelisdev02 hadoop     472902 2021-09-16 19:00 /devsh_loudacre/activations/2011-03.xml
-rw-r--r--   2 scelisdev02 hadoop     456871 2021-09-16 19:00 /devsh_loudacre/activations/2011-04.xml
-rw-r--r--   2 scelisdev02 hadoop     466384 2021-09-16 19:00 /devsh_loudacre/activations/2011-05.xml
-rw-r--r--   2 scelisdev02 hadoop     454844 2021-09-16 19:00 /devsh_loudacre/activations/2011-06.xml
-rw-r--r--   2 scelisdev02 hadoop     466854 2021-09-16 19:00 /devsh_loudacre/activations/2011-07.xml
-rw-r--r--   2 scelisdev02 hadoop     483014 2021-09-16 19:00 /devsh_loudacre/activations/2011-08.xml
-rw-r--r--   2 scelisdev02 hadoop     464367 2021-09-16 19:00 /devsh_loudacre/activations/2011-09.xml
-rw-r--r--   2 scelisdev02 hadoop     500909 2021-09-16 19:00 /devsh_loudacre/activations/2011-10.xml
-rw-r--r--   2 scelisdev02 hadoop     477224 2021-09-16 19:00 /devsh_loudacre/activations/2011-11.xml
-rw-r--r--   2 scelisdev02 hadoop     506646 2021-09-16 19:00 /devsh_loudacre/activations/2011-12.xml
-rw-r--r--   2 scelisdev02 hadoop     979534 2021-09-16 19:00 /devsh_loudacre/activations/2012-01.xml
-rw-r--r--   2 scelisdev02 hadoop     945789 2021-09-16 19:00 /devsh_loudacre/activations/2012-02.xml
-rw-r--r--   2 scelisdev02 hadoop    1010401 2021-09-16 19:00 /devsh_loudacre/activations/2012-03.xml
-rw-r--r--   2 scelisdev02 hadoop     994863 2021-09-16 19:00 /devsh_loudacre/activations/2012-04.xml
-rw-r--r--   2 scelisdev02 hadoop    1005624 2021-09-16 19:00 /devsh_loudacre/activations/2012-05.xml
-rw-r--r--   2 scelisdev02 hadoop     957156 2021-09-16 19:00 /devsh_loudacre/activations/2012-06.xml
-rw-r--r--   2 scelisdev02 hadoop    1028510 2021-09-16 19:00 /devsh_loudacre/activations/2012-07.xml
-rw-r--r--   2 scelisdev02 hadoop    1055421 2021-09-16 19:00 /devsh_loudacre/activations/2012-08.xml
-rw-r--r--   2 scelisdev02 hadoop    1003936 2021-09-16 19:00 /devsh_loudacre/activations/2012-09.xml
-rw-r--r--   2 scelisdev02 hadoop    1066257 2021-09-16 19:00 /devsh_loudacre/activations/2012-10.xml
-rw-r--r--   2 scelisdev02 hadoop    1000719 2021-09-16 19:00 /devsh_loudacre/activations/2012-11.xml
-rw-r--r--   2 scelisdev02 hadoop    1045239 2021-09-16 19:00 /devsh_loudacre/activations/2012-12.xml
-rw-r--r--   2 scelisdev02 hadoop    1081374 2021-09-16 19:00 /devsh_loudacre/activations/2013-01.xml
-rw-r--r--   2 scelisdev02 hadoop     984057 2021-09-16 19:00 /devsh_loudacre/activations/2013-02.xml
-rw-r--r--   2 scelisdev02 hadoop    1115803 2021-09-16 19:00 /devsh_loudacre/activations/2013-03.xml
-rw-r--r--   2 scelisdev02 hadoop    1079565 2021-09-16 19:00 /devsh_loudacre/activations/2013-04.xml
-rw-r--r--   2 scelisdev02 hadoop    1092603 2021-09-16 19:00 /devsh_loudacre/activations/2013-05.xml
-rw-r--r--   2 scelisdev02 hadoop    1066438 2021-09-16 19:00 /devsh_loudacre/activations/2013-06.xml
-rw-r--r--   2 scelisdev02 hadoop    1133909 2021-09-16 19:00 /devsh_loudacre/activations/2013-07.xml
-rw-r--r--   2 scelisdev02 hadoop    1137010 2021-09-16 19:00 /devsh_loudacre/activations/2013-08.xml
-rw-r--r--   2 scelisdev02 hadoop    1059769 2021-09-16 19:00 /devsh_loudacre/activations/2013-09.xml
-rw-r--r--   2 scelisdev02 hadoop    1132497 2021-09-16 19:00 /devsh_loudacre/activations/2013-10.xml
-rw-r--r--   2 scelisdev02 hadoop    6816957 2021-09-16 19:00 /devsh_loudacre/activations/2013-11.xml
-rw-r--r--   2 scelisdev02 hadoop    3734204 2021-09-16 19:00 /devsh_loudacre/activations/2013-12.xml
-rw-r--r--   2 scelisdev02 hadoop    3516581 2021-09-16 19:00 /devsh_loudacre/activations/2014-01.xml
-rw-r--r--   2 scelisdev02 hadoop    2878103 2021-09-16 19:00 /devsh_loudacre/activations/2014-02.xml
-rw-r--r--   2 scelisdev02 hadoop    1316093 2021-09-16 19:00 /devsh_loudacre/activations/2014-03.xml

9. View some of the data you just copied into HDFS.
- imprime las 20 primeras líneas del fichero: /devsh_loudacre/activations/2014-03.xml
scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -cat /devsh_loudacre/activations/2014-03.xml | head -n 20
<activations>
          <activation timestamp="1394866560" type="phone">
            <account-number>50114</account-number>
            <device-id>181c77a1-634c-4451-ad80-781002fd737d</device-id>
            <phone-number>5418104451</phone-number>
            <model>Sorrento F41L</model>
          </activation>
                          <activation timestamp="1394866113" type="phone">
            <account-number>15664</account-number>
            <device-id>60ef6f0b-dc96-4a8d-ac0a-191c18a1cc6b</device-id>
            <phone-number>5100285050</phone-number>
            <model>Sorrento F41L</model>
          </activation>
                          <activation timestamp="1394866096" type="phone">
            <account-number>129553</account-number>
            <device-id>9d359ed7-9549-4308-8ee7-00bc215a47ba</device-id>
            <phone-number>5107873261</phone-number>
            <model>Ronin S4</model>
          </activation>
                          <activation timestamp="1394865838" type="phone">
cat: Unable to write to output stream.    

// cat: Unable to write to output stream.    
// This is because the cat was only able to pipe the first 20 lines because of the limit specified in the head command

- Imprime por páginas
scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -cat /devsh_loudacre/activations/2014-03.xml | more

- Imprime por una cantidad definida por el usuario en modo de ejecución
scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -cat /devsh_loudacre/activations/2014-03.xml | less

scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -tail /devsh_loudacre/activations/2014-03.xml
2349" type="phone">
            <account-number>122521</account-number>
            <device-id>4154b325-0db2-473d-b7a6-9c19028d0176</device-id>
            <phone-number>9281259499</phone-number>
            <model>Ronin S3</model>
          </activation>
                          <activation timestamp="1393661929" type="phone">
            <account-number>127195</account-number>
            <device-id>5c3b2371-dd65-4cdc-85ba-344d0c5965d3</device-id>
            <phone-number>7078927709</phone-number>
            <model>Sorrento F20L</model>
          </activation>
                          <activation timestamp="1393661811" type="phone">
            <account-number>68193</account-number>
            <device-id>cb0fd1b8-9d6a-41b2-b2a5-f67f60834336</device-id>
            <phone-number>6573369255</phone-number>
            <model>Sorrento F41L</model>
          </activation>
                          <activation timestamp="1393661339" type="phone">
            <account-number>122231</account-number>
            <device-id>986c095c-b0df-484c-82b0-194cf84f1358</device-id>
            <phone-number>7020832947</phone-number>
            <model>Sorrento F01L</model>
          </activation>

10. Practice downloading from HDFS by downloading the directory you uploaded
above.
scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -get \
> /devsh_loudacre/activations/ /tmp/devsh_activations

11. Show the directory you downloaded from HDFS to the local filesystem.
scelisdev02@cca175-m:~/training_materials/devsh/data$ ls /tmp/devsh_activations/
2008-10.xml  2009-04.xml  2009-10.xml  2010-04.xml  2010-10.xml  2011-04.xml  2011-10.xml  2012-04.xml  2012-10.xml  2013-04.xml  2013-10.xml
2008-11.xml  2009-05.xml  2009-11.xml  2010-05.xml  2010-11.xml  2011-05.xml  2011-11.xml  2012-05.xml  2012-11.xml  2013-05.xml  2013-11.xml
2008-12.xml  2009-06.xml  2009-12.xml  2010-06.xml  2010-12.xml  2011-06.xml  2011-12.xml  2012-06.xml  2012-12.xml  2013-06.xml  2013-12.xml
2009-01.xml  2009-07.xml  2010-01.xml  2010-07.xml  2011-01.xml  2011-07.xml  2012-01.xml  2012-07.xml  2013-01.xml  2013-07.xml  2014-01.xml
2009-02.xml  2009-08.xml  2010-02.xml  2010-08.xml  2011-02.xml  2011-08.xml  2012-02.xml  2012-08.xml  2013-02.xml  2013-08.xml  2014-02.xml
2009-03.xml  2009-09.xml  2010-03.xml  2010-09.xml  2011-03.xml  2011-09.xml  2012-03.xml  2012-09.xml  2013-03.xml  2013-09.xml  2014-03.xml

12. Remove the directory you uploaded earlier.
scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -rm -r /devsh_loudacre/activations/
Deleted /devsh_loudacre/activations
scelisdev02@cca175-m:~/training_materials/devsh/data$ 

13. There are several other operations available with the hdfs dfs command to perform most common filesystem manipulations such as mv, cp, and mkdir.
Using the hdfs dfs command with no arguments will display the available
scelisdev02@cca175-m:~$ hdfs dfs

solucion del manual:  /home/scelisdev02/training_materials/devsh/exercises/hdfs
-----------------------------------------------
------------------
Hands-On Exercise: Running and Monitoring a YARN Job
------------------

Explore the YARN Cluster
1. In Firefox on your remote desktop, visit the YARN Resource Manager (RM) UI using the provided bookmark (labeled YARN RM), or by going to URL http://localhost:8088/.

2. Take note of the values in the Cluster Metrics and Cluster Node Metrics sections,
which display information such as the number of applications running currently, 
previously run, or waiting to run; the amount of memory used and available; and 
how many worker nodes are in the cluster.

3. Click the Nodes link in the Cluster menu on the left. 
The bottom section will display a list of worker nodes in the cluster. 

4. Click the localhost.localdomain:8042 link under Node HTTP Address to open the Node Manager UI on that node. 
This displays statistics about the selected node, including the amount of available memory, 
currently running applications (there are none), and so on.

5. To return to the Resource Manager UI, click ResourceManager > RM Home on the left.

Submit an Application to the YARN Cluster
6. If you do not have an open terminal window, start one now.

7. Upload the Knowledge Base data files to HDFS.
It may take several seconds for all the files to upload.
scelisdev02@cca175-m:~$ hdfs dfs -put $DEVDATA/kb /devsh_loudacre

scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre
Found 1 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-21 13:42 /devsh_loudacre/kb
scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/kb
Found 300 items
-rw-r--r--   2 scelisdev02 hadoop       1626 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00001.html
-rw-r--r--   2 scelisdev02 hadoop       1298 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00002.html
-rw-r--r--   2 scelisdev02 hadoop       1183 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00003.html
-rw-r--r--   2 scelisdev02 hadoop       1029 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00004.html
-rw-r--r--   2 scelisdev02 hadoop       1155 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00005.html
-rw-r--r--   2 scelisdev02 hadoop       1109 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00006.html
-rw-r--r--   2 scelisdev02 hadoop       1648 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00007.html
-rw-r--r--   2 scelisdev02 hadoop       1303 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00008.html
-rw-r--r--   2 scelisdev02 hadoop       1173 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00009.html
-rw-r--r--   2 scelisdev02 hadoop       1017 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00010.html
-rw-r--r--   2 scelisdev02 hadoop       1149 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00011.html
-rw-r--r--   2 scelisdev02 hadoop       1117 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00012.html
-rw-r--r--   2 scelisdev02 hadoop       1638 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00013.html
-rw-r--r--   2 scelisdev02 hadoop       1282 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00014.html
-rw-r--r--   2 scelisdev02 hadoop       1166 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00015.html
-rw-r--r--   2 scelisdev02 hadoop       1017 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00016.html
-rw-r--r--   2 scelisdev02 hadoop       1160 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00017.html
-rw-r--r--   2 scelisdev02 hadoop       1105 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00018.html
-rw-r--r--   2 scelisdev02 hadoop       1612 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00019.html
-rw-r--r--   2 scelisdev02 hadoop       1274 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00020.html
-rw-r--r--   2 scelisdev02 hadoop       1176 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00021.html
-rw-r--r--   2 scelisdev02 hadoop       1024 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00022.html
-rw-r--r--   2 scelisdev02 hadoop       1149 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00023.html
-rw-r--r--   2 scelisdev02 hadoop       1103 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00024.html
-rw-r--r--   2 scelisdev02 hadoop       1580 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00025.html
-rw-r--r--   2 scelisdev02 hadoop       1242 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00026.html
-rw-r--r--   2 scelisdev02 hadoop       1143 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00027.html
-rw-r--r--   2 scelisdev02 hadoop       1013 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00028.html
-rw-r--r--   2 scelisdev02 hadoop       1130 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00029.html
-rw-r--r--   2 scelisdev02 hadoop       1101 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00030.html
-rw-r--r--   2 scelisdev02 hadoop       1630 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00031.html
-rw-r--r--   2 scelisdev02 hadoop       1258 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00032.html
-rw-r--r--   2 scelisdev02 hadoop       1143 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00033.html
-rw-r--r--   2 scelisdev02 hadoop       1003 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00034.html
-rw-r--r--   2 scelisdev02 hadoop       1118 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00035.html
-rw-r--r--   2 scelisdev02 hadoop       1096 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00036.html
-rw-r--r--   2 scelisdev02 hadoop       1602 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00037.html
-rw-r--r--   2 scelisdev02 hadoop       1247 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00038.html
-rw-r--r--   2 scelisdev02 hadoop       1173 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00039.html
-rw-r--r--   2 scelisdev02 hadoop       1003 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00040.html
-rw-r--r--   2 scelisdev02 hadoop       1136 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00041.html
-rw-r--r--   2 scelisdev02 hadoop       1107 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00042.html
-rw-r--r--   2 scelisdev02 hadoop       1625 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00043.html
-rw-r--r--   2 scelisdev02 hadoop       1266 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00044.html
-rw-r--r--   2 scelisdev02 hadoop       1151 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00045.html
-rw-r--r--   2 scelisdev02 hadoop        992 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00046.html
-rw-r--r--   2 scelisdev02 hadoop       1127 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00047.html
-rw-r--r--   2 scelisdev02 hadoop       1078 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00048.html
-rw-r--r--   2 scelisdev02 hadoop       1612 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00049.html
-rw-r--r--   2 scelisdev02 hadoop       1290 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00050.html
-rw-r--r--   2 scelisdev02 hadoop       1180 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00051.html
-rw-r--r--   2 scelisdev02 hadoop       1011 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00052.html
-rw-r--r--   2 scelisdev02 hadoop       1147 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00053.html
-rw-r--r--   2 scelisdev02 hadoop       1114 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00054.html
-rw-r--r--   2 scelisdev02 hadoop       1643 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00055.html
-rw-r--r--   2 scelisdev02 hadoop       1296 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00056.html
-rw-r--r--   2 scelisdev02 hadoop       1175 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00057.html
-rw-r--r--   2 scelisdev02 hadoop       1038 2021-09-21 13:41 /devsh_loudacre/kb/KBDOC-00058.html
-rw-r--r--   2 scelisdev02 hadoop       1134 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00059.html
-rw-r--r--   2 scelisdev02 hadoop       1105 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00060.html
-rw-r--r--   2 scelisdev02 hadoop       1646 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00061.html
-rw-r--r--   2 scelisdev02 hadoop       1315 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00062.html
-rw-r--r--   2 scelisdev02 hadoop       1189 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00063.html
-rw-r--r--   2 scelisdev02 hadoop       1031 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00064.html
-rw-r--r--   2 scelisdev02 hadoop       1152 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00065.html
-rw-r--r--   2 scelisdev02 hadoop       1097 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00066.html
-rw-r--r--   2 scelisdev02 hadoop       1621 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00067.html
-rw-r--r--   2 scelisdev02 hadoop       1262 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00068.html
-rw-r--r--   2 scelisdev02 hadoop       1161 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00069.html
-rw-r--r--   2 scelisdev02 hadoop       1003 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00070.html
-rw-r--r--   2 scelisdev02 hadoop       1141 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00071.html
-rw-r--r--   2 scelisdev02 hadoop       1100 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00072.html
-rw-r--r--   2 scelisdev02 hadoop       1628 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00073.html
-rw-r--r--   2 scelisdev02 hadoop       1282 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00074.html
-rw-r--r--   2 scelisdev02 hadoop       1187 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00075.html
-rw-r--r--   2 scelisdev02 hadoop       1040 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00076.html
-rw-r--r--   2 scelisdev02 hadoop       1151 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00077.html
-rw-r--r--   2 scelisdev02 hadoop       1120 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00078.html
-rw-r--r--   2 scelisdev02 hadoop       1650 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00079.html
-rw-r--r--   2 scelisdev02 hadoop       1290 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00080.html
-rw-r--r--   2 scelisdev02 hadoop       1196 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00081.html
-rw-r--r--   2 scelisdev02 hadoop       1032 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00082.html
-rw-r--r--   2 scelisdev02 hadoop       1149 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00083.html
-rw-r--r--   2 scelisdev02 hadoop       1112 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00084.html
-rw-r--r--   2 scelisdev02 hadoop       1655 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00085.html
-rw-r--r--   2 scelisdev02 hadoop       1364 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00086.html
-rw-r--r--   2 scelisdev02 hadoop       1230 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00087.html
-rw-r--r--   2 scelisdev02 hadoop       1062 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00088.html
-rw-r--r--   2 scelisdev02 hadoop       1191 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00089.html
-rw-r--r--   2 scelisdev02 hadoop       1159 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00090.html
-rw-r--r--   2 scelisdev02 hadoop       1613 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00091.html
-rw-r--r--   2 scelisdev02 hadoop       1293 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00092.html
-rw-r--r--   2 scelisdev02 hadoop       1165 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00093.html
-rw-r--r--   2 scelisdev02 hadoop       1031 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00094.html
-rw-r--r--   2 scelisdev02 hadoop       1146 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00095.html
-rw-r--r--   2 scelisdev02 hadoop       1124 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00096.html
-rw-r--r--   2 scelisdev02 hadoop       1682 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00097.html
-rw-r--r--   2 scelisdev02 hadoop       1377 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00098.html
-rw-r--r--   2 scelisdev02 hadoop       1229 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00099.html
-rw-r--r--   2 scelisdev02 hadoop       1044 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00100.html
-rw-r--r--   2 scelisdev02 hadoop       1181 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00101.html
-rw-r--r--   2 scelisdev02 hadoop       1168 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00102.html
-rw-r--r--   2 scelisdev02 hadoop       1689 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00103.html
-rw-r--r--   2 scelisdev02 hadoop       1350 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00104.html
-rw-r--r--   2 scelisdev02 hadoop       1229 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00105.html
-rw-r--r--   2 scelisdev02 hadoop       1048 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00106.html
-rw-r--r--   2 scelisdev02 hadoop       1173 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00107.html
-rw-r--r--   2 scelisdev02 hadoop       1151 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00108.html
-rw-r--r--   2 scelisdev02 hadoop       1696 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00109.html
-rw-r--r--   2 scelisdev02 hadoop       1367 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00110.html
-rw-r--r--   2 scelisdev02 hadoop       1243 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00111.html
-rw-r--r--   2 scelisdev02 hadoop       1056 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00112.html
-rw-r--r--   2 scelisdev02 hadoop       1173 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00113.html
-rw-r--r--   2 scelisdev02 hadoop       1164 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00114.html
-rw-r--r--   2 scelisdev02 hadoop       1604 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00115.html
-rw-r--r--   2 scelisdev02 hadoop       1274 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00116.html
-rw-r--r--   2 scelisdev02 hadoop       1176 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00117.html
-rw-r--r--   2 scelisdev02 hadoop       1020 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00118.html
-rw-r--r--   2 scelisdev02 hadoop       1135 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00119.html
-rw-r--r--   2 scelisdev02 hadoop       1095 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00120.html
-rw-r--r--   2 scelisdev02 hadoop       1613 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00121.html
-rw-r--r--   2 scelisdev02 hadoop       1283 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00122.html
-rw-r--r--   2 scelisdev02 hadoop       1162 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00123.html
-rw-r--r--   2 scelisdev02 hadoop       1046 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00124.html
-rw-r--r--   2 scelisdev02 hadoop       1151 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00125.html
-rw-r--r--   2 scelisdev02 hadoop       1122 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00126.html
-rw-r--r--   2 scelisdev02 hadoop       1657 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00127.html
-rw-r--r--   2 scelisdev02 hadoop       1298 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00128.html
-rw-r--r--   2 scelisdev02 hadoop       1189 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00129.html
-rw-r--r--   2 scelisdev02 hadoop       1037 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00130.html
-rw-r--r--   2 scelisdev02 hadoop       1151 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00131.html
-rw-r--r--   2 scelisdev02 hadoop       1125 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00132.html
-rw-r--r--   2 scelisdev02 hadoop       1630 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00133.html
-rw-r--r--   2 scelisdev02 hadoop       1307 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00134.html
-rw-r--r--   2 scelisdev02 hadoop       1208 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00135.html
-rw-r--r--   2 scelisdev02 hadoop       1023 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00136.html
-rw-r--r--   2 scelisdev02 hadoop       1165 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00137.html
-rw-r--r--   2 scelisdev02 hadoop       1097 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00138.html
-rw-r--r--   2 scelisdev02 hadoop       1626 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00139.html
-rw-r--r--   2 scelisdev02 hadoop       1319 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00140.html
-rw-r--r--   2 scelisdev02 hadoop       1199 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00141.html
-rw-r--r--   2 scelisdev02 hadoop       1043 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00142.html
-rw-r--r--   2 scelisdev02 hadoop       1146 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00143.html
-rw-r--r--   2 scelisdev02 hadoop       1115 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00144.html
-rw-r--r--   2 scelisdev02 hadoop       1643 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00145.html
-rw-r--r--   2 scelisdev02 hadoop       1291 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00146.html
-rw-r--r--   2 scelisdev02 hadoop       1195 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00147.html
-rw-r--r--   2 scelisdev02 hadoop       1046 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00148.html
-rw-r--r--   2 scelisdev02 hadoop       1155 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00149.html
-rw-r--r--   2 scelisdev02 hadoop       1114 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00150.html
-rw-r--r--   2 scelisdev02 hadoop       1644 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00151.html
-rw-r--r--   2 scelisdev02 hadoop       1302 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00152.html
-rw-r--r--   2 scelisdev02 hadoop       1194 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00153.html
-rw-r--r--   2 scelisdev02 hadoop       1016 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00154.html
-rw-r--r--   2 scelisdev02 hadoop       1168 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00155.html
-rw-r--r--   2 scelisdev02 hadoop       1139 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00156.html
-rw-r--r--   2 scelisdev02 hadoop       1619 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00157.html
-rw-r--r--   2 scelisdev02 hadoop       1287 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00158.html
-rw-r--r--   2 scelisdev02 hadoop       1187 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00159.html
-rw-r--r--   2 scelisdev02 hadoop       1028 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00160.html
-rw-r--r--   2 scelisdev02 hadoop       1146 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00161.html
-rw-r--r--   2 scelisdev02 hadoop       1135 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00162.html
-rw-r--r--   2 scelisdev02 hadoop       1623 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00163.html
-rw-r--r--   2 scelisdev02 hadoop       1280 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00164.html
-rw-r--r--   2 scelisdev02 hadoop       1175 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00165.html
-rw-r--r--   2 scelisdev02 hadoop       1019 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00166.html
-rw-r--r--   2 scelisdev02 hadoop       1126 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00167.html
-rw-r--r--   2 scelisdev02 hadoop       1099 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00168.html
-rw-r--r--   2 scelisdev02 hadoop       1647 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00169.html
-rw-r--r--   2 scelisdev02 hadoop       1300 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00170.html
-rw-r--r--   2 scelisdev02 hadoop       1210 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00171.html
-rw-r--r--   2 scelisdev02 hadoop       1016 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00172.html
-rw-r--r--   2 scelisdev02 hadoop       1153 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00173.html
-rw-r--r--   2 scelisdev02 hadoop       1120 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00174.html
-rw-r--r--   2 scelisdev02 hadoop       1667 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00175.html
-rw-r--r--   2 scelisdev02 hadoop       1330 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00176.html
-rw-r--r--   2 scelisdev02 hadoop       1254 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00177.html
-rw-r--r--   2 scelisdev02 hadoop       1064 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00178.html
-rw-r--r--   2 scelisdev02 hadoop       1179 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00179.html
-rw-r--r--   2 scelisdev02 hadoop       1125 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00180.html
-rw-r--r--   2 scelisdev02 hadoop       1625 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00181.html
-rw-r--r--   2 scelisdev02 hadoop       1271 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00182.html
-rw-r--r--   2 scelisdev02 hadoop       1165 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00183.html
-rw-r--r--   2 scelisdev02 hadoop       1032 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00184.html
-rw-r--r--   2 scelisdev02 hadoop       1158 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00185.html
-rw-r--r--   2 scelisdev02 hadoop       1099 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00186.html
-rw-r--r--   2 scelisdev02 hadoop       1621 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00187.html
-rw-r--r--   2 scelisdev02 hadoop       1262 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00188.html
-rw-r--r--   2 scelisdev02 hadoop       1167 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00189.html
-rw-r--r--   2 scelisdev02 hadoop       1023 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00190.html
-rw-r--r--   2 scelisdev02 hadoop       1126 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00191.html
-rw-r--r--   2 scelisdev02 hadoop       1126 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00192.html
-rw-r--r--   2 scelisdev02 hadoop       1624 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00193.html
-rw-r--r--   2 scelisdev02 hadoop       1283 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00194.html
-rw-r--r--   2 scelisdev02 hadoop       1178 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00195.html
-rw-r--r--   2 scelisdev02 hadoop       1027 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00196.html
-rw-r--r--   2 scelisdev02 hadoop       1135 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00197.html
-rw-r--r--   2 scelisdev02 hadoop       1091 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00198.html
-rw-r--r--   2 scelisdev02 hadoop       1623 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00199.html
-rw-r--r--   2 scelisdev02 hadoop       1309 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00200.html
-rw-r--r--   2 scelisdev02 hadoop       1191 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00201.html
-rw-r--r--   2 scelisdev02 hadoop       1030 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00202.html
-rw-r--r--   2 scelisdev02 hadoop       1152 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00203.html
-rw-r--r--   2 scelisdev02 hadoop       1100 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00204.html
-rw-r--r--   2 scelisdev02 hadoop       1604 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00205.html
-rw-r--r--   2 scelisdev02 hadoop       1271 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00206.html
-rw-r--r--   2 scelisdev02 hadoop       1160 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00207.html
-rw-r--r--   2 scelisdev02 hadoop       1029 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00208.html
-rw-r--r--   2 scelisdev02 hadoop       1128 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00209.html
-rw-r--r--   2 scelisdev02 hadoop       1091 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00210.html
-rw-r--r--   2 scelisdev02 hadoop       1614 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00211.html
-rw-r--r--   2 scelisdev02 hadoop       1268 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00212.html
-rw-r--r--   2 scelisdev02 hadoop       1160 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00213.html
-rw-r--r--   2 scelisdev02 hadoop       1020 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00214.html
-rw-r--r--   2 scelisdev02 hadoop       1142 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00215.html
-rw-r--r--   2 scelisdev02 hadoop       1090 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00216.html
-rw-r--r--   2 scelisdev02 hadoop       1607 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00217.html
-rw-r--r--   2 scelisdev02 hadoop       1275 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00218.html
-rw-r--r--   2 scelisdev02 hadoop       1178 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00219.html
-rw-r--r--   2 scelisdev02 hadoop       1036 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00220.html
-rw-r--r--   2 scelisdev02 hadoop       1144 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00221.html
-rw-r--r--   2 scelisdev02 hadoop       1112 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00222.html
-rw-r--r--   2 scelisdev02 hadoop       1640 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00223.html
-rw-r--r--   2 scelisdev02 hadoop       1297 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00224.html
-rw-r--r--   2 scelisdev02 hadoop       1195 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00225.html
-rw-r--r--   2 scelisdev02 hadoop       1030 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00226.html
-rw-r--r--   2 scelisdev02 hadoop       1130 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00227.html
-rw-r--r--   2 scelisdev02 hadoop       1132 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00228.html
-rw-r--r--   2 scelisdev02 hadoop       1645 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00229.html
-rw-r--r--   2 scelisdev02 hadoop       1302 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00230.html
-rw-r--r--   2 scelisdev02 hadoop       1197 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00231.html
-rw-r--r--   2 scelisdev02 hadoop       1049 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00232.html
-rw-r--r--   2 scelisdev02 hadoop       1149 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00233.html
-rw-r--r--   2 scelisdev02 hadoop       1112 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00234.html
-rw-r--r--   2 scelisdev02 hadoop       1616 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00235.html
-rw-r--r--   2 scelisdev02 hadoop       1268 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00236.html
-rw-r--r--   2 scelisdev02 hadoop       1172 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00237.html
-rw-r--r--   2 scelisdev02 hadoop        996 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00238.html
-rw-r--r--   2 scelisdev02 hadoop       1110 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00239.html
-rw-r--r--   2 scelisdev02 hadoop       1093 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00240.html
-rw-r--r--   2 scelisdev02 hadoop       1627 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00241.html
-rw-r--r--   2 scelisdev02 hadoop       1297 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00242.html
-rw-r--r--   2 scelisdev02 hadoop       1194 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00243.html
-rw-r--r--   2 scelisdev02 hadoop       1048 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00244.html
-rw-r--r--   2 scelisdev02 hadoop       1150 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00245.html
-rw-r--r--   2 scelisdev02 hadoop       1137 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00246.html
-rw-r--r--   2 scelisdev02 hadoop       1660 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00247.html
-rw-r--r--   2 scelisdev02 hadoop       1300 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00248.html
-rw-r--r--   2 scelisdev02 hadoop       1187 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00249.html
-rw-r--r--   2 scelisdev02 hadoop       1030 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00250.html
-rw-r--r--   2 scelisdev02 hadoop       1155 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00251.html
-rw-r--r--   2 scelisdev02 hadoop       1128 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00252.html
-rw-r--r--   2 scelisdev02 hadoop       1641 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00253.html
-rw-r--r--   2 scelisdev02 hadoop       1309 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00254.html
-rw-r--r--   2 scelisdev02 hadoop       1176 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00255.html
-rw-r--r--   2 scelisdev02 hadoop       1026 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00256.html
-rw-r--r--   2 scelisdev02 hadoop       1146 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00257.html
-rw-r--r--   2 scelisdev02 hadoop       1128 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00258.html
-rw-r--r--   2 scelisdev02 hadoop       1614 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00259.html
-rw-r--r--   2 scelisdev02 hadoop       1297 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00260.html
-rw-r--r--   2 scelisdev02 hadoop       1212 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00261.html
-rw-r--r--   2 scelisdev02 hadoop       1045 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00262.html
-rw-r--r--   2 scelisdev02 hadoop       1165 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00263.html
-rw-r--r--   2 scelisdev02 hadoop       1109 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00264.html
-rw-r--r--   2 scelisdev02 hadoop       1639 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00265.html
-rw-r--r--   2 scelisdev02 hadoop       1302 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00266.html
-rw-r--r--   2 scelisdev02 hadoop       1179 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00267.html
-rw-r--r--   2 scelisdev02 hadoop       1046 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00268.html
-rw-r--r--   2 scelisdev02 hadoop       1132 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00269.html
-rw-r--r--   2 scelisdev02 hadoop       1122 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00270.html
-rw-r--r--   2 scelisdev02 hadoop       1618 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00271.html
-rw-r--r--   2 scelisdev02 hadoop       1268 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00272.html
-rw-r--r--   2 scelisdev02 hadoop       1151 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00273.html
-rw-r--r--   2 scelisdev02 hadoop       1020 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00274.html
-rw-r--r--   2 scelisdev02 hadoop       1105 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00275.html
-rw-r--r--   2 scelisdev02 hadoop       1101 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00276.html
-rw-r--r--   2 scelisdev02 hadoop       1604 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00277.html
-rw-r--r--   2 scelisdev02 hadoop       1253 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00278.html
-rw-r--r--   2 scelisdev02 hadoop       1169 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00279.html
-rw-r--r--   2 scelisdev02 hadoop       1021 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00280.html
-rw-r--r--   2 scelisdev02 hadoop       1135 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00281.html
-rw-r--r--   2 scelisdev02 hadoop       1115 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00282.html
-rw-r--r--   2 scelisdev02 hadoop       1608 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00283.html
-rw-r--r--   2 scelisdev02 hadoop       1263 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00284.html
-rw-r--r--   2 scelisdev02 hadoop       1151 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00285.html
-rw-r--r--   2 scelisdev02 hadoop       1017 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00286.html
-rw-r--r--   2 scelisdev02 hadoop       1129 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00287.html
-rw-r--r--   2 scelisdev02 hadoop       1111 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00288.html
-rw-r--r--   2 scelisdev02 hadoop       1612 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00289.html
-rw-r--r--   2 scelisdev02 hadoop       1253 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00290.html
-rw-r--r--   2 scelisdev02 hadoop       1164 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00291.html
-rw-r--r--   2 scelisdev02 hadoop       1041 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00292.html
-rw-r--r--   2 scelisdev02 hadoop       1110 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00293.html
-rw-r--r--   2 scelisdev02 hadoop       1097 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00294.html
-rw-r--r--   2 scelisdev02 hadoop       1625 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00295.html
-rw-r--r--   2 scelisdev02 hadoop       1270 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00296.html
-rw-r--r--   2 scelisdev02 hadoop       1167 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00297.html
-rw-r--r--   2 scelisdev02 hadoop       1015 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00298.html
-rw-r--r--   2 scelisdev02 hadoop       1126 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00299.html
-rw-r--r--   2 scelisdev02 hadoop       1097 2021-09-21 13:42 /devsh_loudacre/kb/KBDOC-00300.html

8. Run the example wordcount.py program on the YARN cluster to count the frequency of words in the sample data file set:
The spark-submit command is used to submit a Spark program for execution on
the cluster. Since Spark is managed by YARN on the course cluster, this gives us the
opportunity to see how the YARN UI displays information about a running job. 
For now, focus on learning about the YARN UI.
While the application is running, continue with the next steps. 
If it completes before you finish the exercise, go to the terminal, press the up arrow until you get to the
spark-submit command again, and rerun the application.

scelisdev02@cca175-m:~/training_materials/devsh$ spark-submit $DEVSH/exercises/yarn/wordcount.py /devsh_loudacre/kb/*
21/09/21 13:56:55 INFO org.spark_project.jetty.util.log: Logging initialized @3223ms to org.spark_project.jetty.util.log.Slf4jLog
21/09/21 13:56:55 INFO org.spark_project.jetty.server.Server: jetty-9.4.z-SNAPSHOT; built: unknown; git: unknown; jvm 1.8.0_275-b01
21/09/21 13:56:55 INFO org.spark_project.jetty.server.Server: Started @3357ms
21/09/21 13:56:55 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@346cd460{HTTP/1.1, (http/1.1)}{0.0.0.0:35093}
21/09/21 13:56:56 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cca175-m/10.132.0.3:8032
21/09/21 13:56:56 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at cca175-m/10.132.0.3:10200
21/09/21 13:56:56 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
21/09/21 13:56:56 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
21/09/21 13:56:56 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE
21/09/21 13:56:56 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE
21/09/21 13:56:58 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1632225409574_0003
21/09/21 13:57:05 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 300
: 4600
worry: 50
KBDOC-00028</h2>: 1
2200.</li>: 1
content="KBDOC-00109": 1
content="KBDOC-00126": 1
content="KBDOC-00183": 1

.....
.....

21/09/21 13:57:29 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@346cd460{HTTP/1.1, (http/1.1)}{0.0.0.0:0}


9. Reload the YARN RM page in Firefox. Notice that the application you just started is
displayed in the list of applications in the bottom section of the RM home page.

scelisdev02@cca175-m:~/training_materials/devsh$ yarn application -list
21/09/21 14:21:19 INFO client.RMProxy: Connecting to ResourceManager at cca175-m/10.132.0.3:8032
21/09/21 14:21:19 INFO client.AHSProxy: Connecting to Application History server at cca175-m/10.132.0.3:10200
Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):1
                Application-Id      Application-Name        Application-Type          User           Queue                   State             Final-State           Progress                        Tracking-URL
application_1632225409574_0001          PySparkShell                   SPARK    scelisdev02        default                 RUNNING               UNDEFINED                10% http://cca175-m.europe-west1-b.c.cca175-325912.internal:33441
scelisdev02@cca175-m:~/training_materials/devsh$ yarn logs -applicationId application_1632225409574_0001
21/09/21 14:22:15 INFO client.RMProxy: Connecting to ResourceManager at cca175-m/10.132.0.3:8032
21/09/21 14:22:15 INFO client.AHSProxy: Connecting to Application History server at cca175-m/10.132.0.3:10200
Can not find any log file matching the pattern: [ALL] for the container: container_1632225409574_0001_01_000004 within the application: application_1632225409574_0001
End of LogType:prelaunch.err.
******************************************************************************

End of LogType:stdout.
***********************************************************************

Container: container_1632225409574_0001_01_000003 on cca175-w-0.europe-west1-b.c.cca175-325912.internal:8026
LogAggregationType: LOCAL
============================================================================================================
LogType:stderr
LogLastModifiedTime:Tue Sep 21 12:00:41 +0000 2021
LogLength:545
LogContents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
21/09/21 12:00:41 ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
End of LogType:stderr.
***********************************************************************

Container: container_1632225409574_0001_01_000003 on cca175-w-0.europe-west1-b.c.cca175-325912.internal:8026
LogAggregationType: LOCAL
============================================================================================================
LogType:prelaunch.out
LogLastModifiedTime:Tue Sep 21 11:59:34 +0000 2021
LogLength:70
LogContents:
Setting up env variables
Setting up job resources
Launching container
End of LogType:prelaunch.out.
******************************************************************************

End of LogType:prelaunch.err.This log file belongs to a running container (container_1632225409574_0001_01_000002) and so may not be complete.
******************************************************************************


End of LogType:stdout.This log file belongs to a running container (container_1632225409574_0001_01_000002) and so may not be complete.
***********************************************************************

Container: container_1632225409574_0001_01_000002 on cca175-w-1.europe-west1-b.c.cca175-325912.internal:8026
LogAggregationType: LOCAL
============================================================================================================
LogType:stderr
LogLastModifiedTime:Tue Sep 21 11:59:38 +0000 2021
LogLength:444
LogContents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
End of LogType:stderr.This log file belongs to a running container (container_1632225409574_0001_01_000002) and so may not be complete.
***********************************************************************

Container: container_1632225409574_0001_01_000002 on cca175-w-1.europe-west1-b.c.cca175-325912.internal:8026
LogAggregationType: LOCAL
============================================================================================================
LogType:prelaunch.out
LogLastModifiedTime:Tue Sep 21 11:59:36 +0000 2021
LogLength:70
LogContents:
Setting up env variables
Setting up job resources
Launching container
End of LogType:prelaunch.out.This log file belongs to a running container (container_1632225409574_0001_01_000002) and so may not be complete.
******************************************************************************

End of LogType:prelaunch.err.This log file belongs to a running container (container_1632225409574_0001_01_000001) and so may not be complete.
******************************************************************************

End of LogType:stdout.This log file belongs to a running container (container_1632225409574_0001_01_000001) and so may not be complete.
***********************************************************************

Container: container_1632225409574_0001_01_000001 on cca175-w-0.europe-west1-b.c.cca175-325912.internal:8026
LogAggregationType: LOCAL
============================================================================================================
LogType:stderr
LogLastModifiedTime:Tue Sep 21 11:59:33 +0000 2021
LogLength:1051
LogContents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
21/09/21 11:59:32 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cca175-m/10.132.0.3:8030
21/09/21 11:59:33 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
21/09/21 11:59:33 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
21/09/21 11:59:33 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE
21/09/21 11:59:33 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE
End of LogType:stderr.This log file belongs to a running container (container_1632225409574_0001_01_000001) and so may not be complete.
***********************************************************************

Container: container_1632225409574_0001_01_000001 on cca175-w-0.europe-west1-b.c.cca175-325912.internal:8026
LogAggregationType: LOCAL
============================================================================================================
LogType:prelaunch.out
LogLastModifiedTime:Tue Sep 21 11:59:28 +0000 2021
LogLength:70
LogContents:
Setting up env variables
Setting up job resources
Launching container
End of LogType:prelaunch.out.This log file belongs to a running container (container_1632225409574_0001_01_000001) and so may not be complete.
******************************************************************************

(base) hadoop@sc-ubuntu-20-04-2-lts:~/SCProjects/0_SCProjects_github.com_SCelisV$ ping 10.132.0.3
PING 10.132.0.3 (10.132.0.3) 56(84) bytes of data.

NO ME PUDE CONECTAR DESDE EL GCP A LA MÁQUINA PARA VER LA EJECUCION DE YARN

SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

http://cca175-m.europe-west1-b.c.cca175-325912.internal:33441

9. Reload the YARN RM page in Firefox. Notice that the application you just started is
displayed in the list of applications in the bottom section of the RM home page.

10. As you did in the first exercise section, select Nodes.
11. Select the node HTTP address link for localhost.localdomain to open the Node Manager UI.
12. Now that an application is running, you can click List of Applications to see the application you submitted.
13. If your application is still running, try clicking on List of Containers.
This will display the containers the Resource Manager has allocated on the selected node for the current application. 
(No containers will show if no applications are running; if you missed it because the application completed, 
you can run the application again. In the terminal window, use the up arrow key to recall previous commands.)

View the Application Using the yarn Command
14. Open a second terminal window on your remote desktop.
15. View the list of currently running applications.

scelisdev02@cca175-m:~$ yarn application -list
21/09/21 14:52:27 INFO client.RMProxy: Connecting to ResourceManager at cca175-m/10.132.0.3:8032
21/09/21 14:52:27 INFO client.AHSProxy: Connecting to Application History server at cca175-m/10.132.0.3:10200
Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):1
                Application-Id      Application-Name        Application-Type           User           Queue                   State              Final-State             Progress                         Tracking-URL
application_1632225409574_0001          PySparkShell                   SPARK     scelisdev02        default                 RUNNING                UNDEFINED                  10%  http://cca175-m.europe-west1-b.c.cca175-325912.internal:33441
scelisdev02@cca175-m:~$ yarn application -list
21/09/21 14:52:30 INFO client.RMProxy: Connecting to ResourceManager at cca175-m/10.132.0.3:8032
21/09/21 14:52:31 INFO client.AHSProxy: Connecting to Application History server at cca175-m/10.132.0.3:10200
Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):2
                Application-Id      Application-Name        Application-Type           User           Queue                   State              Final-State             Progress                         Tracking-URL
application_1632225409574_0001          PySparkShell                   SPARK     scelisdev02        default                 RUNNING                UNDEFINED                  10%  http://cca175-m.europe-west1-b.c.cca175-325912.internal:33441
application_1632225409574_0006       PythonWordCount                   SPARK     scelisdev02        default                ACCEPTED                UNDEFINED                   0%                                  N/A
scelisdev02@cca175-m:~$ yarn application -list
21/09/21 14:52:37 INFO client.RMProxy: Connecting to ResourceManager at cca175-m/10.132.0.3:8032
21/09/21 14:52:37 INFO client.AHSProxy: Connecting to Application History server at cca175-m/10.132.0.3:10200
Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):2
                Application-Id      Application-Name        Application-Type           User           Queue                   State              Final-State             Progress                         Tracking-URL
application_1632225409574_0001          PySparkShell                   SPARK     scelisdev02        default                 RUNNING                UNDEFINED                  10%  http://cca175-m.europe-west1-b.c.cca175-325912.internal:33441
application_1632225409574_0006       PythonWordCount                   SPARK     scelisdev02        default                 RUNNING                UNDEFINED                  10%  http://cca175-m.europe-west1-b.c.cca175-325912.internal:37305
scelisdev02@cca175-m:~$ yarn application -list
21/09/21 14:52:40 INFO client.RMProxy: Connecting to ResourceManager at cca175-m/10.132.0.3:8032
21/09/21 14:52:41 INFO client.AHSProxy: Connecting to Application History server at cca175-m/10.132.0.3:10200
Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):2
                Application-Id      Application-Name        Application-Type           User           Queue                   State              Final-State             Progress                         Tracking-URL
application_1632225409574_0001          PySparkShell                   SPARK     scelisdev02        default                 RUNNING                UNDEFINED                  10%  http://cca175-m.europe-west1-b.c.cca175-325912.internal:33441
application_1632225409574_0006       PythonWordCount                   SPARK     scelisdev02        default                 RUNNING                UNDEFINED                  10%  http://cca175-m.europe-west1-b.c.cca175-325912.internal:37305

scelisdev02@cca175-m:~$ yarn application -list -appStates ALL
21/09/21 14:53:40 INFO client.RMProxy: Connecting to ResourceManager at cca175-m/10.132.0.3:8032
21/09/21 14:53:40 INFO client.AHSProxy: Connecting to Application History server at cca175-m/10.132.0.3:10200
Total number of applications (application-types: [], states: [NEW, NEW_SAVING, SUBMITTED, ACCEPTED, RUNNING, FINISHED, FAILED, KILLED] and tags: []):6
                Application-Id      Application-Name        Application-Type           User           Queue                   State              Final-State             Progress                         Tracking-URL
application_1632225409574_0004       PythonWordCount                   SPARK     scelisdev02        default                FINISHED                SUCCEEDED                 100%  cca175-m:18080/history/application_1632225409574_0004/1
application_1632225409574_0003       PythonWordCount                   SPARK     scelisdev02        default                FINISHED                SUCCEEDED                 100%  cca175-m:18080/history/application_1632225409574_0003/1
application_1632225409574_0002       PythonWordCount                   SPARK     scelisdev02        default                FINISHED                SUCCEEDED                 100%  cca175-m:18080/history/application_1632225409574_0002/1
application_1632225409574_0001          PySparkShell                   SPARK     scelisdev02        default                 RUNNING                UNDEFINED                  10%  http://cca175-m.europe-west1-b.c.cca175-325912.internal:33441
application_1632225409574_0006       PythonWordCount                   SPARK     scelisdev02        default                FINISHED                SUCCEEDED                 100%  cca175-m:18080/history/application_1632225409574_0006/1
application_1632225409574_0005       PythonWordCount                   SPARK     scelisdev02        default                FINISHED                SUCCEEDED                 100%  cca175-m:18080/history/application_1632225409574_0005/1

scelisdev02@cca175-m:~$ yarn application -status application_1632225409574_0006
21/09/21 14:54:36 INFO client.RMProxy: Connecting to ResourceManager at cca175-m/10.132.0.3:8032
21/09/21 14:54:36 INFO client.AHSProxy: Connecting to Application History server at cca175-m/10.132.0.3:10200
21/09/21 14:54:37 INFO conf.Configuration: resource-types.xml not found
21/09/21 14:54:37 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
21/09/21 14:54:37 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE
21/09/21 14:54:37 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE
Application Report : 
        Application-Id : application_1632225409574_0006
        Application-Name : PythonWordCount
        Application-Type : SPARK
        User : scelisdev02
        Queue : default
        Application Priority : 0
        Start-Time : 1632235950068
        Finish-Time : 1632235973499
        Progress : 100%
        State : FINISHED
        Final-State : SUCCEEDED
        Tracking-URL : cca175-m:18080/history/application_1632225409574_0006/1
        RPC Port : -1
        AM Host : 10.132.0.2
        Aggregate Resource Allocation : 252180 MB-seconds, 59 vcore-seconds
        Aggregate Resource Preempted : 0 MB-seconds, 0 vcore-seconds
        Log Aggregation Status : SUCCEEDED
        Diagnostics : 
        Unmanaged Application : false
        Application Node Label Expression : <Not set>
        AM container Node Label Expression : <DEFAULT_PARTITION>
        TimeoutType : LIFETIME  ExpiryTime : UNLIMITED  RemainingTime : -1seconds



solucion del manual:  /home/scelisdev02/training_materials/devsh/exercises/yarn/solution/run-yarn-commands.sh
---------------------
# upload data file if not previously uploaded: hdfs dfs -put $DEVDATA/kb /devsh_loudacre/
hdfs dfs -put $DEVDATA/kb /devsh_loudacre/
hdfs dfs -ls /devsh_loudacre/kb
spark-submit $DEVSH/exercises/yarn/wordcount.py /devsh_loudacre/kb/*
yarn application -list
yarn application -list -appStates AL

------------------
Hands-On Exercise: Exploring DataFrames Using the Apache Spark Shell
------------------

https://spark.apache.org/docs/2.4.8/sql-programming-guide.html
https://spark.apache.org/docs/2.4.8/api/python/index.html
https://spark.apache.org/docs/2.4.8/api/python/pyspark.sql.html


http://spark.apache.org/docs/2.4.0/api/scala/index.html#org.apache.spark.package
http://spark.apache.org/docs/2.4.0/api/scala/index.html#org.apache.spark.sql.package
http://spark.apache.org/docs/2.4.0/api/scala/index.html#org.apache.spark.sql.Dataset
http://spark.apache.org/docs/2.4.0/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions

7. In the terminal window, start the Spark shell. Start either the Python shell or the Scala shell, not both.
To start the Python shell, use the pyspark command.


En GCP he tenido que hacer esto para usar ipyton y poder cargar el fichero con load

IPYTHON and IPYTHON_OPTS are removed in Spark 2.0+. 
scelisdev02@cca175-m:~$ which ipython
/opt/conda/default/bin/ipython
scelisdev02@cca175-m:~$ which pyspark
/opt/conda/default/bin/pyspark
scelisdev02@cca175-m:~$ IPYTHON=1 /opt/conda/default/bin/pyspark
export IPYTHON=1

 para superiores a 2.0+ set PYSPARK_DRIVER_PYTHON and PYSPARK_DRIVER_PYTHON_OPTS instead

scelisdev02@cca175-m:~$ export PYSPARK_DRIVER_PYTHON=ipython
scelisdev02@cca175-m:~$ PYSPARK_DRIVER_PYTHON_OPTS=ipython
scelisdev02@cca175-m:~$ pyspark

En ubuntu he tenido que hacer esto para que no diera error: ModuleNotFoundError: No module named 'py4j'

echo $PYTHONPATH
echo $SPARK_HOME/python/lib/
/home/hadoop/spark/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/

export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.8.1-src.zip:$PYTHONPATH

$ pyspark

Connected, host fingerprint: ssh-rsa 0 75:69:F5:20:FC:17:28:AD:FD:2E:6B:91:DB:90:43:FD:DF:A6:00:BE:BA:00:D2:2B:70:2B:D0:38:80:38:3F:B9
Last login: Wed Sep 22 10:55:10 2021 from 35.235.243.225
scelisdev02@cca175-m:~$ pyspark
Python 3.7.4 (default, Aug 13 2019, 20:35:49) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.8
      /_/

Using Python version 3.7.4 (default, Aug 13 2019 20:35:49)
SparkSession available as 'spark'.
>>> 

To start the Scala shell, use the spark-shell command.

# averiguar esto: spark-shell --master local [2]

$ spark-shell

Connected, host fingerprint: ssh-rsa 0 75:69:F5:20:FC:17:28:AD:FD:2E:6B:91:DB:90:43:FD:DF:A6:00:BE:BA:00:D2:2B:70:2B:D0:38:80:38:3F:B9
Last login: Thu Sep 23 11:00:58 2021 from 35.235.243.226
scelisdev02@cca175-m:~$ spark-shell
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://cca175-m.europe-west1-b.c.cca175-325912.internal:44079
Spark context available as 'sc' (master = yarn, app id = application_1632394097018_0002).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.8
      /_/
         
Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_275)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 

8. Spark creates a SparkSession object for you called spark. Make sure the object exists. 
pyspark> spark

>>> spark
<pyspark.sql.session.SparkSession object at 0x7fc18aba03d0>

scala> spark
res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@16019c16

Scala will display similar information in a different format:
org.apache.spark.sql.SparkSession =
org.apache.spark.sql.SparkSession@address

Note: In subsequent instructions, both Python and Scala commands will be shown
but not noted explicitly; Python shell commands are in blue and preceded with
pyspark>, and Scala shell commands are in red and preceded with scala>.

9. Using command completion, you can see all the available Spark session methods:
type spark. (spark followed by a dot) and then the TAB key.
Note: You can exit the Scala shell by typing sys.exit. To exit the Python shell, press Ctrl+D or type exit. However, stay in the shell for now to complete the remainder of this exercise.

scala> spark.
baseRelationToDataFrame   conf              emptyDataFrame   implicits         range        sessionState   sql          streams   udf       
catalog                   createDataFrame   emptyDataset     listenerManager   read         sharedState    sqlContext   table     version   
close                     createDataset     experimental     newSession        readStream   sparkContext   stop         time                

Read and Display a JSON File
10. Open a new terminal window (in addition to the terminal running the Spark shell).
11. Review the simple text file you will be using: $DEVDATA/devices.json.

scelisdev02@cca175-m:~/training_materials/devsh/data$ cat $DEVDATA/devices.json

{"devnum":1,"release_dt":"2008-10-21T00:00:00.000-07:00","make":"Sorrento","model":"F00L","dev_type":"phone"}
{"devnum":2,"release_dt":"2010-04-19T00:00:00.000-07:00","make":"Titanic","model":"2100","dev_type":"phone"}
{"devnum":3,"release_dt":"2011-02-18T00:00:00.000-08:00","make":"MeeToo","model":"3.0","dev_type":"phone"}
{"devnum":4,"release_dt":"2011-09-21T00:00:00.000-07:00","make":"MeeToo","model":"3.1","dev_type":"phone"}
{"devnum":5,"release_dt":"2008-10-21T00:00:00.000-07:00","make":"iFruit","model":"1","dev_type":"phone"}
{"devnum":6,"release_dt":"2011-11-02T00:00:00.000-07:00","make":"iFruit","model":"3","dev_type":"phone"}
{"devnum":7,"release_dt":"2010-05-20T00:00:00.000-07:00","make":"iFruit","model":"2","dev_type":"phone"}
{"devnum":8,"release_dt":"2013-07-02T00:00:00.000-07:00","make":"iFruit","model":"5","dev_type":"phone"}
{"devnum":9,"release_dt":"2008-10-21T00:00:00.000-07:00","make":"Titanic","model":"1000","dev_type":"phone"}
{"devnum":10,"release_dt":"2008-10-21T00:00:00.000-07:00","make":"MeeToo","model":"1.0","dev_type":"phone"}
{"devnum":11,"release_dt":"2011-02-28T00:00:00.000-08:00","make":"Sorrento","model":"F21L","dev_type":"phone"}
{"devnum":12,"release_dt":"2012-10-25T00:00:00.000-07:00","make":"iFruit","model":"4","dev_type":"phone"}
{"devnum":13,"release_dt":"2011-11-21T00:00:00.000-08:00","make":"Sorrento","model":"F23L","dev_type":"phone"}
{"devnum":14,"release_dt":"2010-05-25T00:00:00.000-07:00","make":"Titanic","model":"2200","dev_type":"phone"}
{"devnum":15,"release_dt":"2010-06-20T00:00:00.000-07:00","make":"Ronin","model":"Novelty Note 1","dev_type":"phone"}
{"devnum":16,"release_dt":"2012-07-21T00:00:00.000-07:00","make":"Titanic","model":"2500","dev_type":"phone"}
{"devnum":17,"release_dt":"2013-04-11T00:00:00.000-07:00","make":"Ronin","model":"Novelty Note 3","dev_type":"phone"}
{"devnum":18,"release_dt":"2011-10-02T00:00:00.000-07:00","make":"Ronin","model":"Novelty Note 2","dev_type":"phone"}
{"devnum":19,"release_dt":"2013-07-02T00:00:00.000-07:00","make":"Ronin","model":"Novelty Note 4","dev_type":"phone"}
{"devnum":20,"release_dt":"2012-07-21T00:00:00.000-07:00","make":"iFruit","model":"3A","dev_type":"phone"}
{"devnum":21,"release_dt":"2011-02-18T00:00:00.000-08:00","make":"Titanic","model":"2300","dev_type":"phone"}
{"devnum":22,"release_dt":"2012-03-12T00:00:00.000-07:00","make":"Sorrento","model":"F24L","dev_type":"phone"}
{"devnum":23,"release_dt":"2010-11-01T00:00:00.000-07:00","make":"Sorrento","model":"F20L","dev_type":"phone"}
{"devnum":24,"release_dt":"2012-10-21T00:00:00.000-07:00","make":"Sorrento","model":"F32L","dev_type":"phone"}
{"devnum":25,"release_dt":"2011-10-01T00:00:00.000-07:00","make":"Sorrento","model":"F22L","dev_type":"phone"}
{"devnum":26,"release_dt":"2012-06-21T00:00:00.000-07:00","make":"Sorrento","model":"F30L","dev_type":"phone"}
{"devnum":27,"release_dt":"2009-10-25T00:00:00.000-07:00","make":"Sorrento","model":"F10L","dev_type":"phone"}
{"devnum":28,"release_dt":"2012-09-21T00:00:00.000-07:00","make":"Titanic","model":"4000","dev_type":"phone"}
{"devnum":29,"release_dt":"2013-11-01T00:00:00.000-07:00","make":"Sorrento","model":"F41L","dev_type":"phone"}
{"devnum":30,"release_dt":"2013-08-01T00:00:00.000-07:00","make":"Titanic","model":"DeckChairs","dev_type":"phone"}
{"devnum":31,"release_dt":"2012-08-04T00:00:00.000-07:00","make":"MeeToo","model":"4.1","dev_type":"phone"}
{"devnum":32,"release_dt":"2012-07-21T00:00:00.000-07:00","make":"MeeToo","model":"4.0","dev_type":"phone"}
{"devnum":33,"release_dt":"2010-05-15T00:00:00.000-07:00","make":"MeeToo","model":"2.0","dev_type":"phone"}
{"devnum":34,"release_dt":"2009-12-21T00:00:00.000-08:00","make":"Titanic","model":"2000","dev_type":"phone"}
{"devnum":35,"release_dt":"2012-09-21T00:00:00.000-07:00","make":"MeeToo","model":"5.0","dev_type":"phone"}
{"devnum":36,"release_dt":"2013-08-01T00:00:00.000-07:00","make":"MeeToo","model":"5.1","dev_type":"phone"}
{"devnum":37,"release_dt":"2012-08-04T00:00:00.000-07:00","make":"Titanic","model":"3000","dev_type":"phone"}
{"devnum":38,"release_dt":"2008-11-25T00:00:00.000-08:00","make":"Titanic","model":"1100","dev_type":"phone"}
{"devnum":39,"release_dt":"2013-03-11T00:00:00.000-07:00","make":"Sorrento","model":"F33L","dev_type":"phone"}
{"devnum":40,"release_dt":"2013-02-11T00:00:00.000-08:00","make":"iFruit","model":"4A","dev_type":"phone"}
{"devnum":41,"release_dt":"2012-07-04T00:00:00.000-07:00","make":"Sorrento","model":"F31L","dev_type":"phone"}
{"devnum":42,"release_dt":"2013-06-26T00:00:00.000-07:00","make":"Sorrento","model":"F40L","dev_type":"phone"}
{"devnum":43,"release_dt":"2009-03-11T00:00:00.000-07:00","make":"Sorrento","model":"F01L","dev_type":"phone"}
{"devnum":44,"release_dt":"2010-04-10T00:00:00.000-07:00","make":"Sorrento","model":"F11L","dev_type":"phone"}
{"devnum":45,"release_dt":"2011-09-21T00:00:00.000-07:00","make":"Titanic","model":"2400","dev_type":"phone"}
{"devnum":46,"release_dt":"2013-10-02T00:00:00.000-07:00","make":"Ronin","model":"S4","dev_type":"phone"}
{"devnum":47,"release_dt":"2010-05-20T00:00:00.000-07:00","make":"Ronin","model":"S1","dev_type":"phone"}
{"devnum":48,"release_dt":"2013-02-11T00:00:00.000-08:00","make":"Ronin","model":"S3","dev_type":"phone"}
{"devnum":49,"release_dt":"2012-07-21T00:00:00.000-07:00","make":"Ronin","model":"S2","dev_type":"phone"}
{"devnum":50,"release_dt":"2013-10-02T00:00:00.000-07:00","make":"iFruit","model":"5A","dev_type":"phone"}

12. Upload the data file to the /devsh_loudacre directory in HDFS:
scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -put $DEVDATA/devices.json /devsh_loudacre/

13. In the Spark shell, create a new DataFrame based on the devices.json file in HDFS.

>>> devDF = spark.read.json("/devsh_loudacre/devices.json")

scala> val devDF = spark.read.json("/devsh_loudacre/devices.json")
devDF: org.apache.spark.sql.DataFrame = [dev_type: string, devnum: bigint ... 3 more fields]

14. Spark has not yet read the data in the file, but it has scanned the file to infer the schema. 
View the schema, and note that the column names match the record field names in the JSON file.

>>> devDF.printSchema()                                                         
root
 |-- dev_type: string (nullable = true)
 |-- devnum: long (nullable = true)
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)
 |-- release_dt: string (nullable = true)


scala> devDF.printSchema
root
 |-- dev_type: string (nullable = true)
 |-- devnum: long (nullable = true)
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)
 |-- release_dt: string (nullable = true)


15. Display the data in the DataFrame using the show function. If you don’t pass an argument to show, Spark will display the first 20 rows in the DataFrame. 
For this step, display the first five rows. Note that the data is displayed in tabular form, using the column names defined in the schema.

> devDF.show(5)

scala> devDF.show(5)
+--------+------+--------+-----+--------------------+
|dev_type|devnum|    make|model|          release_dt|
+--------+------+--------+-----+--------------------+
|   phone|     1|Sorrento| F00L|2008-10-21T00:00:...|
|   phone|     2| Titanic| 2100|2010-04-19T00:00:...|
|   phone|     3|  MeeToo|  3.0|2011-02-18T00:00:...|
|   phone|     4|  MeeToo|  3.1|2011-09-21T00:00:...|
|   phone|     5|  iFruit|    1|2008-10-21T00:00:...|
+--------+------+--------+-----+--------------------+
only showing top 5 rows

16. The show and printSchema operations are actions—that is, they return a value from the distributed DataFrame to the Spark driver. 
Both functions display the data in a nicely formatted table. These functions are intended for interactive use in the
shell, but do not allow you to actually work with the data that is returned. 
Try using the take action instead, which returns an array (Scala) or list (Python) of Row objects. 
You can display the data by iterating through the collection.

>>> rows = devDF.take(5)
>>> type(rows)
<class 'list'>
>>> for row in rows: print(row)
... 
Row(dev_type='phone', devnum=1, make='Sorrento', model='F00L', release_dt='2008-10-21T00:00:00.000-07:00')
Row(dev_type='phone', devnum=2, make='Titanic', model='2100', release_dt='2010-04-19T00:00:00.000-07:00')
Row(dev_type='phone', devnum=3, make='MeeToo', model='3.0', release_dt='2011-02-18T00:00:00.000-08:00')
Row(dev_type='phone', devnum=4, make='MeeToo', model='3.1', release_dt='2011-09-21T00:00:00.000-07:00')
Row(dev_type='phone', devnum=5, make='iFruit', model='1', release_dt='2008-10-21T00:00:00.000-07:00')

scala> val rows = devDF.take(5)
rows: Array[org.apache.spark.sql.Row] = Array([phone,1,Sorrento,F00L,2008-10-21T00:00:00.000-07:00], [phone,2,Titanic,2100,2010-04-19T00:00:00.000-07:00], [phone,3,MeeToo,3.0,2011-02-18T00:00:00.000-08:00], [phone,4,MeeToo,3.1,2011-09-21T00:00:00.000-07:00], [phone,5,iFruit,1,2008-10-21T00:00:00.000-07:00])

scala> rows.foreach(println)
[phone,1,Sorrento,F00L,2008-10-21T00:00:00.000-07:00]
[phone,2,Titanic,2100,2010-04-19T00:00:00.000-07:00]
[phone,3,MeeToo,3.0,2011-02-18T00:00:00.000-08:00]
[phone,4,MeeToo,3.1,2011-09-21T00:00:00.000-07:00]
[phone,5,iFruit,1,2008-10-21T00:00:00.000-07:00]

Query a DataFrame
17. Use the count action to return the number of items in the DataFrame.
> devDF.count()

scala> devDF.count
res7: Long = 50

18. DataFrame transformations typically return another DataFrame. 
Try using a select transformation to return a DataFrame with only the make and model columns, then display its schema. 
Note that only the selected columns are in the schema.

>>> makeModelDF = devDF.select("make","model")
>>> makeModelDF.printSchema()
root
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)

>>> makeModelDF.count()
50

scala> val makeModelDF = devDF.select("make","model")
makeModelDF: org.apache.spark.sql.DataFrame = [make: string, model: string]

scala> makeModelDF.printSchema
root
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)

scala> makeModelDF.count
res10: Long = 50

19. A query is a series of one or more transformations followed by an action. 
Spark does not execute the query until you call the action operation. 
Display the first 20 lines of the final DataFrame in the series using the show action.

>>> makeModelDF.show()
+--------+--------------+
|    make|         model|
+--------+--------------+
|Sorrento|          F00L|
| Titanic|          2100|
|  MeeToo|           3.0|
|  MeeToo|           3.1|
|  iFruit|             1|
|  iFruit|             3|
|  iFruit|             2|
|  iFruit|             5|
| Titanic|          1000|
|  MeeToo|           1.0|
|Sorrento|          F21L|
|  iFruit|             4|
|Sorrento|          F23L|
| Titanic|          2200|
|   Ronin|Novelty Note 1|
| Titanic|          2500|
|   Ronin|Novelty Note 3|
|   Ronin|Novelty Note 2|
|   Ronin|Novelty Note 4|
|  iFruit|            3A|
+--------+--------------+
only showing top 20 rows

scala> makeModelDF.show
+--------+--------------+
|    make|         model|
+--------+--------------+
|Sorrento|          F00L|
| Titanic|          2100|
|  MeeToo|           3.0|
|  MeeToo|           3.1|
|  iFruit|             1|
|  iFruit|             3|
|  iFruit|             2|
|  iFruit|             5|
| Titanic|          1000|
|  MeeToo|           1.0|
|Sorrento|          F21L|
|  iFruit|             4|
|Sorrento|          F23L|
| Titanic|          2200|
|   Ronin|Novelty Note 1|
| Titanic|          2500|
|   Ronin|Novelty Note 3|
|   Ronin|Novelty Note 2|
|   Ronin|Novelty Note 4|
|  iFruit|            3A|
+--------+--------------+
only showing top 20 rows

20. Transformations in a query can be chained together. Execute a single command to show the results of a query using select and where.
The resulting DataFrame will contain only the columns devnum, make, and model, and only the rows where the
make is Ronin.

>>> devDF.select("devnum","make","model").where("make = 'Ronin'").show()
+------+-----+--------------+
|devnum| make|         model|
+------+-----+--------------+
|    15|Ronin|Novelty Note 1|
|    17|Ronin|Novelty Note 3|
|    18|Ronin|Novelty Note 2|
|    19|Ronin|Novelty Note 4|
|    46|Ronin|            S4|
|    47|Ronin|            S1|
|    48|Ronin|            S3|
|    49|Ronin|            S2|
+------+-----+--------------+

scala> devDF.select("devnum","make","model").where("make = 'Ronin'").show()
+------+-----+--------------+
|devnum| make|         model|
+------+-----+--------------+
|    15|Ronin|Novelty Note 1|
|    17|Ronin|Novelty Note 3|
|    18|Ronin|Novelty Note 2|
|    19|Ronin|Novelty Note 4|
|    46|Ronin|            S4|
|    47|Ronin|            S1|
|    48|Ronin|            S3|
|    49|Ronin|            S2|
+------+-----+--------------+

solucion del manual:  /home/scelisdev02/training_materials/devsh/exercises/spark-shell/solution
-----------------------------------------------

------------------
Hands-On Exercise: Working with DataFrames and Schemas
------------------

Create a DataFrame Based on a Hive Table
1. This exercise uses a DataFrame based on the accounts table in the devsh Hive database. 
You can review the schema using the Beeline SQL command line to access Hive.
In a terminal session (not one that is running the Spark shell), enter the following command:

----
Error con beeline

scelisdev02@cca175-m:~/training_materials/devsh/data$ beeline -u jdbc:hive2://localhost:10000 -e "DESCRIBE devsh.accounts"
Connecting to jdbc:hive2://localhost:10000
Connected to: Apache Hive (version 2.3.7)
Driver: Hive JDBC (version 2.3.7)
Transaction isolation: TRANSACTION_REPEATABLE_READ
. . . . . . . . . . . . . . . .> 
. . . . . . . . . . . . . . . .> 
Closing: 0: jdbc:hive2://localhost:10000

scelisdev02@cca175-m:~/training_materials/devsh/data$ hive
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin)

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true

hive> describe devsh.accounts;
OK
acct_num                int                                         
acct_create_dt          timestamp                                   
acct_close_dt           timestamp                                   
first_name              varchar(255)                                
last_name               varchar(255)                                
address                 varchar(255)                                
city                    varchar(255)                                
state                   varchar(255)                                
zipcode                 varchar(255)                                
phone_number            varchar(255)                                
created                 timestamp                                   
modified                timestamp                                   
Time taken: 0.282 seconds, Fetched: 12 row(s)

>>> accountsDF = spark.read.table("devsh.accounts")
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used

scala> val accountsDF = spark.read.table("devsh.accounts")
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used
accountsDF: org.apache.spark.sql.DataFrame = [acct_num: int, acct_create_dt: timestamp ... 10 more fields]

Error con beeline
----

scelisdev02@cca175-m:/etc/hive/conf.dist$  hive -e "SELECT * FROM devsh.accounts LIMIT 10"
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin)

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true
Query ID = scelisdev02_20210923205456_430cdcbd-ba6f-47d5-85d9-81b648e764e7
Total jobs = 1
Launching Job 1 out of 1
Status: Running (Executing on YARN cluster with App id application_1632429311159_0006)

----------------------------------------------------------------------------------------------
        VERTICES      MODE        STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED  
----------------------------------------------------------------------------------------------
Map 1 .......... container     SUCCEEDED      1          1        0        0       0       0  
----------------------------------------------------------------------------------------------
VERTICES: 01/01  [==========================>>] 100%  ELAPSED TIME: 5.14 s     
----------------------------------------------------------------------------------------------
OK
1       2008-10-23 16:05:05     NULL    Donald  Becton  2275 Washburn Street    Oakland CA      94660   5100032418      2014-03-18 13:29:47  2014-03-18 13:29:47
2       2008-11-12 03:00:01     NULL    Donna   Jones   3885 Elliott Street     San Francisco   CA      94171   4150835799      2014-03-18 13:29:47   2014-03-18 13:29:47
3       2008-12-21 09:19:50     NULL    Dorthy  Chalmers        4073 Whaley Lane        San Mateo       CA      94479   6506877757      2014-03-18 13:29:47   2014-03-18 13:29:47
4       2008-11-28 00:08:09     NULL    Leila   Spencer 1447 Ross Street        San Mateo       CA      94444   6503198619      2014-03-18 13:29:47   2014-03-18 13:29:47
5       2008-11-15 23:06:06     NULL    Anita   Laughlin        2767 Hill Street        Richmond        CA      94872   5107754354      2014-03-18 13:29:47   2014-03-18 13:29:47
6       2008-11-20 12:39:33     2014-03-01 07:37:48     Stevie  Bridge  3977 Linda Street       Sacramento      CA      94264   9162111862   2014-03-18 13:29:47      2014-03-18 13:29:47
7       2008-12-09 10:32:12     2010-10-16 10:01:51     David   Eggers  2109 Ross Street        Oakland CA      94508   5103935529      2014-03-18 13:29:47   2014-03-18 13:29:47
8       2008-12-15 08:49:38     NULL    Dorothy Koopman 1985 Pratt Avenue       San Mateo       CA      94469   6502406661      2014-03-18 13:29:47   2014-03-18 13:29:47
9       2008-11-07 17:58:55     2014-02-14 01:26:52     Kara    Kohl    235 Fort Street Palo Alto       CA      94312   6502384894      2014-03-18 13:29:47   2014-03-18 13:29:47
10      2008-12-02 23:28:01     NULL    Diane   Nelson  921 Sardis Sta  Oakland CA      94577   5102711264      2014-03-18 13:29:47     2014-03-18 13:29:47
Time taken: 17.603 seconds, Fetched: 10 row(s)

scelisdev02@cca175-m:~$  hive -e "DESCRIBE devsh.accounts"
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin)

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true
OK
acct_num                int                                         
acct_create_dt          timestamp                                   
acct_close_dt           timestamp                                   
first_name              varchar(255)                                
last_name               varchar(255)                                
address                 varchar(255)                                
city                    varchar(255)                                
state                   varchar(255)                                
zipcode                 varchar(255)                                
phone_number            varchar(255)                                
created                 timestamp                                   
modified                timestamp                                   
Time taken: 1.129 seconds, Fetched: 12 row(s)
scelisdev02@cca175-m:~$ 

2. If it is not already running, start the Spark shell (either Scala or Python, as you prefer).

3. Create a new DataFrame using the Hive devsh.accounts table.
>>> accountsDF = spark.read.table("devsh.accounts")

4. Print the schema and the first few rows of the DataFrame, and note that the schema aligns with that of the Hive table.
>>> accountsDF.printSchema()
root
 |-- acct_num: integer (nullable = true)
 |-- acct_create_dt: timestamp (nullable = true)
 |-- acct_close_dt: timestamp (nullable = true)
 |-- first_name: string (nullable = true)
 |-- last_name: string (nullable = true)
 |-- address: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- zipcode: string (nullable = true)
 |-- phone_number: string (nullable = true)
 |-- created: timestamp (nullable = true)
 |-- modified: timestamp (nullable = true)

5. Create a new DataFrame with rows from the accounts data where the zip code is 94913, and save the result to CSV files in the /devsh_loudacre/accounts_zip94913 HDFS directory. 
You can do this in a single command, as shown below, or with multiple commands.

>>> accountsDF.where("zipcode = 94913").write.option("header", "true").csv("/devsh_loudacre/accounts_zip94913")

scala> accountsDF.where("zipcode = 94913").write.option("header", "true").csv("/devsh_loudacre/accounts_zip94913")

6. Use hdfs in a separate terminal window to view the /devsh_loudacre/accounts_zip94913 directory in HDFS and the data in one of the saved files. Confirm that the CSV file includes a header line, and that only records for the selected zip code are included.

scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/
Found 3 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-23 21:40 /devsh_loudacre/accounts_zip94913
-rw-r--r--   2 scelisdev02 hadoop       5483 2021-09-23 11:25 /devsh_loudacre/devices.json
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-21 13:42 /devsh_loudacre/kb


7. Optional: Try creating a new DataFrame based on the CSV files you created above.
Compare the schema of the original accountsDF and the new DataFrame. 
What’s different? 
Try again, this time setting the inferSchema option to true and compare again.

>>> acczip94913DF = spark.read.csv("/devsh_loudacre/accounts_zip94913")
>>> acczip94913DF.printSchema()
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)
 |-- _c9: string (nullable = true)
 |-- _c10: string (nullable = true)
 |-- _c11: string (nullable = true)

scala> val acczip94913DF = spark.read.csv("/devsh_loudacre/accounts_zip94913")
acczip94913DF: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 10 more fields]

scala> acczip94913DF.printSchema
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)
 |-- _c9: string (nullable = true)
 |-- _c10: string (nullable = true)
 |-- _c11: string (nullable = true)

Try again, this time setting the inferSchema option to true and compare again.

>>> acczip94913DF = spark.read.option("inferschema", "true").csv("/devsh_loudacre/accounts_zip94913") 
>>> acczip94913DF.printSchema()
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)
 |-- _c9: string (nullable = true)
 |-- _c10: string (nullable = true)
 |-- _c11: string (nullable = true)

scala> val acczip94913DF = spark.read.options(Map("inferSchema"->"true", "header"->"true")).csv("/devsh_loudacre/accounts_zip94913")
acczip94913DF: org.apache.spark.sql.DataFrame = [acct_num: int, acct_create_dt: timestamp ... 10 more fields]

scala> acczip94913DF.printSchema
root
 |-- acct_num: integer (nullable = true)
 |-- acct_create_dt: timestamp (nullable = true)
 |-- acct_close_dt: timestamp (nullable = true)
 |-- first_name: string (nullable = true)
 |-- last_name: string (nullable = true)
 |-- address: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- zipcode: integer (nullable = true)
 |-- phone_number: long (nullable = true)
 |-- created: timestamp (nullable = true)
 |-- modified: timestamp (nullable = true)

Define a Schema for a DataFrame
8. If you have not done so yet, review the data in the HDFS file /devsh_loudacre/devices.json.
9. Create a new DataFrame based on the devices.json file. (This command could take several seconds while it infers the schema.)

scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/devices*
{"devnum":50,"release_dt":"2013-10-02T00:00:00.000-07:00","make":"iFruit","model":"5A","dev_type":"phone"}

>>> devDF = spark.read.json("/devsh_loudacre/devices.json")

scala> val devDF = spark.read.json("/devsh_loudacre/devices.json")
devDF: org.apache.spark.sql.DataFrame = [dev_type: string, devnum: bigint ... 3 more fields]

10. View the schema of the devDF DataFrame. Note the column names and types that Spark inferred from the JSON file. 
In particular, note that the release_dt column is of type string, whereas the data in the column actually represents a timestamp.

>>> devDF.printSchema()
root
 |-- devnum: long (nullable = true)
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)
 |-- release_dt: timestamp (nullable = true)
 |-- dev_type: string (nullable = true)

11. Define a schema that correctly specifies the column types for this DataFrame. 
Start by importing the package with the definitions of necessary classes and types.
>>> from pyspark.sql.types import *

scala> import org.apache.spark.sql.types._
import org.apache.spark.sql.types._

12. Next, create a collection of StructField objects, which represent column definitions. 
The release_dt column should be a timestamp.
>>> devColumns = [
... StructField("devnum",LongType()),
... StructField("make",StringType()),
... StructField("model",StringType()),
... StructField("release_dt",TimestampType()),
... StructField("dev_type",StringType())]

scala> val devColumns = List(
     | StructField("devnum",LongType),
     | StructField("make",StringType),
     | StructField("model",StringType),
     | StructField("release_dt",TimestampType),
     | StructField("dev_type",StringType))
devColumns: List[org.apache.spark.sql.types.StructField] = List(StructField(devnum,LongType,true), StructField(make,StringType,true), StructField(model,StringType,true), StructField(release_dt,TimestampType,true), StructField(dev_type,StringType,true))

scala> val devColumns = List(StructField("devnum", LongType), StructField("make", StringType), StructField("model", StringType), StructField("release_dt",TimestampType), StructField("dev_type", StringType))
devColumns: List[org.apache.spark.sql.types.StructField] = List(StructField(devnum,LongType,true), StructField(make,StringType,true), StructField(model,StringType,true), StructField(release_dt,TimestampType,true), StructField(dev_type,StringType,true))


13. Create a schema (a StructType object) using the column definition list.
>>> devSchema = StructType(devColumns)

scala> val devSchema = StructType(devColumns)
devSchema: org.apache.spark.sql.types.StructType = StructType(StructField(devnum,LongType,true), StructField(make,StringType,true), StructField(model,StringType,true), StructField(release_dt,TimestampType,true), StructField(dev_type,StringType,true))

14. Recreate the devDF DataFrame, this time using the new schema.
>>> devDF = spark.read.schema(devSchema).json("/devsh_loudacre/devices.json")

scala> val devDF = spark.read.schema(devSchema).json("/devsh_loudacre/devices.json")
devDF: org.apache.spark.sql.DataFrame = [devnum: bigint, make: string ... 3 more fields]

15. View the schema and data of the new DataFrame, and confirm that the release_dt column type is now timestamp.

>>> devDF.printSchema()
root
 |-- devnum: long (nullable = true)
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)
 |-- release_dt: timestamp (nullable = true)
 |-- dev_type: string (nullable = true)

scala> devDF.printSchema
root
 |-- devnum: long (nullable = true)
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)
 |-- release_dt: timestamp (nullable = true)
 |-- dev_type: string (nullable = true)

16. Now that the device data uses the correct schema, write the data in Parquet format, which automatically embeds the schema. 
Save the Parquet data files into an HDFS directory called /devsh_loudacre/devices_parquet.

>>> devDF.write.save("/devsh_loudacre/devices_parquet")
scala> devDF.write.save("/devsh_loudacre/devices_parquet")

scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/
Found 4 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-23 21:40 /devsh_loudacre/accounts_zip94913
-rw-r--r--   2 scelisdev02 hadoop       5483 2021-09-23 11:25 /devsh_loudacre/devices.json
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-24 09:21 /devsh_loudacre/devices_parquet
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-21 13:42 /devsh_loudacre/kb

17. Optional: In a separate terminal window, use parquet-tools to view the schema of the saved files. 
First download the HDFS directory (or an individual file), then run the command.

scelisdev02@cca175-m:~$ hdfs dfs -get /devsh_loudacre/devices_parquet /tmp/
scelisdev02@cca175-m:/tmp$ ls -l dev*
devices_parquet:
total 4
-rw-r--r--. 1 scelisdev02 scelisdev02 1020 Sep 24 09:26 part-00000-76e7f2db-023a-4099-ab30-ef7332b78c2d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02    0 Sep 24 09:26 _SUCCESS

Esto lo hice en la máquina virtual no en cloud
[training@dev ~]$ parquet-tools schema /tmp/devices_parquet/
message spark_schema{
  optional binary dev_type (UTF8);
  optional int64 devnum;
  optional binary make (UTF8);
  optional binary model (UTF8);
  optional int96 release_dt;
}

Note that the type of the release_dt column is noted as int96; this is how Spark
denotes a timestamp type in Parquet.
For more information about parquet-tools, run parquet-tools --help.

18. Create a new DataFrame using the Parquet files you saved in devices_parquet and view its schema. 
Note that Spark is able to correctly infer the timestamp type of the release_dt column from Parquet’s embedded schema.

>>> parquetDF = spark.read.parquet("/devsh_loudacre/devices_parquet")
>>> parquetDF.printSchema()                                                     
root
 |-- devnum: long (nullable = true)
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)
 |-- release_dt: timestamp (nullable = true)
 |-- dev_type: string (nullable = true)

scala> val parDF = spark.read.parquet("/devsh_loudacre/devices_parquet")
parDF: org.apache.spark.sql.DataFrame = [devnum: bigint, make: string ... 3 more fields]

scala> parDF.printSchema
root
 |-- devnum: long (nullable = true)
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)
 |-- release_dt: timestamp (nullable = true)
 |-- dev_type: string (nullable = true)

solucion del manual: /home/scelisdev02/training_materials/devsh/exercises/dataframes/solution
-----------------------------------------------
------------------
Hands-On Exercise: Analyzing Data with DataFrame Queries
------------------

Query DataFrames Using Column Expressions
1. Optional: Review the API docs for the Column class (which is in the Python module pyspark.sql and the Scala package org.apache.spark.sql). Take note of
the various options available.
2. Start the Spark shell in a terminal if you do not already have it running.
3. Create a new DataFrame called accountsDF based on the Hive devsh.accounts table.
4. Try a simple query with select, using both column reference syntaxes.

>>> accountsDF = spark.read.table("devsh.accounts")
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used
>>> accountsDF.select(accountsDF["first_name"]).show()
+----------+                                                                    
|first_name|
+----------+
|    Donald|
|     Donna|
|    Dorthy|
|     Leila|
|     Anita|
|    Stevie|
|     David|
|   Dorothy|
|      Kara|
|     Diane|
|    Robert|
|    Marcia|
|    Andres|
|       Ann|
|    Joseph|
|     Sarah|
|      Lucy|
|    Roland|
|     Leona|
|   Forrest|
+----------+
>>> accountsDF.select(accountsDF.first_name).show()
+----------+
|first_name|
+----------+
|    Donald|
|     Donna|
|    Dorthy|
|     Leila|
|     Anita|
|    Stevie|
|     David|
|   Dorothy|
|      Kara|
|     Diane|
|    Robert|
|    Marcia|
|    Andres|
|       Ann|
|    Joseph|
|     Sarah|
|      Lucy|
|    Roland|
|     Leona|
|   Forrest|
+----------+
only showing top 20 rows

scala> val accountsDF = spark.read.table("devsh.accounts")
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used
accountsDF: org.apache.spark.sql.DataFrame = [acct_num: int, acct_create_dt: timestamp ... 10 more fields]

scala> accountsDF.select(accountsDF("first_name")).show
+----------+                                                                    
|first_name|
+----------+
|    Donald|
|     Donna|
|    Dorthy|
|     Leila|
|     Anita|
|    Stevie|
|     David|
|   Dorothy|
|      Kara|
|     Diane|
|    Robert|
|    Marcia|
|    Andres|
|       Ann|
|    Joseph|
|     Sarah|
|      Lucy|
|    Roland|
|     Leona|
|   Forrest|
+----------+

scala> accountsDF.select(($"first_name")).show
+----------+
|first_name|
+----------+
|    Donald|
|     Donna|
|    Dorthy|
|     Leila|
|     Anita|
|    Stevie|
|     David|
|   Dorothy|
|      Kara|
|     Diane|
|    Robert|
|    Marcia|
|    Andres|
|       Ann|
|    Joseph|
|     Sarah|
|      Lucy|
|    Roland|
|     Leona|
|   Forrest|
+----------+
only showing top 20 rows

5. To explore column expressions, create a column object to work with, based on the first_name column in the accountsDF DataFrame.

>>> fnCol = accountsDF.first_name
>>> fnCol
Column<b'first_name'>

scala> val fnCol = accountsDF("first_name")
fnCol: org.apache.spark.sql.Column = first_name

6. Note that the object type is Column. To see available methods and attributes, use tab completion—that is, enter fnCol. followed by TAB.

fnCol. + <TAB>
>>> fnCol.
fnCol.alias(             fnCol.bitwiseAND(        fnCol.desc_nulls_first(  fnCol.isNotNull(         fnCol.over(
fnCol.asc(               fnCol.bitwiseOR(         fnCol.desc_nulls_last(   fnCol.isNull(            fnCol.rlike(
fnCol.asc_nulls_first(   fnCol.bitwiseXOR(        fnCol.endswith(          fnCol.isin(              fnCol.startswith(
fnCol.asc_nulls_last(    fnCol.cast(              fnCol.eqNullSafe(        fnCol.like(              fnCol.substr(
fnCol.astype(            fnCol.contains(          fnCol.getField(          fnCol.name(              fnCol.when(
fnCol.between(           fnCol.desc(              fnCol.getItem(           fnCol.otherwise(         

scala> fnCol.
!==   /     >       asc               bitwiseXOR         divide       expr       isInCollection   like       notEqual    startsWith   ||   
%     <     >=      asc_nulls_first   cast               endsWith     geq        isNaN            lt         or          substr            
&&    <=    alias   asc_nulls_last    contains           eqNullSafe   getField   isNotNull        minus      otherwise   toString          
*     <=>   and     between           desc               equalTo      getItem    isNull           mod        over        unary_!           
+     =!=   apply   bitwiseAND        desc_nulls_first   equals       gt         isin             multiply   plus        unary_-           
-     ===   as      bitwiseOR         desc_nulls_last    explain      hashCode   leq              name       rlike       when              


7. New Column objects are created when you perform operations on existing columns. Create a new Column object based on a column expression that identifies users whose first name is Lucy using the equality operator on the fnCol object you created above.

lucyCol = fnCol == "Lucy"
lycyCol.show()

scala> val lucyCol = (fnCol === "Lucy")
lucyCol: org.apache.spark.sql.Column = (first_name = Lucy)

8. Use the lucyCol column expression in a select statement. Because lucyCol is based on a boolean expression, the column values will be true or false

>>> accountsDF.printSchema()
root
 |-- acct_num: integer (nullable = true)
 |-- acct_create_dt: timestamp (nullable = true)
 |-- acct_close_dt: timestamp (nullable = true)
 |-- first_name: string (nullable = true)
 |-- last_name: string (nullable = true)
 |-- address: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- zipcode: string (nullable = true)
 |-- phone_number: string (nullable = true)
 |-- created: timestamp (nullable = true)
 |-- modified: timestamp (nullable = true)

>>> accountsDF.select(accountsDF.first_name, accountsDF.last_name, lucyCol).show()
+----------+---------+-------------------+
|first_name|last_name|(first_name = Lucy)|
+----------+---------+-------------------+
|    Donald|   Becton|              false|
|     Donna|    Jones|              false|
|    Dorthy| Chalmers|              false|
|     Leila|  Spencer|              false|
|     Anita| Laughlin|              false|
|    Stevie|   Bridge|              false|
|     David|   Eggers|              false|
|   Dorothy|  Koopman|              false|
|      Kara|     Kohl|              false|
|     Diane|   Nelson|              false|
|    Robert|   Fisher|              false|
|    Marcia|  Roberts|              false|
|    Andres|    Cruse|              false|
|       Ann|    Moore|              false|
|    Joseph|   Lackey|              false|
|     Sarah|   Duvall|              false|
|      Lucy|   Corley|               true|
|    Roland| Crawford|              false|
|     Leona|     Bray|              false|
|   Forrest|   Becker|              false|
+----------+---------+-------------------+
only showing top 20 rows

scala> lucyCol.desc
res5: org.apache.spark.sql.Column = (first_name = Lucy) DESC NULLS LAST

scala> accountsDF.select($"first_name",$"last_name",lucyCol).show
+----------+---------+-------------------+
|first_name|last_name|(first_name = Lucy)|
+----------+---------+-------------------+
|    Donald|   Becton|              false|
|     Donna|    Jones|              false|
|    Dorthy| Chalmers|              false|
|     Leila|  Spencer|              false|
|     Anita| Laughlin|              false|
|    Stevie|   Bridge|              false|
|     David|   Eggers|              false|
|   Dorothy|  Koopman|              false|
|      Kara|     Kohl|              false|
|     Diane|   Nelson|              false|
|    Robert|   Fisher|              false|
|    Marcia|  Roberts|              false|
|    Andres|    Cruse|              false|
|       Ann|    Moore|              false|
|    Joseph|   Lackey|              false|
|     Sarah|   Duvall|              false|
|      Lucy|   Corley|               true|
|    Roland| Crawford|              false|
|     Leona|     Bray|              false|
|   Forrest|   Becker|              false|
+----------+---------+-------------------+
only showing top 20 rows

9. The where operation requires a boolean-based column expression. 
Use the lucyCol column expression in a where transformation and view the data in the resulting DataFrame. 
Confirm that only users named Lucy are in the data

>>> accountsDF.select(accountsDF.first_name, accountsDF.last_name, lucyCol).where(lucyCol).show()
+----------+---------+-------------------+
|first_name|last_name|(first_name = Lucy)|
+----------+---------+-------------------+
|      Lucy|   Corley|               true|
|      Lucy|    Davis|               true|
|      Lucy|  Casiano|               true|
|      Lucy|      Lee|               true|
|      Lucy| Hastings|               true|
|      Lucy|  Hampton|               true|
|      Lucy|  McBride|               true|
|      Lucy|   Curley|               true|
|      Lucy|  Bottoms|               true|
|      Lucy|    Osuna|               true|
|      Lucy|   Cannon|               true|
|      Lucy|  Wallace|               true|
|      Lucy|    Licon|               true|
|      Lucy|     Wood|               true|
|      Lucy|  Johnson|               true|
|      Lucy|   Kohler|               true|
|      Lucy|    Lopez|               true|
|      Lucy|Kimbrough|               true|
|      Lucy|    Tartt|               true|
|      Lucy| Douglass|               true|
+----------+---------+-------------------+
only showing top 20 rows

scala> accountsDF.select($"first_name",$"last_name",lucyCol).where(lucyCol).show()
+----------+---------+-------------------+
|first_name|last_name|(first_name = Lucy)|
+----------+---------+-------------------+
|      Lucy|   Corley|               true|
|      Lucy|    Davis|               true|
|      Lucy|  Casiano|               true|
|      Lucy|      Lee|               true|
|      Lucy| Hastings|               true|
|      Lucy|  Hampton|               true|
|      Lucy|  McBride|               true|
|      Lucy|   Curley|               true|
|      Lucy|  Bottoms|               true|
|      Lucy|    Osuna|               true|
|      Lucy|   Cannon|               true|
|      Lucy|  Wallace|               true|
|      Lucy|    Licon|               true|
|      Lucy|     Wood|               true|
|      Lucy|  Johnson|               true|
|      Lucy|   Kohler|               true|
|      Lucy|    Lopez|               true|
|      Lucy|Kimbrough|               true|
|      Lucy|    Tartt|               true|
|      Lucy| Douglass|               true|
+----------+---------+-------------------+
only showing top 20 rows

10. Column expressions do not need to be assigned to a variable. Try the same query without using the lucyCol variable.

>>> accountsDF.select(accountsDF.first_name, accountsDF.last_name, lucyCol).where(accountsDF.first_name == "Lucy").show()
>>> accountsDF.select(accountsDF.first_name, accountsDF.last_name, lucyCol).where(fnCol == "Lucy").show()
>>> accountsDF.where(fnCol == "Lucy").show()

scala> accountsDF.select($"first_name", $"last_name", lucyCol).where($"first_name" === "Lucy").show()
scala> accountsDF.select($"first_name", $"last_name", lucyCol).where(fnCol === "Lucy").show()
scala> accountsDF.where(fnCol === "Lucy").show()

11. Column expressions are not limited to where operations like those above. 
They can be used in any transformation for which a simple column could be used, such as a select. 
Try selecting the city and state columns, and the first three characters of the phone_number column (in the U.S., the first three digits of a phone number are known as the area code). 
Use the substr operator on the phone_number column to extract the area code

>>> accountsDF.select("city", "state", accountsDF.phone_number.substr(1,3)).show(5)
+-------------+-----+-----------------------------+
|         city|state|substring(phone_number, 1, 3)|
+-------------+-----+-----------------------------+
|      Oakland|   CA|                          510|
|San Francisco|   CA|                          415|
|    San Mateo|   CA|                          650|
|    San Mateo|   CA|                          650|
|     Richmond|   CA|                          510|
+-------------+-----+-----------------------------+
only showing top 5 rows

scala> accountsDF.select($"city", $"state", $"phone_number".substr(1,3)).show(5)
+-------------+-----+-----------------------------+
|         city|state|substring(phone_number, 1, 3)|
+-------------+-----+-----------------------------+
|      Oakland|   CA|                          510|
|San Francisco|   CA|                          415|
|    San Mateo|   CA|                          650|
|    San Mateo|   CA|                          650|
|     Richmond|   CA|                          510|
+-------------+-----+-----------------------------+
only showing top 5 rows

12. Notice that in the last step, the values returned by the query were correct, but the column name was substring(phone_number, 1, 3), which is long and hard to work with. 
Repeat the same query, using the alias operator to rename that column as area_code.

>>> accountsDF.select("city", "state", accountsDF.phone_number.substr(1,3).alias("area_code")).show(5)
+-------------+-----+---------+
|         city|state|area_code|
+-------------+-----+---------+
|      Oakland|   CA|      510|
|San Francisco|   CA|      415|
|    San Mateo|   CA|      650|
|    San Mateo|   CA|      650|
|     Richmond|   CA|      510|
+-------------+-----+---------+
only showing top 5 rows

scala> accountsDF.select($"city", $"state", $"phone_number".substr(1,3).alias("area_code")).show(5)
+-------------+-----+---------+
|         city|state|area_code|
+-------------+-----+---------+
|      Oakland|   CA|      510|
|San Francisco|   CA|      415|
|    San Mateo|   CA|      650|
|    San Mateo|   CA|      650|
|     Richmond|   CA|      510|
+-------------+-----+---------+
only showing top 5 rows

13. Perform a query that results in a DataFrame with just first_name and last_name columns, and only includes users 
whose first and last names both begin with the same two letters. 
(For example, the user Roberta Roget would be included, because both her first and last names begin with “Ro”.)

>>> accountsDF.where(accountsDF.first_name.substr(1,2) == accountsDF.last_name.substr(1,2)).select("first_name","last_name").show(5)

scala> accountsDF.where($"first_name".substr(1,2) === $"last_name".substr(1,2)).select("first_name","last_name").show(5)

Group and Count Data by Name
14. Query the accountsDF DataFrame using groupBy with count to find out the total number people sharing each last name. 
(Note that the count aggregation transformation returns a DataFrame, unlike the count DataFrame action, which
returns a single value to the driver.)


>>> accountsDF.groupBy("last_name").count().show(5)
+---------+-----+                                                               
|last_name|count|
+---------+-----+
| Francois|    6|
|   Bohner|    3|
|    Tyler|   50|
|   Maddox|   60|
| Abramson|    3|
+---------+-----+
only showing top 5 rows

scala> accountsDF.groupBy("last_name").count.show(5)
+---------+-----+                                                               
|last_name|count|
+---------+-----+
| Francois|    6|
|   Bohner|    3|
|    Tyler|   50|
|   Maddox|   60|
| Abramson|    3|
+---------+-----+
only showing top 5 rows

15. You can also group by multiple columns. Query accountsDF again, this time counting the number of people who share the same last and first name.

>>> accountsDF.groupBy("last_name","first_name").count().show(5)
+---------+----------+-----+                                                    
|last_name|first_name|count|
+---------+----------+-----+
| Williams|   Lillian|    6|
|Robertson| Rosemarie|    3|
|     Bell|    Joanna|    3|
|    Spano|    Amanda|    3|
|    Beane|     Marie|    3|
+---------+----------+-----+
only showing top 5 rows

scala> accountsDF.groupBy("last_name","first_name").count.show(5)
+---------+----------+-----+                                                    
|last_name|first_name|count|
+---------+----------+-----+
| Williams|   Lillian|    6|
|Robertson| Rosemarie|    3|
|     Bell|    Joanna|    3|
|    Spano|    Amanda|    3|
|    Beane|     Marie|    3|
+---------+----------+-----+
only showing top 5 rows

Join Account Data with Cellular Towers by Zip Code 
16. In this section, you will join the accounts data that you have been using with data about cell tower base station locations, 
which is in the base_stations.parquet file. Start by reviewing the schema and a few records of the data. 
Use the parquet-tools command in a separate terminal window (not the one running the Spark shell).

[training@dev ~]$ parquet-tools schema $DEVDATA/base_stations.parquet
message spark_schema{
  optional int32 id;
  optional binary zip (UTF8);
  optional binary city (UTF8);
  optional binary state (UTF8);
  optional double lat;
  optional double lon;
}
[training@dev ~]$ parquet-tools head $DEVDATA/base_stations.parquet
id = 1
zip = 86502
city = Chambers
state = AZ
lat = 35.2375
lon = -109.523

id = 2
zip = 86514
city = Teec Nos Pos
state = AZ
lat = 36.7797
lon = -109.359

...
...
17. Upload the data file to HDFS.

scelisdev02@cca175-m:~/training_materials/devsh/exercises/analyze/solution$  hdfs dfs -ls /devsh_loudacre/base*
-rw-r--r--   2 scelisdev02 hadoop      13149 2021-09-27 09:27 /devsh_loudacre/base_stations.parquet

18. In your Spark shell, create a new DataFrame called baseDF using the base stations data. 
Review the baseDF schema and data to ensure it matches the data in the Parquet file.

>>> baseDF = spark.read.parquet("/devsh_loudacre/base_stations.parquet")
>>> baseDF.printSchema()
root
 |-- id: integer (nullable = true)
 |-- zip: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- lat: double (nullable = true)
 |-- lon: double (nullable = true)

scala> val baseDF = spark.read.parquet("/devsh_loudacre/base_stations.parquet")
baseDF: org.apache.spark.sql.DataFrame = [id: int, zip: string ... 4 more fields]

scala> baseDF.printSchema
root
 |-- id: integer (nullable = true)
 |-- zip: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- lat: double (nullable = true)
 |-- lon: double (nullable = true)

19. Some account holders live in zip codes that have a base station. Join baseDF and accountsDF to find those users. 
For each of those users, include their account ID, first name, last name, and the ID and location data (latitude and longitude) for the
base station in their zip code.

>>> accountsDF.select("acct_num","first_name","last_name","zipcode").join(baseDF, baseDF.zip == accountsDF.zipcode).show()
+--------+----------+-----------+-------+---+-----+-------------+-----+-------+---------+
|acct_num|first_name|  last_name|zipcode| id|  zip|         city|state|    lat|      lon|
+--------+----------+-----------+-------+---+-----+-------------+-----+-------+---------+
|      37|    Cheryl|       West|  94622|231|94622|      Oakland|   CA| 37.799|-122.2337|
|      56|   Terrell|   Hardiman|  94261|127|94261|   Sacramento|   CA|38.3774|-121.4444|
|      60|     Irwin|Clatterbuck|  94403|185|94403|    San Mateo|   CA|37.5395|-122.2998|
|      68|    Walter|    Greiner|  94701| 35|94701|     Berkeley|   CA|37.8606|-122.2967|
|     126|     Keith|     Branan|  94623| 34|94623|      Oakland|   CA|37.6802|-121.9215|
|     130|   Maynard|     Penley|  94120|165|94120|San Francisco|   CA|37.7848|-122.7278|
|     179|     Julia|    Nowicki|  94059|182|94059| Redwood City|   CA|37.3811|-122.3348|
|     183| Elizabeth|   McMillan|  94150|173|94150|San Francisco|   CA|37.7848|-122.7278|
|     196|     Bruce|     Vargas|  94975|211|94975|     Petaluma|   CA|38.4631|  -122.99|
|     233|     Arden|      Black|  94088|190|94088|    Sunnyvale|   CA|37.1894|-121.7053|
|     241|    Conrad|  Fairchild|  94116|164|94116|San Francisco|   CA|37.7441|-122.4863|
|     275|   Chelsea|      Gaona|  94040|189|94040|Mountain View|   CA|37.3855| -122.088|
|     410|   Kenneth|    Nowicki|  94110|163|94110|San Francisco|   CA|37.7509|-122.4153|
|     415|     Jason|       King|  94529| 39|94529|      Concord|   CA|37.7772|-121.9554|
|     435|    Andrew|      Oakes|  94130|167|94130|San Francisco|   CA|37.8231|-122.3693|
|     481|     Grace|     Brooks|  95675| 37|95675|  River Pines|   CA|38.5463| -120.743|
|     489|    George|       Sams|  94088|190|94088|    Sunnyvale|   CA|37.1894|-121.7053|
|     530|       Ann|   Stennett|  95190|202|95190|     San Jose|   CA|37.3894|-121.8868|
|     543|      Joan|     Benito|  95915|121|95915|       Belden|   CA|39.9324|-121.3144|
|     556|  Gretchen|    Akridge|  94074|184|94074| San Gregorio|   CA|37.3255|-122.3556|
+--------+----------+-----------+-------+---+-----+-------------+-----+-------+---------+
only showing top 20 rows


scala> accountsDF.select("acct_num","first_name","last_name","zipcode").join(baseDF,$"zip" === $"zipcode").show()
+--------+----------+-----------+-------+---+-----+-------------+-----+-------+---------+
|acct_num|first_name|  last_name|zipcode| id|  zip|         city|state|    lat|      lon|
+--------+----------+-----------+-------+---+-----+-------------+-----+-------+---------+
|      37|    Cheryl|       West|  94622|231|94622|      Oakland|   CA| 37.799|-122.2337|
|      56|   Terrell|   Hardiman|  94261|127|94261|   Sacramento|   CA|38.3774|-121.4444|
|      60|     Irwin|Clatterbuck|  94403|185|94403|    San Mateo|   CA|37.5395|-122.2998|
|      68|    Walter|    Greiner|  94701| 35|94701|     Berkeley|   CA|37.8606|-122.2967|
|     126|     Keith|     Branan|  94623| 34|94623|      Oakland|   CA|37.6802|-121.9215|
|     130|   Maynard|     Penley|  94120|165|94120|San Francisco|   CA|37.7848|-122.7278|
|     179|     Julia|    Nowicki|  94059|182|94059| Redwood City|   CA|37.3811|-122.3348|
|     183| Elizabeth|   McMillan|  94150|173|94150|San Francisco|   CA|37.7848|-122.7278|
|     196|     Bruce|     Vargas|  94975|211|94975|     Petaluma|   CA|38.4631|  -122.99|
|     233|     Arden|      Black|  94088|190|94088|    Sunnyvale|   CA|37.1894|-121.7053|
|     241|    Conrad|  Fairchild|  94116|164|94116|San Francisco|   CA|37.7441|-122.4863|
|     275|   Chelsea|      Gaona|  94040|189|94040|Mountain View|   CA|37.3855| -122.088|
|     410|   Kenneth|    Nowicki|  94110|163|94110|San Francisco|   CA|37.7509|-122.4153|
|     415|     Jason|       King|  94529| 39|94529|      Concord|   CA|37.7772|-121.9554|
|     435|    Andrew|      Oakes|  94130|167|94130|San Francisco|   CA|37.8231|-122.3693|
|     481|     Grace|     Brooks|  95675| 37|95675|  River Pines|   CA|38.5463| -120.743|
|     489|    George|       Sams|  94088|190|94088|    Sunnyvale|   CA|37.1894|-121.7053|
|     530|       Ann|   Stennett|  95190|202|95190|     San Jose|   CA|37.3894|-121.8868|
|     543|      Joan|     Benito|  95915|121|95915|       Belden|   CA|39.9324|-121.3144|
|     556|  Gretchen|    Akridge|  94074|184|94074| San Gregorio|   CA|37.3255|-122.3556|
+--------+----------+-----------+-------+---+-----+-------------+-----+-------+---------+
Count Active Devices

20. The accountdevice CSV dataset contains a list of all the devices used by all accounts. 
Each row in the data set includes a row ID, an account ID, an ID for the type of device, the date the device was activated for the account, and the specific device ID.
The CSV data file is in the $DEVDATA/accountdevice directory. 
Review the data in the data set, then upload the directory and its contents to the HDFS directory / devsh_loudacre/accountdevice.

scelisdev02@cca175-m:~/training_materials/devsh/data$ ls
accountdevice       base_stations.parquet  devicestatus_stream      frostroad.txt  makes1.txt   weblogs
activations         devices.csv            devicestatus_stream.txt  kb             makes2.txt   writedevicestatus-devsh.py
activations_stream  devices.json           devicestatus.txt         loudacre.sql   static_data

scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -put accountdevice /devsh_loudacre/accountdevice

scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -ls /devsh_loudacre/accountdevice
Found 1 items
-rw-r--r--   2 scelisdev02 hadoop   12928268 2021-09-27 10:07 /devsh_loudacre/accountdevice/part-00000-f3b62dad-1054-4b2e-81fd-26e54c2ae76a.csv

21. Create a DataFrame based on the accountdevice data files.

accountDeviceDF = spark.read.csv("/devsh_loudacre/accountdevice")
accountDeviceDF.printSchema()
accountDeviceDF = spark.read.option("header","true").option("inferSchema","true").csv("/devsh_loudacre/accountdevice")
accountDeviceDF.printSchema()
root
 |-- id: integer (nullable = true)
 |-- account_id: integer (nullable = true)
 |-- device_id: integer (nullable = true)
 |-- activation_date: long (nullable = true)
 |-- account_device_id: string (nullable = true)


scala> val accountDeviceDF = spark.read.options(Map("inferSchema"->"true", "header"->"true")).csv("/devsh_loudacre/accountdevice")
accountDeviceDF: org.apache.spark.sql.DataFrame = [id: int, account_id: int ... 3 more fields]

scala> accountDeviceDF.printSchema
root
 |-- id: integer (nullable = true)
 |-- account_id: integer (nullable = true)
 |-- device_id: integer (nullable = true)
 |-- activation_date: long (nullable = true)
 |-- account_device_id: string (nullable = true)

22. Use the account device data and the DataFrames you created previously in this exercise to find the total number of each device model 
across all active accounts— that is, accounts that have not been closed. 
The new DataFrame should be sorted from most to least common model. 
Save the data as Parquet files in a directory called /devsh_loudacre/top_devices with the following columns:

Column Name - Description - Example Value
device_id - The ID number of each known device - 18
(including those that might not be in use by any account)
make - The manufacturer name for the device - Ronin 
model - The model name for the device - Novelty Note 2
active_num - The total number of the model used by active accounts - 2092
Hints:
• Active accounts are those with a null value for acct_close_dt (account close date) in the accounts table.
• The account_id column in the device accounts data corresponds to the acct_num column in accounts table.
• The device_id column in the device accounts data corresponds to the devnum column in the list of known devices 
in the /devsh_loudacre/devices.json file.
• When you count devices, use withColumnRenamed to rename the count column to active_num. 
(The count column name is ambiguous because it is both a function and a column.)
• The query to complete this exercise is somewhat complicated and includes a sequence of many transformations. 
You may wish to assign variables to the intermediate DataFrames resulting from the transformations that make up the
query to make the code easier to work with and debug.

• Active accounts are those with a null value for acct_close_dt (account close date) in the accounts table.
# DataFrame con las cuentas activas - cuentas que no han sido cerradas
activeAccountsDF = accountsDF.where(accountsDF.acct_close_dt.isNull())

• The account_id column in the device accounts data corresponds to the acct_num column in accounts table.
# JOIN activeAccountsDF and accountDeviceDF x account_id: integer == acct_num: integer

>>> activeAccountsDF.printSchema()
root
 |-- acct_num: integer (nullable = true)
 |-- acct_create_dt: timestamp (nullable = true)
 |-- acct_close_dt: timestamp (nullable = true)
 |-- first_name: string (nullable = true)
 |-- last_name: string (nullable = true)
 |-- address: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- zipcode: string (nullable = true)
 |-- phone_number: string (nullable = true)
 |-- created: timestamp (nullable = true)
 |-- modified: timestamp (nullable = true)
 
>>> accountDeviceDF.printSchema()
root
 |-- id: integer (nullable = true)
 |-- account_id: integer (nullable = true)
 |-- device_id: integer (nullable = true)
 |-- activation_date: long (nullable = true)
 |-- account_device_id: string (nullable = true)

+--------+----------+---------+-------------+----+----------+---------+---------------+--------------------+
>>> activeAccountsDF.select("acct_num","first_name","last_name","acct_close_dt").join(accountDeviceDF,  accountDeviceDF.account_id == activeAccountsDF.acct_num).show()
+--------+----------+---------+-------------+----+----------+---------+---------------+--------------------+
|acct_num|first_name|last_name|acct_close_dt|  id|account_id|device_id|activation_date|   account_device_id|
+--------+----------+---------+-------------+----+----------+---------+---------------+--------------------+
|     471|       Kim|  Johnson|         null| 675|       471|        9|  1230299662000|fd049fc6-b891-4c4...|
|     471|       Kim|  Johnson|         null| 676|       471|       29|  1393933392000|7462dbb8-4583-410...|
|     496|  Jennifer|   Sander|         null| 710|       496|       38|  1231930424000|c97988fe-3113-459...|
|     496|  Jennifer|   Sander|         null| 711|       496|       29|  1392791200000|31f33dc9-9f81-4b8...|
|    1088|     Silas|     Dews|         null|1608|      1088|       10|  1237268358000|f547a609-3cbc-4b6...|
|    1238|   Tiffany|   Avalos|         null|1828|      1238|        1|  1246893601000|84a3df86-4b7f-4e1...|
|    1580|     Randi|  Clouser|         null|2332|      1580|       10|  1257072530000|ee9d911c-c949-435...|
|    1591|     Rusty|  Bingham|         null|2347|      1591|        9|  1251312904000|a38cd4f7-2f02-467...|
|    1645|    Eugene|   Monday|         null|2435|      1645|        1|  1244787310000|120c4b22-bf85-469...|
|    1829|    Angela|Silverman|         null|2705|      1829|       38|  1249166286000|f591df26-322e-423...|
|    1829|    Angela|Silverman|         null|2706|      1829|       29|  1387801187000|fe647bcf-a48f-471...|
|    2142|     Jonie|   Reeves|         null|3182|      2142|        1|  1248077979000|c8653785-cfbf-413...|
|    2142|     Jonie|   Reeves|         null|3183|      2142|        5|  1348072487000|8236e192-dc31-435...|
|    2866|      Jose|   Morris|         null|4257|      2866|       43|  1247315530000|f69ce74a-7650-4a4...|
|    2866|      Jose|   Morris|         null|4258|      2866|       29|  1393716073000|5a9ca22f-64e2-424...|
|    3175|      Anne|   Garcia|         null|4730|      3175|       43|  1256136858000|bc378322-37c3-44b...|
|    3749| Elizabeth|  Fincher|         null|5581|      3749|       34|  1290489151000|0595acd9-3334-4d4...|
|    3794|     Perry|     Howe|         null|5654|      3794|       15|  1278530505000|92882573-e5df-425...|
|    3918|     Brian|  Jackson|         null|5842|      3918|        2|  1279086333000|6618f5d2-2e4d-4ed...|
|    3918|     Brian|  Jackson|         null|5843|      3918|       29|  1393583694000|5930f7b4-15cc-4ce...|
+--------+----------+---------+-------------+----+----------+---------+---------------+--------------------+
only showing top 20 rows

>>> activeAcctDevsDF = activeAccountsDF.join(accountDeviceDF,  accountDeviceDF.account_id == activeAccountsDF.acct_num).select("device_id")
>>> activeAcctDevsDF.show()
+---------+
|device_id|
+---------+
|       29|
|        9|
|        5|
|        5|
|       38|
|       29|
|        5|
|        5|
|       10|
|        1|
|        1|
|        5|
|       29|
|       38|
|       29|
|        5|
|        1|
|       38|
|       10|
|        5|
+---------+

• The device_id column in the device accounts data corresponds to the devnum column in the list of known devices 
# JOIN accountDeviceDF vs devDF x device_id: integer == devnum: long (nullable = true)
 
>>> accountDeviceDF.printSchema()
root
 |-- id: integer (nullable = true)
 |-- account_id: integer (nullable = true)
 |-- device_id: integer (nullable = true)
 |-- activation_date: long (nullable = true)
 |-- account_device_id: string (nullable = true)

>>> devDF = spark.read.json("/devsh_loudacre/devices.json")
>>> devDF.printSchema()                                                         
root
 |-- dev_type: string (nullable = true)
 |-- devnum: long (nullable = true)
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)
 |-- release_dt: string (nullable = true)

>>> accountDeviceDF.select("id", "account_id", "device_id", "activation_date", "account_device_id").join(devDF, devDF.devnum == accountDeviceDF.device_id).show()
+-----+----------+---------+---------------+--------------------+--------+------+--------+--------------+--------------------+
|   id|account_id|device_id|activation_date|   account_device_id|dev_type|devnum|    make|         model|          release_dt|
+-----+----------+---------+---------------+--------------------+--------+------+--------+--------------+--------------------+
|48692|     32443|       29|  1393242509000|7351fed1-f344-4cd...|   phone|    29|Sorrento|          F41L|2013-11-01T00:00:...|
|48693|     32444|        4|  1353649861000|6da22278-ff7a-461...|   phone|     4|  MeeToo|           3.1|2011-09-21T00:00:...|
|48694|     32445|        9|  1331819465000|cb993b85-6775-407...|   phone|     9| Titanic|          1000|2008-10-21T00:00:...|
|48695|     32446|       43|  1336860950000|48ea2c09-a0df-4d1...|   phone|    43|Sorrento|          F01L|2009-03-11T00:00:...|
|48696|     32446|       29|  1383650663000|4b49c0a6-d141-42e...|   phone|    29|Sorrento|          F41L|2013-11-01T00:00:...|
|48697|     32447|        6|  1342578469000|cc8e8361-3d67-4be...|   phone|     6|  iFruit|             3|2011-11-02T00:00:...|
|48698|     32447|       29|  1386643231000|b40dba90-b073-405...|   phone|    29|Sorrento|          F41L|2013-11-01T00:00:...|
|48699|     32448|        5|  1350883104000|f088d30f-1e1c-47f...|   phone|     5|  iFruit|             1|2008-10-21T00:00:...|
|48700|     32448|       29|  1383321310000|2805df93-2e89-433...|   phone|    29|Sorrento|          F41L|2013-11-01T00:00:...|
|48701|     32449|       34|  1333225574000|e0e7edbe-77fc-421...|   phone|    34| Titanic|          2000|2009-12-21T00:00:...|
|48702|     32450|       13|  1341361160000|b3ce579f-48e3-43e...|   phone|    13|Sorrento|          F23L|2011-11-21T00:00:...|
|48703|     32451|       21|  1329524457000|857bbe4f-028c-475...|   phone|    21| Titanic|          2300|2011-02-18T00:00:...|
|48704|     32451|       29|  1383361574000|826e4946-48a1-43c...|   phone|    29|Sorrento|          F41L|2013-11-01T00:00:...|
|48705|     32452|       16|  1348532412000|a3c1d199-f70f-44f...|   phone|    16| Titanic|          2500|2012-07-21T00:00:...|
|48706|     32452|       29|  1391579929000|d3e29743-fb26-496...|   phone|    29|Sorrento|          F41L|2013-11-01T00:00:...|
|48707|     32453|        3|  1327154730000|1c1011ff-9760-410...|   phone|     3|  MeeToo|           3.0|2011-02-18T00:00:...|
|48708|     32454|       14|  1335835131000|253febd2-c2c6-406...|   phone|    14| Titanic|          2200|2010-05-25T00:00:...|
|48709|     32455|       14|  1327343339000|2c4db96f-4afa-411...|   phone|    14| Titanic|          2200|2010-05-25T00:00:...|
|48710|     32455|       29|  1392305172000|96f69b89-adbf-46b...|   phone|    29|Sorrento|          F41L|2013-11-01T00:00:...|
|48711|     32456|       18|  1351503504000|f7e3fe6a-86bf-4ce...|   phone|    18|   Ronin|Novelty Note 2|2011-10-02T00:00:...|
+-----+----------+---------+---------------+--------------------+--------+------+--------+--------------+--------------------+
only showing top 20 rows

JOIN de activeAccountsDF, activeAccountsDF, devDF
>>> activeAccountsDF.select("acct_num","first_name","last_name","acct_close_dt").join(accountDeviceDF,  accountDeviceDF.account_id == activeAccountsDF.acct_num).join(devDF, devDF.devnum == accountDeviceDF.device_id).show()
+--------+----------+---------+-------------+----+----------+---------+---------------+--------------------+--------+------+--------+--------------+--------------------+
|acct_num|first_name|last_name|acct_close_dt|  id|account_id|device_id|activation_date|   account_device_id|dev_type|devnum|    make|         model|          release_dt|
+--------+----------+---------+-------------+----+----------+---------+---------------+--------------------+--------+------+--------+--------------+--------------------+
|     471|       Kim|  Johnson|         null| 675|       471|        9|  1230299662000|fd049fc6-b891-4c4...|   phone|     9| Titanic|          1000|2008-10-21T00:00:...|
|     471|       Kim|  Johnson|         null| 676|       471|       29|  1393933392000|7462dbb8-4583-410...|   phone|    29|Sorrento|          F41L|2013-11-01T00:00:...|
|     496|  Jennifer|   Sander|         null| 710|       496|       38|  1231930424000|c97988fe-3113-459...|   phone|    38| Titanic|          1100|2008-11-25T00:00:...|
|     496|  Jennifer|   Sander|         null| 711|       496|       29|  1392791200000|31f33dc9-9f81-4b8...|   phone|    29|Sorrento|          F41L|2013-11-01T00:00:...|
|    1088|     Silas|     Dews|         null|1608|      1088|       10|  1237268358000|f547a609-3cbc-4b6...|   phone|    10|  MeeToo|           1.0|2008-10-21T00:00:...|
|    1238|   Tiffany|   Avalos|         null|1828|      1238|        1|  1246893601000|84a3df86-4b7f-4e1...|   phone|     1|Sorrento|          F00L|2008-10-21T00:00:...|
|    1580|     Randi|  Clouser|         null|2332|      1580|       10|  1257072530000|ee9d911c-c949-435...|   phone|    10|  MeeToo|           1.0|2008-10-21T00:00:...|
|    1591|     Rusty|  Bingham|         null|2347|      1591|        9|  1251312904000|a38cd4f7-2f02-467...|   phone|     9| Titanic|          1000|2008-10-21T00:00:...|
|    1645|    Eugene|   Monday|         null|2435|      1645|        1|  1244787310000|120c4b22-bf85-469...|   phone|     1|Sorrento|          F00L|2008-10-21T00:00:...|
|    1829|    Angela|Silverman|         null|2705|      1829|       38|  1249166286000|f591df26-322e-423...|   phone|    38| Titanic|          1100|2008-11-25T00:00:...|
|    1829|    Angela|Silverman|         null|2706|      1829|       29|  1387801187000|fe647bcf-a48f-471...|   phone|    29|Sorrento|          F41L|2013-11-01T00:00:...|
|    2142|     Jonie|   Reeves|         null|3182|      2142|        1|  1248077979000|c8653785-cfbf-413...|   phone|     1|Sorrento|          F00L|2008-10-21T00:00:...|
|    2142|     Jonie|   Reeves|         null|3183|      2142|        5|  1348072487000|8236e192-dc31-435...|   phone|     5|  iFruit|             1|2008-10-21T00:00:...|
|    2866|      Jose|   Morris|         null|4257|      2866|       43|  1247315530000|f69ce74a-7650-4a4...|   phone|    43|Sorrento|          F01L|2009-03-11T00:00:...|
|    2866|      Jose|   Morris|         null|4258|      2866|       29|  1393716073000|5a9ca22f-64e2-424...|   phone|    29|Sorrento|          F41L|2013-11-01T00:00:...|
|    3175|      Anne|   Garcia|         null|4730|      3175|       43|  1256136858000|bc378322-37c3-44b...|   phone|    43|Sorrento|          F01L|2009-03-11T00:00:...|
|    3749| Elizabeth|  Fincher|         null|5581|      3749|       34|  1290489151000|0595acd9-3334-4d4...|   phone|    34| Titanic|          2000|2009-12-21T00:00:...|
|    3794|     Perry|     Howe|         null|5654|      3794|       15|  1278530505000|92882573-e5df-425...|   phone|    15|   Ronin|Novelty Note 1|2010-06-20T00:00:...|
|    3918|     Brian|  Jackson|         null|5842|      3918|        2|  1279086333000|6618f5d2-2e4d-4ed...|   phone|     2| Titanic|          2100|2010-04-19T00:00:...|
|    3918|     Brian|  Jackson|         null|5843|      3918|       29|  1393583694000|5930f7b4-15cc-4ce...|   phone|    29|Sorrento|          F41L|2013-11-01T00:00:...|
+--------+----------+---------+-------------+----+----------+---------+---------------+--------------------+--------+------+--------+--------------+--------------------+
only showing top 20 rows

• When you count devices, use withColumnRenamed to rename the count column to active_num.  
(The count column name is ambiguous because it is both a function and a column.)
# Only active accounts
>>> sumDevicesDF = activeAcctDevsDF.groupBy("device_id").count().withColumnRenamed("count", "active_num")
>>> sumDevicesDF.show()
+---------+----------+                                                          
|device_id|active_num|
+---------+----------+
|       31|      1445|
|       34|      3257|
|       28|      1396|
|       27|      3245|
|       26|      1560|
|       44|      2968|
|       12|      1207|
|       22|      1874|
|       47|      2963|
|        1|      3620|
|       13|      2084|
|        6|      2231|
|       16|      1447|
|        3|      2622|
|       20|      1476|
|       40|       914|
|       48|       970|
|        5|      3643|
|       19|       680|
|       41|      1496|
+---------+----------+
only showing top 20 rows

# Order by count in descending order
>>> orderDevicesDF = sumDevicesDF.orderBy(sumDevicesDF.active_num.desc())
>>> orderDevicesDF.show()
+---------+----------+                                                          
|device_id|active_num|
+---------+----------+
|       29|     30901|
|       38|      3727|
|        9|      3644|
|        5|      3643|
|        1|      3620|
|       10|      3547|
|       43|      3424|
|       34|      3257|
|       27|      3245|
|        2|      3087|
|       14|      3066|
|       33|      3033|
|        7|      2973|
|       15|      2972|
|       44|      2968|
|       47|      2963|
|       23|      2718|
|       11|      2691|
|       21|      2670|
|        3|      2622|
+---------+----------+
only showing top 20 rows

# Create a DataFrame based on the devices.json file
>>> devDF = spark.read.json("/devsh_loudacre/devices.json")
>>> devDF.show()
+--------+------+--------+--------------+--------------------+
|dev_type|devnum|    make|         model|          release_dt|
+--------+------+--------+--------------+--------------------+
|   phone|     1|Sorrento|          F00L|2008-10-21T00:00:...|
|   phone|     2| Titanic|          2100|2010-04-19T00:00:...|
|   phone|     3|  MeeToo|           3.0|2011-02-18T00:00:...|
|   phone|     4|  MeeToo|           3.1|2011-09-21T00:00:...|
|   phone|     5|  iFruit|             1|2008-10-21T00:00:...|
|   phone|     6|  iFruit|             3|2011-11-02T00:00:...|
|   phone|     7|  iFruit|             2|2010-05-20T00:00:...|
|   phone|     8|  iFruit|             5|2013-07-02T00:00:...|
|   phone|     9| Titanic|          1000|2008-10-21T00:00:...|
|   phone|    10|  MeeToo|           1.0|2008-10-21T00:00:...|
|   phone|    11|Sorrento|          F21L|2011-02-28T00:00:...|
|   phone|    12|  iFruit|             4|2012-10-25T00:00:...|
|   phone|    13|Sorrento|          F23L|2011-11-21T00:00:...|
|   phone|    14| Titanic|          2200|2010-05-25T00:00:...|
|   phone|    15|   Ronin|Novelty Note 1|2010-06-20T00:00:...|
|   phone|    16| Titanic|          2500|2012-07-21T00:00:...|
|   phone|    17|   Ronin|Novelty Note 3|2013-04-11T00:00:...|
|   phone|    18|   Ronin|Novelty Note 2|2011-10-02T00:00:...|
|   phone|    19|   Ronin|Novelty Note 4|2013-07-02T00:00:...|
|   phone|    20|  iFruit|            3A|2012-07-21T00:00:...|
+--------+------+--------+--------------+--------------------+
only showing top 20 rows

JOIN the list of device model totals with the list of devices to get the make and model for each device
>>> joinDevicesDF = orderDevicesDF.join(devDF, sumDevicesDF.device_id == devDF.devnum)
>>> joinDevicesDF.show()
+---------+----------+--------+------+--------+--------------+--------------------+
|device_id|active_num|dev_type|devnum|    make|         model|          release_dt|
+---------+----------+--------+------+--------+--------------+--------------------+
|       29|     30901|   phone|    29|Sorrento|          F41L|2013-11-01T00:00:...|
|       38|      3727|   phone|    38| Titanic|          1100|2008-11-25T00:00:...|
|        9|      3644|   phone|     9| Titanic|          1000|2008-10-21T00:00:...|
|        5|      3643|   phone|     5|  iFruit|             1|2008-10-21T00:00:...|
|        1|      3620|   phone|     1|Sorrento|          F00L|2008-10-21T00:00:...|
|       10|      3547|   phone|    10|  MeeToo|           1.0|2008-10-21T00:00:...|
|       43|      3424|   phone|    43|Sorrento|          F01L|2009-03-11T00:00:...|
|       34|      3257|   phone|    34| Titanic|          2000|2009-12-21T00:00:...|
|       27|      3245|   phone|    27|Sorrento|          F10L|2009-10-25T00:00:...|
|        2|      3087|   phone|     2| Titanic|          2100|2010-04-19T00:00:...|
|       14|      3066|   phone|    14| Titanic|          2200|2010-05-25T00:00:...|
|       33|      3033|   phone|    33|  MeeToo|           2.0|2010-05-15T00:00:...|
|        7|      2973|   phone|     7|  iFruit|             2|2010-05-20T00:00:...|
|       15|      2972|   phone|    15|   Ronin|Novelty Note 1|2010-06-20T00:00:...|
|       44|      2968|   phone|    44|Sorrento|          F11L|2010-04-10T00:00:...|
|       47|      2963|   phone|    47|   Ronin|            S1|2010-05-20T00:00:...|
|       23|      2718|   phone|    23|Sorrento|          F20L|2010-11-01T00:00:...|
|       11|      2691|   phone|    11|Sorrento|          F21L|2011-02-28T00:00:...|
|       21|      2670|   phone|    21| Titanic|          2300|2011-02-18T00:00:...|
|        3|      2622|   phone|     3|  MeeToo|           3.0|2011-02-18T00:00:...|
+---------+----------+--------+------+--------+--------------+--------------------+
only showing top 20 rows

Write out the data with the correct Columns
Use overwrite mode so solution can be run multiple times
>>> joinDevicesDF.select("device_id", "make", "model", joinDevicesDF.active_num).show()
+---------+--------+--------------+----------+                                  
|device_id|    make|         model|active_num|
+---------+--------+--------------+----------+
|       29|Sorrento|          F41L|     30901|
|       38| Titanic|          1100|      3727|
|        9| Titanic|          1000|      3644|
|        5|  iFruit|             1|      3643|
|        1|Sorrento|          F00L|      3620|
|       10|  MeeToo|           1.0|      3547|
|       43|Sorrento|          F01L|      3424|
|       34| Titanic|          2000|      3257|
|       27|Sorrento|          F10L|      3245|
|        2| Titanic|          2100|      3087|
|       14| Titanic|          2200|      3066|
|       33|  MeeToo|           2.0|      3033|
|        7|  iFruit|             2|      2973|
|       15|   Ronin|Novelty Note 1|      2972|
|       44|Sorrento|          F11L|      2968|
|       47|   Ronin|            S1|      2963|
|       23|Sorrento|          F20L|      2718|
|       11|Sorrento|          F21L|      2691|
|       21| Titanic|          2300|      2670|
|        3|  MeeToo|           3.0|      2622|
+---------+--------+--------------+----------+
only showing top 20 rows

>>> joinDevicesDF.select("device_id", "make", "model", joinDevicesDF.active_num).write.mode("overwrite").save("/devsh_loudacre/top_devices")

scelisdev02@cca175-m:~/training_materials/devsh/exercises$ hdfs dfs -ls /devsh_loudacre/
Found 12 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-27 13:40 /devsh_loudacre/top_devices

scelisdev02@cca175-m:~/training_materials/devsh/exercises$ hdfs dfs -get /devsh_loudacre/top_devices /tmp/
scelisdev02@cca175-m:~/training_materials/devsh/exercises$ ls -l /tmp/top_devices/
total 200
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00000-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 13:43 part-00001-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 13:43 part-00002-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 13:43 part-00003-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00004-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 13:43 part-00005-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00006-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 13:43 part-00007-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00008-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 13:43 part-00009-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 13:43 part-00010-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 13:43 part-00011-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 13:43 part-00012-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1217 Sep 27 13:43 part-00013-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00014-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 13:43 part-00015-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00016-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00017-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 13:43 part-00018-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 13:43 part-00019-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 13:43 part-00020-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1217 Sep 27 13:43 part-00021-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 13:43 part-00022-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 13:43 part-00023-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00024-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00025-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00026-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00027-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00028-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1116 Sep 27 13:43 part-00029-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 13:43 part-00030-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 13:43 part-00031-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 13:43 part-00032-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 13:43 part-00033-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 13:43 part-00034-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 13:43 part-00035-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 13:43 part-00036-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 13:43 part-00037-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00038-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 13:43 part-00039-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1116 Sep 27 13:43 part-00040-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00041-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1217 Sep 27 13:43 part-00042-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 13:43 part-00043-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1217 Sep 27 13:43 part-00044-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 13:43 part-00045-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1199 Sep 27 13:43 part-00046-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 13:43 part-00047-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 13:43 part-00048-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1116 Sep 27 13:43 part-00049-8ca8720e-ad09-485f-a864-6079dc699024-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02    0 Sep 27 13:43 _SUCCESS


[training@dev ~]$ parquet-tools head /tmp/top_devices
device_id = 6
make = iFruit
model = 3
count = 2231
....
....


scala> val accountDeviceDF = spark.read.options(Map("inferSchema"->"true", "header"->"true")).csv("/devsh_loudacre/accountdevice")
accountDeviceDF: org.apache.spark.sql.DataFrame = [id: int, account_id: int ... 3 more fields]

scala> accountDeviceDF.printSchema
root
 |-- id: integer (nullable = true)
 |-- account_id: integer (nullable = true)
 |-- device_id: integer (nullable = true)
 |-- activation_date: long (nullable = true)
 |-- account_device_id: string (nullable = true)

scala> val activeAccountsDF = accountsDF.where(accountsDF("acct_close_dt").isNull)
activeAccountsDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [acct_num: int, acct_create_dt: timestamp ... 10 more fields]

scala> val activeAcctDevsDF = activeAccountsDF.join(accountDeviceDF,  accountDeviceDF("account_id") === activeAccountsDF("acct_num")).select("device_id")
activeAcctDevsDF: org.apache.spark.sql.DataFrame = [device_id: int]

scala> val sumDevicesDF = activeAcctDevsDF.groupBy("device_id").count().withColumnRenamed("count", "active_num")
sumDevicesDF: org.apache.spark.sql.DataFrame = [device_id: int, active_num: bigint]

scala> val orderDevicesDF = sumDevicesDF.orderBy($"active_num".desc)
orderDevicesDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [device_id: int, active_num: bigint]

scala> val devDF = spark.read.json("/devsh_loudacre/devices.json")
devDF: org.apache.spark.sql.DataFrame = [dev_type: string, devnum: bigint ... 3 more fields]

scala> val joinDevicesDF = orderDevicesDF.join(devDF, orderDevicesDF("device_id") === devDF("devnum"))
joinDevicesDF: org.apache.spark.sql.DataFrame = [device_id: int, active_num: bigint ... 5 more fields]

scala> joinDevicesDF.select("device_id", "make", "model", "active_num").write.mode("overwrite").save("/devsh_loudacre/top_devices")

scelisdev02@cca175-m:~/training_materials/devsh/exercises$  rm -r /tmp/top_devices
scelisdev02@cca175-m:~/training_materials/devsh/exercises$ hdfs dfs -get /devsh_loudacre/top_devices /tmp/
scelisdev02@cca175-m:~/training_materials/devsh/exercises$  rm -r /tmp/top_devices
scelisdev02@cca175-m:~/training_materials/devsh/exercises$ hdfs dfs -get /devsh_loudacre/top_devices /tmp/
scelisdev02@cca175-m:~/training_materials/devsh/exercises$  ls -l /tmp/top_devices
total 200
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00000-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 15:45 part-00001-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 15:45 part-00002-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 15:45 part-00003-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00004-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 15:45 part-00005-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00006-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 15:45 part-00007-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00008-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 15:45 part-00009-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 15:45 part-00010-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 15:45 part-00011-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 15:45 part-00012-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1217 Sep 27 15:45 part-00013-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00014-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 15:45 part-00015-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00016-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00017-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 15:45 part-00018-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 15:45 part-00019-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 15:45 part-00020-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1217 Sep 27 15:45 part-00021-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 15:45 part-00022-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 15:45 part-00023-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00024-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00025-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00026-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet  
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00027-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00028-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1116 Sep 27 15:45 part-00029-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 15:45 part-00030-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 15:45 part-00031-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 15:45 part-00032-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 15:45 part-00033-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 15:45 part-00034-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1143 Sep 27 15:45 part-00035-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 15:45 part-00036-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 15:45 part-00037-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00038-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 15:45 part-00039-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1116 Sep 27 15:45 part-00040-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00041-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1217 Sep 27 15:45 part-00042-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1152 Sep 27 15:45 part-00043-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1217 Sep 27 15:45 part-00044-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 15:45 part-00045-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1199 Sep 27 15:45 part-00046-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1125 Sep 27 15:45 part-00047-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1107 Sep 27 15:45 part-00048-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02 1116 Sep 27 15:45 part-00049-b0af4d24-682a-4edc-a57a-14dfdf7e094d-c000.snappy.parquet
-rw-r--r--. 1 scelisdev02 scelisdev02    0 Sep 27 15:45 _SUCCESS

solucion del manual: /home/scelisdev02/training_materials/devsh/exercises/analyze/solution
-----------------------------------------------

------------------
Hands-On Exercise: Working With RDDs
------------------

Review the API Documentation for RDD Operations
1. Review the API docs for the RDD class (which is in the Python module pyspark, and the Scala package org.apache.spark.rdd). 
  https://spark.apache.org/docs/2.4.8/api/python/pyspark.html?highlight=rdd#pyspark.RDD
  http://spark.apache.org/docs/2.4.0/api/scala/index.html#org.apache.spark.rdd.package

Read and Display Data from a Text File
2. Review the simple text file you will be using by viewing (without editing) the file in a separate window (not the Spark shell). 
The file is located at $DEVDATA/frostroad.txt.

scelisdev02@cca175-m:~/training_materials/devsh/exercises$ cat $DEVDATA/frostroad.txt
Two roads diverged in a yellow wood,
And sorry I could not travel both
And be one traveler, long I stood
And looked down one as far as I could
To where it bent in the undergrowth;

Then took the other, as just as fair,
And having perhaps the better claim,
Because it was grassy and wanted wear;
Though as for that the passing there
Had worn them really about the same,

And both that morning equally lay
In leaves no step had trodden black.
Oh, I kept the first for another day!
Yet knowing how way leads on to way,
I doubted if I should ever come back.

I shall be telling this with a sigh
Somewhere ages and ages hence:
Two roads diverged in a wood, and I--
I took the one less traveled by,
And that has made all the difference.

3. In a terminal window on your remote desktop, upload the text file to HDFS directory /devsh_loudacre.
scelisdev02@cca175-m:~/training_materials/devsh/exercises$ hdfs dfs -put $DEVDATA/frostroad.txt /devsh_loudacre/

4. In the Spark shell, define an RDD based on the frostroad.txt text file.

>>> myRDD = sc.textFile("/devsh_loudacre/frostroad.txt")
>>> myRDD
/devsh_loudacre/frostroad.txt MapPartitionsRDD[603] at textFile at NativeMethodAccessorImpl.java:0

scala> val myRDD = sc.textFile("/devsh_loudacre/frostroad.txt")
myRDD: org.apache.spark.rdd.RDD[String] = /devsh_loudacre/frostroad.txt MapPartitionsRDD[66] at textFile at <console>:24

Using command completion, you can see all the available transformations and actions you can perform on an RDD. 
Type myRDD. and then the TAB key.

>>> myRDD.
Display all 106 possibilities? (y or n)
myRDD.aggregate(                           myRDD.id(                                  myRDD.sampleStdev(
myRDD.aggregateByKey(                      myRDD.intersection(                        myRDD.sampleVariance(
myRDD.barrier(                             myRDD.isCheckpointed(                      myRDD.saveAsHadoopDataset(
myRDD.cache(                               myRDD.isEmpty(                             myRDD.saveAsHadoopFile(
myRDD.cartesian(                           myRDD.isLocallyCheckpointed(               myRDD.saveAsNewAPIHadoopDataset(
myRDD.checkpoint(                          myRDD.is_cached                            myRDD.saveAsNewAPIHadoopFile(
myRDD.coalesce(                            myRDD.is_checkpointed                      myRDD.saveAsPickleFile(
myRDD.cogroup(                             myRDD.join(                                myRDD.saveAsSequenceFile(
myRDD.collect(                             myRDD.keyBy(                               myRDD.saveAsTextFile(
myRDD.collectAsMap(                        myRDD.keys(                                myRDD.setName(
myRDD.combineByKey(                        myRDD.leftOuterJoin(                       myRDD.sortBy(
myRDD.context                              myRDD.localCheckpoint(                     myRDD.sortByKey(
myRDD.count(                               myRDD.lookup(                              myRDD.stats(
myRDD.countApprox(                         myRDD.map(                                 myRDD.stdev(
myRDD.countApproxDistinct(                 myRDD.mapPartitions(                       myRDD.subtract(
myRDD.countByKey(                          myRDD.mapPartitionsWithIndex(              myRDD.subtractByKey(
myRDD.countByValue(                        myRDD.mapPartitionsWithSplit(              myRDD.sum(
myRDD.ctx                                  myRDD.mapValues(                           myRDD.sumApprox(
myRDD.distinct(                            myRDD.max(                                 myRDD.take(
myRDD.filter(                              myRDD.mean(                                myRDD.takeOrdered(
myRDD.first(                               myRDD.meanApprox(                          myRDD.takeSample(
myRDD.flatMap(                             myRDD.min(                                 myRDD.toDF(
myRDD.flatMapValues(                       myRDD.name(                                myRDD.toDebugString(
myRDD.fold(                                myRDD.partitionBy(                         myRDD.toLocalIterator(
myRDD.foldByKey(                           myRDD.partitioner                          myRDD.top(
myRDD.foreach(                             myRDD.persist(                             myRDD.treeAggregate(
myRDD.foreachPartition(                    myRDD.pipe(                                myRDD.treeReduce(
myRDD.fullOuterJoin(                       myRDD.randomSplit(                         myRDD.union(
myRDD.getCheckpointFile(                   myRDD.reduce(                              myRDD.unpersist(
myRDD.getNumPartitions(                    myRDD.reduceByKey(                         myRDD.values(
myRDD.getStorageLevel(                     myRDD.reduceByKeyLocally(                  myRDD.variance(
myRDD.glom(                                myRDD.repartition(                         myRDD.zip(
myRDD.groupBy(                             myRDD.repartitionAndSortWithinPartitions(  myRDD.zipWithIndex(
myRDD.groupByKey(                          myRDD.rightOuterJoin(                      myRDD.zipWithUniqueId(
myRDD.groupWith(                           myRDD.sample(                              
myRDD.histogram(                           myRDD.sampleByKey(       


scala> myRDD.
++             count                 foreachAsync            keyBy                    productArity       subtract          treeReduce        
aggregate      countApprox           foreachPartition        localCheckpoint          productElement     take              union             
barrier        countApproxDistinct   foreachPartitionAsync   map                      productIterator    takeAsync         unpersist         
cache          countAsync            getCheckpointFile       mapPartitions            productPrefix      takeOrdered       zip               
canEqual       countByValue          getNumPartitions        mapPartitionsWithIndex   randomSplit        takeSample        zipPartitions     
cartesian      countByValueApprox    getStorageLevel         max                      reduce             toDF              zipWithIndex      
checkpoint     dependencies          glom                    min                      repartition        toDS              zipWithUniqueId   
coalesce       distinct              groupBy                 name                     sample             toDebugString                       
collect        filter                id                      partitioner              saveAsObjectFile   toJavaRDD                           
collectAsync   first                 intersection            partitions               saveAsTextFile     toLocalIterator                     
compute        flatMap               isCheckpointed          persist                  setName            toString                            
context        fold                  isEmpty                 pipe                     sortBy             top                                 
copy           foreach               iterator                preferredLocations       sparkContext       treeAggregate                       

scala> myRDD.
!=             copy                  foreachPartition        map                      randomSplit        toJavaRDD         
##             count                 foreachPartitionAsync   mapPartitions            reduce             toLocalIterator   
+              countApprox           formatted               mapPartitionsWithIndex   repartition        toString          
++             countApproxDistinct   getCheckpointFile       max                      sample             top               
->             countAsync            getClass                min                      saveAsObjectFile   treeAggregate     
==             countByValue          getNumPartitions        name                     saveAsTextFile     treeReduce        
aggregate      countByValueApprox    getStorageLevel         ne                       setName            union             
asInstanceOf   dependencies          glom                    notify                   sortBy             unpersist         
barrier        distinct              groupBy                 notifyAll                sparkContext       wait              
cache          ensuring              hashCode                partitioner              subtract           zip               
canEqual       eq                    id                      partitions               synchronized       zipPartitions     
cartesian      equals                intersection            persist                  take               zipWithIndex      
checkpoint     filter                isCheckpointed          pipe                     takeAsync          zipWithUniqueId   
coalesce       first                 isEmpty                 preferredLocations       takeOrdered        →                 
collect        flatMap               isInstanceOf            productArity             takeSample                           
collectAsync   fold                  iterator                productElement           toDF                                 
compute        foreach               keyBy                   productIterator          toDS                                 
context        foreachAsync          localCheckpoint         productPrefix            toDebugString    

6. Spark has not yet read the file. It will not do so until you perform an action on the RDD. 
Try counting the number of elements in the RDD using the count action:

The count operation causes the RDD to be materialized (created and populated).
>>> myRDD.count()
23

scala> myRDD.count
count   countApprox   countApproxDistinct   countAsync   countByValue   countByValueApprox

scala> myRDD.count
   def count(): Long

scala> myRDD.count()
res20: Long = 23

7. Call the collect operation to return all data in the RDD to the Spark driver. 
Take note of the type of the return value; in Python will be a list of strings, and in Scala it will be an array of strings.
Note: collect returns the entire set of data. This is convenient for very small RDDs like this one, 
but be careful using collect for more typical large sets of data.

>>> lines = myRDD.collect()
>>> type(lines)
<class 'list'>

scala> lines
res21: Array[String] = Array(Two roads diverged in a yellow wood,, And sorry I could not travel both, And be one traveler, long I stood, And looked down one as far as I could, To where it bent in the undergrowth;, "", Then took the other, as just as fair,, And having perhaps the better claim,, Because it was grassy and wanted wear;, Though as for that the passing there, Had worn them really about the same,, "", And both that morning equally lay, In leaves no step had trodden black., Oh, I kept the first for another day!, Yet knowing how way leads on to way,, I doubted if I should ever come back., "", I shall be telling this with a sigh, Somewhere ages and ages hence:, Two roads diverged in a wood, and I--, I took the one less traveled by,, And that has made all...

8. Display the contents of the collected data by looping through the collection.

>>> for line in lines: print(line)
... 
Two roads diverged in a yellow wood,
And sorry I could not travel both
And be one traveler, long I stood
And looked down one as far as I could
To where it bent in the undergrowth;

Then took the other, as just as fair,
And having perhaps the better claim,
Because it was grassy and wanted wear;
Though as for that the passing there
Had worn them really about the same,

And both that morning equally lay
In leaves no step had trodden black.
Oh, I kept the first for another day!
Yet knowing how way leads on to way,
I doubted if I should ever come back.

I shall be telling this with a sigh
Somewhere ages and ages hence:
Two roads diverged in a wood, and I--
I took the one less traveled by,
And that has made all the difference.

scala> for (line <- lines) println(line)
Two roads diverged in a yellow wood,
And sorry I could not travel both
And be one traveler, long I stood
And looked down one as far as I could
To where it bent in the undergrowth;

Then took the other, as just as fair,
And having perhaps the better claim,
Because it was grassy and wanted wear;
Though as for that the passing there
Had worn them really about the same,

And both that morning equally lay
In leaves no step had trodden black.
Oh, I kept the first for another day!
Yet knowing how way leads on to way,
I doubted if I should ever come back.

I shall be telling this with a sigh
Somewhere ages and ages hence:
Two roads diverged in a wood, and I--
I took the one less traveled by,
And that has made all the difference.

Transform Data in an RDD
9. In this exercise, you will load two text files containing the names of various cell phone makes, and append one to the other. 
Review the two text files you will be using by viewing (without editing) the file in a separate window. 
The files are makes1.txt and makes2.txt in the $DEVDATA directory.

scelisdev02@cca175-m:~/training_materials/devsh/exercises$ cat $DEVDATA/makes1.txt 
Sorrento
Titanic
Sorrento
Titanic
MeeToo
MeeToo
MeeToo
Titanic
MeeToo
MeeToo
Titanic
Sorrento
Sorrento
scelisdev02@cca175-m:~/training_materials/devsh/exercises$ cat $DEVDATA/makes2.txt 
Titanic
MeeToo
MeeToo
iFruit
iFruit
Titanic
MeeToo
iFruit
Titanic
Ronin
Titanic
Titanic
Titanic
scelisdev02@cca175-m:~/training_materials/devsh/exercises$ 

10. Upload the two text file to HDFS directory /devsh_loudacre.

scelisdev02@cca175-m:~/training_materials/devsh/exercises$ hdfs dfs -put $DEVDATA/makes*.txt /devsh_loudacre/

11. In Spark, create an RDD called makes1RDD based on the /devsh_loudacre/makes1.txt file.

>>> makes1RDD = sc.textFile("/devsh_loudacre/makes1.txt")

scala> val makes1RDD = sc.textFile("/devsh_loudacre/makes1.txt")
makes1RDD: org.apache.spark.rdd.RDD[String] = /devsh_loudacre/makes1.txt MapPartitionsRDD[68] at textFile at <console>:24

12. Display the contents of the makes1RDD data using collect and then looping through returned collection.

>>> for make in makes1RDD.collect(): print(make)
... 
Sorrento
Titanic
Sorrento
Titanic
MeeToo
MeeToo
MeeToo
Titanic
MeeToo
MeeToo
Titanic
Sorrento
Sorrento

scala> for (make <- makes1RDD.collect()) println(make)
Sorrento
Titanic
Sorrento
Titanic
MeeToo
MeeToo
MeeToo
Titanic
MeeToo
MeeToo
Titanic
Sorrento
Sorrento

13. Repeat the previous steps to create and display an RDD called makes2RDD based on the second file, /devsh_loudacre/makes2.txt.

>>> makes2RDD = sc.textFile("/devsh_loudacre/makes2.txt")
>>> for make in makes2RDD.collect(): print(make)
... 
Titanic
MeeToo
MeeToo
iFruit
iFruit
Titanic
MeeToo
iFruit
Titanic
Ronin
Titanic
Titanic
Titanic

scala> val makes2RDD = sc.textFile("/devsh_loudacre/makes2.txt")
makes2RDD: org.apache.spark.rdd.RDD[String] = /devsh_loudacre/makes2.txt MapPartitionsRDD[70] at textFile at <console>:24

scala> for (make <- makes2RDD.collect()) println(make)
Titanic
MeeToo
MeeToo
iFruit
iFruit
Titanic
MeeToo
iFruit
Titanic
Ronin
Titanic
Titanic
Titanic

14. Create a new RDD by appending the second RDD to the first using the union transformation.

>>> allMakesRDD = makes1RDD.union(makes2RDD)
>>> allMakesRDD.collect()
['Sorrento', 'Titanic', 'Sorrento', 'Titanic', 'MeeToo', 'MeeToo', 'MeeToo', 'Titanic', 'MeeToo', 'MeeToo', 'Titanic', 'Sorrento', 'Sorrento', 'Titanic', 'MeeToo', 'MeeToo', 'iFruit', 'iFruit', 'Titanic', 'MeeToo', 'iFruit', 'Titanic', 'Ronin', 'Titanic', 'Titanic', 'Titanic']
>>> for make in allMakesRDD.collect(): print(make)
... 
Sorrento
Titanic
Sorrento
Titanic
MeeToo
MeeToo
MeeToo
Titanic
MeeToo
MeeToo
Titanic
Sorrento
Sorrento
Titanic
MeeToo
MeeToo
iFruit
iFruit
Titanic
MeeToo
iFruit
Titanic
Ronin
Titanic
Titanic
Titanic
>>> 

scala> allMakesRDD.collect()
res26: Array[String] = Array(Sorrento, Titanic, Sorrento, Titanic, MeeToo, MeeToo, MeeToo, Titanic, MeeToo, MeeToo, Titanic, Sorrento, Sorrento, Titanic, MeeToo, MeeToo, iFruit, iFruit, Titanic, MeeToo, iFruit, Titanic, Ronin, Titanic, Titanic, Titanic)

scala> for (make <- allMakesRDD.collect()) println(make)
Sorrento
Titanic
Sorrento
Titanic
MeeToo
MeeToo
MeeToo
Titanic
MeeToo
MeeToo
Titanic
Sorrento
Sorrento
Titanic
MeeToo
MeeToo
iFruit
iFruit
Titanic
MeeToo
iFruit
Titanic
Ronin
Titanic
Titanic
Titanic

16. Use the distinct transformation to remove duplicates from allMakesRDD.
Collect and display the contents to confirm that duplicate elements were removed.

>>> distinctRDD = allMakesRDD.distinct()
>>> for makes in distinctRDD.collect(): print(makes)
... 
MeeToo
Sorrento
Titanic
iFruit
Ronin

scala> val distinctRDD = allMakesRDD.distinct()
distinctRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[74] at distinct at <console>:25

scala> for (make <- distinctRDD.collect()) println(make)
Sorrento
Titanic
Ronin
MeeToo
iFruit

17. Optional: Try performing different transformations on the RDDs you created above,
such as intersection, subtract, or zip. 
See the RDD API documentation for details.

>>> intersectionRDD = makes1RDD.intersection(makes2RDD)
>>> for makes in intersectionRDD.collect(): print(makes)
... 
MeeToo
Titanic

scala> val intersectionRDD = makes1RDD.intersection(makes2RDD)
intersectionRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[80] at intersection at <console>:27

scala> for (make <- intersectionRDD.collect()) println(make)
Titanic
MeeToo

>>> subtractRDD = makes1RDD.subtract(makes2RDD)
>>> for makes in subtractRDD.collect(): print(makes)
... 
Sorrento
Sorrento
Sorrento
Sorrento

scala> val subtractRDD = makes1RDD.subtract(makes2RDD)
subtractRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[84] at subtract at <console>:27

scala> for (make <- subtractRDD.collect()) println(make)
Sorrento
Sorrento
Sorrento
Sorrento

>>> zipRDD = makes1RDD.zip(makes2RDD)
>>> for makes in zipRDD.collect(): print(makes)
... 
('Sorrento', 'Titanic')
('Titanic', 'MeeToo')
('Sorrento', 'MeeToo')
('Titanic', 'iFruit')
('MeeToo', 'iFruit')
('MeeToo', 'Titanic')
('MeeToo', 'MeeToo')
('Titanic', 'iFruit')
('MeeToo', 'Titanic')
('MeeToo', 'Ronin')
('Titanic', 'Titanic')
('Sorrento', 'Titanic')
('Sorrento', 'Titanic')

scala> val zipRDD = makes1RDD.zip(makes2RDD)
zipRDD: org.apache.spark.rdd.RDD[(String, String)] = ZippedPartitionsRDD2[85] at zip at <console>:27

scala> for (make <- zipRDD.collect()) println(make)
(Sorrento,Titanic)
(Titanic,MeeToo)
(Sorrento,MeeToo)
(Titanic,iFruit)
(MeeToo,iFruit)
(MeeToo,Titanic)
(MeeToo,MeeToo)
(Titanic,iFruit)
(MeeToo,Titanic)
(MeeToo,Ronin)
(Titanic,Titanic)
(Sorrento,Titanic)
(Sorrento,Titanic)

solucion del manual: /home/scelisdev02/training_materials/devsh/exercises/rdds/solution
-----------------------------------------------

------------------
Hands-On Exercise: Transforming Data Using RDDs
------------------

Explore the Loudacre Web Log Files

1. In this section you will be using data in $DEVDATA/weblogs. Review one of the .log files in the directory. 
Note the format of the lines:

cat $DEVDATA/weblogs/2013-09-15.log |more

IP Address- UserID - TimeStamp                 -Request                       - estado
3.94.78.5 - 69827 [15/Sep/2013:23:58:36 +0100] "GET /KBDOC-00033.html HTTP/1.0" 200 14417 "http://www.loudacre.com"  "Loudacre Mobile Browser iFruit 1"

2. Copy the weblogs directory from the local filesystem to the /devsh_loudacre HDFS directory

hdfs dfs -put $DEVDATA/weblogs /devsh_loudacre/

3. In Spark, create an RDD from the uploaded web logs data files in the /devsh_loudacre/weblogs/ directory in HDFS.

>>> logsRDD = sc.textFile("/devsh_loudacre/weblogs/")
>>> logsRDD
/devsh_loudacre/weblogs/ MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0

scala> val logsRDD = sc.textFile("devsh_loudacre/weblogs/")
logsRDD: org.apache.spark.rdd.RDD[String] = devsh_loudacre/weblogs/ MapPartitionsRDD[1] at textFile at <console>:24

4. Create an RDD containing only lines that are requests for JPG files. 
Use the filter operation with a transformation function that takes a string RDD element and returns a boolean value.

>>> jpglogsRDD = logsRDD.filter(lambda line: ".jpg" in line)

scala> val jpglogsRDD = logsRDD.filter(line => line.contains(".jpg"))

5. Use take to return the first five lines of the data in jpglogsRDD. 
The return value is a list of strings (in Python) or array of strings (in Scala).

>>> jpgLines = jpglogsRDD.take(5)

scala> val jpgLines = jpglogsRDD.take(5)
jpgLines: Array[String] = Array(217.150.149.167 - 4712 [15/Sep/2013:23:56:06 +0100] "GET /ronin_s4.jpg HTTP/1.0" 200 5552 "http://www.loudacre.com"  "Loudacre Mobile Browser MeeToo 1.0", 104.184.210.93 - 28402 [15/Sep/2013:23:42:53 +0100] "GET /titanic_2200.jpg HTTP/1.0" 200 19466 "http://www.loudacre.com"  "Loudacre Mobile Browser MeeToo 2.0", 37.91.137.134 - 36171 [15/Sep/2013:23:39:33 +0100] "GET /ronin_novelty_note_3.jpg HTTP/1.0" 200 7432 "http://www.loudacre.com"  "Loudacre Mobile Browser iFruit 3", 177.43.223.203 - 90653 [15/Sep/2013:23:31:17 +0100] "GET /ifruit_3.jpg HTTP/1.0" 200 19578 "http://www.loudacre.com"  "Loudacre Mobile Browser Sorrento F31L", 19.250.65.76 - 44388 [15/Sep/2013:23:31:10 +0100] "GET /sorrento_f24l.jpg HTTP/1.0" 200 5730 "http://...

6. Loop through and display the strings returned by take.

>>> for line in jpgLines: print(line)
... 
217.150.149.167 - 4712 [15/Sep/2013:23:56:06 +0100] "GET /ronin_s4.jpg HTTP/1.0" 200 5552 "http://www.loudacre.com"  "Loudacre Mobile Browser MeeToo 1.0"
104.184.210.93 - 28402 [15/Sep/2013:23:42:53 +0100] "GET /titanic_2200.jpg HTTP/1.0" 200 19466 "http://www.loudacre.com"  "Loudacre Mobile Browser MeeToo 2.0"
37.91.137.134 - 36171 [15/Sep/2013:23:39:33 +0100] "GET /ronin_novelty_note_3.jpg HTTP/1.0" 200 7432 "http://www.loudacre.com"  "Loudacre Mobile Browser iFruit 3"
177.43.223.203 - 90653 [15/Sep/2013:23:31:17 +0100] "GET /ifruit_3.jpg HTTP/1.0" 200 19578 "http://www.loudacre.com"  "Loudacre Mobile Browser Sorrento F31L"
19.250.65.76 - 44388 [15/Sep/2013:23:31:10 +0100] "GET /sorrento_f24l.jpg HTTP/1.0" 200 5730 "http://www.loudacre.com"  "Loudacre Mobile Browser iFruit 3A"

scala> jpgLines.foreach(println)
217.150.149.167 - 4712 [15/Sep/2013:23:56:06 +0100] "GET /ronin_s4.jpg HTTP/1.0" 200 5552 "http://www.loudacre.com"  "Loudacre Mobile Browser MeeToo 1.0"
104.184.210.93 - 28402 [15/Sep/2013:23:42:53 +0100] "GET /titanic_2200.jpg HTTP/1.0" 200 19466 "http://www.loudacre.com"  "Loudacre Mobile Browser MeeToo 2.0"
37.91.137.134 - 36171 [15/Sep/2013:23:39:33 +0100] "GET /ronin_novelty_note_3.jpg HTTP/1.0" 200 7432 "http://www.loudacre.com"  "Loudacre Mobile Browser iFruit 3"
177.43.223.203 - 90653 [15/Sep/2013:23:31:17 +0100] "GET /ifruit_3.jpg HTTP/1.0" 200 19578 "http://www.loudacre.com"  "Loudacre Mobile Browser Sorrento F31L"
19.250.65.76 - 44388 [15/Sep/2013:23:31:10 +0100] "GET /sorrento_f24l.jpg HTTP/1.0" 200 5730 "http://www.loudacre.com"  "Loudacre Mobile Browser iFruit 3A"

7. Use the map transformation to define a new RDD. 
Start with a simple map function that returns the length of each line in the log file. 
This results in an RDD of integers.

>>> lineLengthsRDD = logsRDD.map(lambda line: len(line))
>>> lineLengthsRDD.isEmpty()
False
>>> lineLengthsRDD.count()
1079891                                                                         

scala> val lineLengthsRDD =
     | logsRDD.map(line => line.length)
lineLengthsRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[3] at map at <console>:26

scala> lineLengthsRDD.count
res2: Long = 1079891                                                            

scala> lineLengthsRDD.isEmpty
   def isEmpty(): Boolean

scala> lineLengthsRDD.isEmpty
res3: Boolean = false

8. Loop through and display the first five elements (integers) in the RDD.

>>> lenLines = lineLengthsRDD.take(5)
>>> for line in lenLines: print(line)
... 
151
143
154
147
160

scala> val lineLen = lineLengthsRDD.take(5)
lineLen: Array[Int] = Array(151, 143, 154, 147, 160)

scala> lineLen.foreach(println)
151
143
154
147
160

9. Calculating line lengths is not very useful. Instead, try mapping each string in logsRDD by splitting the strings based on spaces. 
The result will be an RDD in which each element is a list of strings (in Python) or an array of strings (in Scala).
Each string represents a “field” in the web log line.

>>> lineFieldsRDD = logsRDD.map(lambda line: line.split(' '))

scala> val lineFieldsRDD = logsRDD.map(line => line.split(' '))
lineFieldsRDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[4] at map at <console>:25

10. Return the first five elements of lineFieldsRDD. 
The result will be a list of lists of strings (in Python) or an array of arrays of strings (in Scala).

>>> lineFields = lineFieldsRDD.take(5)
>>> splitLine = lineFieldsRDD.take(5)

scala> val lineFields = lineFieldsRDD.take(5)
lineFields: Array[Array[String]] = Array(Array(3.94.78.5, -, 69827, [15/Sep/2013:23:58:36, +0100], "GET, /KBDOC-00033.html, HTTP/1.0", 200, 14417, "http://www.loudacre.com", "", "Loudacre, Mobile, Browser, iFruit, 1"), Array(3.94.78.5, -, 69827, [15/Sep/2013:23:58:36, +0100], "GET, /theme.css, HTTP/1.0", 200, 3576, "http://www.loudacre.com", "", "Loudacre, Mobile, Browser, iFruit, 1"), Array(19.38.140.62, -, 21475, [15/Sep/2013:23:58:34, +0100], "GET, /KBDOC-00277.html, HTTP/1.0", 200, 15517, "http://www.loudacre.com", "", "Loudacre, Mobile, Browser, Ronin, S1"), Array(19.38.140.62, -, 21475, [15/Sep/2013:23:58:34, +0100], "GET, /theme.css, HTTP/1.0", 200, 13353, "http://www.loudacre.com", "", "Loudacre, Mobile, Browser, Ronin, S1"), Array(129.133.56.105, -, 24...

scala> val splitLine = lineFieldsRDD.take(5)
splitLine: Array[Array[String]] = Array(Array(3.94.78.5, -, 69827, [15/Sep/2013:23:58:36, +0100], "GET, /KBDOC-00033.html, HTTP/1.0", 200, 14417, "http://www.loudacre.com", "", "Loudacre, Mobile, Browser, iFruit, 1"), Array(3.94.78.5, -, 69827, [15/Sep/2013:23:58:36, +0100], "GET, /theme.css, HTTP/1.0", 200, 3576, "http://www.loudacre.com", "", "Loudacre, Mobile, Browser, iFruit, 1"), Array(19.38.140.62, -, 21475, [15/Sep/2013:23:58:34, +0100], "GET, /KBDOC-00277.html, HTTP/1.0", 200, 15517, "http://www.loudacre.com", "", "Loudacre, Mobile, Browser, Ronin, S1"), Array(19.38.140.62, -, 21475, [15/Sep/2013:23:58:34, +0100], "GET, /theme.css, HTTP/1.0", 200, 13353, "http://www.loudacre.com", "", "Loudacre, Mobile, Browser, Ronin, S1"), Array(129.133.56.105, -, 248...

11. Display the contents of the return from take.
this time you have a set of compound values (arrays or lists containing strings). 
Therefore, to display them properly, you will need to loop through the arrays/lists in lineFields, and then
loop through each string in the array/list. To make it easier to read the output, use ------- to separate each set of field values.

>>> for line in splitLine: print(line)
... 
['3.94.78.5', '-', '69827', '[15/Sep/2013:23:58:36', '+0100]', '"GET', '/KBDOC-00033.html', 'HTTP/1.0"', '200', '14417', '"http://www.loudacre.com"', '', '"Loudacre', 'Mobile', 'Browser', 'iFruit', '1"']
['3.94.78.5', '-', '69827', '[15/Sep/2013:23:58:36', '+0100]', '"GET', '/theme.css', 'HTTP/1.0"', '200', '3576', '"http://www.loudacre.com"', '', '"Loudacre', 'Mobile', 'Browser', 'iFruit', '1"']
['19.38.140.62', '-', '21475', '[15/Sep/2013:23:58:34', '+0100]', '"GET', '/KBDOC-00277.html', 'HTTP/1.0"', '200', '15517', '"http://www.loudacre.com"', '', '"Loudacre', 'Mobile', 'Browser', 'Ronin', 'S1"']
['19.38.140.62', '-', '21475', '[15/Sep/2013:23:58:34', '+0100]', '"GET', '/theme.css', 'HTTP/1.0"', '200', '13353', '"http://www.loudacre.com"', '', '"Loudacre', 'Mobile', 'Browser', 'Ronin', 'S1"']
['129.133.56.105', '-', '2489', '[15/Sep/2013:23:58:34', '+0100]', '"GET', '/KBDOC-00033.html', 'HTTP/1.0"', '200', '10590', '"http://www.loudacre.com"', '', '"Loudacre', 'Mobile', 'Browser', 'Sorrento', 'F00L"']


>>> for fields in lineFields:
...     print("-------")
...     for field in fields: print(field)
... 
-------
3.94.78.5
-
69827
[15/Sep/2013:23:58:36
+0100]
"GET
/KBDOC-00033.html
HTTP/1.0"
200
14417
"http://www.loudacre.com"

"Loudacre
Mobile
Browser
iFruit
1"
-------
3.94.78.5
-
69827
[15/Sep/2013:23:58:36
+0100]
"GET
/theme.css
HTTP/1.0"
200
3576
"http://www.loudacre.com"

"Loudacre
Mobile
Browser
iFruit
1"
-------
19.38.140.62
-
21475
[15/Sep/2013:23:58:34
+0100]
"GET
/KBDOC-00277.html
HTTP/1.0"
200
15517
"http://www.loudacre.com"

"Loudacre
Mobile
Browser
Ronin
S1"
-------
19.38.140.62
-
21475
[15/Sep/2013:23:58:34
+0100]
"GET
/theme.css
HTTP/1.0"
200
13353
"http://www.loudacre.com"

"Loudacre
Mobile
Browser
Ronin
S1"
-------
129.133.56.105
-
2489
[15/Sep/2013:23:58:34
+0100]
"GET
/KBDOC-00033.html
HTTP/1.0"
200
10590
"http://www.loudacre.com"

"Loudacre
Mobile
Browser
Sorrento
F00L"

scala> splitLine.foreach(println)
[Ljava.lang.String;@593e926f
[Ljava.lang.String;@7d9b0c1f
[Ljava.lang.String;@b74ce29
[Ljava.lang.String;@14788eec
[Ljava.lang.String;@3978cbe6

scala> for (fields <- lineFields) {
     | println("-------")
     | fields.foreach(println)
     | }

-------
3.94.78.5
-
69827
[15/Sep/2013:23:58:36
+0100]
"GET
/KBDOC-00033.html
HTTP/1.0"
200
14417
"http://www.loudacre.com"

"Loudacre
Mobile
Browser
iFruit
1"
-------
3.94.78.5
-
69827
[15/Sep/2013:23:58:36
+0100]
"GET
/theme.css
HTTP/1.0"
200
3576
"http://www.loudacre.com"
"Loudacre
Mobile
Browser
iFruit
1"
-------
19.38.140.62
-
21475
[15/Sep/2013:23:58:34
+0100]
"GET
/KBDOC-00277.html
HTTP/1.0"
200
15517
"http://www.loudacre.com"

"Loudacre
Mobile
Browser
Ronin
S1"
-------
19.38.140.62
-
21475
[15/Sep/2013:23:58:34
+0100]
"GET
/theme.css
HTTP/1.0"
200
13353
"http://www.loudacre.com"

"Loudacre
Mobile
Browser
Ronin
S1"
-------
129.133.56.105
-
2489
[15/Sep/2013:23:58:34
+0100]
"GET
/KBDOC-00033.html
HTTP/1.0"
200
10590
"http://www.loudacre.com"

"Loudacre
Mobile
Browser
Sorrento
F00L"


12. Now that you know how map works, create a new RDD containing just the IP addresses from each line in the log file. 
(The IP address is the first space-delimited field in each line.)

>>> ipsRDD = logsRDD.map(lambda line: line.split(' ')[0])
>>> for ip in ipsRDD.take(5): print(ip)
... 
3.94.78.5
3.94.78.5
19.38.140.62
19.38.140.62
129.133.56.105

scala> val ipsRDD = logsRDD.map(line => line.split(' ')(0))
ipsRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at map at <console>:25

scala> ipsRDD.take(5).foreach(println)
3.94.78.5
3.94.78.5
19.38.140.62
19.38.140.62
129.133.56.105

13. Finally, save the list of IP addresses as a text file:

>>> ipsRDD.saveAsTextFile("/devsh_loudacre/iplist")
scala> ipsRDD.saveAsTextFile("/devsh_loudacre/iplist")

• Note: If you re-run this command, you will not be able to save to the same directory because it already exists. 
Be sure to delete the directory in HDFS before saving again

14. List the contents of the /devsh_loudacre/iplist HDFS folder. 
Review the contents of one of the files to confirm that they were created correctly.

scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/iplist
Found 183 items
-rw-r--r--   2 scelisdev02 hadoop          0 2021-09-28 13:29 /devsh_loudacre/iplist/_SUCCESS
-rw-r--r--   2 scelisdev02 hadoop      49265 2021-09-28 13:29 /devsh_loudacre/iplist/part-00000
-rw-r--r--   2 scelisdev02 hadoop      45854 2021-09-28 13:29 /devsh_loudacre/iplist/part-00001

scelisdev02@cca175-m:~$ hdfs dfs -rm -r /devsh_loudacre/iplist
Deleted /devsh_loudacre/iplist
scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/iplist
Found 183 items
-rw-r--r--   2 scelisdev02 hadoop          0 2021-09-28 13:35 /devsh_loudacre/iplist/_SUCCESS
-rw-r--r--   2 scelisdev02 hadoop      49265 2021-09-28 13:35 /devsh_loudacre/iplist/part-00000
-rw-r--r--   2 scelisdev02 hadoop      45854 2021-09-28 13:35 /devsh_loudacre/iplist/part-00001

scelisdev02@cca175-m:~$ hdfs dfs -cat /devsh_loudacre/iplist/part-00000

110.188.167.158
88.20.201.223
88.20.201.223
221.204.143.23
221.204.143.23
236.195.132.32
236.195.132.32
236.195.132.32
236.195.132.32
71.165.253.85
71.165.253.85
71.165.253.85
71.165.253.85
236.13.121.129

Map Weblog Entries to IP Address/User ID Pairs

15. Use RDD transformations to create a dataset consisting of the IP address and corresponding user ID for each request for an HTML file. 
(Filter for files with the .html extension; disregard requests for other file types.) 
The user ID is the third field in each log file line. Save the data into a comma-separated text file in the directory /devsh_loudacre/userips_csv. 
Make sure the data is saved in the form of comma-separated strings:

ipusersRDD = logsRDD.filter(lambda line: ".html" in line).map(lambda line: line.split(' ')[0]+","+line.split(' ')[2])


>>> htmllogsRDD = logsRDD.filter(lambda line: ".html" in line)
>>> htmlLines = htmllogsRDD.take(5)
>>> for line in htmlLines: print(line)
... 
3.94.78.5 - 69827 [15/Sep/2013:23:58:36 +0100] "GET /KBDOC-00033.html HTTP/1.0" 200 14417 "http://www.loudacre.com"  "Loudacre Mobile Browser iFruit 1"
19.38.140.62 - 21475 [15/Sep/2013:23:58:34 +0100] "GET /KBDOC-00277.html HTTP/1.0" 200 15517 "http://www.loudacre.com"  "Loudacre Mobile Browser Ronin S1"
129.133.56.105 - 2489 [15/Sep/2013:23:58:34 +0100] "GET /KBDOC-00033.html HTTP/1.0" 200 10590 "http://www.loudacre.com"  "Loudacre Mobile Browser Sorrento F00L"
217.150.149.167 - 4712 [15/Sep/2013:23:56:06 +0100] "GET /ronin_s4_sales.html HTTP/1.0" 200 845 "http://www.loudacre.com"  "Loudacre Mobile Browser MeeToo 1.0"
209.151.12.34 - 45922 [15/Sep/2013:23:55:09 +0100] "GET /KBDOC-00259.html HTTP/1.0" 200 19362 "http://www.loudacre.com"  "Loudacre Mobile Browser Sorrento F11L"
>>> ipusersRDD = logsRDD.map(lambda line: line.split(' ')[0]+","+line.split(' ')[2])
>>> ipusersFields = ipusersRDD.take(5)
>>> for fields in ipusersFields:
...     print("-------")
...     for field in fields: print(field)
... 
-------
3
.
9
4
.
7
8
.
5
,
6
9
8
2
7
-------
3
.
9
4
.
7
8
.
5
,
6
9
8
2
7
-------
1
9
.
3
8
.
1
4
0
.
6
2
,
2
1
4
7
5
-------
1
9
.
3
8
.
1
4
0
.
6
2
,
2
1
4
7
5
-------
1
2
9
.
1
3
3
.
5
6
.
1
0
5
,
2
4
8
9

hdfs dfs -rm -r /devsh_loudacre/userips_csv
>>> ipusersRDD.saveAsTextFile("/devsh_loudacre/userips_csv")  
hdfs dfs -ls /devsh_loudacre/userips_csv
hdfs dfs -cat /devsh_loudacre/userips_csv

scala> val htmllogsRDD = logsRDD.filter(line => line.contains(".html"))
htmllogsRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[8] at filter at <console>:25

scala> val htmlLines = htmllogsRDD.take(5)
htmlLines: Array[String] = Array(3.94.78.5 - 69827 [15/Sep/2013:23:58:36 +0100] "GET /KBDOC-00033.html HTTP/1.0" 200 14417 "http://www.loudacre.com"  "Loudacre Mobile Browser iFruit 1", 19.38.140.62 - 21475 [15/Sep/2013:23:58:34 +0100] "GET /KBDOC-00277.html HTTP/1.0" 200 15517 "http://www.loudacre.com"  "Loudacre Mobile Browser Ronin S1", 129.133.56.105 - 2489 [15/Sep/2013:23:58:34 +0100] "GET /KBDOC-00033.html HTTP/1.0" 200 10590 "http://www.loudacre.com"  "Loudacre Mobile Browser Sorrento F00L", 217.150.149.167 - 4712 [15/Sep/2013:23:56:06 +0100] "GET /ronin_s4_sales.html HTTP/1.0" 200 845 "http://www.loudacre.com"  "Loudacre Mobile Browser MeeToo 1.0", 209.151.12.34 - 45922 [15/Sep/2013:23:55:09 +0100] "GET /KBDOC-00259.html HTTP/1.0" 200 19362 "http://www....

scala> htmlLines.foreach(println)
3.94.78.5 - 69827 [15/Sep/2013:23:58:36 +0100] "GET /KBDOC-00033.html HTTP/1.0" 200 14417 "http://www.loudacre.com"  "Loudacre Mobile Browser iFruit 1"
19.38.140.62 - 21475 [15/Sep/2013:23:58:34 +0100] "GET /KBDOC-00277.html HTTP/1.0" 200 15517 "http://www.loudacre.com"  "Loudacre Mobile Browser Ronin S1"
129.133.56.105 - 2489 [15/Sep/2013:23:58:34 +0100] "GET /KBDOC-00033.html HTTP/1.0" 200 10590 "http://www.loudacre.com"  "Loudacre Mobile Browser Sorrento F00L"
217.150.149.167 - 4712 [15/Sep/2013:23:56:06 +0100] "GET /ronin_s4_sales.html HTTP/1.0" 200 845 "http://www.loudacre.com"  "Loudacre Mobile Browser MeeToo 1.0"
209.151.12.34 - 45922 [15/Sep/2013:23:55:09 +0100] "GET /KBDOC-00259.html HTTP/1.0" 200 19362 "http://www.loudacre.com"  "Loudacre Mobile Browser Sorrento F11L"

scala> val ipusersRDD = logsRDD.map(line => line.split(' ')(0) + "," + line.split(' ')(2))
ipusersRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at map at <console>:25

scala> ipusersRDD.take(5).foreach(println)
3.94.78.5,69827
3.94.78.5,69827
19.38.140.62,21475
19.38.140.62,21475
129.133.56.105,2489

hdfs dfs -rm -r /devsh_loudacre/userips_csv 

scala> ipusersRDD.saveAsTextFile("/devsh_loudacre/userips_csv")
      
hdfs dfs -ls /devsh_loudacre/userips_csv/

scelisdev02@cca175-m:~/training_materials/devsh/exercises/transform-rdds/solution$ hdfs dfs -cat /devsh_loudacre/userips_csv/part-00000 |more
3.94.78.5,69827
3.94.78.5,69827
19.38.140.62,21475
19.38.140.62,21475
129.133.56.105,2489
129.133.56.105,2489
217.150.149.167,4712

16. Now that the data is in CSV format, it can easily be used by Spark SQL. 
Load the new CSV files in /devsh_loudacre/userips_csv created above into a DataFrame, then view the data and schema.

>>> usersipDF =  spark.read.option("inferschema", "true").csv("/devsh_loudacre/userips_csv")
>>> usersipDF.printSchema()
root
 |-- _c0: string (nullable = true)
 |-- _c1: integer (nullable = true)

>>> usersipDF.show()
+---------------+------+                                                        
|            _c0|   _c1|
+---------------+------+
| 186.63.149.248|111180|
| 186.63.149.248|111180|
|   52.38.78.242| 61365|
|   52.38.78.242| 61365|
|   52.38.78.242| 61365|
|   52.38.78.242| 61365|
|  13.52.250.110| 23316|
|  13.52.250.110| 23316|
|   243.96.65.66| 74292|
|   243.96.65.66| 74292|
|247.201.244.216| 59837|
|247.201.244.216| 59837|
|247.201.244.216| 59837|
|247.201.244.216| 59837|
| 29.149.166.116| 52347|
| 29.149.166.116| 52347|
| 51.174.107.161|109719|
| 51.174.107.161|109719|
|199.192.126.151| 90270|
|199.192.126.151| 90270|
+---------------+------+
only showing top 20 rows


scala> val usersipDF = spark.read.options(Map("inferSchema"->"true")).csv("/devsh_loudacre/userips_csv")
usersipDF: org.apache.spark.sql.DataFrame = [_c0: string, _c1: int]             

scala> usersipDF
   val usersipDF: org.apache.spark.sql.DataFrame

scala> usersipDF.printSchema
root
 |-- _c0: string (nullable = true)
 |-- _c1: integer (nullable = true)

scala> usersipDF.show()
+---------------+------+
|            _c0|   _c1|
+---------------+------+
| 186.63.149.248|111180|
| 186.63.149.248|111180|
|   52.38.78.242| 61365|
|   52.38.78.242| 61365|
|   52.38.78.242| 61365|
|   52.38.78.242| 61365|
|  13.52.250.110| 23316|
|  13.52.250.110| 23316|
|   243.96.65.66| 74292|
|   243.96.65.66| 74292|
|247.201.244.216| 59837|
|247.201.244.216| 59837|
|247.201.244.216| 59837|
|247.201.244.216| 59837|
| 29.149.166.116| 52347|
| 29.149.166.116| 52347|
| 51.174.107.161|109719|
| 51.174.107.161|109719|
|199.192.126.151| 90270|
|199.192.126.151| 90270|
+---------------+------+
only showing top 20 rows

Bonus Exercise 1: Clean Device Status Data
If you have more time, attempt this extra bonus exercise.
One common use of core Spark RDDs is data scrubbing—converting the data into a format that can be used in Spark SQL. 
In this bonus exercise, you will process data in order to get it into a standardized format for later processing.
Review the contents of the file $DEVDATA/devicestatus.txt. 
This file contains data collected from mobile devices on Loudacre’s network, including device ID, current status, location, and so on. 
Because Loudacre previously acquired other mobile providers’ networks, the data from different subnetworks has different formats. 
Note that the records in this file have different field delimiters: some use commas, some use pipes (|), and so on. 
Your task is the following:

17. Upload the devicestatus.txt file to HDFS.

$DEVDATA/devicestatus.txt

scelisdev02@cca175-m:~/training_materials$ hdfs dfs -put $DEVDATA/devicestatus.txt /devsh_loudacre/
scelisdev02@cca175-m:~/training_materials$ hdfs dfs -ls /devsh_loudacre/
Found 20 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-27 10:07 /devsh_loudacre/accountdevice
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-23 21:40 /devsh_loudacre/accounts_zip94913
-rw-r--r--   2 scelisdev02 hadoop      13149 2021-09-27 09:27 /devsh_loudacre/base_stations.parquet
-rw-r--r--   2 scelisdev02 hadoop       2221 2021-09-24 13:45 /devsh_loudacre/devices.csv
-rw-r--r--   2 scelisdev02 hadoop       5483 2021-09-23 11:25 /devsh_loudacre/devices.json
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-24 09:21 /devsh_loudacre/devices_parquet
-rw-r--r--   2 scelisdev02 hadoop   13954723 2021-09-29 08:50 /devsh_loudacre/devicestatus.txt

18. Determine which delimiter to use (the 20th character—position 19—is the first use of the delimiter).

extraer la posición 19 del fichero... para saber todos los delimitadores
hacer split por ese caracter y 

>>> devstatusRDD = sc.textFile("/devsh_loudacre/devicestatus.txt")
>>> for line in devstatusRDD.take(2):
...     print(line)
... 
2014-03-15:10:10:20,Sorrento F41L,8cc3b47e-bd01-4482-b500-28f2342679af,7,24,39,enabled,disabled,connected,55,67,12,33.6894754264,-117.543308253
2014-03-15:10:10:20|MeeToo 1.0|ef8c7564-0a1a-4650-a655-c8bbd5f8f943|0|31|63|70|39|27|enabled|enabled|enabled|37.4321088904|-121.485029632

>>> delimitadorRDD = devstatusRDD.filter(lambda line: len(line) > 20)
>>> for line in delimitadorRDD.take(2):
...     print(line)
... 
2014-03-15:10:10:20,Sorrento F41L,8cc3b47e-bd01-4482-b500-28f2342679af,7,24,39,enabled,disabled,connected,55,67,12,33.6894754264,-117.543308253
2014-03-15:10:10:20|MeeToo 1.0|ef8c7564-0a1a-4650-a655-c8bbd5f8f943|0|31|63|70|39|27|enabled|enabled|enabled|37.4321088904|-121.485029632

>>> lineFieldsRDD = devstatusRDD.map(lambda line: line.split(line[19:20]))
>>> for fields in lineFieldsRDD.take(2):
...     print("-------")
...     for field in fields: print(field)
... 
-------
2014-03-15:10:10:20
Sorrento F41L
8cc3b47e-bd01-4482-b500-28f2342679af
7
24
39
enabled
disabled
connected
55
67
12
33.6894754264
-117.543308253
-------
2014-03-15:10:10:20
MeeToo 1.0
ef8c7564-0a1a-4650-a655-c8bbd5f8f943
0
31
63
70
39
27
enabled
enabled
enabled
37.4321088904
-121.485029632


scala> val devstatusRDD = sc.textFile("/devsh_loudacre/devicestatus.txt")
devstatusRDD: org.apache.spark.rdd.RDD[String] = /devsh_loudacre/devicestatus.txt MapPartitionsRDD[1] at textFile at <console>:24

scala> val delimitadorRDD = devstatusRDD.filter(line => line.length > 20)
delimitadorRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <console>:25

scala> val lineFieldsRDD = devstatusRDD.map(line => line.split(line.charAt(19)))
lineFieldsRDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[4] at map at <console>:25

19. Filter out any records which do not parse correctly (hint: each record should have exactly 14 values).

>>> valuesFieldsRDD = lineFieldsRDD.filter(lambda values: len(values) == 14)
>>> for line in valuesFieldsRDD.take(2):
...     print(line)
... 
['2014-03-15:10:10:20', 'Sorrento F41L', '8cc3b47e-bd01-4482-b500-28f2342679af', '7', '24', '39', 'enabled', 'disabled', 'connected', '55', '67', '12', '33.6894754264', '-117.543308253']
['2014-03-15:10:10:20', 'MeeToo 1.0', 'ef8c7564-0a1a-4650-a655-c8bbd5f8f943', '0', '31', '63', '70', '39', '27', 'enabled', 'enabled', 'enabled', '37.4321088904', '-121.485029632']

scala> val valuesFieldsRDD = lineFieldsRDD.filter(values => values.length == 14)
valuesFieldsRDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[5] at filter at <console>:25

20. Extract the date (first field), model (second field), device ID (third field), and latitude and longitude (13 th and 14 th fields respectively).

21. The second field contains the device manufacturer and model name (such as Ronin S2). 
Split this field by spaces to separate the manufacturer from the model (for example, manufacturer Ronin, model S2). Keep just the manufacturer name.

>>> dataRDD = valuesFieldsRDD.map(lambda line: (line[0], line[1].split(' ')[0], line[2], line[12], line[13]) )
>>> for data in dataRDD.take(2): print(data)
... 
('2014-03-15:10:10:20', 'Sorrento', '8cc3b47e-bd01-4482-b500-28f2342679af', '33.6894754264', '-117.543308253')
('2014-03-15:10:10:20', 'MeeToo', 'ef8c7564-0a1a-4650-a655-c8bbd5f8f943', '37.4321088904', '-121.485029632')

scala> val dataRDD = valuesFieldsRDD.map(line =>  (line(0), line(1).split(' ')(0), line(2), line(12), line(13)) )
dataRDD: org.apache.spark.rdd.RDD[(String, String, String, String, String)] = MapPartitionsRDD[6] at map at <console>:25

22. Save the extracted data to comma-delimited text files in the
/devsh_loudacre/devicestatus_etl directory on HDFS.

>>> dataRDD.saveAsTextFile("/devsh_loudacre/devicestatus_etl")
>>> dataRDD.map(lambda line: ','.join(line)).saveAsTextFile("/devsh_loudacre/devicestatus_etl")

scala> dataRDD.map(line => line.toString).map(s => s.substring(1,s.length-1)).saveAsTextFile("/devsh_loudacre/devicestatus_etl")

23. Confirm that the data in the file(s) was saved correctly. The lines in the file should all look similar to this, with all fields delimited by commas.

scelisdev02@cca175-m:~/training_materials$ hdfs dfs -ls /devsh_loudacre/devicestatus_etl
Found 3 items
-rw-r--r--   2 scelisdev02 hadoop          0 2021-09-29 10:11 /devsh_loudacre/devicestatus_etl/_SUCCESS
-rw-r--r--   2 scelisdev02 hadoop    5417019 2021-09-29 10:11 /devsh_loudacre/devicestatus_etl/part-00000
-rw-r--r--   2 scelisdev02 hadoop    5417087 2021-09-29 10:11 /devsh_loudacre/devicestatus_etl/part-00001

scelisdev02@cca175-m:~/training_materials$ hdfs dfs -cat /devsh_loudacre/devicestatus_etl/part-00000

('2014-03-15:10:15:48', 'Ronin', '435fd8ea-d109-45dd-ab80-7f6c3355d9b6', '42.6935687789', '-122.379621799')
('2014-03-15:10:15:48', 'Titanic', 'c47c4f40-a520-49bb-acc4-003943249481', '37.1494857648', '-119.371337872')
('2014-03-15:10:15:48', 'Sorrento', '9bf0cee1-6946-47ce-a061-29966819fdd4', '0', '0')
('2014-03-15:10:15:48', 'Titanic', 'f2a74037-e291-48ab-945f-052932be3251', '34.4338253254', '-118.133911894')
('2014-03-15:10:15:48', 'Titanic', '058caca7-0bb9-466b-9f6b-df7eb1deb3ed', '34.0322887096', '-117.382940806')

scelisdev02@cca175-m:~/training_materials$ hdfs dfs -cat /devsh_loudacre/devicestatus_etl/part-00000

2014-03-15:10:14:00,Sorrento,d768e05e-b9c8-4a97-b2f9-a95a9dab83cc,36.575150792,-120.889983679
2014-03-15:10:14:00,Sorrento,b15b2297-ab13-48c3-a1b3-b661e55141fe,37.970373416,-121.83435139
2014-03-15:10:14:00,MeeToo,22f75025-720b-42f3-807e-7f903704d141,38.9599583892,-120.918615683
2014-03-15:10:14:00,Sorrento,a33bc727-eed8-469a-aa8b-d1d73af39a6e,38.3365167034,-121.945349435


>>> devstatusRDD = sc.textFile("/devsh_loudacre/devicestatus.txt")
>>> cleanstatusRDD = devstatusRDD.filter(lambda line: len(line) > 20).map(lambda line: line.split(line[19:20])).filter(lambda values: len(values) == 14)
>>> for line in cleanstatusRDD.take(2): print(line)
... 
['2014-03-15:10:10:20', 'Sorrento F41L', '8cc3b47e-bd01-4482-b500-28f2342679af', '7', '24', '39', 'enabled', 'disabled', 'connected', '55', '67', '12', '33.6894754264', '-117.543308253']
['2014-03-15:10:10:20', 'MeeToo 1.0', 'ef8c7564-0a1a-4650-a655-c8bbd5f8f943', '0', '31', '63', '70', '39', '27', 'enabled', 'enabled', 'enabled', '37.4321088904', '-121.485029632']
>>> dataRDD = cleanstatusRDD.map(lambda line: (line[0], line[1].split(' ')[0], line[2], line[12], line[13]) )
>>> dataRDD.map(lambda line: ','.join(line)).saveAsTextFile("/devsh_loudacre/devicestatus_etl")

scala> val devstatusRDD = sc.textFile("/devsh_loudacre/devicestatus.txt")
devstatusRDD: org.apache.spark.rdd.RDD[String] = /devsh_loudacre/devicestatus.txt MapPartitionsRDD[1] at textFile at <console>:24

scala> val cleanstatusRDD = devstatusRDD.filter(line => line.length > 20).map(line => line.split(line.charAt(19))).filter(values => values.length == 14)
cleanstatusRDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[4] at filter at <console>:25

scala> val dataRDD = cleanstatusRDD.map(line =>  (line(0), line(1).split(' ')(0), line(2), line(12), line(13)) )
dataRDD: org.apache.spark.rdd.RDD[(String, String, String, String, String)] = MapPartitionsRDD[5] at map at <console>:25

scala> dataRDD.map(line => line.toString).map(s => s.substring(1,s.length-1)).saveAsTextFile("/devsh_loudacre/devicestatus_etl")
                                                                                
Bonus Exercise 2: Convert Multi-line XML files to CSV files 
One of the common uses for RDDs in core Spark is to transform data from unstructured or semi-structured 
sources or formats that are not supported by Spark SQL to structured formats you can use with Spark SQL. 
In this bonus exercise, you will convert a set of whole-file XML records to a CSV file that can be read into a DataFrame.


24. Review the data on the local Linux filesystem in the directory $DEVDATA/activations. 
Each XML file contains data for all the devices activated by customers during a specific month.

scelisdev02@cca175-m:~/training_materials$ ls $DEVDATA/activations
2008-10.xml  2009-04.xml  2009-10.xml  2010-04.xml  2010-10.xml  2011-04.xml  2011-10.xml  2012-04.xml  2012-10.xml  2013-04.xml  2013-10.xml
2008-11.xml  2009-05.xml  2009-11.xml  2010-05.xml  2010-11.xml  2011-05.xml  2011-11.xml  2012-05.xml  2012-11.xml  2013-05.xml  2013-11.xml
2008-12.xml  2009-06.xml  2009-12.xml  2010-06.xml  2010-12.xml  2011-06.xml  2011-12.xml  2012-06.xml  2012-12.xml  2013-06.xml  2013-12.xml
2009-01.xml  2009-07.xml  2010-01.xml  2010-07.xml  2011-01.xml  2011-07.xml  2012-01.xml  2012-07.xml  2013-01.xml  2013-07.xml  2014-01.xml
2009-02.xml  2009-08.xml  2010-02.xml  2010-08.xml  2011-02.xml  2011-08.xml  2012-02.xml  2012-08.xml  2013-02.xml  2013-08.xml  2014-02.xml
2009-03.xml  2009-09.xml  2010-03.xml  2010-09.xml  2011-03.xml  2011-09.xml  2012-03.xml  2012-09.xml  2013-03.xml  2013-09.xml  2014-03.xml
scelisdev02@cca175-m:~/training_materials$ cat $DEVDATA/activations/2008-10.xml
<activations>
          <activation timestamp="1225462088" type="phone">
            <account-number>9763</account-number>
            <device-id>debea35e-3ecd-4ee7-b0dd-ad428d953f32</device-id>
            <phone-number>7600763387</phone-number>
            <model>MeeToo 1.0</model>
          </activation>
                          <activation timestamp="1225461447" type="phone">
            <account-number>426</account-number>
            <device-id>38a1566d-524e-4137-bad8-b597d09b54b0</device-id>
            <phone-number>5102521038</phone-number>
            <model>Titanic 1000</model>
          </activation>

25. Copy the entire activations directory to /devsh_loudacre in HDFS.

scelisdev02@cca175-m:~/training_materials$ hdfs dfs -put $DEVDATA/activations /devsh_loudacre

Follow the steps below to write code to go through a set of activation XML files and extract the account number and device model for each activation, and save the list to a file as account_number:model.

26. Start with the ActivationModels stub script in the bonus exercise directory:
$DEVSH/exercises/transform-rdds/bonus-xml. 

scelisdev02@cca175-m:~/training_materials$ vi $DEVSH/exercises/transform-rdds/bonus-xml/ActivationModels.pyspark


# Given a string containing XML, parse the string, and 
# return an iterator of activation XML records (Elements) contained in the string

>>> import xml.etree.ElementTree as ElementTree
>>> def getActivations(s):
...     filetree = ElementTree.fromstring(s)
...     return filetree.getiterator('activation')

# Given an activation record (XML Element), return the model name
>>> def getModel(activation):
...     return activation.find('model').text

# Given an activation record (XML Element), return the account number 
>>> def getAccount(activation):
...     return activation.find('account-number').text

scala> import scala.xml._
import scala.xml._

// Given a string containing XML, parse the string, and
// return an iterator of activation XML records (Nodes) contained in the string

scala> def getActivations(xmlstring: String): Iterator[Node] = {
     |     val nodes = XML.loadString(xmlstring) \\ "activation"
     |     nodes.toIterator
     | }
getActivations: (xmlstring: String)Iterator[scala.xml.Node]

// Given an activation record (XML Node), return the model name
scala> def getModel(activation: Node): String = {
     |    (activation \ "model").text
     | }
getModel: (activation: scala.xml.Node)String

// Given an activation record (XML Node), return the account number
scala> def getAccount(activation: Node): String = {
     |    (activation \ "account-number").text
     | }
getAccount: (activation: scala.xml.Node)String


27. Use wholeTextFiles to create an RDD from the activations dataset. 
The resulting RDD will consist of tuples, in which the first value is the name of the file,
and the second value is the contents of the file (XML) as a string.

# Read XML files into an RDD 

>>> files="/devsh_loudacre/activations"
>>> activationFiles = sc.wholeTextFiles(files)
>>> activationFiles
org.apache.spark.api.java.JavaPairRDD@7ef30252

for line in activationFiles.take(2): print (line)

>>> for line in activationFiles.take(2): print (line)
... 
('hdfs://cca175-m/devsh_loudacre/activations/2008-10.xml', '<activations>\n\t  <activation timestamp="1225462088" type="phone">\n\t    <account-number>9763</account-number>\n\t    <device-id>debea35e-3ecd-4ee7-b0dd-ad428d953f32</device-id>\n\t    <phone-number>7600763387</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225461447" type="phone">\n\t    <account-number>426</account-number>\n\t    <device-id>38a1566d-524e-4137-bad8-b597d09b54b0</device-id>\n\t    <phone-number>5102521038</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225446947" type="phone">\n\t    <account-number>383</account-number>\n\t    <device-id>513024a3-a828-4674-9ff6-041e9a851f18</device-id>\n\t    <phone-number>9162206560</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225445908" type="phone">\n\t    <account-number>404</account-number>\n\t    <device-id>93d7d0eb-7551-4e11-ab59-91f34c04378f</device-id>\n\t    <phone-number>6506862748</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225444099" type="phone">\n\t    <account-number>393</account-number>\n\t    <device-id>75faa3a2-31d6-48ec-a1a9-da4c882c7b75</device-id>\n\t    <phone-number>6504786402</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225421991" type="phone">\n\t    <account-number>53</account-number>\n\t    <device-id>a6482e48-d347-4ec5-b010-9d9a3b13d999</device-id>\n\t    <phone-number>6503155055</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225412236" type="phone">\n\t    <account-number>96</account-number>\n\t    <device-id>351a0384-9af6-4621-ada8-3eba9fb2734c</device-id>\n\t    <phone-number>4152163825</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225406348" type="phone">\n\t    <account-number>283</account-number>\n\t    <device-id>04ad366c-c023-4d01-8493-8ec97d3ff5cc</device-id>\n\t    <phone-number>5109683453</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225400017" type="phone">\n\t    <account-number>464</account-number>\n\t    <device-id>e4ceefc4-8765-4beb-8b10-9b3d73eec535</device-id>\n\t    <phone-number>6504367851</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225386391" type="phone">\n\t    <account-number>475</account-number>\n\t    <device-id>31fe0811-c4fb-40b8-bc59-8386fe2f5d8e</device-id>\n\t    <phone-number>6504313792</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225371984" type="phone">\n\t    <account-number>479</account-number>\n\t    <device-id>38f76751-3236-4e99-80c9-c4b0a030108e</device-id>\n\t    <phone-number>6507811206</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225355475" type="phone">\n\t    <account-number>306</account-number>\n\t    <device-id>d743093b-8bcf-4ede-a475-4bc29936bc5e</device-id>\n\t    <phone-number>4158787317</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225349055" type="phone">\n\t    <account-number>14045</account-number>\n\t    <device-id>1f568e8d-268a-4171-8a50-94c70fbfdf90</device-id>\n\t    <phone-number>9098824980</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225347793" type="phone">\n\t    <account-number>6482</account-number>\n\t    <device-id>c14cbc3d-8b55-49bb-931d-7b2472aaa750</device-id>\n\t    <phone-number>4082887060</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225345397" type="phone">\n\t    <account-number>349</account-number>\n\t    <device-id>04c2fc8e-2f4f-4c7a-ad26-146c26deec87</device-id>\n\t    <phone-number>6502434241</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225333766" type="phone">\n\t    <account-number>124</account-number>\n\t    <device-id>74d1341a-8da9-4de7-86f6-ec5285665ef2</device-id>\n\t    <phone-number>5103920538</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225333728" type="phone">\n\t    <account-number>247</account-number>\n\t    <device-id>8ca7ba2f-5b8a-4574-abbb-de5a18221894</device-id>\n\t    <phone-number>6509604844</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225320127" type="phone">\n\t    <account-number>88</account-number>\n\t    <device-id>9f7daca9-5faf-4cfd-b863-85d510464945</device-id>\n\t    <phone-number>6501458572</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225319701" type="phone">\n\t    <account-number>2939</account-number>\n\t    <device-id>18970270-86f0-4bac-ba03-24eb8518f082</device-id>\n\t    <phone-number>9161294398</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225317616" type="phone">\n\t    <account-number>314</account-number>\n\t    <device-id>ae3cb61f-5318-4b74-bb05-9de91a66bb10</device-id>\n\t    <phone-number>6509019772</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225315853" type="phone">\n\t    <account-number>396</account-number>\n\t    <device-id>767d106f-4a25-4177-afe5-8f081fc68cd7</device-id>\n\t    <phone-number>6500312075</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225278009" type="phone">\n\t    <account-number>6881</account-number>\n\t    <device-id>fb8f6fe5-c2f5-43e5-a0b5-032c1b437989</device-id>\n\t    <phone-number>5625307326</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225275474" type="phone">\n\t    <account-number>234</account-number>\n\t    <device-id>31485143-2104-4f4d-bceb-4866f484e27f</device-id>\n\t    <phone-number>9166314637</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225246830" type="phone">\n\t    <account-number>34209</account-number>\n\t    <device-id>7dcd293d-b59d-48ea-bfe5-af623fa67b81</device-id>\n\t    <phone-number>7022095507</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225242164" type="phone">\n\t    <account-number>369</account-number>\n\t    <device-id>8cadd7a9-b799-4ebb-8314-2b8a9716ed86</device-id>\n\t    <phone-number>5108109471</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225230097" type="phone">\n\t    <account-number>255</account-number>\n\t    <device-id>6d515f2b-4fc8-4916-916f-f7ec7750ee3e</device-id>\n\t    <phone-number>4159552347</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225229238" type="phone">\n\t    <account-number>13</account-number>\n\t    <device-id>9163f7b0-9957-47f2-8a56-cb195ea1adc4</device-id>\n\t    <phone-number>4153335996</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225174761" type="phone">\n\t    <account-number>142</account-number>\n\t    <device-id>558b13f3-783f-4434-a85f-97413146f835</device-id>\n\t    <phone-number>5109700907</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225161449" type="phone">\n\t    <account-number>11</account-number>\n\t    <device-id>678dc546-57ca-47fa-a387-184248f610dd</device-id>\n\t    <phone-number>5109726353</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225157362" type="phone">\n\t    <account-number>61</account-number>\n\t    <device-id>be33c35d-1fa6-4f61-83e4-439a095ae4cc</device-id>\n\t    <phone-number>9162581613</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225152905" type="phone">\n\t    <account-number>1554</account-number>\n\t    <device-id>ce9468cc-8d98-4dc6-90b5-e72a44c64a21</device-id>\n\t    <phone-number>4153912267</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225149813" type="phone">\n\t    <account-number>165</account-number>\n\t    <device-id>52c76b69-c8d2-479c-88af-cbf0e95e3e5e</device-id>\n\t    <phone-number>5104562352</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225120582" type="phone">\n\t    <account-number>128224</account-number>\n\t    <device-id>416578b7-5684-4848-b97a-3587784eaa12</device-id>\n\t    <phone-number>2133389934</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225118290" type="phone">\n\t    <account-number>394</account-number>\n\t    <device-id>6b879605-0cdc-4200-834f-205b74b40dca</device-id>\n\t    <phone-number>7070593543</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225102200" type="phone">\n\t    <account-number>433</account-number>\n\t    <device-id>fd12614c-b21c-4b26-a825-4bfd63492149</device-id>\n\t    <phone-number>9165391636</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225086639" type="phone">\n\t    <account-number>448</account-number>\n\t    <device-id>39f94f2b-3a58-49bd-bc48-3885cd42badc</device-id>\n\t    <phone-number>6503935352</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225078279" type="phone">\n\t    <account-number>440</account-number>\n\t    <device-id>0bf181b2-25b7-4541-87d8-15f6d7a0ccb9</device-id>\n\t    <phone-number>6505467053</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225067942" type="phone">\n\t    <account-number>240</account-number>\n\t    <device-id>3966e7ae-0954-4ea7-bde6-b35ee0f39c30</device-id>\n\t    <phone-number>7079354042</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225061652" type="phone">\n\t    <account-number>460</account-number>\n\t    <device-id>5b8530d4-f467-42e3-818d-c81e20cfd1c1</device-id>\n\t    <phone-number>6500282551</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225058673" type="phone">\n\t    <account-number>275</account-number>\n\t    <device-id>42be5c11-8931-46bf-af06-c87c3f580337</device-id>\n\t    <phone-number>4155493875</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225047870" type="phone">\n\t    <account-number>4638</account-number>\n\t    <device-id>7164d9d1-d2e0-439d-9aee-1fd20da48df3</device-id>\n\t    <phone-number>5101970154</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225039607" type="phone">\n\t    <account-number>171</account-number>\n\t    <device-id>dc0e941a-90d4-4c84-ac7d-383e276dad84</device-id>\n\t    <phone-number>5105209310</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225033098" type="phone">\n\t    <account-number>213</account-number>\n\t    <device-id>5c42d2ac-d92f-4ef1-acee-47d39a909b11</device-id>\n\t    <phone-number>5101004303</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224994511" type="phone">\n\t    <account-number>324</account-number>\n\t    <device-id>71a6ed8d-a751-44bf-8b38-320a49c12a9b</device-id>\n\t    <phone-number>6501654645</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224990765" type="phone">\n\t    <account-number>98121</account-number>\n\t    <device-id>9a6d0b23-8845-460d-aa47-91c82d8fc480</device-id>\n\t    <phone-number>6618825932</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224983389" type="phone">\n\t    <account-number>363</account-number>\n\t    <device-id>254ab846-f1d4-4c85-a658-46ee5e848c53</device-id>\n\t    <phone-number>6505506140</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224956920" type="phone">\n\t    <account-number>52</account-number>\n\t    <device-id>330f46cc-de2a-49d3-aa93-6ff761de7d5a</device-id>\n\t    <phone-number>4151228388</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224956441" type="phone">\n\t    <account-number>286</account-number>\n\t    <device-id>78d4f9fe-b6e3-40c2-91d5-18ff95a074e5</device-id>\n\t    <phone-number>5107058174</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224946254" type="phone">\n\t    <account-number>13702</account-number>\n\t    <device-id>b024cf7d-496f-4f6e-bbc1-772d13ca1617</device-id>\n\t    <phone-number>5416107165</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224944970" type="phone">\n\t    <account-number>345</account-number>\n\t    <device-id>cd1c8a9d-066c-4f6f-b08f-dc119d827699</device-id>\n\t    <phone-number>5103859765</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224938467" type="phone">\n\t    <account-number>36906</account-number>\n\t    <device-id>9043cf20-d816-4f17-af72-020767ae14e0</device-id>\n\t    <phone-number>5411008209</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224907589" type="phone">\n\t    <account-number>413</account-number>\n\t    <device-id>0e487cf9-6923-4891-94f7-7b12695b72fd</device-id>\n\t    <phone-number>5107080443</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224847655" type="phone">\n\t    <account-number>459</account-number>\n\t    <device-id>5aff8521-f91b-48f6-a69a-156caa236bd0</device-id>\n\t    <phone-number>4157500638</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224831370" type="phone">\n\t    <account-number>144</account-number>\n\t    <device-id>e7f833e9-276e-4f3b-9710-9d3106a5141a</device-id>\n\t    <phone-number>5108355566</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224817110" type="phone">\n\t    <account-number>373</account-number>\n\t    <device-id>0f7b8628-873a-4d1b-9b86-0bfe40f2f668</device-id>\n\t    <phone-number>5109247675</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224812967" type="phone">\n\t    <account-number>46849</account-number>\n\t    <device-id>b99f88ef-5ad2-4c62-bbc6-ca2756b4467c</device-id>\n\t    <phone-number>5414191932</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224805157" type="phone">\n\t    <account-number>252</account-number>\n\t    <device-id>6c71ca45-ddae-4191-98e8-01f3bc2a1220</device-id>\n\t    <phone-number>5109508609</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224803105" type="phone">\n\t    <account-number>1</account-number>\n\t    <device-id>7eb61253-55cd-4309-8f17-129089fee461</device-id>\n\t    <phone-number>5100032418</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224782486" type="phone">\n\t    <account-number>21566</account-number>\n\t    <device-id>2bebd773-39ac-4d31-a23e-06eb2991ac26</device-id>\n\t    <phone-number>9165053364</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224781467" type="phone">\n\t    <account-number>154</account-number>\n\t    <device-id>1923ef7c-3067-4f7c-93f7-c867bd31a39f</device-id>\n\t    <phone-number>5106577100</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224778387" type="phone">\n\t    <account-number>46168</account-number>\n\t    <device-id>fb7f3575-c523-47bf-be76-889a07a5af9c</device-id>\n\t    <phone-number>5037703631</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224771580" type="phone">\n\t    <account-number>3838</account-number>\n\t    <device-id>46ec037a-c72d-4e4a-aff6-a7498dbd8b1f</device-id>\n\t    <phone-number>6501253886</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224769655" type="phone">\n\t    <account-number>21821</account-number>\n\t    <device-id>2cef3c7a-6fbe-42d1-9a7f-19854a959700</device-id>\n\t    <phone-number>6262624197</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224747397" type="phone">\n\t    <account-number>204</account-number>\n\t    <device-id>04d374ba-3441-4aa4-88d9-057c0721ba0f</device-id>\n\t    <phone-number>7076669246</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224736965" type="phone">\n\t    <account-number>131</account-number>\n\t    <device-id>a53f23f2-a57f-4655-90fd-f53a0a05d587</device-id>\n\t    <phone-number>4151707647</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224729378" type="phone">\n\t    <account-number>474</account-number>\n\t    <device-id>4bd55037-72e7-432f-a1b1-2a94a4df9f16</device-id>\n\t    <phone-number>5102971778</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224725663" type="phone">\n\t    <account-number>146</account-number>\n\t    <device-id>5c66d63f-3384-49d6-8cba-65a28b34ba30</device-id>\n\t    <phone-number>7070285635</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224719408" type="phone">\n\t    <account-number>2063</account-number>\n\t    <device-id>21b927a6-c4f6-43c0-bc11-abe77abfd6bc</device-id>\n\t    <phone-number>6503298970</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224688572" type="phone">\n\t    <account-number>374</account-number>\n\t    <device-id>cc8ef6ff-93de-4a55-b2ed-888b3bd9740b</device-id>\n\t    <phone-number>7073442709</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224652877" type="phone">\n\t    <account-number>89</account-number>\n\t    <device-id>bbc9f0e7-8315-4ada-832d-56db3083d507</device-id>\n\t    <phone-number>5104067433</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224652432" type="phone">\n\t    <account-number>194</account-number>\n\t    <device-id>7997d06a-664d-45f2-9c1e-4896b1450e28</device-id>\n\t    <phone-number>7071451183</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224641243" type="phone">\n\t    <account-number>269</account-number>\n\t    <device-id>07895a7d-1b66-4b7c-84d7-b1592a03437c</device-id>\n\t    <phone-number>4153026986</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224628700" type="phone">\n\t    <account-number>155</account-number>\n\t    <device-id>cd84b041-5d4e-4fc5-977e-46231507e173</device-id>\n\t    <phone-number>4153927390</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1224585351" type="phone">\n\t    <account-number>405</account-number>\n\t    <device-id>b53bb445-5bc6-489b-9dd5-9de6370a19fe</device-id>\n\t    <phone-number>4151697001</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t</activations>')
('hdfs://cca175-m/devsh_loudacre/activations/2008-11.xml', '<activations>\n\t  <activation timestamp="1228068581" type="phone">\n\t    <account-number>93893</account-number>\n\t    <device-id>b1d33f97-4f7f-4760-b6d5-2354b8ede7f9</device-id>\n\t    <phone-number>8313151801</phone-number>\n\t    <model>Titanic 1100</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1228064129" type="phone">\n\t    <account-number>10524</account-number>\n\t    <device-id>e2e9a69e-9e9d-4cf0-a3be-b948c8f9bcd0</device-id>\n\t    <phone-number>3102505580</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1228062743" type="phone">\n\t    <account-number>116</account-number>\n\t    <device-id>f0f3bf10-4849-4409-a68b-d70be46729bc</device-id>\n\t    <phone-number>5101189988</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1228062084" type="phone">\n\t    <account-number>428</account-number>\n\t    <device-id>fab0463c-aa81-4b0a-86dc-9b2dc336f9ea</device-id>\n\t    <phone-number>5106385454</phone-number>\n\t    <model>Titanic 1100</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1228061914" type="phone">\n\t    <account-number>237</account-number>\n\t    <device-id>65a54ea6-21d3-406d-806a-fb060675e525</device-id>\n\t    <phone-number>6503135322</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1228057594" type="phone">\n\t    <account-number>51</account-number>\n\t    <device-id>f4f80206-62ee-4a62-81fe-fae3f67d9ac5</device-id>\n\t    <phone-number>6509811731</phone-number>\n\t    <model>Titanic 1100</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1228048822" type="phone">\n\t    <account-number>195</account-number>\n\t    <device-id>a1a52d81-a215-4109-b2ea-05c5c3f88023</device-id>\n\t    <phone-number>9163594089</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1228048751" type="phone">\n\t    <account-number>38</account-number>\n\t    <device-id>f4edd2b5-f875-4ddf-ab05-e88b42304cfe</device-id>\n\t    <phone-number>4153177265</phone-number>\n\t    <model>Titanic 1100</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1228045159" type="phone">\n\t    <account-number>147</account-number>\n\t    <device-id>8030b68d-9fe2-4a32-b7ef-a4d01458bad5</device-id>\n\t    <phone-number>7074994246</phone-number>\n\t    <model>Titanic 1100</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1228044426" type="phone">\n\t    <account-number>120</account-number>\n\t    <device-id>3bb4eab1-6059-4563-ba83-652ed13183d8</device-id>\n\t    <phone-number>4159244097</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1228043667" type="phone">\n\t    <account-number>8386</account-number>\n\t    <device-id>7fe71ddb-f628-404b-92b1-d83185144941</device-id>\n\t    <phone-number>8055015287</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1228037258" type="phone">\n\t    <account-number>412</account-number>\n\t    <device-id>c73b3e86-4b6b-4570-93df-0c63c88b9832</device-id>\n\t    <phone-number>6503191552</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1228024901" type="phone">\n\t    <account-number>458</account-number>\n\t    <device-id>8ab5b4b5-c37c-44b4-af3f-383df36dd933</device-id>\n\t    <phone-number>6505547003</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1228011938" type="phone">\n\t    <account-number>14</account-number>\n\t    <device-id>356562ec-25f7-4fa0-80c3-04dde199f28d</device-id>\n\t    <phone-number>9163145431</phone-number>\n\t    <model>Titanic 1100</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227994723" type="phone">\n\t    <account-number>265</account-number>\n\t    <device-id>6529c933-a516-4806-8789-b1cec9a906bc</device-id>\n\t    <phone-number>5102513154</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227991927" type="phone">\n\t    <account-number>128</account-number>\n\t    <device-id>d04b2829-82d6-40e8-99a4-6f36de7f37aa</device-id>\n\t    <phone-number>5104700295</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227990060" type="phone">\n\t    <account-number>257</account-number>\n\t    <device-id>481fac04-9b3d-4149-8c3c-7e44e0193e77</device-id>\n\t    <phone-number>6500930741</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227977355" type="phone">\n\t    <account-number>23566</account-number>\n\t    <device-id>34b9b80f-14bd-4a8d-ac1e-450d6f027b33</device-id>\n\t    <phone-number>5414803587</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227970298" type="phone">\n\t    <account-number>107295</account-number>\n\t    <device-id>f5acd15f-03af-47bc-a058-96b3579d043d</device-id>\n\t    <phone-number>9164044669</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227961962" type="phone">\n\t    <account-number>27</account-number>\n\t    <device-id>39e73634-abb2-4908-9f0c-b03a21385522</device-id>\n\t    <phone-number>5106498890</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227958445" type="phone">\n\t    <account-number>333</account-number>\n\t    <device-id>50a49dc9-ace7-4bda-9bd2-3f5fc287628d</device-id>\n\t    <phone-number>9169038715</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227957356" type="phone">\n\t    <account-number>121392</account-number>\n\t    <device-id>5c83cc16-01ab-4d3a-9396-beb184e2a9f6</device-id>\n\t    <phone-number>2138590851</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227919061" type="phone">\n\t    <account-number>335</account-number>\n\t    <device-id>fa1d9c67-b73a-487a-a82d-1fce0b10d697</device-id>\n\t    <phone-number>5108634189</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227913705" type="phone">\n\t    <account-number>923</account-number>\n\t    <device-id>80619a86-288a-4ff8-b8e3-dc163b7dd6e6</device-id>\n\t    <phone-number>6504194308</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227908713" type="phone">\n\t    <account-number>41</account-number>\n\t    <device-id>7cde8c96-c803-4e48-b52d-8e0c0a4b24e1</device-id>\n\t    <phone-number>5109215080</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227888786" type="phone">\n\t    <account-number>397</account-number>\n\t    <device-id>e9953f37-bef4-4093-8045-0a998b8ab001</device-id>\n\t    <phone-number>5107577696</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227873981" type="phone">\n\t    <account-number>149</account-number>\n\t    <device-id>5fbac38d-5150-4499-a087-91a3c2ba15da</device-id>\n\t    <phone-number>4151278270</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227865367" type="phone">\n\t    <account-number>476</account-number>\n\t    <device-id>5fb78138-dc78-4103-a091-55a9d4bf5efa</device-id>\n\t    <phone-number>4157817332</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227859689" type="phone">\n\t    <account-number>4</account-number>\n\t    <device-id>c0b0c922-7fd4-46bb-a6d6-68d4064f22d5</device-id>\n\t    <phone-number>6503198619</phone-number>\n\t    <model>Titanic 1100</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227830952" type="phone">\n\t    <account-number>114</account-number>\n\t    <device-id>f538a7a6-f6cd-48af-b313-5773be6b7d0d</device-id>\n\t    <phone-number>5106048663</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227826151" type="phone">\n\t    <account-number>189</account-number>\n\t    <device-id>9b134b4c-07d8-42c1-af92-eeb1fd211c98</device-id>\n\t    <phone-number>5100136428</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227819767" type="phone">\n\t    <account-number>49</account-number>\n\t    <device-id>612810c3-b7e3-43e9-83bc-998a4cda0e6b</device-id>\n\t    <phone-number>5107438525</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227806793" type="phone">\n\t    <account-number>292</account-number>\n\t    <device-id>45a59589-828c-4d75-b297-e1b7ad511f04</device-id>\n\t    <phone-number>4159862789</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227802883" type="phone">\n\t    <account-number>56</account-number>\n\t    <device-id>4c0342b7-af47-4464-9e22-49a1a261ffcd</device-id>\n\t    <phone-number>9162005319</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227786174" type="phone">\n\t    <account-number>322</account-number>\n\t    <device-id>6dd56894-b7d8-490c-95f8-46e6251f764f</device-id>\n\t    <phone-number>7075317849</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227785572" type="phone">\n\t    <account-number>372</account-number>\n\t    <device-id>dd1d353e-abbc-4adc-93a0-ea30060766d6</device-id>\n\t    <phone-number>9161076178</phone-number>\n\t    <model>Titanic 1100</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227782087" type="phone">\n\t    <account-number>270</account-number>\n\t    <device-id>19452078-e2b8-41ad-af17-a77b5a1b5652</device-id>\n\t    <phone-number>4150621455</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227769405" type="phone">\n\t    <account-number>339</account-number>\n\t    <device-id>f1bffcf8-f105-4983-9d47-cc96c048ce0d</device-id>\n\t    <phone-number>4154302141</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227762105" type="phone">\n\t    <account-number>2031</account-number>\n\t    <device-id>af56cf3e-77a5-4688-b784-c92f34314c83</device-id>\n\t    <phone-number>4151135639</phone-number>\n\t    <model>Titanic 1100</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227738017" type="phone">\n\t    <account-number>353</account-number>\n\t    <device-id>a1fa8883-5a06-4504-a6b1-ff5a6a034bfb</device-id>\n\t    <phone-number>5101143561</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227720362" type="phone">\n\t    <account-number>125</account-number>\n\t    <device-id>903a774b-dd0f-4cf2-890b-c9ba4dba28dc</device-id>\n\t    <phone-number>7070672305</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227719309" type="phone">\n\t    <account-number>99887</account-number>\n\t    <device-id>e7b4d7ce-6ef8-4962-a592-43e0ebe6aece</device-id>\n\t    <phone-number>5415417522</phone-number>\n\t    <model>Titanic 1100</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227717892" type="phone">\n\t    <account-number>472</account-number>\n\t    <device-id>deb0c95c-f6aa-4bcb-8b72-7a853ccb75c0</device-id>\n\t    <phone-number>5105433467</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227711502" type="phone">\n\t    <account-number>8939</account-number>\n\t    <device-id>04c3c276-3075-418b-8a51-b93883909896</device-id>\n\t    <phone-number>5104872043</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227698854" type="phone">\n\t    <account-number>370</account-number>\n\t    <device-id>2e16c752-0c31-4441-8b7b-f9fb1200a647</device-id>\n\t    <phone-number>5109097241</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227688988" type="phone">\n\t    <account-number>28434</account-number>\n\t    <device-id>a7d5b546-fa0e-4902-8d43-e1ac83bdcec1</device-id>\n\t    <phone-number>6199063286</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227676109" type="phone">\n\t    <account-number>300</account-number>\n\t    <device-id>3c2503ae-836e-449b-acd6-776483d2355e</device-id>\n\t    <phone-number>5108557383</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227672804" type="phone">\n\t    <account-number>289</account-number>\n\t    <device-id>f98d0b0f-cf84-4464-a412-303db57039a0</device-id>\n\t    <phone-number>9164554012</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227650267" type="phone">\n\t    <account-number>151</account-number>\n\t    <device-id>2b064958-4cc8-49fc-8aca-ee7c909c8fbe</device-id>\n\t    <phone-number>5100707947</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227646491" type="phone">\n\t    <account-number>12066</account-number>\n\t    <device-id>cc3d0cbf-03a3-4dad-b534-36346ca08983</device-id>\n\t    <phone-number>3108467208</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227641513" type="phone">\n\t    <account-number>22255</account-number>\n\t    <device-id>3dbd189d-787b-4d14-9dc1-620bd08cc104</device-id>\n\t    <phone-number>6197367895</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227634697" type="phone">\n\t    <account-number>364</account-number>\n\t    <device-id>1492df55-6de9-4862-83ba-daabcba71a98</device-id>\n\t    <phone-number>9164414254</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227618293" type="phone">\n\t    <account-number>409</account-number>\n\t    <device-id>608da6ca-1252-48cf-a87b-89f500e063a5</device-id>\n\t    <phone-number>6502425864</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227617658" type="phone">\n\t    <account-number>290</account-number>\n\t    <device-id>085cc0f2-a13a-420b-b673-656d96d3d0ad</device-id>\n\t    <phone-number>5101843345</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227605543" type="phone">\n\t    <account-number>2603</account-number>\n\t    <device-id>3f45619a-2958-459f-8808-3c022da95ba6</device-id>\n\t    <phone-number>4156590803</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227584515" type="phone">\n\t    <account-number>96006</account-number>\n\t    <device-id>2cb7096f-d1f6-431d-bed6-e71469a27210</device-id>\n\t    <phone-number>7473762531</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227584445" type="phone">\n\t    <account-number>414</account-number>\n\t    <device-id>89682648-98b3-4dc8-8c37-5cdf0d716a25</device-id>\n\t    <phone-number>5106847547</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227577650" type="phone">\n\t    <account-number>271</account-number>\n\t    <device-id>27143f4c-17ab-4027-a01a-ce35d456ea8a</device-id>\n\t    <phone-number>5105340442</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227569822" type="phone">\n\t    <account-number>384</account-number>\n\t    <device-id>1a7b0b16-20e4-4204-b31b-7bc7aa33053d</device-id>\n\t    <phone-number>6507494892</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227566876" type="phone">\n\t    <account-number>132</account-number>\n\t    <device-id>640b3f2e-0903-4f33-82e8-074e0fa047ca</device-id>\n\t    <phone-number>5101808531</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227556137" type="phone">\n\t    <account-number>297</account-number>\n\t    <device-id>093a8b70-55ca-4248-970a-09ecbf651b4a</device-id>\n\t    <phone-number>5100478907</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227554198" type="phone">\n\t    <account-number>82</account-number>\n\t    <device-id>6ef9260a-b7f6-44c3-b50c-bbd7e69b81dd</device-id>\n\t    <phone-number>4159613291</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227547822" type="phone">\n\t    <account-number>69821</account-number>\n\t    <device-id>cfec70f7-14bf-4335-b925-752764177d82</device-id>\n\t    <phone-number>5039528927</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227545979" type="phone">\n\t    <account-number>175</account-number>\n\t    <device-id>1775cf6b-05c1-4bf2-b431-316688ff7736</device-id>\n\t    <phone-number>7071408613</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227534768" type="phone">\n\t    <account-number>73</account-number>\n\t    <device-id>544ec3d6-5526-417c-9786-cfefc63662b9</device-id>\n\t    <phone-number>5101152162</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227533625" type="phone">\n\t    <account-number>355</account-number>\n\t    <device-id>534f8d7b-4df7-41ea-a97b-537b484847a3</device-id>\n\t    <phone-number>5109162875</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227526739" type="phone">\n\t    <account-number>26873</account-number>\n\t    <device-id>2c02a50b-48c5-41a9-96fa-30c6aea016e2</device-id>\n\t    <phone-number>4150858149</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227473968" type="phone">\n\t    <account-number>299</account-number>\n\t    <device-id>cd37b386-0656-4bf5-97a6-f97fb7139ec2</device-id>\n\t    <phone-number>7071317120</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227468354" type="phone">\n\t    <account-number>199</account-number>\n\t    <device-id>f6e9cf31-f92d-478f-ab89-955835c23683</device-id>\n\t    <phone-number>5104964687</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227462453" type="phone">\n\t    <account-number>449</account-number>\n\t    <device-id>34703d51-ff43-4acb-bd9f-e10100da3841</device-id>\n\t    <phone-number>9161654125</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227460449" type="phone">\n\t    <account-number>356</account-number>\n\t    <device-id>94f908e5-ff2b-4661-b892-0d4110b290e3</device-id>\n\t    <phone-number>4155612599</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227444381" type="phone">\n\t    <account-number>123</account-number>\n\t    <device-id>425ffa78-f4da-49b1-8735-340afbdaca14</device-id>\n\t    <phone-number>7075556653</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227432368" type="phone">\n\t    <account-number>467</account-number>\n\t    <device-id>24966a23-9df0-4ef6-9813-469a54266548</device-id>\n\t    <phone-number>4154428964</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227413006" type="phone">\n\t    <account-number>407</account-number>\n\t    <device-id>2af5dd88-c6de-4f66-b0cb-536444bbf88f</device-id>\n\t    <phone-number>6502784421</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227366193" type="phone">\n\t    <account-number>463</account-number>\n\t    <device-id>6334f701-4fc0-481c-9c5e-33e26f29eb11</device-id>\n\t    <phone-number>5103168445</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227352294" type="phone">\n\t    <account-number>429</account-number>\n\t    <device-id>070e0ef1-dd8d-4a6f-8d00-70ba4ad450a5</device-id>\n\t    <phone-number>5108860050</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227349216" type="phone">\n\t    <account-number>180</account-number>\n\t    <device-id>7cd06085-e53d-4ef9-9a44-7be0a1f3fb08</device-id>\n\t    <phone-number>5101190952</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227336876" type="phone">\n\t    <account-number>12528</account-number>\n\t    <device-id>86a2f1ae-503f-4a35-9c19-324f238212c3</device-id>\n\t    <phone-number>8187754640</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227306460" type="phone">\n\t    <account-number>382</account-number>\n\t    <device-id>be9a9b25-89d4-431a-b5ad-dd916b1148fd</device-id>\n\t    <phone-number>4157383968</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227300540" type="phone">\n\t    <account-number>305</account-number>\n\t    <device-id>82f7fd24-f481-4cd7-9fa9-783178c4b633</device-id>\n\t    <phone-number>5103095729</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227281886" type="phone">\n\t    <account-number>293</account-number>\n\t    <device-id>c3dbe092-409c-4202-a2ef-59c0891a1a51</device-id>\n\t    <phone-number>4159400192</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227268428" type="phone">\n\t    <account-number>55861</account-number>\n\t    <device-id>35ae54fb-ac24-4c67-b412-723974e194b6</device-id>\n\t    <phone-number>9517956606</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227241205" type="phone">\n\t    <account-number>6386</account-number>\n\t    <device-id>5765680b-0631-434a-9a47-ff1c2ed02ef0</device-id>\n\t    <phone-number>6618001716</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227215603" type="phone">\n\t    <account-number>79</account-number>\n\t    <device-id>3809fda6-9db0-4b47-9800-ef07910221b3</device-id>\n\t    <phone-number>5107606902</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227213573" type="phone">\n\t    <account-number>6</account-number>\n\t    <device-id>7d11b8e3-d67d-45f9-b705-fbbbf16a0e02</device-id>\n\t    <phone-number>9162111862</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227202172" type="phone">\n\t    <account-number>241</account-number>\n\t    <device-id>4ae0824e-42a6-4aa1-89c8-243493a09f71</device-id>\n\t    <phone-number>4155201906</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227192768" type="phone">\n\t    <account-number>107404</account-number>\n\t    <device-id>a34f5997-e02c-4ba1-a00f-9f408565c685</device-id>\n\t    <phone-number>9512160493</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227187959" type="phone">\n\t    <account-number>350</account-number>\n\t    <device-id>7f6102fc-8bd1-42eb-8144-ea1629847dc6</device-id>\n\t    <phone-number>6505008683</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227179626" type="phone">\n\t    <account-number>375</account-number>\n\t    <device-id>a28234e2-3308-4444-a06a-205fa00d9c39</device-id>\n\t    <phone-number>5105517777</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227176378" type="phone">\n\t    <account-number>14501</account-number>\n\t    <device-id>0fd40479-9747-4104-86f0-7800541087d9</device-id>\n\t    <phone-number>3102530687</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227173189" type="phone">\n\t    <account-number>59</account-number>\n\t    <device-id>598f5e0d-db2b-4f28-aaa5-6a43159761de</device-id>\n\t    <phone-number>5103147786</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227170543" type="phone">\n\t    <account-number>113</account-number>\n\t    <device-id>35d3d77f-74bd-4f27-b428-1feaa3d0b11c</device-id>\n\t    <phone-number>5100575073</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227129895" type="phone">\n\t    <account-number>135</account-number>\n\t    <device-id>69a087c1-14bb-4fea-b40b-dfca0035f999</device-id>\n\t    <phone-number>5109553651</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227108365" type="phone">\n\t    <account-number>28028</account-number>\n\t    <device-id>61e44226-4dee-4b79-991b-c40b50e215ac</device-id>\n\t    <phone-number>5103107438</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227106221" type="phone">\n\t    <account-number>432</account-number>\n\t    <device-id>df27313a-6ed2-43ac-aeb8-073a2e1b5a57</device-id>\n\t    <phone-number>5107285542</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227105488" type="phone">\n\t    <account-number>403</account-number>\n\t    <device-id>6303b0ea-bf39-4e34-bdf8-a1fe15de3027</device-id>\n\t    <phone-number>4156669660</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227103092" type="phone">\n\t    <account-number>159</account-number>\n\t    <device-id>d89af7a3-6da5-4acb-b999-ff7f21e37f3b</device-id>\n\t    <phone-number>6505892403</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227096692" type="phone">\n\t    <account-number>26001</account-number>\n\t    <device-id>1fa1500d-e7b7-436b-a185-0789c6f2f42c</device-id>\n\t    <phone-number>6572805809</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227083068" type="phone">\n\t    <account-number>68034</account-number>\n\t    <device-id>85539f8e-2a88-4948-8912-ff1fc71047e0</device-id>\n\t    <phone-number>9287870035</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227078349" type="phone">\n\t    <account-number>262</account-number>\n\t    <device-id>f1b3ecdb-7327-4c9b-8ddd-4083bd1e9c61</device-id>\n\t    <phone-number>9161509342</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227075700" type="phone">\n\t    <account-number>71</account-number>\n\t    <device-id>566c8e28-1583-4772-89b4-47a3e66bee04</device-id>\n\t    <phone-number>9160627649</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227071165" type="phone">\n\t    <account-number>25</account-number>\n\t    <device-id>4fc1d7ec-d0b7-4149-9899-e10aa0892c50</device-id>\n\t    <phone-number>5106716204</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227061834" type="phone">\n\t    <account-number>29389</account-number>\n\t    <device-id>b8a7d48e-e5f6-42c8-8600-68f498d050a4</device-id>\n\t    <phone-number>5039714527</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227047358" type="phone">\n\t    <account-number>123408</account-number>\n\t    <device-id>9cb8b0d4-917b-4465-8024-f6c44ea944f4</device-id>\n\t    <phone-number>9161495797</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227006656" type="phone">\n\t    <account-number>215</account-number>\n\t    <device-id>142fff48-c94b-4ff7-af7a-26a623b2c148</device-id>\n\t    <phone-number>7073644115</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1227003406" type="phone">\n\t    <account-number>284</account-number>\n\t    <device-id>5649d503-08a0-440d-a632-db739313f761</device-id>\n\t    <phone-number>9166026450</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226962113" type="phone">\n\t    <account-number>223</account-number>\n\t    <device-id>9407e003-e05a-438a-bcb4-7c74054ab870</device-id>\n\t    <phone-number>6505293784</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226954331" type="phone">\n\t    <account-number>44</account-number>\n\t    <device-id>7c5c8c9b-db06-4bc9-8880-8b683a131c51</device-id>\n\t    <phone-number>5102680691</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226932538" type="phone">\n\t    <account-number>72</account-number>\n\t    <device-id>4baf9d0b-d32d-4887-bba9-93521564eb92</device-id>\n\t    <phone-number>6509765463</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226925399" type="phone">\n\t    <account-number>134</account-number>\n\t    <device-id>b33ae619-9c6c-4223-99de-059ff1722ef2</device-id>\n\t    <phone-number>5105155205</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226916311" type="phone">\n\t    <account-number>337</account-number>\n\t    <device-id>e0e4c76e-dea5-4dde-a5c1-395107750227</device-id>\n\t    <phone-number>4159428403</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226909219" type="phone">\n\t    <account-number>376</account-number>\n\t    <device-id>86442bf9-76b7-4bae-a060-b68260636cba</device-id>\n\t    <phone-number>9166411409</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226893551" type="phone">\n\t    <account-number>185</account-number>\n\t    <device-id>1964df5f-8936-4928-9cd5-fbd5eaf4136e</device-id>\n\t    <phone-number>7071779156</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226889691" type="phone">\n\t    <account-number>207</account-number>\n\t    <device-id>597a1c52-d120-49da-a03f-c9273edd1a4a</device-id>\n\t    <phone-number>5107830293</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226871468" type="phone">\n\t    <account-number>307</account-number>\n\t    <device-id>78822b46-a072-4889-a5ae-06c632572c5e</device-id>\n\t    <phone-number>7072884903</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226851411" type="phone">\n\t    <account-number>81</account-number>\n\t    <device-id>34bd60bc-2f8c-4073-ac6b-65cced23b6e4</device-id>\n\t    <phone-number>6503149681</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226848084" type="phone">\n\t    <account-number>313</account-number>\n\t    <device-id>93eb2d30-b700-4402-93d4-0e0ead5fd7cd</device-id>\n\t    <phone-number>5103285924</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226830822" type="phone">\n\t    <account-number>386</account-number>\n\t    <device-id>e4f0f7ae-86f6-41af-8842-ca931488c672</device-id>\n\t    <phone-number>7071257114</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226819166" type="phone">\n\t    <account-number>5</account-number>\n\t    <device-id>b72a7271-864e-46dc-b6bf-f5dd810f27ee</device-id>\n\t    <phone-number>5107754354</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226797238" type="phone">\n\t    <account-number>58552</account-number>\n\t    <device-id>9e94d18e-4b97-45a0-a367-63bd1aeb5643</device-id>\n\t    <phone-number>4152418283</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226796125" type="phone">\n\t    <account-number>112</account-number>\n\t    <device-id>bada37ea-8de7-4634-b705-d19b674b10dd</device-id>\n\t    <phone-number>4150293906</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226793051" type="phone">\n\t    <account-number>99</account-number>\n\t    <device-id>3aef4890-c93e-4972-a905-6cc3def1d19b</device-id>\n\t    <phone-number>5102145517</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226785810" type="phone">\n\t    <account-number>32</account-number>\n\t    <device-id>b3a6847c-c77d-4710-bfec-3eb3d991a15f</device-id>\n\t    <phone-number>5108387425</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226780830" type="phone">\n\t    <account-number>139</account-number>\n\t    <device-id>3e705bfa-2697-4690-af02-7ad9451c4f9c</device-id>\n\t    <phone-number>6508780391</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226780554" type="phone">\n\t    <account-number>48</account-number>\n\t    <device-id>81a60aeb-0d08-4e54-9704-52740fd8a387</device-id>\n\t    <phone-number>7078755344</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226778018" type="phone">\n\t    <account-number>2748</account-number>\n\t    <device-id>20b7c339-847d-443b-b149-2f70b724e810</device-id>\n\t    <phone-number>2095640777</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226777227" type="phone">\n\t    <account-number>307</account-number>\n\t    <device-id>516c6a73-7e08-47e1-a700-3ed97d9e9b5e</device-id>\n\t    <phone-number>7072884903</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226764693" type="phone">\n\t    <account-number>1297</account-number>\n\t    <device-id>cac5ab39-a0fe-4d9d-a58c-eb9761e86a80</device-id>\n\t    <phone-number>2092833894</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226763980" type="phone">\n\t    <account-number>26</account-number>\n\t    <device-id>5c63c053-1c35-4170-8287-c6b027e6bd88</device-id>\n\t    <phone-number>5101367449</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226762615" type="phone">\n\t    <account-number>359</account-number>\n\t    <device-id>b95942a1-90de-4786-b808-56c34ed0a28b</device-id>\n\t    <phone-number>4151951455</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226756930" type="phone">\n\t    <account-number>67</account-number>\n\t    <device-id>e3177e85-d0c9-401e-b553-2c4f06ab0967</device-id>\n\t    <phone-number>5101032076</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226749833" type="phone">\n\t    <account-number>366</account-number>\n\t    <device-id>741f9ea9-73bc-4bc5-8d44-d1701bb3962e</device-id>\n\t    <phone-number>5105723627</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226738474" type="phone">\n\t    <account-number>60954</account-number>\n\t    <device-id>74318a1a-0fcc-4dd7-860c-6d5f05bafe90</device-id>\n\t    <phone-number>9289510220</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226709473" type="phone">\n\t    <account-number>58</account-number>\n\t    <device-id>92fa28ab-9532-490c-8af0-7e2df769a3a9</device-id>\n\t    <phone-number>6500393228</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226701950" type="phone">\n\t    <account-number>22</account-number>\n\t    <device-id>99ac2986-d93c-4efc-913b-8b5be52aa1d5</device-id>\n\t    <phone-number>5102858788</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226691746" type="phone">\n\t    <account-number>198</account-number>\n\t    <device-id>f719ef62-dbd3-4e8c-8e1a-0b68e6ad8b63</device-id>\n\t    <phone-number>7078932972</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226686607" type="phone">\n\t    <account-number>399</account-number>\n\t    <device-id>676d7ae1-5e9d-441c-b638-514962388d76</device-id>\n\t    <phone-number>5104255852</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226683325" type="phone">\n\t    <account-number>15</account-number>\n\t    <device-id>76f2da6d-b1a3-45ef-9022-564c494e577b</device-id>\n\t    <phone-number>5108428188</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226683091" type="phone">\n\t    <account-number>20132</account-number>\n\t    <device-id>c6aa6562-2dfe-4b21-97ee-57530ea1ccfa</device-id>\n\t    <phone-number>7026728206</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226681383" type="phone">\n\t    <account-number>187</account-number>\n\t    <device-id>0a5b7989-f95d-4bb1-a041-d8c09e7377eb</device-id>\n\t    <phone-number>4152861127</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226673056" type="phone">\n\t    <account-number>77</account-number>\n\t    <device-id>0df68239-cbfb-4a4b-91ea-4fb016e8a90d</device-id>\n\t    <phone-number>4158623685</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226671661" type="phone">\n\t    <account-number>69</account-number>\n\t    <device-id>ae85e5a4-3907-4eba-8e87-d917dc10f75c</device-id>\n\t    <phone-number>6501143930</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226671206" type="phone">\n\t    <account-number>427</account-number>\n\t    <device-id>32256bae-b495-4bfc-b84f-4701d9f14b84</device-id>\n\t    <phone-number>5109657043</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226664086" type="phone">\n\t    <account-number>59690</account-number>\n\t    <device-id>00d22746-31d0-44f9-ab35-afe489ed0ebd</device-id>\n\t    <phone-number>9284251886</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226663330" type="phone">\n\t    <account-number>93</account-number>\n\t    <device-id>38eb1692-ae73-41da-a44b-a54bc7b5fcd7</device-id>\n\t    <phone-number>4151631986</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226659780" type="phone">\n\t    <account-number>365</account-number>\n\t    <device-id>9793666e-8ebf-44b3-8fb5-f091b69d248a</device-id>\n\t    <phone-number>4154589394</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226648453" type="phone">\n\t    <account-number>19</account-number>\n\t    <device-id>80481be6-a76b-4303-9ec1-0dd437d64af7</device-id>\n\t    <phone-number>7070013038</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226648345" type="phone">\n\t    <account-number>11036</account-number>\n\t    <device-id>dc05b2a4-ff2f-4341-9668-5e92cb443fd7</device-id>\n\t    <phone-number>5596126490</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226642674" type="phone">\n\t    <account-number>228</account-number>\n\t    <device-id>7965e62f-69f0-4ca2-8715-450439b8f894</device-id>\n\t    <phone-number>7079548081</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226617254" type="phone">\n\t    <account-number>163</account-number>\n\t    <device-id>29b77ba9-b33f-46e1-8c38-6c8ed3b2eb7f</device-id>\n\t    <phone-number>5101691548</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226614688" type="phone">\n\t    <account-number>115</account-number>\n\t    <device-id>e6da1951-d219-4d13-9fe0-15f52cd809da</device-id>\n\t    <phone-number>5104207058</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226580160" type="phone">\n\t    <account-number>232</account-number>\n\t    <device-id>92211cce-98c9-4431-b2b7-784d9ac8c595</device-id>\n\t    <phone-number>7070880122</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226569940" type="phone">\n\t    <account-number>172</account-number>\n\t    <device-id>d4f4fb01-0a3e-4204-a3c9-2ba6db057aab</device-id>\n\t    <phone-number>6501120813</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226567178" type="phone">\n\t    <account-number>227</account-number>\n\t    <device-id>ca9f3028-29c8-45d3-a772-94f6fc85e428</device-id>\n\t    <phone-number>6505304437</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226566444" type="phone">\n\t    <account-number>416</account-number>\n\t    <device-id>e4477dd5-6a56-4212-bebc-3b777902f236</device-id>\n\t    <phone-number>9161891477</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226563714" type="phone">\n\t    <account-number>24</account-number>\n\t    <device-id>1d63f147-8b34-4d15-ab2f-85ee959d865c</device-id>\n\t    <phone-number>9164156733</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226551791" type="phone">\n\t    <account-number>120</account-number>\n\t    <device-id>08e673f7-a50f-4f31-8014-b8acc2704208</device-id>\n\t    <phone-number>4159244097</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226551381" type="phone">\n\t    <account-number>98868</account-number>\n\t    <device-id>7cf000db-5b25-4cc8-90d8-ccacd9893be5</device-id>\n\t    <phone-number>5590745938</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226549910" type="phone">\n\t    <account-number>203</account-number>\n\t    <device-id>55947b65-eef9-4f96-ac39-e01e6b39bc22</device-id>\n\t    <phone-number>9161423776</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226547737" type="phone">\n\t    <account-number>117312</account-number>\n\t    <device-id>4740a190-b0bb-4b8d-9d65-df2947e97f0c</device-id>\n\t    <phone-number>9094875766</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226544444" type="phone">\n\t    <account-number>73802</account-number>\n\t    <device-id>7077ffa4-df09-4116-8726-dab9028f3837</device-id>\n\t    <phone-number>5622397068</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226510981" type="phone">\n\t    <account-number>74</account-number>\n\t    <device-id>a47b68ca-f04a-4bbc-9c23-f310ba79507a</device-id>\n\t    <phone-number>7070315981</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226499154" type="phone">\n\t    <account-number>230</account-number>\n\t    <device-id>c21ec1cd-3410-457b-9802-4460b93018c5</device-id>\n\t    <phone-number>5109331191</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226495786" type="phone">\n\t    <account-number>209</account-number>\n\t    <device-id>8e112291-98ca-42b8-9f4b-bec204b92e62</device-id>\n\t    <phone-number>6503019051</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226487601" type="phone">\n\t    <account-number>2</account-number>\n\t    <device-id>658e3fac-8e85-40e0-9d08-ee3ee537b802</device-id>\n\t    <phone-number>4150835799</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226483895" type="phone">\n\t    <account-number>248</account-number>\n\t    <device-id>5d88d215-7202-4e32-8ea5-3bdead672eb9</device-id>\n\t    <phone-number>6504229844</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226461022" type="phone">\n\t    <account-number>385</account-number>\n\t    <device-id>3b6aba06-c94a-41b1-bdb7-7942509e7b34</device-id>\n\t    <phone-number>4159688943</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226457773" type="phone">\n\t    <account-number>55</account-number>\n\t    <device-id>abec39c0-13c3-4cf2-961d-86aad783d013</device-id>\n\t    <phone-number>4157329877</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226455820" type="phone">\n\t    <account-number>35033</account-number>\n\t    <device-id>43018e5c-eae5-4da8-9811-6f5f42b414cf</device-id>\n\t    <phone-number>5417412680</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226450138" type="phone">\n\t    <account-number>603</account-number>\n\t    <device-id>93075018-1a5e-48fc-89bb-bdd6ee776dfb</device-id>\n\t    <phone-number>9160477234</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226448622" type="phone">\n\t    <account-number>23</account-number>\n\t    <device-id>72f2975b-7c59-44a5-b6a2-a9a1a66a2511</device-id>\n\t    <phone-number>7075012743</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226442291" type="phone">\n\t    <account-number>321</account-number>\n\t    <device-id>8f51e7f1-a33a-4c3b-84c4-668a78338329</device-id>\n\t    <phone-number>7071292900</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226441466" type="phone">\n\t    <account-number>45</account-number>\n\t    <device-id>df31a000-8e1d-4764-8c82-da1be2add014</device-id>\n\t    <phone-number>5104985494</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226438801" type="phone">\n\t    <account-number>423</account-number>\n\t    <device-id>f051d5dd-2810-4015-9cc2-0f9fe652961d</device-id>\n\t    <phone-number>4152056786</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226437308" type="phone">\n\t    <account-number>133</account-number>\n\t    <device-id>1b465200-0f33-48af-b0ea-a4eb48822e73</device-id>\n\t    <phone-number>9165399923</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226435965" type="phone">\n\t    <account-number>91361</account-number>\n\t    <device-id>1b7ccd65-60bb-4375-8350-ad9d0c748015</device-id>\n\t    <phone-number>5410342188</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226400678" type="phone">\n\t    <account-number>77074</account-number>\n\t    <device-id>64656abd-09e7-419b-815c-89cfd3b84b3d</device-id>\n\t    <phone-number>7751032857</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226395121" type="phone">\n\t    <account-number>34236</account-number>\n\t    <device-id>c1d5c382-c5df-4811-9ac6-5caffd3413e8</device-id>\n\t    <phone-number>9288617245</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226393810" type="phone">\n\t    <account-number>40</account-number>\n\t    <device-id>ecee34cc-c256-47df-9b49-e3641ea6abfb</device-id>\n\t    <phone-number>6503025128</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226386793" type="phone">\n\t    <account-number>104667</account-number>\n\t    <device-id>50ab923c-64be-4bd7-9745-74337e61cc04</device-id>\n\t    <phone-number>9282848787</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226356818" type="phone">\n\t    <account-number>277</account-number>\n\t    <device-id>5dc403c5-5f4c-45e8-8a4d-754f6b320298</device-id>\n\t    <phone-number>6501404020</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226353107" type="phone">\n\t    <account-number>425</account-number>\n\t    <device-id>553f5785-bdb0-45d6-b78e-81ef7fe68ef2</device-id>\n\t    <phone-number>5109357429</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226350144" type="phone">\n\t    <account-number>3923</account-number>\n\t    <device-id>e71b8017-a85c-4545-ae34-77a38e2340c6</device-id>\n\t    <phone-number>8318546247</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226348570" type="phone">\n\t    <account-number>418</account-number>\n\t    <device-id>fc2b89d2-fe75-474d-824c-59eb4e2b4a53</device-id>\n\t    <phone-number>5109285552</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226336866" type="phone">\n\t    <account-number>42</account-number>\n\t    <device-id>c1df873f-6cd1-4612-9a9b-f543d644764c</device-id>\n\t    <phone-number>7076948029</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226304654" type="phone">\n\t    <account-number>54</account-number>\n\t    <device-id>dc21cc20-1ea5-4033-b835-3b9bc5fd8ff0</device-id>\n\t    <phone-number>5109192624</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226299375" type="phone">\n\t    <account-number>164</account-number>\n\t    <device-id>f01d0ecc-4457-4555-adf4-737f649d311c</device-id>\n\t    <phone-number>4152651216</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226288136" type="phone">\n\t    <account-number>347</account-number>\n\t    <device-id>ed025276-9620-488f-a028-f7e8d525ecd9</device-id>\n\t    <phone-number>6505897633</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226285409" type="phone">\n\t    <account-number>298</account-number>\n\t    <device-id>a6d9f8ad-342b-410c-9e2d-5f9052256968</device-id>\n\t    <phone-number>6501273168</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226274283" type="phone">\n\t    <account-number>437</account-number>\n\t    <device-id>61ae630f-dce4-415b-bb93-8f2c572802ff</device-id>\n\t    <phone-number>4153295574</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226264371" type="phone">\n\t    <account-number>470</account-number>\n\t    <device-id>1a159975-8854-4d76-8f39-2635f634d568</device-id>\n\t    <phone-number>5106250427</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226249916" type="phone">\n\t    <account-number>441</account-number>\n\t    <device-id>fef4da54-6c53-4956-b9b6-4caf37593381</device-id>\n\t    <phone-number>6509248815</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226249894" type="phone">\n\t    <account-number>357</account-number>\n\t    <device-id>358d3835-21d1-4628-a824-d45d91c30870</device-id>\n\t    <phone-number>4156327428</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226227609" type="phone">\n\t    <account-number>84</account-number>\n\t    <device-id>6bf36b20-47bf-45e3-bbf3-9ce12825a976</device-id>\n\t    <phone-number>7073642335</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226217478" type="phone">\n\t    <account-number>415</account-number>\n\t    <device-id>df6a17f1-f0c7-4c39-8448-7649bb0e25b2</device-id>\n\t    <phone-number>5105797948</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226199975" type="phone">\n\t    <account-number>79755</account-number>\n\t    <device-id>072d5bea-a024-43d4-99b7-a29ce35c4b7e</device-id>\n\t    <phone-number>5108703226</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226192621" type="phone">\n\t    <account-number>996</account-number>\n\t    <device-id>509fcdc1-449a-4491-b7e0-3ad0834dfa3f</device-id>\n\t    <phone-number>9165890769</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226176865" type="phone">\n\t    <account-number>444</account-number>\n\t    <device-id>390bb050-352a-4435-8bb6-f92f6381ec90</device-id>\n\t    <phone-number>6508511040</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226175515" type="phone">\n\t    <account-number>222</account-number>\n\t    <device-id>dc543ab8-a8dc-4bb7-a82a-837854dd8856</device-id>\n\t    <phone-number>4150290275</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226171726" type="phone">\n\t    <account-number>683</account-number>\n\t    <device-id>63f873d5-64cc-4c30-8ce7-555acfd8b7d2</device-id>\n\t    <phone-number>9167064619</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226167360" type="phone">\n\t    <account-number>166</account-number>\n\t    <device-id>dffafa6b-dd92-453a-9a0c-d73b8e331ad6</device-id>\n\t    <phone-number>5101598812</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226159489" type="phone">\n\t    <account-number>295</account-number>\n\t    <device-id>dbee1bb5-fe22-4db9-978f-726ef86867dc</device-id>\n\t    <phone-number>4151012695</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226156087" type="phone">\n\t    <account-number>221</account-number>\n\t    <device-id>c9bb3215-f336-443a-8c7e-48f720f80b8d</device-id>\n\t    <phone-number>5106109641</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226149518" type="phone">\n\t    <account-number>121</account-number>\n\t    <device-id>4a5f211d-4d97-40ed-9f19-7b6055fd0ec0</device-id>\n\t    <phone-number>5102940162</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226121535" type="phone">\n\t    <account-number>217</account-number>\n\t    <device-id>1a13e2eb-67ff-475d-8bc3-c5171b9131de</device-id>\n\t    <phone-number>6507384769</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226119719" type="phone">\n\t    <account-number>50586</account-number>\n\t    <device-id>b030537f-1225-4fbf-a3fe-81596cd737f0</device-id>\n\t    <phone-number>9284139222</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226114365" type="phone">\n\t    <account-number>20645</account-number>\n\t    <device-id>c23f07d5-6f5a-4409-832d-df84941fbba2</device-id>\n\t    <phone-number>8183912100</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226109535" type="phone">\n\t    <account-number>9</account-number>\n\t    <device-id>45fe71c6-c56f-4355-9e2b-94f3c215da50</device-id>\n\t    <phone-number>6502384894</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226095062" type="phone">\n\t    <account-number>477</account-number>\n\t    <device-id>4fb6f5ab-9d13-4bb7-8d68-bbfc78fb2441</device-id>\n\t    <phone-number>7074792970</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226079702" type="phone">\n\t    <account-number>336</account-number>\n\t    <device-id>04203efe-dc38-4ec5-bd55-25b938383257</device-id>\n\t    <phone-number>9168017797</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226071899" type="phone">\n\t    <account-number>200</account-number>\n\t    <device-id>abd4c8f2-0826-4086-b8ef-3d3091cd81bb</device-id>\n\t    <phone-number>6504123769</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226070347" type="phone">\n\t    <account-number>294</account-number>\n\t    <device-id>8d15e50b-fa8c-4e8b-b64f-aa0623f46774</device-id>\n\t    <phone-number>6502001026</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226068607" type="phone">\n\t    <account-number>1055</account-number>\n\t    <device-id>f52612d8-8f20-4e08-9b25-36fd0a127128</device-id>\n\t    <phone-number>5108647878</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226054759" type="phone">\n\t    <account-number>64</account-number>\n\t    <device-id>27c42d82-e80f-48f0-9128-d6daecb51e62</device-id>\n\t    <phone-number>5102689749</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226053047" type="phone">\n\t    <account-number>315</account-number>\n\t    <device-id>0b3bd672-0359-4579-83e3-1e5da3d0c1cf</device-id>\n\t    <phone-number>9167212066</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226046715" type="phone">\n\t    <account-number>259</account-number>\n\t    <device-id>d12103ad-1357-4f46-aa6c-abf97b5650e4</device-id>\n\t    <phone-number>5100845932</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226039107" type="phone">\n\t    <account-number>122</account-number>\n\t    <device-id>0349b72c-9db9-4075-8232-0203ea2fa787</device-id>\n\t    <phone-number>5109441371</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226038286" type="phone">\n\t    <account-number>387</account-number>\n\t    <device-id>85b17392-3d8e-406e-bb8b-52114d25d730</device-id>\n\t    <phone-number>7073839638</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226023227" type="phone">\n\t    <account-number>211</account-number>\n\t    <device-id>a719fd7a-815d-4ebc-812a-38e1b070e02e</device-id>\n\t    <phone-number>7079726497</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226010573" type="phone">\n\t    <account-number>107180</account-number>\n\t    <device-id>9cf9e6a2-7413-4ec0-a0ef-471d325bf1c4</device-id>\n\t    <phone-number>4080170748</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1226002218" type="phone">\n\t    <account-number>50876</account-number>\n\t    <device-id>3bcab81b-48d3-4099-be1c-ab4bcfd24959</device-id>\n\t    <phone-number>6265924615</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225995937" type="phone">\n\t    <account-number>54084</account-number>\n\t    <device-id>98e323b0-baf8-403a-ba17-21169f90cf1e</device-id>\n\t    <phone-number>8187200809</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225994818" type="phone">\n\t    <account-number>368</account-number>\n\t    <device-id>b394c845-065b-4a91-a261-d8de81d653aa</device-id>\n\t    <phone-number>5108104669</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225993483" type="phone">\n\t    <account-number>231</account-number>\n\t    <device-id>c759fab6-38c5-416f-9293-62c03ce6bc86</device-id>\n\t    <phone-number>4150983324</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225984043" type="phone">\n\t    <account-number>16907</account-number>\n\t    <device-id>f3f64cb0-db9b-49ae-9037-31867ae81bae</device-id>\n\t    <phone-number>5624167437</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225979086" type="phone">\n\t    <account-number>97</account-number>\n\t    <device-id>a899a82b-3e5e-4b5a-b1c9-c97e672832fa</device-id>\n\t    <phone-number>5102439375</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225963218" type="phone">\n\t    <account-number>243</account-number>\n\t    <device-id>15782d47-26d5-4cdd-98ad-149943eedcba</device-id>\n\t    <phone-number>6505061680</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225930843" type="phone">\n\t    <account-number>417</account-number>\n\t    <device-id>2bace1d1-7c02-4532-8ad3-6b88b56ee9ec</device-id>\n\t    <phone-number>4156254158</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225924381" type="phone">\n\t    <account-number>251</account-number>\n\t    <device-id>c6cc51fb-2142-4d61-a490-1de24eee0ee9</device-id>\n\t    <phone-number>6505597078</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225917470" type="phone">\n\t    <account-number>102</account-number>\n\t    <device-id>d69a813f-8d3f-45e0-b0a7-085c780f8fac</device-id>\n\t    <phone-number>5102393293</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225912561" type="phone">\n\t    <account-number>18</account-number>\n\t    <device-id>e7487a33-911c-40f9-a43c-86277c641879</device-id>\n\t    <phone-number>6506033983</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225909491" type="phone">\n\t    <account-number>254</account-number>\n\t    <device-id>a95049c9-a338-402b-93c2-4c8c56f7ee7c</device-id>\n\t    <phone-number>4156978598</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225903244" type="phone">\n\t    <account-number>118329</account-number>\n\t    <device-id>33b16389-d814-4e18-8f72-1f3884bd5f64</device-id>\n\t    <phone-number>5414036682</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225875775" type="phone">\n\t    <account-number>47</account-number>\n\t    <device-id>f0cd06c6-af21-4344-89c7-aa802e2a8011</device-id>\n\t    <phone-number>5106329832</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225861542" type="phone">\n\t    <account-number>206</account-number>\n\t    <device-id>494dadd7-ccff-49e5-8653-16ccbaae3ebf</device-id>\n\t    <phone-number>5107385381</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225860989" type="phone">\n\t    <account-number>162</account-number>\n\t    <device-id>9b9671bc-9af0-44cc-99d4-cd38fd35c609</device-id>\n\t    <phone-number>5107380008</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225851138" type="phone">\n\t    <account-number>273</account-number>\n\t    <device-id>97c8d28e-ef21-4487-9587-fe739cf6e101</device-id>\n\t    <phone-number>5104066441</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225820175" type="phone">\n\t    <account-number>352</account-number>\n\t    <device-id>cc4cdc54-c135-415d-b085-763f21b35730</device-id>\n\t    <phone-number>4152732908</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225819365" type="phone">\n\t    <account-number>267</account-number>\n\t    <device-id>c7fa6e19-3f7f-4f4b-abf2-02683b2e82f9</device-id>\n\t    <phone-number>9169252484</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225817792" type="phone">\n\t    <account-number>274</account-number>\n\t    <device-id>7de27b2f-18bd-410f-8f0b-194e2a98bdbe</device-id>\n\t    <phone-number>5102866680</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225811266" type="phone">\n\t    <account-number>430</account-number>\n\t    <device-id>8cffad95-3fd0-4c02-8be2-346bcd27f631</device-id>\n\t    <phone-number>5102654195</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225803113" type="phone">\n\t    <account-number>816</account-number>\n\t    <device-id>c21c33ac-1191-4537-a78f-3438d2416949</device-id>\n\t    <phone-number>9160228886</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225780486" type="phone">\n\t    <account-number>466</account-number>\n\t    <device-id>5c0f57cc-78fe-47a4-ae2f-712508bdb416</device-id>\n\t    <phone-number>5100225594</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225777069" type="phone">\n\t    <account-number>224</account-number>\n\t    <device-id>95a5492c-7b56-4256-b95d-c82f75867217</device-id>\n\t    <phone-number>5108762113</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225772409" type="phone">\n\t    <account-number>421</account-number>\n\t    <device-id>82ca09a5-6c66-4639-ad78-419355589d9f</device-id>\n\t    <phone-number>7072912797</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225770080" type="phone">\n\t    <account-number>21337</account-number>\n\t    <device-id>8c8a1155-dcfb-4253-846f-a369d69a4c11</device-id>\n\t    <phone-number>8183556078</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225766882" type="phone">\n\t    <account-number>329</account-number>\n\t    <device-id>89c491b1-1dbf-42eb-a9b7-51b378b8af72</device-id>\n\t    <phone-number>6505755572</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225762527" type="phone">\n\t    <account-number>478</account-number>\n\t    <device-id>fe031e8e-f998-4624-9c74-5d27ac53d3ab</device-id>\n\t    <phone-number>7079435879</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225753656" type="phone">\n\t    <account-number>432</account-number>\n\t    <device-id>5fa0eebd-8648-482c-839a-7cd9ebe95b71</device-id>\n\t    <phone-number>5107285542</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225751275" type="phone">\n\t    <account-number>170</account-number>\n\t    <device-id>783af6a7-6475-4c74-b327-4672247a3549</device-id>\n\t    <phone-number>9167863246</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225746212" type="phone">\n\t    <account-number>391</account-number>\n\t    <device-id>402f16a7-b65c-45c7-afe2-e573bcaead8c</device-id>\n\t    <phone-number>6501569040</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225742879" type="phone">\n\t    <account-number>109</account-number>\n\t    <device-id>c6e1d63e-2518-48f7-b28f-a7413729c98f</device-id>\n\t    <phone-number>7079737340</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225732684" type="phone">\n\t    <account-number>287</account-number>\n\t    <device-id>ec1bbfdb-11a5-408b-b747-202c99211f0f</device-id>\n\t    <phone-number>9169106818</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225731179" type="phone">\n\t    <account-number>86</account-number>\n\t    <device-id>dadd3e5a-d3ba-46bc-b30f-5ab78b37c4d9</device-id>\n\t    <phone-number>5106698545</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225730773" type="phone">\n\t    <account-number>140</account-number>\n\t    <device-id>ffd6d3de-ec18-410a-8d62-8aaea087cc49</device-id>\n\t    <phone-number>4154577235</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225729029" type="phone">\n\t    <account-number>446</account-number>\n\t    <device-id>f5bcdf73-64fb-43f9-8c17-d06be8bc6365</device-id>\n\t    <phone-number>5102926102</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225712341" type="phone">\n\t    <account-number>12803</account-number>\n\t    <device-id>6e079507-a217-42aa-b85f-5035e210b786</device-id>\n\t    <phone-number>4245093328</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225696578" type="phone">\n\t    <account-number>443</account-number>\n\t    <device-id>3fe47112-a956-46d0-88a8-600ac7ab949f</device-id>\n\t    <phone-number>5105377753</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225683016" type="phone">\n\t    <account-number>456</account-number>\n\t    <device-id>78853763-cdd7-4e5f-9227-4544cb865711</device-id>\n\t    <phone-number>5109807377</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225677680" type="phone">\n\t    <account-number>381</account-number>\n\t    <device-id>7c70dc79-d33c-4a78-b440-c05b5b011037</device-id>\n\t    <phone-number>4153365303</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225676641" type="phone">\n\t    <account-number>98</account-number>\n\t    <device-id>57f5315e-2975-4c3e-93ba-ff04a1f2e8ac</device-id>\n\t    <phone-number>5103197687</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225675890" type="phone">\n\t    <account-number>362</account-number>\n\t    <device-id>5979526a-6f45-4339-ab3f-ea494d36556f</device-id>\n\t    <phone-number>4159253838</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225672491" type="phone">\n\t    <account-number>246</account-number>\n\t    <device-id>cc3afbc4-fde7-465d-99a9-b1390dbef73c</device-id>\n\t    <phone-number>5109530656</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225670934" type="phone">\n\t    <account-number>29609</account-number>\n\t    <device-id>5d274380-1baf-49a1-b79f-6beea7a26e8f</device-id>\n\t    <phone-number>5106050290</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225669984" type="phone">\n\t    <account-number>68</account-number>\n\t    <device-id>94e3daa4-0331-489e-b182-99785acb0c50</device-id>\n\t    <phone-number>5102984254</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225666507" type="phone">\n\t    <account-number>218</account-number>\n\t    <device-id>77d7f474-7075-472b-a71b-55e2e42a6956</device-id>\n\t    <phone-number>7076585071</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225647825" type="phone">\n\t    <account-number>173</account-number>\n\t    <device-id>ce6ac725-3868-417d-9fe1-645e905c8fb8</device-id>\n\t    <phone-number>5109427954</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225641325" type="phone">\n\t    <account-number>20</account-number>\n\t    <device-id>f88e538f-e4b9-4395-8f06-219f187ec34e</device-id>\n\t    <phone-number>7075831640</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225633957" type="phone">\n\t    <account-number>127</account-number>\n\t    <device-id>ff993835-f44b-48c9-9f06-6ecedf2962cc</device-id>\n\t    <phone-number>7077814054</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225627855" type="phone">\n\t    <account-number>358</account-number>\n\t    <device-id>34e559f9-953a-4efb-bfde-bb9cb6a3d908</device-id>\n\t    <phone-number>9168169852</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225626120" type="phone">\n\t    <account-number>111</account-number>\n\t    <device-id>96da7e9f-50a7-4518-82d1-5a7ddf2e07f4</device-id>\n\t    <phone-number>5108252790</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225610504" type="phone">\n\t    <account-number>190</account-number>\n\t    <device-id>44ee9903-241d-4363-b267-610d5db0b3aa</device-id>\n\t    <phone-number>5105126542</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225597092" type="phone">\n\t    <account-number>453</account-number>\n\t    <device-id>9280d946-b2de-4e9f-8657-dbb234b1e2a4</device-id>\n\t    <phone-number>7073559747</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225578485" type="phone">\n\t    <account-number>160</account-number>\n\t    <device-id>1aad9107-83a5-4047-8f7f-6ab416a32bee</device-id>\n\t    <phone-number>4158441466</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225570113" type="phone">\n\t    <account-number>367</account-number>\n\t    <device-id>c2361109-6f4f-4e96-9c2f-53496ca34f59</device-id>\n\t    <phone-number>9168238433</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225568764" type="phone">\n\t    <account-number>169</account-number>\n\t    <device-id>838f6857-f3c6-4d38-95e4-3557f6f2128a</device-id>\n\t    <phone-number>7075173347</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225564434" type="phone">\n\t    <account-number>150</account-number>\n\t    <device-id>7b15d752-5e69-4292-b818-c7c2f196c78b</device-id>\n\t    <phone-number>5102799018</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225562113" type="phone">\n\t    <account-number>34</account-number>\n\t    <device-id>c719a38a-2292-47bb-aa5e-711f678e616f</device-id>\n\t    <phone-number>5109458176</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225557123" type="phone">\n\t    <account-number>308</account-number>\n\t    <device-id>6986b267-0c34-44af-9f24-646d46fa3bf4</device-id>\n\t    <phone-number>7075382511</phone-number>\n\t    <model>Sorrento F00L</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225546475" type="phone">\n\t    <account-number>344</account-number>\n\t    <device-id>e17e1abd-2998-4f8e-9950-7ce4ac1f412a</device-id>\n\t    <phone-number>4157994884</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225532588" type="phone">\n\t    <account-number>328</account-number>\n\t    <device-id>d5910942-e9c0-46df-b3f2-93b12c068347</device-id>\n\t    <phone-number>5104718508</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225521652" type="phone">\n\t    <account-number>341</account-number>\n\t    <device-id>bd49bdda-135b-4f93-af88-f07d39eed6ef</device-id>\n\t    <phone-number>5107739396</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225515059" type="phone">\n\t    <account-number>192</account-number>\n\t    <device-id>14278cd8-6239-45b1-a59a-b5d710af2bd0</device-id>\n\t    <phone-number>9168053145</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225504008" type="phone">\n\t    <account-number>66</account-number>\n\t    <device-id>ed051262-392a-49ab-a627-3845d7d742d3</device-id>\n\t    <phone-number>5101166999</phone-number>\n\t    <model>Titanic 1000</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225492440" type="phone">\n\t    <account-number>153</account-number>\n\t    <device-id>4fa2d027-ae7d-4d8b-9ea3-42fe6214a9c8</device-id>\n\t    <phone-number>5100978341</phone-number>\n\t    <model>MeeToo 1.0</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225488157" type="phone">\n\t    <account-number>25406</account-number>\n\t    <device-id>d0983035-7470-43a1-a94f-6e1ec8af6790</device-id>\n\t    <phone-number>9096011974</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t\t  <activation timestamp="1225487136" type="phone">\n\t    <account-number>348</account-number>\n\t    <device-id>288b0949-e3cf-4ac2-8aad-2c4ec811ad07</device-id>\n\t    <phone-number>7079223784</phone-number>\n\t    <model>iFruit 1</model>\n\t  </activation>\n\t  \t</activations>')
>>>


activationFiles
org.apache.spark.api.java.JavaPairRDD@7ef30252


scala> val files="/devsh_loudacre/activations"
files: String = /devsh_loudacre/activations

scala> val activationFiles = sc.wholeTextFiles(files)
activationFiles: org.apache.spark.rdd.RDD[(String, String)] = /devsh_loudacre/activations MapPartitionsRDD[13] at wholeTextFiles at <console>:29

scala> for (line <- activationFiles.take(2)) print (line)
(hdfs://cca175-m/devsh_loudacre/activations/2008-10.xml,<activations>
          <activation timestamp="1225462088" type="phone">
            <account-number>9763</account-number>
            <device-id>debea35e-3ecd-4ee7-b0dd-ad428d953f32</device-id>
            <phone-number>7600763387</phone-number>
            <model>MeeToo 1.0</model>
          </activation>
                          <activation timestamp="1225461447" type="phone">
            <account-number>426</account-number>
            <device-id>38a1566d-524e-4137-bad8-b597d09b54b0</device-id>
            <phone-number>5102521038</phone-number>
            <model>Titanic 1000</model>
          </activation>
...
...

28. Each XML file can contain many activation records; use flatMap to map the contents of each file to a collection of XML records by calling the provided getActivations function. 
getActivations takes an XML string, parses it, and returns a collection of XML records; flatMap maps each record to a separate RDD
element.

>>> activationRecords = activationFiles.flatMap(lambda pair: getActivations(pair[1]))
>>> activationRecords
PythonRDD[9] at RDD at PythonRDD.scala:53
>>> for line in activationRecords.take(2): print (line)
... 
<Element 'activation' at 0x7f6fa30976b0>
<Element 'activation' at 0x7f6fa3097bf0>

scala> val activationTrees = activationFiles.flatMap(pair => getActivations(pair._2))
activationTrees: org.apache.spark.rdd.RDD[scala.xml.Node] = MapPartitionsRDD[14] at flatMap at <console>:30

scala> for (line <- activationTrees.take(2)) print (line)
<activation type="phone" timestamp="1225462088">
            <account-number>9763</account-number>
            <device-id>debea35e-3ecd-4ee7-b0dd-ad428d953f32</device-id>
            <phone-number>7600763387</phone-number>
            <model>MeeToo 1.0</model>
          </activation><activation type="phone" timestamp="1225461447">
            <account-number>426</account-number>
            <device-id>38a1566d-524e-4137-bad8-b597d09b54b0</device-id>
            <phone-number>5102521038</phone-number>
            <model>Titanic 1000</model>
          </activation>

29. Map each activation record to a string in the format account-number:model.
Use the provided getAccount and getModel functions to find the values from the activation record.

>>> models = activationRecords.map(lambda record: getAccount(record) + ":" + getModel(record))
>>> models
PythonRDD[11] at RDD at PythonRDD.scala:53
>>> for line in models.take(2): print (line)
... 
9763:MeeToo 1.0
426:Titanic 1000

scala> val models = activationTrees.map(record => getAccount(record) + ":" + getModel(record))
models: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[15] at map at <console>:32


30. Save the formatted strings to a text file in the directory /devsh_loudacre/account-models.

>>> models.saveAsTextFile("/devsh_loudacre/account-models")

scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/account-models
Found 3 items
-rw-r--r--   2 scelisdev02 hadoop          0 2021-09-29 12:37 /devsh_loudacre/account-models/_SUCCESS
-rw-r--r--   2 scelisdev02 hadoop    1858986 2021-09-29 12:37 /devsh_loudacre/account-models/part-00000
-rw-r--r--   2 scelisdev02 hadoop    1885226 2021-09-29 12:37 /devsh_loudacre/account-models/part-00001
scelisdev02@cca175-m:~$ hdfs dfs -cat /devsh_loudacre/account-models/part-00000 |more
9763:MeeToo 1.0
426:Titanic 1000
383:Sorrento F00L
...
...

scala> models.saveAsTextFile("/devsh_loudacre/account-models")

------------------
Hands-On Exercise: Joining Data Using Pair RDDs
------------------

solucion del manual: /home/scelisdev02/training_materials/devsh/exercises/pair-rdds
-----------------------------------------------
GCP - Viernes 25/09/2021 Free trial status: €144.18 
GCP - Lunes 27/09/2021 Free trial status: €126.73 credit and 77 days remaining -
GCP - Lunes 27/09/2021 Free trial status: €126.73 credit and 76 days remaining -
GCP - Martes 28/09/2021 Free trial status: €111.05 credit and 76 days remaining - 
GCP - Martes 28/09/2021 Free trial status: €107.75 credit and 76 days remaining - 
GCP - Miercoles 29/09/2021 Free trial status: €102.07 credit and 75 days remaining - 
GCP - Miercoles 29/09/2021 Upgrade your account to avoid a break in service (€97.67 credit and 75 days left in your trial). 
GCP - Jueves 30/09/2021 Upgrade your account to avoid a break in service (€88.76 credit and 74 days left in your trial). 
GCP - Viernes 01/10/2021 Upgrade your account to avoid a break in service (€84.04 credit and 73 days left in your trial).
GCP - Viernes 01/10/2021 Upgrade your account to avoid a break in service (€84.04 credit and 73 days left in your trial). 

Instance "cca175-m" is underutilized. 
You can save an estimated $32 per month by switching to the machine type: custom (4 vCPUs, 15.75 GB memory)

Explore Web Log Files
In this section, you will create a pair RDD based on data in the weblogs data files, and use that RDD to explore the data.
Tip: In this exercise, you will be reducing and joining large datasets, which can take a lot of time and may result in memory errors resulting from the limited resources available in the course exercise environment. 
To avoid this problem, perform these exercises with a subset of the web log files by using a wildcard: 
textFile("/ devsh_loudacre/weblogs/*2.log") includes only filenames ending with 2.log.

scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -put weblogs /devsh_loudacre/weblogs/
scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -cat weblogs /devsh_loudacre/weblogs/2014-03-15.log |more
cat: `weblogs': No such file or directory
234.206.18.239 - 8495 [15/Mar/2014:23:59:30 +0100] "GET /KBDOC-00082.html HTTP/1.0" 200 9054 "http://www.loudacre.com"  "Loudacre Mobile Brows
er Titanic 2200"
234.206.18.239 - 8495 [15/Mar/2014:23:59:30 +0100] "GET /theme.css HTTP/1.0" 200 4552 "http://www.loudacre.com"  "Loudacre Mobile Browser Tita
nic 2200"
104.213.2.248 - 22676 [15/Mar/2014:23:58:52 +0100] "GET /KBDOC-00086.html HTTP/1.0" 200 11413 "http://www.loudacre.com"  "Loudacre Mobile Brow
ser Ronin Novelty Note 1"
104.213.2.248 - 22676 [15/Mar/2014:23:58:52 +0100] "GET /theme.css HTTP/1.0" 200 14482 "http://www.loudacre.com"  "Loudacre Mobile Browser Ron
in Novelty Note 1"

1. Using map-reduce logic, count the number of requests from each user.
a. Use map to create a pair RDD with the user ID as the key and the integer 1 as the value. 
(The user ID is the third field in each line.) Your data will look something like this:
(userid,1)
(userid,1)
(userid,1)

Paso 0. Crear el RDD con los ficheros
>>> weblogfiles=("/devsh_loudacre/weblogs/*2.log")
>>> type(weblogfiles)
<class 'str'>

>>> weblogsRDD = sc.textFile(weblogfiles)
>>> type(weblogsRDD)
<class 'pyspark.rdd.RDD'>

>>> for line in weblogsRDD.take(2): print(line)
... 
131.166.169.114 - 67858 [23/Sep/2013:00:00:00 +0100] "GET /ifruit_3a_sales.html HTTP/1.0" 200 9509 "http://www.loudacre.com"  "Loudacre Mobile Browser Sorrento F01L"
131.166.169.114 - 67858 [23/Sep/2013:00:00:00 +0100] "GET /theme.css HTTP/1.0" 200 13428 "http://www.loudacre.com"  "Loudacre Mobile Browser Sorrento F01L"

scala> val weblogfiles=("/devsh_loudacre/weblogs/*2.log")
weblogfiles: String = /devsh_loudacre/weblogs/*2.log

scala> val weblogsRDD = sc.textFile(weblogfiles)
weblogsRDD: org.apache.spark.rdd.RDD[String] = /devsh_loudacre/weblogs/*2.log MapPartitionsRDD[1] at textFile at <console>:26

scala> for (line <- weblogsRDD.take(2)) print(line)
200.31.229.239 - 26069 [02/Oct/2013:23:59:48 +0100] "GET /titanic_1100_sales.html HTTP/1.0" 200 17619 "http://www.loudacre.com"  "Loudacre Mobile Browser Titanic 2400"200.31.229.239 - 26069 [02/Oct/2013:23:59:48 +0100] "GET /theme.css HTTP/1.0" 200 12437 "http://www.loudacre.com"  "Loudacre Mobile Browser Titanic 2400"
scala> 

Paso 1. Para extraer la key que en nuestro caso es el usuario - Crear la lista
>>> weblogs1RDD = weblogsRDD.map(lambda line: line.split(' '))
>>> type(weblogs1RDD)
<class 'pyspark.rdd.PipelinedRDD'>

>>> for line in weblogs1RDD.take(2): print(line)
... 
['131.166.169.114', '-', '67858', '[23/Sep/2013:00:00:00', '+0100]', '"GET', '/ifruit_3a_sales.html', 'HTTP/1.0"', '200', '9509', '"http://www.loudacre.com"', '', '"Loudacre', 'Mobile', 'Browser', 'Sorrento', 'F01L"']
['131.166.169.114', '-', '67858', '[23/Sep/2013:00:00:00', '+0100]', '"GET', '/theme.css', 'HTTP/1.0"', '200', '13428', '"http://www.loudacre.com"', '', '"Loudacre', 'Mobile', 'Browser', 'Sorrento', 'F01L"']


scala> val weblogs1RDD = weblogsRDD.map(line => line.split(' '))
weblogs1RDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:25

scala> for (line <- weblogs1RDD.take(2)) print(line)
[Ljava.lang.String;@33650388[Ljava.lang.String;@5a575e30

Paso 2. crear la k,v:  usuario [2], 1
>>> weblogs2RDD = weblogs1RDD.map(lambda words: (words[2],1))
>>> type(weblogs2RDD)
<class 'pyspark.rdd.PipelinedRDD'>
>>> for line in weblogs2RDD.take(2): print(line)
... 
('67858', 1)
('67858', 1)

scala> val weblogs2RDD = weblogs1RDD.map(words => (words(2),1))
weblogs2RDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[4] at map at <console>:25

scala> for (line <- weblogs2RDD.take(2)) print(line)
(26069,1)(26069,1)
scala> 

b. Use reduceByKey to sum the values for each user ID. Your RDD data will be similar to this:
(userid,5)
(userid,7)
(userid,2)

Paso 3. hacer el reduce, esto me crea un RDD por userID
>>> weblogs3RDD = weblogs2RDD.reduceByKey(lambda count1,count2: count1 + count2)
>>> type(weblogs3RDD)
<class 'pyspark.rdd.PipelinedRDD'>
>>> for line in weblogs3RDD.take(2): print(line)
... 
('91', 174)
('73079', 4)

scala> val weblogs3RDD = weblogs2RDD.reduceByKey((v1,v2) => v1 + v2)
weblogs3RDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[6] at reduceByKey at <console>:25
scala> weblogs3RDD.getClass
res6: Class[_ <: org.apache.spark.rdd.RDD[(String, Int)]] = class org.apache.spark.rdd.ShuffledRDD

scala> for (line <- weblogs3RDD.take(2)) print(line)
(99066,2)(27120,6)                                                              

2. Use countByKey to determine how many users visited the site for each frequency.
That is, how many users visited once, twice, three times, and so on.
a. Use map to reverse the key and value, like this:
(5,userid)
(7,userid)
(2,userid)
…
b. Use the countByKey action to return a map of frequency: user-count pairs.

Paso 4. crear un dictionaries con la frecuencia, count por key
>>> weblogs4dict = weblogs3RDD.map(lambda x: (x[0],x[1])).countByKey()
>>> type(weblogs4RDD)
<class 'collections.defaultdict'>

print(weblogs4dict)
defaultdict(<class 'int'>, {'91': 1, '73079': 1, '33': 1, '91460': 1, '53120': 1, '11878': 1, '12756': 1, '64927': 1, '43769': 1, '44794': 1, '17493': 1, '134': 1, '62655': 1, '48997': 1, '22': 1, '4967': 1, '9135': 1, '12358': 1, '106': 1, '35908': 1, '80752': 1, '93690': 1, '28939': 1, '54': 1, '42800': 1, '21446': 1, '40086': 1, '3706': 1, '100': 1, '58717': 1, '42221': 1, '62343': 1, '68276': 1, '53': 1, '19692': 1, '41453': 1, '4582': 1, '2953': 1, '184': 1, '18920': 1, '23675': 1, '8067': 1, '9310': 1, '10391': 1, '59468': 1, '1381': 1, '40': 1, '4951': 1, '19349': 1, '17341': 1, '48115': 1, '38027': 1, '62624': 1, '54808': 1, '65754': 1, '4998': 1, '24789': 1, '17856': 1, '67653': 1, '51158': 1, '11616': 1, '57851': 1, '83571': 1, '6523': 1, '868': 1, '34623': 1, '30828': 1, '40645': 1, '22834': 1, '68238': 1, '21033': 1, '202': 1, '50937': 1, '32949': 1, '9656': 1, '53827': 1, '19486': 1, '96353': 1, '66188': 1, '38366': 1, '21071': 1, '53459': 1, '26580': 1, '22650': 1, '14556': 1, '4251': 1, '7762': 1, '69715': 1, '31261': 1, '31774': 1, '2764': 1, '4458': 1, '24593': 1, '49686': 1, '8669': 1, '23350': 1, '56808': 1, '12353': 1, '5875': 1, '50165': 1, '16232': 1, '34928': 1, '12712': 1, '68870': 1, '47528': 1, '17001': 1, '34657': 1, '65696': 1, '60386': 1, '12111': 1, 

scala> val weblogs4collection = weblogs3RDD.map(x => (x._2, x._1)).countByKey()
weblogs4collection: scala.collection.Map[Int,Long] = Map(138 -> 6, 170 -> 3, 5 -> 26, 120 -> 5, 10 -> 878, 142 -> 8, 174 -> 2, 24 -> 6, 14 -> 308, 110 -> 4, 20 -> 41, 152 -> 11, 164 -> 3, 106 -> 1, 132 -> 12, 116 -> 1, 6 -> 2162, 160 -> 8, 21 -> 1, 156 -> 5, 9 -> 14, 188 -> 1, 124 -> 7, 13 -> 7, 134 -> 9, 128 -> 9, 2 -> 7239, 166 -> 3, 148 -> 2, 17 -> 4, 176 -> 2, 22 -> 17, 118 -> 4, 27 -> 1, 12 -> 549, 144 -> 7, 86 -> 1, 172 -> 6, 7 -> 14, 140 -> 14, 130 -> 10, 3 -> 36, 162 -> 5, 112 -> 1, 18 -> 76, 150 -> 11, 16 -> 155, 154 -> 5, 11 -> 12, 104 -> 1, 158 -> 6, 8 -> 1409, 168 -> 4, 146 -> 8, 190 -> 1, 19 -> 2, 4 -> 4155, 126 -> 5, 136 -> 11, 15 -> 8, 178 -> 1, 122 -> 4, 100 -> 1)

scala> weblogs4collection.getClass()
res8: Class[_ <: scala.collection.Map[Int,Long]] = class scala.collection.immutable.HashMap$HashTrieMap

scala> print (weblogs4collection)
Map(138 -> 6, 170 -> 3, 5 -> 26, 120 -> 5, 10 -> 878, 142 -> 8, 174 -> 2, 24 -> 6, 14 -> 308, 110 -> 4, 20 -> 41, 152 -> 11, 164 -> 3, 106 -> 1, 132 -> 12, 116 -> 1, 6 -> 2162, 160 -> 8, 21 -> 1, 156 -> 5, 9 -> 14, 188 -> 1, 124 -> 7, 13 -> 7, 134 -> 9, 128 -> 9, 2 -> 7239, 166 -> 3, 148 -> 2, 17 -> 4, 176 -> 2, 22 -> 17, 118 -> 4, 27 -> 1, 12 -> 549, 144 -> 7, 86 -> 1, 172 -> 6, 7 -> 14, 140 -> 14, 130 -> 10, 3 -> 36, 162 -> 5, 112 -> 1, 18 -> 76, 150 -> 11, 16 -> 155, 154 -> 5, 11 -> 12, 104 -> 1, 158 -> 6, 8 -> 1409, 168 -> 4, 146 -> 8, 190 -> 1, 19 -> 2, 4 -> 4155, 126 -> 5, 136 -> 11, 15 -> 8, 178 -> 1, 122 -> 4, 100 -> 1)

3. Create an RDD where the user ID is the key, and the value is the list of all the IP addresses that user has connected from. 
(IP address is the first field in each request line.)
• Hint: Map to (userid,ipaddress) and then use groupByKey.
(userid,20.1.34.55)
(userid,245.33.1.1)
(userid,65.50.196.141)

Paso 5. Para extraer la key que en este caso es la ip y el usuario - Crear la lista
>>> weblogs5RDD = weblogsRDD.map(lambda line: line.split(' '))
>>> type(weblogs5RDD)
<class 'pyspark.rdd.PipelinedRDD'>
>>> for line in weblogs5RDD.take(2): print(line)
... 
['131.166.169.114', '-', '67858', '[23/Sep/2013:00:00:00', '+0100]', '"GET', '/ifruit_3a_sales.html', 'HTTP/1.0"', '200', '9509', '"http://www.loudacre.com"', '', '"Loudacre', 'Mobile', 'Browser', 'Sorrento', 'F01L"']
['131.166.169.114', '-', '67858', '[23/Sep/2013:00:00:00', '+0100]', '"GET', '/theme.css', 'HTTP/1.0"', '200', '13428', '"http://www.loudacre.com"', '', '"Loudacre', 'Mobile', 'Browser', 'Sorrento', 'F01L"']

scala> val weblogs5RDD = weblogsRDD.map(line => line.split(' '))
weblogs5RDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[16] at map at <console>:25

scala> for (line <- weblogs5RDD.take(2)) print(line)
[Ljava.lang.String;@16150bb7[Ljava.lang.String;@2018453c                 

Paso 6. extraer la ip y el usuario
>>> weblogs6RDD = weblogs1RDD.map(lambda words: (words[2], words[0]))
>>> type(weblogs6RDD)
<class 'pyspark.rdd.PipelinedRDD'>
>>> for line in weblogs6RDD.take(2): print(line)
... 
('67858', '131.166.169.114')
('67858', '131.166.169.114')

scala> val weblogs6RDD = weblogs1RDD.map(words => (words(2), words(0)))
weblogs6RDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[17] at map at <console>:25

scala> for (line <- weblogs6RDD.take(2)) print(line)
(26069,200.31.229.239)(26069,200.31.229.239)

Paso 7. Group by por key
>>> weblogs7RDD = weblogs6RDD.groupByKey()
>>> type(weblogs7RDD)
<class 'pyspark.rdd.PipelinedRDD'>
>>> for line in weblogs7RDD.take(2): print(line)
... 
('91', <pyspark.resultiterable.ResultIterable object at 0x7fed23680fd0>)        
('73079', <pyspark.resultiterable.ResultIterable object at 0x7fed23680090>)

Paso 8. Para poder ver como es el iterable hay que hacer el siguiente map
>>> weblogs8list = weblogs7RDD.map(lambda x : (x[0], list(x[1]))).collect()
>>> type(weblogs8list)
<class 'list'>

>>> for line in weblogs8list: print(line)
... 
('91', ['79.35.153.191', '79.35.153.191', '78.100.117.34', '78.100.117.34', '247.63.197.120', '247.63.197.120', '245.112.66.109', '245.112.66.109', '253.250.101.198', '253.250.101.198', '73.87.9.86', '73.87.9.86', '160.169.235.236', '160.169.235.236', '2.192.76.227', '2.192.76.227', '179.70.66.121', '179.70.66.121', '34.237.117.136', '34.237.117.136', '213.31.158.178', '213.31.158.178', '165.245.82.201', '165.245.82.201', '114.5.167.156', '114.5.167.156', '231.176.83.205', '231.176.83.205', '126.223.208.40', '126.223.208.40', '168.172.75.135', '168.172.75.135', '1.213.123.177', '1.213.123.177', '161.45.11.66', '161.45.11.66', '247.41.208.12', '247.41.208.12', '117.170.14.102', '117.170.14.102', '117.159.99.194', '117.159.99.194', '131.5.78.133', '131.5.78.133', '145.204.203.140', '145.204.203.140', '100.220.38.137', '100.220.38.137', '158.27.171.75', '158.27.171.75', '48.196.159.251', '48.196.159.251', '139.101.245.131', '139.101.245.131', '139.206.104.182', '139.206.104.182', '141.120.13.38', '141.120.13.38', '2.168.239.109', '2.168.239.109', '255.223.6.239', '255.223.6.239', '146.127.142.246', '146.127.142.246', '98.174.211.196', '98.174.211.196', '245.237.251.248', '245.237.251.248', '120.161.185.152', '120.161.185.152', '143.171.133.206', '143.171.133.206', '190.118.68.67', '190.118.68.67', '168.208.49.222', '168.208.49.222', '90.186.143.145', '90.186.143.145', '189.161.190.180', '189.161.190.180', '81.11.227.176', '81.11.227.176', '84.25.149.174', '84.25.149.174', '39.156.59.246', '39.156.59.246', '203.34.35.204', '203.34.35.204', '238.124.121.60', '238.124.121.60', '155.71.231.245', '155.71.231.245', '132.43.177.8', '132.43.177.8', '77.111.79.143', '77.111.79.143', '97.11.49.64', '97.11.49.64', '162.188.95.236', '162.188.95.236', '109.60.40.108', '109.60.40.108', '202.64.3.85', '202.64.3.85', '39.162.52.224', '39.162.52.224', '217.131.98.66', '217.131.98.66', '153.74.71.40', '153.74.71.40', '131.241.135.60', '131.241.135.60', '110.200.230.249', '110.200.230.249', '66.215.27.173', '66.215.27.173', '58.186.54.67', '58.186.54.67', '102.141.181.216', '102.141.181.216', '3.202.124.125', '3.202.124.125', '181.42.242.176', '181.42.242.176', '191.197.23.35', '191.197.23.35', '9.224.98.204', '9.224.98.204', '189.110.130.167', '189.110.130.167', '240.9.6.55', '240.9.6.55', '37.100.214.30', '37.100.214.30', '163.158.70.178', '163.158.70.178', '235.14.229.200', '235.14.229.200', '152.242.150.111', '152.242.150.111', '147.249.31.141', '147.249.31.141', '66.188.39.198', '66.188.39.198', '21.72.55.153', '21.72.55.153', '96.113.70.218', '96.113.70.218', '44.113.201.157', '44.113.201.157', '222.253.17.90', '222.253.17.90', '18.229.40.246', '18.229.40.246', '174.27.171.150', '174.27.171.150', '145.71.216.142', '145.71.216.142', '67.39.116.101', '67.39.116.101', '129.62.183.89', '129.62.183.89', '231.110.204.57', '231.110.204.57', '196.214.239.110', '196.214.239.110', '89.189.143.191', '89.189.143.191', '77.74.108.14', '77.74.108.14', '193.74.79.166', '193.74.79.166', '129.223.160.166', '129.223.160.166'])
('73079', ['179.49.255.2', '179.49.255.2', '53.238.11.142', '53.238.11.142'])

scala> val weblogs7RDD = weblogs6RDD.groupByKey()
weblogs7RDD: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[18] at groupByKey at <console>:25

scala> for (line <- weblogs7RDD.take(2)) print(line)
(99066,CompactBuffer(195.67.16.237, 195.67.16.237))(27120,CompactBuffer(13.195.52.33, 13.195.52.33, 13.195.52.33, 13.195.52.33, 129.153.29.20, 129.153.29.20))

9. imprimir 10 registros del RDD --- otra forma de hacer el Paso 8.

>>> for (userid,ips) in weblogs7RDD.take(2):
...    print(userid, ":")
...    for ip in ips: print("\t",ip)
... 
91 :
         79.35.153.191
         79.35.153.191
         78.100.117.34
         78.100.117.34
        ....
        ....
         193.74.79.166
         193.74.79.166
         129.223.160.166
         129.223.160.166
73079 :
         179.49.255.2
         179.49.255.2
         53.238.11.142
         53.238.11.142         

scala> for (x <- weblogs7RDD.take(2)) {
     |    println(x._1 + ":")
     |    for (ip <- x._2) println("\t"+ip)
     | }
99066:
        195.67.16.237
        195.67.16.237
27120:
        13.195.52.33
        13.195.52.33
        13.195.52.33
        13.195.52.33
        129.153.29.20
        129.153.29.20


Join Web Log Data with Account Data
Review the accounts data located in /warehouse/tablespace/external/hive/devsh.db/accounts, 
which contains the data in the Hive devsh.accounts table.
The first field in each line is the user ID, which corresponds to the user ID in the web server logs. 
The other fields include account details such as creation date, first and last name, and so on.

scelisdev02@cca175-m:~/training_materials/devsh/exercises/pair-rdds/solution$ hdfs dfs -ls /user/hive/warehouse/devsh.db/
Found 1 items
drwxrwxrwt   - scelisdev02 hadoop          0 2021-09-21 10:41 /user/hive/warehouse/devsh.db/accounts
scelisdev02@cca175-m:~/training_materials/devsh/exercises/pair-rdds/solution$ 

4. Join the accounts data with the weblog data to produce a dataset keyed by user ID which contains the user account information and the number of website hits for that user.
a. Create an RDD, based on the accounts data, consisting of key/value-array pairs: (userid,[values…])

Paso 0. Crear el RDD con los datos de la tabla accounts de hive

>>> accountsdata = "/user/hive/warehouse/devsh.db/accounts"
>>> type(accountsdata)
<class 'str'>

>>> accountsRDD = sc.textFile(accountsdata)
>>> type(accountsRDD)
<class 'pyspark.rdd.RDD'>

>>> for line in accountsRDD.take(2): print(line)
... 
1,2008-10-23 16:05:05.0,\N,Donald,Becton,2275 Washburn Street,Oakland,CA,94660,5100032418,2014-03-18 13:29:47.0,2014-03-18 13:29:47.0
2,2008-11-12 03:00:01.0,\N,Donna,Jones,3885 Elliott Street,San Francisco,CA,94171,4150835799,2014-03-18 13:29:47.0,2014-03-18 13:29:47.0

scala> val accountsdata = "/user/hive/warehouse/devsh.db/accounts"
accountsdata: String = /user/hive/warehouse/devsh.db/accounts

scala> val accountsRDD = sc.textFile(accountsdata)
accountsRDD: org.apache.spark.rdd.RDD[String] = /user/hive/warehouse/devsh.db/accounts MapPartitionsRDD[20] at textFile at <console>:26

scala> for (line <- accountsRDD.take(2)) print(line)
1,2008-10-23 16:05:05.0,\N,Donald,Becton,2275 Washburn Street,Oakland,CA,94660,5100032418,2014-03-18 13:29:47.0,2014-03-18 13:29:47.02,2008-11-12 03:00:01.0,\N,Donna,Jones,3885 Elliott Street,San Francisco,CA,94171,4150835799,2014-03-18 13:29:47.0,2014-03-18 13:29:47.0

Paso 1. Crear la lista a partir del RDD y hacer el split por campos
>>> accounts1RDD = accountsRDD.map(lambda s: s.split(','))
>>> type(accounts1RDD)
<class 'pyspark.rdd.PipelinedRDD'>

>>> for line in accounts1RDD.take(2): print(line)
... 
['1', '2008-10-23 16:05:05.0', '\\N', 'Donald', 'Becton', '2275 Washburn Street', 'Oakland', 'CA', '94660', '5100032418', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0']
['2', '2008-11-12 03:00:01.0', '\\N', 'Donna', 'Jones', '3885 Elliott Street', 'San Francisco', 'CA', '94171', '4150835799', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0']

scala> val accounts1RDD = accountsRDD.map(s => s.split(','))
accounts1RDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[21] at map at <console>:25

scala> for (line <- accounts1RDD.take(2)) print(line)
[Ljava.lang.String;@31c6414d[Ljava.lang.String;@f81c7ee

Paso 2. Crea el RDD por userID, campo [0]
>>> accounts2RDD = accounts1RDD.map(lambda account: (account[0], account))
>>> type(accounts2RDD)
<class 'pyspark.rdd.PipelinedRDD'>
>>> for line in accounts2RDD.take(2): print(line)
... 
('1', ['1', '2008-10-23 16:05:05.0', '\\N', 'Donald', 'Becton', '2275 Washburn Street', 'Oakland', 'CA', '94660', '5100032418', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])
('2', ['2', '2008-11-12 03:00:01.0', '\\N', 'Donna', 'Jones', '3885 Elliott Street', 'San Francisco', 'CA', '94171', '4150835799', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])

scala> val accounts2RDD = accounts1RDD.map(x => (x(0), x))
accounts2RDD: org.apache.spark.rdd.RDD[(String, Array[String])] = MapPartitionsRDD[16] at map at <console>:25

scala> for (line <- accounts2RDD.take(2)) print(line)
(1,[Ljava.lang.String;@7f500542)(2,[Ljava.lang.String;@435876cf)                

b. Join the pair RDD with the set of user-id/hit-count pairs calculated in the first step.

>>> accountsjoinRDD = accounts2RDD.join(weblogs3RDD)
>>> type(accountsjoinRDD)
<class 'pyspark.rdd.PipelinedRDD'>
>>> for line in accountsjoinRDD.take(2): print(line)
... 
('48', (['48', '2008-11-15 12:22:34.0', '\\N', 'Jerome', 'Cope', '4453 Jenna Lane', 'Santa Rosa', 'CA', '94914', '7078755344', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'], 188))
('52', (['52', '2008-10-25 10:48:40.0', '\\N', 'Odessa', 'True', '3764 Kessla Way', 'San Francisco', 'CA', '94002', '4151228388', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'], 160))

scala> val accountsjoinRDD = accounts2RDD.join(weblogs3RDD)
accountsjoinRDD: org.apache.spark.rdd.RDD[(String, (Array[String], Int))] = MapPartitionsRDD[19] at join at <console>:27

scala> for (line <- accountsjoinRDD.take(2)) print(line)
(27120,([Ljava.lang.String;@3ee86eb7,6))(41094,([Ljava.lang.String;@678f4876,2))

c. Display the user ID, hit count, and first name (4 th value) and last name (5 th value) for the first five elements. 

>>> for (userid,(values, count)) in accountsjoinRDD.take(2) :
...     print(userid, count, values[3], values[4])
... 
48 188 Jerome Cope                                                              
52 160 Odessa True

scala> for (x <- accountsjoinRDD.take(2)) {
     |    printf("%s %s %s %s\n",x._1, x._2._2,  x._2._1(3), x._2._1(4))
     | }
27120 6 Dale Hanson
41094 2 Billy Jordan

Bonus Exercises
If you have more time, attempt the following extra bonus exercises:
1. Use keyBy to create an RDD of account data with the postal code (9 th field in the CSV file) as the key.

>>> accountsdata = "/user/hive/warehouse/devsh.db/accounts"
>>> accountsRDD = sc.textFile(accountsdata)
>>> for line in accountsRDD.take(2): print(line)
... 
1,2008-10-23 16:05:05.0,\N,Donald,Becton,2275 Washburn Street,Oakland,CA,94660,5100032418,2014-03-18 13:29:47.0,2014-03-18 13:29:47.0
2,2008-11-12 03:00:01.0,\N,Donna,Jones,3885 Elliott Street,San Francisco,CA,94171,4150835799,2014-03-18 13:29:47.0,2014-03-18 13:29:47.0

>>> accounts1RDD = accountsRDD.map(lambda s: s.split(','))
>>> for line in accounts1RDD.take(2): print(line)
... 
['1', '2008-10-23 16:05:05.0', '\\N', 'Donald', 'Becton', '2275 Washburn Street', 'Oakland', 'CA', '94660', '5100032418', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0']
['2', '2008-11-12 03:00:01.0', '\\N', 'Donna', 'Jones', '3885 Elliott Street', 'San Francisco', 'CA', '94171', '4150835799', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0']

>>> accounts2RDD = accounts1RDD.keyBy(lambda x: x[8])
>>> for line in accounts2RDD.take(2): print(line)
... 
('94660', ['1', '2008-10-23 16:05:05.0', '\\N', 'Donald', 'Becton', '2275 Washburn Street', 'Oakland', 'CA', '94660', '5100032418', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])
('94171', ['2', '2008-11-12 03:00:01.0', '\\N', 'Donna', 'Jones', '3885 Elliott Street', 'San Francisco', 'CA', '94171', '4150835799', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])

scala> val accountsdata = "/user/hive/warehouse/devsh.db/accounts"
accountsdata: String = /user/hive/warehouse/devsh.db/accounts

scala> val accountsRDD = sc.textFile(accountsdata)
accountsRDD: org.apache.spark.rdd.RDD[String] = /user/hive/warehouse/devsh.db/accounts MapPartitionsRDD[21] at textFile at <console>:26

scala> for (line <- accountsRDD.take(2)) print(line)
1,2008-10-23 16:05:05.0,\N,Donald,Becton,2275 Washburn Street,Oakland,CA,94660,5100032418,2014-03-18 13:29:47.0,2014-03-18 13:29:47.02,2008-11-12 03:00:01.0,\N,Donna,Jones,3885 Elliott Street,San Francisco,CA,94171,4150835799,2014-03-18 13:29:47.0,2014-03-18 13:29:47.0

scala> val accounts1RDD = accountsRDD.map(s => s.split(','))
accounts1RDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[22] at map at <console>:25

scala> for (line <- accounts1RDD.take(2)) print(line)
[Ljava.lang.String;@bdb63c6[Ljava.lang.String;@5863bcee


------------------
|OJO ---- este lo estaba haciendo en el cluster, pero no lo termine - Hands-On Exercise: Joining Data Using Pair RDDs
|OJO ---- lo empiezo de nuevo en local - Hands-On Exercise: Joining Data Using Pair RDDs
|------------------
|
|(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ pyspark
|>>> weblogfiles=("../files/weblogs/*2.log")
|>>> type(weblogfiles)
|<class 'str'>
|
|Use map to create a pair RDD with the user ID as the key and the integer 1 as the value. 
|(The user ID is the third field in each line.) 
|
|>>> weblogsRDD = sc.textFile(weblogfiles)
|>>> type(weblogsRDD)
|<class 'pyspark.rdd.RDD'>
|>>> for line in weblogsRDD.take(2): print(line)
|... 
|131.166.169.114 - 67858 [23/Sep/2013:00:00:00 +0100] "GET /ifruit_3a_sales.html HTTP/1.0" 200 9509 "http://www.loudacre.com"  "Loudacre Mobile Browser Sorrento F01L"
|131.166.169.114 - 67858 [23/Sep/2013:00:00:00 +0100] "GET /theme.css HTTP/1.0" 200 13428 "http://www.loudacre.com"  "Loudacre Mobile Browser Sorrento F01L"
|
|>>> weblogsRDDMap = weblogsRDD.map(lambda line : line.split(' '))
|>>> weblogsRDDMap = weblogsRDD.map(lambda _ : _.split(' '))
|>>> for line in weblogsRDDMap.take(2): print(line)
|['131.166.169.114', '-', '67858', '[23/Sep/2013:00:00:00', '+0100]', '"GET', '/ifruit_3a_sales.html', 'HTTP/1.0"', '200', '9509', '"http://www.loudacre.com"', '', '"Loudacre', 'Mobile', 'Browser', 'Sorrento', 'F01L"']
|['131.166.169.114', '-', '67858', '[23/Sep/2013:00:00:00', '+0100]', '"GET', '/theme.css', 'HTTP/1.0"', '200', '13428', '"http://www.loudacre.com"', '', '"Loudacre', 'Mobile', 'Browser', 'Sorrento', 'F01L"']
|
|>>> weblogsRDDMap2 = weblogsRDDMap.map(lambda line: (line[2], 1))
|>>> for line in weblogsRDDMap2.take(2): print(line)
|... 
|('67858', 1)
|('67858', 1)
|>>> weblogsRDDMap2 = weblogsRDDMap.map(lambda _: (_[2], 1))
|>>> for line in weblogsRDDMap2.take(2): print(line)
|... 
|('67858', 1)
|('67858', 1)
|
|Use reduceByKey to sum the values for each user ID
|>>> weblogsRDDReduce = weblogsRDDMap2.reduceByKey(lambda v1,v2: v1+v2)
|>>> for line in weblogsRDDReduce.take(20): print(line)
|... 
|('91', 174)                                                                     
|('73079', 4)
|('33', 146)
|('91460', 10)
|('53120', 10)
|('11878', 10)
|('12756', 10)
|('64927', 14)
|('43769', 8)
|('44794', 6)
|
|Use map to create a pair RDD with the user ID as the key and the integer 1 as the value. 
|(The user ID is the third field in each line.)
|Use reduceByKey to sum the values for each user ID.
|
|>>> weblogsRDD = sc.textFile(weblogfiles).map(lambda line : line.split(' ')).map(lambda line: (line[2], 1)).reduceByKey(lambda v1,v2: v1+v2)
|>>> weblogsRDD.getNumPartitions()
|18
|>>> for i in weblogsRDD.take(10) :print(i)
|... 
|('91', 174)
|('73079', 4)
|('33', 146)
|('91460', 10)
|('53120', 10)
|('11878', 10)
|('12756', 10)
|('64927', 14)
|('43769', 8)
|('44794', 6)
|
|Use countByKey to determine how many users visited the site for each frequency.
|
|>>> freq = weblogsRDD.map(lambda x: (x[0],x[1])).countByKey()
|>>> type(freq)
|<class 'collections.defaultdict'>
|>>> for i in freq :print(i)
|(los últimos que muestra)
|... 
|110068
|50355
|38667
|19137
|72025
|23483
|54346
|68488
|67847
|54484
|51902
|
|Use countByKey to determine how many users visited the site for each frequency.
|>>> freq = weblogsRDD.countByKey().items()
|>>> for i in freq :print(i)
|(los últimos que muestra)
|... 
|('110068', 1)
|('50355', 1)
|('38667', 1)
|('19137', 1)
|('72025', 1)
|('23483', 1)
|('54346', 1)
|('68488', 1)
|('67847', 1)
|('54484', 1)
|('51902', 1)
|
|Create an RDD where the user ID is the key, and the value is the list of all the IP addresses that user has connected from. 
|(IP address is the first field in each request line.)
|• Hint: Map to (userid,ipaddress) and then use groupByKey.
|
|>>> weblogsRDD = sc.textFile(weblogfiles)
|>>> userIPRDD = weblogsRDD.map(lambda line: line.split(' ')).map(lambda words: (words[2],words[0]))
|>>> type(userIPRDD)
|<class 'pyspark.rdd.PipelinedRDD'>
|>>> for i in userIPRDD.take(10) : print(i)
|... 
|('67858', '131.166.169.114')
|('67858', '131.166.169.114')
|('67858', '131.166.169.114')
|('67858', '131.166.169.114')
|('3579', '254.253.248.193')
|('3579', '254.253.248.193')
|('3579', '254.253.248.193')
|('3579', '254.253.248.193')
|('8897', '200.192.65.234')
|('8897', '200.192.65.234')
|
|>>> groupRDD = userIPRDD.groupByKey()
|>>> type(groupRDD)
|<class 'pyspark.rdd.PipelinedRDD'>
|>>> for i in groupRDD.take(10) : print(i)
|... 
|('91', <pyspark.resultiterable.ResultIterable object at 0x7fd2c9ce8ee0>)
|('73079', <pyspark.resultiterable.ResultIterable object at 0x7fd2c9ce8f40>)
|('33', <pyspark.resultiterable.ResultIterable object at 0x7fd2c9ce8940>)
|('91460', <pyspark.resultiterable.ResultIterable object at 0x7fd2c9ce8730>)
|('53120', <pyspark.resultiterable.ResultIterable object at 0x7fd2c9ce8760>)
|('11878', <pyspark.resultiterable.ResultIterable object at 0x7fd2c9ce8a00>)
|('12756', <pyspark.resultiterable.ResultIterable object at 0x7fd2c9ce86d0>)
|('64927', <pyspark.resultiterable.ResultIterable object at 0x7fd2c9ce85e0>)
|('43769', <pyspark.resultiterable.ResultIterable object at 0x7fd2c9ce8dc0>)
|('44794', <pyspark.resultiterable.ResultIterable object at 0x7fd2c9ce8a30>)
|
|>>> UserlistIPs = groupRDD.map(lambda x : (x[0], list(x[1]))).collect()
|>>> type(UserlistIPs)
|<class 'list'>
|>>> for i in UserlistIPs : print(i)
|(los últimos que muestra)
|... 
|('110068', ['65.74.157.30', '65.74.157.30'])
|('50355', ['114.189.148.231', '114.189.148.231'])
|('38667', ['222.137.210.19', '222.137.210.19'])
|('19137', ['167.133.104.126', '167.133.104.126'])
|('72025', ['27.58.231.20', '27.58.231.20'])
|('23483', ['212.12.102.5', '212.12.102.5'])
|('54346', ['35.142.155.133', '35.142.155.133'])
|('68488', ['234.119.158.222', '234.119.158.222', '234.119.158.222', '234.119.158.222'])
|('67847', ['198.134.12.14', '198.134.12.14'])
|('54484', ['45.220.118.230', '45.220.118.230'])
|('51902', ['136.230.111.239', '136.230.111.239'])
|
|--hasta aquí el ejercicio finalizado en pyspark, dejo esta nota para recordar que el flatMap es diferente al map
|
|>>> weblogsRDDFlat = weblogsRDD.flatMap(lambda line: line.split(' '))
|>>> for line in weblogsRDDFlat.take(20): print(line)
|... 
|131.166.169.114
|-
|67858
|[23/Sep/2013:00:00:00
|+0100]
|"GET
|/ifruit_3a_sales.html
|HTTP/1.0"
|200
|9509
|"http://www.loudacre.com"
|
|"Loudacre
|Mobile
|Browser
|Sorrento
|F01L"
|131.166.169.114
|-
|67858
|
|>>> weblogsRDDMap = weblogsRDDFlat.map(lambda _ : _.split(' '))
|>>> weblogsRDDMap = weblogsRDDFlat.map(lambda line : line.split(' '))
|>>> for line in weblogsRDDMap.take(20): print(line)
|... 
|['131.166.169.114']
|['-']
|['67858']
|['[23/Sep/2013:00:00:00']
|['+0100]']
|['"GET']
|['/ifruit_3a_sales.html']
|['HTTP/1.0"']
|['200']
|['9509']
|['"http://www.loudacre.com"']
|['']
|['"Loudacre']
|['Mobile']
|['Browser']
|['Sorrento']
|['F01L"']
|['131.166.169.114']
|['-']
|['67858']
|
|>>> type (weblogsRDDFlat)
|<class 'pyspark.rdd.PipelinedRDD'>
|
|--hasta aquí el ejercicio finalizado en pyspark, dejo esta nota para recordar que el flatMap es diferente al map
|
|(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ spark-shell
|
|scala> val weblogfiles=("../files/weblogs/*2.log")
|weblogfiles: String = ../files/weblogs/*2.log
|
|scala> val weblogsRDD = sc.textFile(weblogfiles)
|weblogsRDD: org.apache.spark.rdd.RDD[String] = ../files/weblogs/*2.log MapPartitionsRDD[1] at textFile at <console>:26
|
|scala> weblogsRDD.getClass
|res0: Class[_ <: org.apache.spark.rdd.RDD[String]] = class org.apache.spark.rdd.MapPartitionsRDD
|
|
|scala> for (line <- weblogsRDD.take(2)) print(line)
|131.166.169.114 - 67858 [23/Sep/2013:00:00:00 +0100] "GET /ifruit_3a_sales.html HTTP/1.0" 200 9509 "http://www.loudacre.com"  "Loudacre Mobile Browser Sorrento F01L"131.166.169.114 - 67858 [23/Sep/2013:00:00:00 +0100] "GET /theme.css HTTP/1.0" 200 13428 "http://www.loudacre.com"  "Loudacre Mobile Browser Sorrento F01L"
|scala> 
|
|scala> val weblogsRDDFlat = weblogsRDD.flatMap(line => line.split(' '))
|weblogsRDDFlat: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at flatMap at <console>:25
|scala> for (line <- weblogsRDDFlat.take(20)) print(line)
|131.166.169.114-67858[23/Sep/2013:00:00:00+0100]"GET/ifruit_3a_sales.htmlHTTP/1.0"2009509"http://www.loudacre.com""LoudacreMobileBrowserSorrentoF01L"131.166.169.114-67858
|
|scala> weblogsRDDFlat.getClass
|res5: Class[_ <: org.apache.spark.rdd.RDD[String]] = class org.apache.spark.rdd.MapPartitionsRDD
|
|
|scala> val weblogsRDDMap = weblogsRDDFlat.map(word => (word,2))
|weblogsRDDMap: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[5] at map at <console>:25
|
|scala> for (line <- weblogsRDDMap.take(20)) print(line)
|(131.166.169.114,2)(-,2)(67858,2)([23/Sep/2013:00:00:00,2)(+0100],2)("GET,2)(/ifruit_3a_sales.html,2)(HTTP/1.0",2)(200,2)(9509,2)("http://www.loudacre.com",2)(,2)("Loudacre,2)(Mobile,2)(Browser,2)(Sorrento,2)(F01L",2)(131.166.169.114,2)(-,2)(67858,2)
|
|scala> weblogsRDDMap.getClass
|res8: Class[_ <: org.apache.spark.rdd.RDD[(String, Int)]] = class org.apache.spark.rdd.MapPartitionsRDD
|
|scala> val weblogsRDDReduce = weblogsRDDMap.reduceByKey((v1,v2) => v1+v2)
|weblogsRDDReduce: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[5] at reduceByKey at <console>:25
|
|scala> for (line <- weblogsRDDReduce.take(20)) print(line)
|(27120,12)(3492,12)(27.110.232.235,4)(54.145.72.110,4)(181.19.18.122,4)([12/Nov/2013:16:06:55,4)([22/Nov/2013:11:30:27,4)([12/Dec/2013:22:21:01,4)([22/Nov/2013:18:18:55,4)([02/Dec/2013:06:18:57,4)(130.81.171.233,4)(41094,4)([02/Nov/2013:11:36:12,4)(22.66.178.196,4)([02/Feb/2014:09:28:46,4)([22/Sep/2013:10:36:12,8)(171.162.46.189,4)([12/Feb/2014:22:37:44,4)([12/Dec/2013:09:41:00,4)(18306,16)
|
|scala> weblogsRDDReduce.getClass
|res10: Class[_ <: org.apache.spark.rdd.RDD[(String, Int)]] = class org.apache.spark.rdd.ShuffledRDD
|
|
|scala> weblogsRDD.getClass
|res14: Class[_ <: org.apache.spark.rdd.RDD[(String, Int)]] = class org.apache.spark.rdd.ShuffledRDD
|
|Esta es una forma de hacerlo todo a la vez
|
|scala> val weblogsRDD = sc.textFile(weblogfiles).
     || flatMap(_.split(' ')).
     || map((_,1)).
     || reduceByKey(_+_)
|weblogsRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[26] at reduceByKey at <console>:29
|
|scala> val weblogsRDD = sc.textFile(weblogfiles).flatMap(_.split(' ')).map((_,1)).reduceByKey(_+_)
|weblogsRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[31] at reduceByKey at <console>:26
|
|scala> print(weblogsRDD)
|ShuffledRDD[31] at reduceByKey at <console>:26
|
|scala> for (i <- weblogsRDD.take(10)) print(i)
|(27120,6)(3492,6)(27.110.232.235,2)(54.145.72.110,2)(181.19.18.122,2)([12/Nov/2013:16:06:55,2)([22/Nov/2013:11:30:27,2)([12/Dec/2013:22:21:01,2)([22/Nov/2013:18:18:55,2)([02/Dec/2013:06:18:57,2)
|
|scala> val weblogsRDD = sc.textFile(weblogfiles).
     || flatMap(line => line.split(' ')).
     || map(word => (word,1)).
     || reduceByKey((v1,v2) => v1+v2)
|weblogsRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:29
|
|scala> val weblogsRDD = sc.textFile(weblogfiles).flatMap(line => line.split(' ')).map(word => (word,1)).reduceByKey((v1,v2) => v1+v2)
|weblogsRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[36] at reduceByKey at <console>:26
|
|scala> for (i <- weblogsRDD.take(10)) print(i)
|(27120,6)(3492,6)(27.110.232.235,2)(54.145.72.110,2)(181.19.18.122,2)([12/Nov/2013:16:06:55,2)([22/Nov/2013:11:30:27,2)([12/Dec/2013:22:21:01,2)([22/Nov/2013:18:18:55,2)([02/Dec/2013:06:18:57,2)
|
|
|// Group IPs by user ID
|val userIPs = logsRDD.map(line => line.split(' ')).map(words => (words(2),words(0))).groupByKey()
|
|// print out the first 10 user ids, and their IP list
|for (pair <- userIPs.take(10)) {
   |println(pair._1 + ":")
   |for (ip <- pair._2) println("\t"+ip)
|}
|
|// 
|
|Join Web Log Data with Account Data
|Review the accounts data located in /warehouse/tablespace/external/hive/ devsh.db/accounts, 
|which contains the data in the Hive devsh.accounts table.
|The first field in each line is the user ID, which corresponds to the user ID in the web server logs. 
|The other fields include account details such as creation date, first and last name, and so on.
|
|4. Join the accounts data with the weblog data to produce a dataset keyed by user ID
|which contains the user account information and the number of website hits for that user.
|
|a. Create an RDD, based on the accounts data, consisting of key/value-array pairs: (userid,[values…])
|filename = "/user/hive/warehouse/devsh.db/accounts"
|accountsRDD = sc.textFile(filename)
|for i in accountsRDD.take(10) : print(i)
|
|accountsRDD = sc.textFile(filename).  map(lambda x: x.split(',')).  map(lambda x: (x[0], x))
|for i in accountsRDD.take(10) : print(i)
|
|b. Join the pair RDD with the set of user-id/hit-count pairs calculated in the first step.
|joinedRDD = weblogsRDD.join(accountsRDD)
|for i in joinedRDD.take(10) : print(i)
|
|c. Display the user ID, hit count, and first name (4 th value) and last name (5 th value) for the first five elements. 
|
|for (userid,(values, count)) in joinedRDD.take(2) : print(userid, count, values[3], values[4])
|
|------------------
|OJO ---- este lo estaba haciendo en el cluster, pero lo repetí en local - Hands-On Exercise: Joining Data Using Pair RDDs
|OJO ---- lo empiezo de nuevo en local - Hands-On Exercise: Joining Data Using Pair RDDs
------------------
solucion del manual: /home/scelisdev02/training_materials/devsh/exercises/pair-rdds
-----------------------------------------------

Bonus Exercises
If you have more time, attempt the following extra bonus exercises:
1. Use keyBy to create an RDD of account data with the postal code (9 th field in the CSV file) as the key.

>>> accountsdata = "/user/hive/warehouse/devsh.db/accounts"
>>> accountsRDD = sc.textFile(accountsdata)
>>> for line in accountsRDD.take(2): print(line)
... 
1,2008-10-23 16:05:05.0,\N,Donald,Becton,2275 Washburn Street,Oakland,CA,94660,5100032418,2014-03-18 13:29:47.0,2014-03-18 13:29:47.0
2,2008-11-12 03:00:01.0,\N,Donna,Jones,3885 Elliott Street,San Francisco,CA,94171,4150835799,2014-03-18 13:29:47.0,2014-03-18 13:29:47.0

>>> accounts1RDD = accountsRDD.map(lambda s: s.split(','))
>>> for line in accounts1RDD.take(2): print(line)
... 
['1', '2008-10-23 16:05:05.0', '\\N', 'Donald', 'Becton', '2275 Washburn Street', 'Oakland', 'CA', '94660', '5100032418', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0']
['2', '2008-11-12 03:00:01.0', '\\N', 'Donna', 'Jones', '3885 Elliott Street', 'San Francisco', 'CA', '94171', '4150835799', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0']

>>> accounts2RDD = accounts1RDD.keyBy(lambda x: x[8])
>>> for line in accounts2RDD.take(2): print(line)
... 
('94660', ['1', '2008-10-23 16:05:05.0', '\\N', 'Donald', 'Becton', '2275 Washburn Street', 'Oakland', 'CA', '94660', '5100032418', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])
('94171', ['2', '2008-11-12 03:00:01.0', '\\N', 'Donna', 'Jones', '3885 Elliott Street', 'San Francisco', 'CA', '94171', '4150835799', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])

    ...: sc 
    ...: filename = "/user/hive/warehouse/devsh.db/accounts" 
    ...: bonusRDD = sc.textFile(filename). map(lambda x : x.split(',')) . keyBy(lambda x : x[8]) 
    ...: for i in bonusRDD.take(10) :print(i) 


('94660', ['1', '2008-10-23 16:05:05.0', '\\N', 'Donald', 'Becton', '2275 Washburn Street', 'Oakland', 'CA', '94660', '5100032418', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])
('94171', ['2', '2008-11-12 03:00:01.0', '\\N', 'Donna', 'Jones', '3885 Elliott Street', 'San Francisco', 'CA', '94171', '4150835799', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])
('94479', ['3', '2008-12-21 09:19:50.0', '\\N', 'Dorthy', 'Chalmers', '4073 Whaley Lane', 'San Mateo', 'CA', '94479', '6506877757', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])
('94444', ['4', '2008-11-28 00:08:09.0', '\\N', 'Leila', 'Spencer', '1447 Ross Street', 'San Mateo', 'CA', '94444', '6503198619', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])
('94872', ['5', '2008-11-15 23:06:06.0', '\\N', 'Anita', 'Laughlin', '2767 Hill Street', 'Richmond', 'CA', '94872', '5107754354', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])
('94264', ['6', '2008-11-20 12:39:33.0', '2014-03-01 07:37:48.0', 'Stevie', 'Bridge', '3977 Linda Street', 'Sacramento', 'CA', '94264', '9162111862', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])
('94508', ['7', '2008-12-09 10:32:12.0', '2010-10-16 10:01:51.0', 'David', 'Eggers', '2109 Ross Street', 'Oakland', 'CA', '94508', '5103935529', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])
('94469', ['8', '2008-12-15 08:49:38.0', '\\N', 'Dorothy', 'Koopman', '1985 Pratt Avenue', 'San Mateo', 'CA', '94469', '6502406661', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])
('94312', ['9', '2008-11-07 17:58:55.0', '2014-02-14 01:26:52.0', 'Kara', 'Kohl', '235 Fort Street', 'Palo Alto', 'CA', '94312', '6502384894', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])
('94577', ['10', '2008-12-02 23:28:01.0', '\\N', 'Diane', 'Nelson', '921 Sardis Sta', 'Oakland', 'CA', '94577', '5102711264', '2014-03-18 13:29:47.0', '2014-03-18 13:29:47.0'])

    ...: bonusRDD2 = bonusRDD.mapValues(lambda x: x[4] + ',' + x[3]) 
    ...: for i in bonusRDD2.take(10) : print(i) 

('94660', 'Becton,Donald')
('94171', 'Jones,Donna')
('94479', 'Chalmers,Dorthy')
('94444', 'Spencer,Leila')
('94872', 'Laughlin,Anita')
('94264', 'Bridge,Stevie')
('94508', 'Eggers,David')
('94469', 'Koopman,Dorothy')
('94312', 'Kohl,Kara')
('94577', 'Nelson,Diane') 

2. Create a pair RDD with postal code as the key and a list of names (Last Name,First Name) in that postal code as the value.
• Hint: First name and last name are the 4 and 5 fields respectively.
• Optional: Try using the mapValues operation.

  ...: bonusRDD2 = bonusRDD.mapValues(lambda x: x[4] + ',' + x[3]) .groupByKey() 
  ...: for i in bonusRDD2.take(10) : print(i) 

('94143', <pyspark.resultiterable.ResultIterable object at 0x7f5faf9bf910>)     
('94980', <pyspark.resultiterable.ResultIterable object at 0x7f5faf9bf950>)
('94988', <pyspark.resultiterable.ResultIterable object at 0x7f5fafa29350>)
('94283', <pyspark.resultiterable.ResultIterable object at 0x7f5faf098710>)
('94430', <pyspark.resultiterable.ResultIterable object at 0x7f5faf9cfe50>)
('94360', <pyspark.resultiterable.ResultIterable object at 0x7f5faf9cf790>)
('94922', <pyspark.resultiterable.ResultIterable object at 0x7f5faf090950>)
('94139', <pyspark.resultiterable.ResultIterable object at 0x7f5faf9bfa50>)
('94441', <pyspark.resultiterable.ResultIterable object at 0x7f5faf093c90>)
('94815', <pyspark.resultiterable.ResultIterable object at 0x7f5faf180f10>)


3. Sort the data by postal code, then for the first five postal codes, display the code and list the names in that postal zone. 

for (x, y) in bonusRDD2.sortByKey().take(5):
   print ("\n---") ,x
   for i in y: print(i)

---
Allen,Harvey
Prinz,Daniel
Pascale,Robert
Brookes,Donna
Mackenzie,James
Chamberlain,Robert
Cunningham,Richard
Sewell,Bailey
Marin,Daniel


: # %load ppbonus 
    ...: sc 
    ...: filename = "/user/hive/warehouse/devsh.db/accounts" 
    ...: bonusRDD = sc.textFile(filename). map(lambda x : x.split(',')) . keyBy(lambda x : x[8]) 
    ...: bonusRDD2 = bonusRDD.mapValues(lambda x: x[4] + ',' + x[3]) .groupByKey() 
    ...: #for i in bonusRDD2.take(10) : print(i) 
    ...: for (x, y) in bonusRDD2.sortByKey().take(5): 
    ...:    print ("\n---") ,x 
    ...:    for i in y: print(i) 
    ...:                                                                                                                                      
                                                                                
---
Allen,Harvey
Prinz,Daniel
Pascale,Robert
Brookes,Donna
Mackenzie,James
Chamberlain,Robert
Cunningham,Richard
Sewell,Bailey
Marin,Daniel

scala> val accountsdata = "/user/hive/warehouse/devsh.db/accounts"
accountsdata: String = /user/hive/warehouse/devsh.db/accounts

scala> val accountsRDD = sc.textFile(accountsdata)
accountsRDD: org.apache.spark.rdd.RDD[String] = /user/hive/warehouse/devsh.db/accounts MapPartitionsRDD[21] at textFile at <console>:26

scala> for (line <- accountsRDD.take(2)) print(line)
1,2008-10-23 16:05:05.0,\N,Donald,Becton,2275 Washburn Street,Oakland,CA,94660,5100032418,2014-03-18 13:29:47.0,2014-03-18 13:29:47.02,2008-11-12 03:00:01.0,\N,Donna,Jones,3885 Elliott Street,San Francisco,CA,94171,4150835799,2014-03-18 13:29:47.0,2014-03-18 13:29:47.0

scala> val accounts1RDD = accountsRDD.map(s => s.split(','))
accounts1RDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[22] at map at <console>:25

scala> for (line <- accounts1RDD.take(2)) print(line)
[Ljava.lang.String;@bdb63c6[Ljava.lang.String;@5863bcee

val filename = "/user/hive/warehouse/devsh.db/accounts"
val bonusRDD = sc.textFile(filename). map(x => x.split(',')) . keyBy(x => x(8))
for (i <- bonusRDD.take(10)) print(i)

(94660,Becton,Donald)(94171,Jones,Donna)(94479,Chalmers,Dorthy)(94444,Spencer,Leila)(94872,Laughlin,Anita)(94264,Bridge,Stevie)(94508,Eggers,David)(94469,Koopman,Dorothy)(94312,Kohl,Kara)(94577,Nelson,Diane)

2. Create a pair RDD with postal code as the key and a list of names (Last Name,First Name) in that postal code as the value.
• Hint: First name and last name are the 4 and 5 fields respectively.
• Optional: Try using the mapValues operation.

scala>
val bonusRDD2 = bonusRDD.mapValues(x => x(4) + ',' + x(3)) .groupByKey()
for (i <- bonusRDD.take(10)) print(i)

(95461,CompactBuffer(Dorazio,Charles, Malec,Derrick, Toney,Carolyn, Wicker,Eileen, Hollier,Harold, Rush,Eddie, Copley,Elizabeth, Cowles,Terry, Stewart,Monique, Boston,Shana, Bradford,Robert, Evans,Michael, Walker,James, Strange,Michael, Benally,Josephine, Sanchez,Steven, Johnson,Rebecca))(94039,CompactBuffer(Gonzalez,Kurtis, Whitlow,Aldo, Lovell,Lynn, McDermott,James, Roberts,Leo, Scott,Randy, John,Arturo, Tapia,Kurt, Rosario,Benjamin, Clark,Roy, Conrad,Thomas, Pulido,Jill, Jackson,Robert, Jones,Robin, Meadows,Charles, McCrary,Marjorie, Figueroa,Janet, Evans,Raymond, Day,Kathy))(94372,CompactBuffer(Fong,Lisa, Burges,Frank, Grogan,Larry, Young,Phyllis, Joyce,Bridget, Rose,Richard, Morrison,Margie, Regan,Patty, Carrero,Lee, Brown,Thomas, Mason,Patrick, Milligan,Teresa, Caton,Carole, Calhoun,Beth, Lando,Christian))(97031,CompactBuffer(Madrigal,Tonya, Jorge,Kathleen, Jenkins,Nannie, Jones,Thomas, Askew,Justin, Kelly,Rose, Coleman,Josephine, Tran,Peter, Rose,Charles, Reinhardt,Shelley, Johnson,Robert, Dunn,Joaquin, Bair,Stephan, Miller,Marvin))(92888,CompactBuffer(Howe,Barbara, Tipton,Robert, Wilson,Cheryl, Cardwell,Dante, Vaughn,Andre, Rogers,Melody, Crandall,Elizabeth, Booth,Hannah, Smotherman,Jenny, Johnson,Rueben, Ryan,Douglas, White,Mark, Hopkins,Leslie, Meade,Deborah, Holland,Christine, Otten,Elias, Adames,Grace, Washington,Hollie, Obrien,Mitzi, Chance,Thomas, Rutan,Tamara))(92694,CompactBuffer(Larson,Lucius, Thomas,Luis, Reagan,Caroline, Musial,Clementine, Bailey,Christina, Rushing,Ruth, Fernandez,Sam, Singleton,Karen, Edwards,Roman, Hoagland,Max, Hall,Kathleen, Green,John, Merritt,Melvin, Beebe,James, Morris,Joe))(95452,CompactBuffer(Fernandez,Timothy, Childs,Amy, Raines,Hosea, Smith,Barbara, Duvall,Rigoberto, Morgan,Chris, Allbritton,Stacy, Grantham,Paula, Radke,Gloria, Bowman,Jose, Martinez,Bobbie, Hill,Teri, Payne,Gerard, Little,Cathy, Sloan,Annetta, Evans,Betty, Croteau,Andrew))(91587,CompactBuffer(Doyle,John, Ries,Joe, Hardin,Joe, Klahn,Christopher, Lindsay,Robert, Miguel,Martha, Parker,Louis, Hickmon,Billy, Cooper,Jessica, Hughes,Andrew, Hughes,Terry, Dillon,Matt, Pearson,Nita, Hansen,Pamela, Stanfill,Susan, Monks,Jackie))(93080,CompactBuffer(Dixon,Carlos, Austin,Elaine, Kornegay,Alfred, Ray,Sheila, Samuel,Archie, Gray,Sherry, Nino,Elbert, Burpo,Theresa, Meyer,Valentin, Simmons,Ruth, Vance,Kyle, Klein,Ricky, Shade,Helen, Capers,Kevin, Parker,Joe, Pittman,Amber))(91348,CompactBuffer(Valley,Genevieve, Alford,Travis, Llewellyn,Herman, Sill,Nathaniel, Sunderland,Barry, McCall,April, Elson,Lucas, Cole,Reggie, Henry,Anna, Chaput,Ruth, Viveiros,Garry, Carlson,Paul, Kestner,Gladys, Conti,Barbara, Collins,Jay, Broadnax,Randy, Ward,Bennie))

3. Sort the data by postal code, then for the first five postal codes, display the code and list the names in that postal zone. 

for (x <- bonusRDD2.sortByKey().take(5)) {
   println("---" + x._1)
   x._2.foreach(println)
}

---85000                                                                        
Allen,Harvey
Prinz,Daniel
Pascale,Robert
Brookes,Donna
Mackenzie,James
Chamberlain,Robert
Cunningham,Richard
Sewell,Bailey
Marin,Daniel
---85001
Mendelsohn,Frances
Watson,Mary
Brookover,Donald
Hathaway,Brandon
Leonard,Crystal
Moran,Carrie
Kirksey,Marie
Lance,Issac

Barnes,Vesta
Fiore,Eva
Tucker,Keith
Medford,Danielle
Spell,Norman
Soto,Shelley
Frantz,Kathy
Wilkins,Timothy
Snyder,Joseph
Flores,Delbert
Eakes,Gail
Daniels,Bert
Carpenter,Vincent


val filename = "/user/hive/warehouse/devsh.db/accounts"
val bonusRDD = sc.textFile(filename). map(x => x.split(',')) . keyBy(x => x(8))
val bonusRDD2 = bonusRDD.mapValues(x => x(4) + ',' + x(3)) .groupByKey()
for (x <- bonusRDD2.sortByKey().take(5)) {
   println("---" + x._1)
   x._2.foreach(println)
}

scala> :load ssbonus
Loading ssbonus...
filename: String = /user/hive/warehouse/devsh.db/accounts
bonusRDD: org.apache.spark.rdd.RDD[(String, Array[String])] = MapPartitionsRDD[22] at keyBy at ssbonus:26
bonusRDD2: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[24] at groupByKey at ssbonus:25
---85000                                                                        
Allen,Harvey
Prinz,Daniel
Pascale,Robert
Brookes,Donna
Mackenzie,James
Chamberlain,Robert
Cunningham,Richard
Sewell,Bailey
Marin,Daniel
---85001
Mendelsohn,Frances
Watson,Mary
Brookover,Donald
Hathaway,Brandon
Leonard,Crystal
Moran,Carrie
Kirksey,Marie


solucion del manual: /home/scelisdev02/training_materials/devsh/exercises/pair-rdds/bonus/solution/PairRDDsBonus.pyspark
                                                                                                   PairRDDsBonus.scalaspark
-----------------------------------------------
------------------
Hands-On Exercise: Querying Tables and Views with SQL
------------------
Show Tables and Columns Using the Catalog API

1. View the list of current Hive tables and temporary views in the devsh database

In [5]: for table in spark.catalog.listTables("devsh"): 
   ...:     print(table) 
   ...:      
Table(name='accounts', database='devsh', description=None, tableType='EXTERNAL', isTemporary=False)


ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used
+--------+--------+-----------+---------+-----------+
|    name|database|description|tableType|isTemporary|
+--------+--------+-----------+---------+-----------+
|accounts|   devsh|       null| EXTERNAL|      false|
+--------+--------+-----------+---------+-----------+

2. List the schema (column definitions) of the devsh.accounts table.

   ...: for i in spark.catalog.listColumns("accounts","devsh"): 
   ...:     print(i) 

Column(name='acct_num', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False)
Column(name='acct_create_dt', description=None, dataType='timestamp', nullable=True, isPartition=False, isBucket=False)
Column(name='acct_close_dt', description=None, dataType='timestamp', nullable=True, isPartition=False, isBucket=False)
Column(name='first_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)
Column(name='last_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)
Column(name='address', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)
Column(name='city', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)
Column(name='state', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)
Column(name='zipcode', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)
Column(name='phone_number', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)
Column(name='created', description=None, dataType='timestamp', nullable=True, isPartition=False, isBucket=False)
Column(name='modified', description=None, dataType='timestamp', nullable=True, isPartition=False, isBucket=False)

spark.catalog.listColumns("devsh","accounts").show

+--------------+-----------+---------+--------+-----------+--------+
|          name|description| dataType|nullable|isPartition|isBucket|
+--------------+-----------+---------+--------+-----------+--------+
|      acct_num|       null|      int|    true|      false|   false|
|acct_create_dt|       null|timestamp|    true|      false|   false|
| acct_close_dt|       null|timestamp|    true|      false|   false|
|    first_name|       null|   string|    true|      false|   false|
|     last_name|       null|   string|    true|      false|   false|
|       address|       null|   string|    true|      false|   false|
|          city|       null|   string|    true|      false|   false|
|         state|       null|   string|    true|      false|   false|
|       zipcode|       null|   string|    true|      false|   false|
|  phone_number|       null|   string|    true|      false|   false|
|       created|       null|timestamp|    true|      false|   false|
|      modified|       null|timestamp|    true|      false|   false|
+--------------+-----------+---------+--------+-----------+--------+

3. Create a new DataFrame based on the devsh.accounts table, and confirm that its schema matches that of the column list above.
    ...: myDF = spark.read.table("devsh.accounts") 
    ...: myDF.printSchema() 

  root
 |-- acct_num: integer (nullable = true)
 |-- acct_create_dt: timestamp (nullable = true)
 |-- acct_close_dt: timestamp (nullable = true)
 |-- first_name: string (nullable = true)
 |-- last_name: string (nullable = true)
 |-- address: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- zipcode: string (nullable = true)
 |-- phone_number: string (nullable = true)
 |-- created: timestamp (nullable = true)
 |-- modified: timestamp (nullable = true) 


val myDF = spark.read.table("devsh.accounts") 
myDF.printSchema

Perform a SQL Query on a Table

4. Create a new DataFrame by performing a simple SQL query on the devsh.accounts table. 
Confirm that the schema and data is correct.

   ...: myDF = spark.sql("SELECT * FROM devsh.accounts") 
   ...: myDF.printSchema() 
   ...: myDF.show(5) 

  root
 |-- acct_num: integer (nullable = true)
 |-- acct_create_dt: timestamp (nullable = true)
 |-- acct_close_dt: timestamp (nullable = true)
 |-- first_name: string (nullable = true)
 |-- last_name: string (nullable = true)
 |-- address: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- zipcode: string (nullable = true)
 |-- phone_number: string (nullable = true)
 |-- created: timestamp (nullable = true)
 |-- modified: timestamp (nullable = true) 


 +--------+-------------------+-------------+----------+---------+--------------------+-------------+-----+-------+------------+-------------------+-------------------+
|acct_num|     acct_create_dt|acct_close_dt|first_name|last_name|             address|         city|state|zipcode|phone_number|            created|           modified|
+--------+-------------------+-------------+----------+---------+--------------------+-------------+-----+-------+------------+-------------------+-------------------+
|       1|2008-10-23 16:05:05|         null|    Donald|   Becton|2275 Washburn Street|      Oakland|   CA|  94660|  5100032418|2014-03-18 13:29:47|2014-03-18 13:29:47|
|       2|2008-11-12 03:00:01|         null|     Donna|    Jones| 3885 Elliott Street|San Francisco|   CA|  94171|  4150835799|2014-03-18 13:29:47|2014-03-18 13:29:47|
|       3|2008-12-21 09:19:50|         null|    Dorthy| Chalmers|    4073 Whaley Lane|    San Mateo|   CA|  94479|  6506877757|2014-03-18 13:29:47|2014-03-18 13:29:47|
|       4|2008-11-28 00:08:09|         null|     Leila|  Spencer|    1447 Ross Street|    San Mateo|   CA|  94444|  6503198619|2014-03-18 13:29:47|2014-03-18 13:29:47|
|       5|2008-11-15 23:06:06|         null|     Anita| Laughlin|    2767 Hill Street|     Richmond|   CA|  94872|  5107754354|2014-03-18 13:29:47|2014-03-18 13:29:47|
+--------+-------------------+-------------+----------+---------+--------------------+-------------+-----+-------+------------+-------------------+-------------------+
only showing top 5 rows


val myDF = spark.sql("SELECT * FROM devsh.accounts") 
myDF.printSchema
myDF.show(5) 

5. Optional: Perform the equivalent query using the DataFrame API, 
and compare the schema and data in the results to those of the query above.

    ...: myDF2 = myDF.select("first_name","last_name") 
    ...: myDF2.printSchema() 
    ...: myDF2.show(5) 
    ...:  

val myDF2 = myDF.select("first_name","last_name")
myDF2.printSchema()
myDF2.show(5)

myDF2: org.apache.spark.sql.DataFrame = [first_name: string, last_name: string]
root
 |-- first_name: string (nullable = true)
 |-- last_name: string (nullable = true)

+----------+---------+
|first_name|last_name|
+----------+---------+
|    Donald|   Becton|
|     Donna|    Jones|
|    Dorthy| Chalmers|
|     Leila|  Spencer|
|     Anita| Laughlin|
+----------+---------+
only showing top 5 rows

Create and query a view
6. Create a DataFrame called accountDeviceDF based on the CSV files in /devsh_loudacre/accountdevice. 
(Be sure to use the headers and inferred schema to determine the column names and types.)

   ...: myDF = spark.read.option("header","true").option("inferSchema","true").csv("/devsh_loudacre/accountdevice") 
   ...: myDF.printSchema() 
root                                                                            
 |-- id: integer (nullable = true)
 |-- account_id: integer (nullable = true)
 |-- device_id: integer (nullable = true)
 |-- activation_date: long (nullable = true)
 |-- account_device_id: string (nullable = true)

val myDF = spark.read.option("header","true").option("inferSchema","true").csv("/devsh_loudacre/accountdevice")
myDF.printSchema()

7. Create a temporary view on the accountDeviceDF DataFrame called account_dev.

    myDF.createTempView("account_dev")

    scala> myDF.createOrReplaceTempView("account_dev")

8. Confirm the view was created correctly by listing the tables and views in the devsh database as you did earlier. 
Notice that the account_dev table type is TEMPORARY

In [32]: for i in spark.catalog.listTables("devsh"):  
    ...:     print(i) 
    ...:                                                                                                                                      
Table(name='accounts', database='devsh', description=None, tableType='EXTERNAL', isTemporary=False)
Table(name='account_dev', database=None, description=None, tableType='TEMPORARY', isTemporary=True)

scala> spark.catalog.listTables("devsh").show
+-----------+--------+-----------+---------+-----------+
|       name|database|description|tableType|isTemporary|
+-----------+--------+-----------+---------+-----------+
|   accounts|   devsh|       null| EXTERNAL|      false|
|account_dev|    null|       null|TEMPORARY|       true|
+-----------+--------+-----------+---------+-----------+

9. Using a SQL query, create a new DataFrame based on the first five rows of the account_dev view, and display the results.

In [34]: myDF = spark.sql("select * from account_dev limit 5").show()

+-----+----------+---------+---------------+--------------------+
|   id|account_id|device_id|activation_date|   account_device_id|
+-----+----------+---------+---------------+--------------------+
|48692|     32443|       29|  1393242509000|7351fed1-f344-4cd...|
|48693|     32444|        4|  1353649861000|6da22278-ff7a-461...|
|48694|     32445|        9|  1331819465000|cb993b85-6775-407...|
|48695|     32446|       43|  1336860950000|48ea2c09-a0df-4d1...|
|48696|     32446|       29|  1383650663000|4b49c0a6-d141-42e...|
+-----+----------+---------+---------------+--------------------+
only showing top 5 rows

val myDF = spark.sql("select * from account_dev limit 5").show

Use SQL to Join Two Tables
10. Join the devsh.accounts table and account_dev view using the account ID, and display the results. 
(Note that the SQL string in the command below must be entered on a single line in the Spark shell.)

myDF = spark.sql("SELECT acct_num, first_name, last_name, account_device_id FROM devsh.accounts JOIN account_dev ON acct_num = account_id")
myDF.show()

val myDF = spark.sql("SELECT acct_num, first_name, last_name, account_device_id FROM devsh.accounts JOIN account_dev ON acct_num = account_id")
myDF.show

11. Save nameDevDF as a Hive table in the devsh called name_dev (with the file path as /devsh_loudacre/name_dev).

"""
write a hive
scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/name_dev
"""
myDF.write.option("path","/devsh_loudacre/name_dev").saveAsTable("name_dev")
myDF.write.mode("append").option("path","/devsh_loudacre/name_dev").saveAsTable("name_dev")
"""
write a parquet
scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/name_dev_parquet
"""
myDF.write.save("/devsh_loudacre/name_dev_parquet")

write a hive
scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/name_dev
"""
myDF.write.mode("append").option("path","/devsh_loudacre/name_dev").saveAsTable("name_dev")
myDF.write.mode("overwrite").option("path","/devsh_loudacre/name_dev").saveAsTable("name_dev")
"""
write a parquet
scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/name_dev_parquet
"""
myDF.write.mode("overwrite").save("name_dev_parque")

12. Use the Catalog API to confirm that the table was created correctly with the right schema.

In [56]: for i in spark.catalog.listTables("devsh"): print(i)                                                                                 
Table(name='accounts', database='devsh', description=None, tableType='EXTERNAL', isTemporary=False)
Table(name='name_dev', database='devsh', description=None, tableType='EXTERNAL', isTemporary=False)
Table(name='account_dev', database=None, description=None, tableType='TEMPORARY', isTemporary=True)

In [57]: spark.sql("DESCRIBE devsh.name_dev").show()                                                                                          
+-----------------+---------+-------+
|         col_name|data_type|comment|
+-----------------+---------+-------+
|         acct_num|      int|   null|
|       first_name|   string|   null|
|        last_name|   string|   null|
|account_device_id|   string|   null|
+-----------------+---------+-------+

scala> spark.catalog.listTables("devsh").show

scala> spark.sql("DESCRIBE devsh.name_dev").show()

14. Optional: Exit and restart the shell and confirm that the account_dev temporary view is no longer available.

In [2]: for i in spark.catalog.listTables("devsh"): print(i)
Table(name='accounts', database='devsh', description=None, tableType='EXTERNAL', isTemporary=False)
Table(name='name_dev', database='devsh', description=None, tableType='EXTERNAL', isTemporary=False)

scala> spark.catalog.listTables("devsh").show
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used
+--------+--------+-----------+---------+-----------+
|    name|database|description|tableType|isTemporary|
+--------+--------+-----------+---------+-----------+
|accounts|   devsh|       null| EXTERNAL|      false|
|name_dev|   devsh|       null| EXTERNAL|      false|
+--------+--------+-----------+---------+-----------+

solucion del manual: /home/scelisdev02/training_materials/devsh/exercises/spark-sql 
-----------------------------------------------
------------------
Hands-On Exercise: Using Datasets in Scala
------------------

Explore Datasets Using Web Log Data
Find all the account IDs and the IP addresses from which those accounts logged in to the web site from Loudacre’s web log data.
1. Create a case class for account ID/IP address pairs.

scala> case class AccountIP (id: Int, ip: String)
defined class AccountIP

2. Create an RDD of AccountIP objects by using the web log data in / devsh_loudacre/weblogs. 
Split the data by spaces and use the first field as IP address and the third as account ID.

scala> val accountIPRDD = sc.
     |     textFile("/devsh_loudacre/weblogs").
     |     map(line => line.split(' ')).
     |     map(fields =>
     |     new AccountIP(fields(2).toInt,fields(0)))
accountIPRDD: org.apache.spark.rdd.RDD[AccountIP] = MapPartitionsRDD[36] at map at <console>:37

3. Create a Dataset of AccountIP objects using the new RDD.

scala> val accountIPDS = spark.createDataset(accountIPRDD)
accountIPDS: org.apache.spark.sql.Dataset[AccountIP] = [id: int, ip: string]

4. View the schema and data in the new Dataset.

scala> accountIPDS.printSchema
root
 |-- id: integer (nullable = false)
 |-- ip: string (nullable = true)

scala> accountIPDS.show
+-----+---------------+                                                         
|   id|             ip|
+-----+---------------+
|69827|      3.94.78.5|
|69827|      3.94.78.5|
|21475|   19.38.140.62|
|21475|   19.38.140.62|
| 2489| 129.133.56.105|
| 2489| 129.133.56.105|
| 4712|217.150.149.167|
| 4712|217.150.149.167|
| 4712|217.150.149.167|
| 4712|217.150.149.167|
|45922|  209.151.12.34|
|45922|  209.151.12.34|
|  144|  184.97.84.245|
|  144|  184.97.84.245|
|33908|   233.60.251.2|
|33908|   233.60.251.2|
|51340|160.134.139.204|
|51340|160.134.139.204|
|13392|  19.209.18.222|
|13392|  19.209.18.222|
+-----+---------------+
only showing top 20 rows

5. Compare the result types of a typed transformation—distinct—and an untyped transformation—groupBy/count.


scala> val distinctIPDS = accountIPDS.distinct
distinctIPDS: org.apache.spark.sql.Dataset[AccountIP] = [id: int, ip: string]

scala> distinctIPDS.printSchema
root
 |-- id: integer (nullable = false)
 |-- ip: string (nullable = true)


scala> distinctIPDS.show
+-----+---------------+                                                         
|   id|             ip|
+-----+---------------+
|59751|100.249.249.195|
|48040|   239.133.7.31|
|19940|   108.70.38.24|
| 5008|117.160.239.165|
|51366| 208.105.130.72|
|95580|    60.6.251.40|
|27216|   216.89.61.81|
|   80| 37.133.154.226|
|49330|   3.128.97.218|
|42259|  12.69.221.179|
|35186|207.168.188.124|
|58344|     20.7.9.198|
|91682|   73.240.165.1|
|44794|   6.200.40.248|
|99108| 16.115.252.127|
|26740|   79.71.179.66|
|  120| 129.253.238.61|
|   58|  178.1.172.126|
| 5950|  71.101.91.243|
|   36|   48.111.220.5|
+-----+---------------+
only showing top 20 rows

scala> val accountIPCountDS = distinctIPDS.groupBy("id","ip").count
accountIPCountDS: org.apache.spark.sql.DataFrame = [id: int, ip: string ... 1 more field]

scala> accountIPCountDS.printSchema
root
 |-- id: integer (nullable = false)
 |-- ip: string (nullable = true)
 |-- count: long (nullable = false)


scala> accountIPCountDS.show
+-----+---------------+-----+                                                   
|   id|             ip|count|
+-----+---------------+-----+
|59751|100.249.249.195|    1|
|48040|   239.133.7.31|    1|
|19940|   108.70.38.24|    1|
| 5008|117.160.239.165|    1|
|51366| 208.105.130.72|    1|
|95580|    60.6.251.40|    1|
|27216|   216.89.61.81|    1|
|   80| 37.133.154.226|    1|
|49330|   3.128.97.218|    1|
|42259|  12.69.221.179|    1|
|35186|207.168.188.124|    1|
|58344|     20.7.9.198|    1|
|91682|   73.240.165.1|    1|
|44794|   6.200.40.248|    1|
|99108| 16.115.252.127|    1|
|26740|   79.71.179.66|    1|
|  120| 129.253.238.61|    1|
|   58|  178.1.172.126|    1|
| 5950|  71.101.91.243|    1|
|   36|   48.111.220.5|    1|
+-----+---------------+-----+
only showing top 20 rows

6. Save the accountIPDS Dataset as a Parquet file, then read the file back into a DataFrame. 
Note that the type of the original Dataset (AccountIP) is not preserved, but the types of the columns are.

hdfs dfs -rm -R /devsh_loudacre/accountIPS
scala> accountIPDS.write.save("/devsh_loudacre/accountIPS")
                                                                                
scala> val accountIPDF = spark.read.load("/devsh_loudacre/accountIPS")
accountIPDF: org.apache.spark.sql.DataFrame = [id: int, ip: string]

scala> accountIPDF.printSchema
root
 |-- id: integer (nullable = true)
 |-- ip: string (nullable = true)


scala> accountIPDF.show
+------+---------------+
|    id|             ip|
+------+---------------+
|100277|  15.254.228.58|
|100277|  15.254.228.58|
| 32439|  86.18.208.153|
| 32439|  86.18.208.153|
|     6|  208.145.20.46|
|     6|  208.145.20.46|
|    67|   211.37.67.40|
|    67|   211.37.67.40|
|    72|    46.55.5.181|
|    72|    46.55.5.181|
| 32838| 17.131.173.177|
| 32838| 17.131.173.177|
|    42| 214.97.110.146|
|    42| 214.97.110.146|
| 36489| 179.75.101.241|
| 36489| 179.75.101.241|
|   162|126.245.125.242|
|   162|126.245.125.242|
|   198|   12.16.98.222|
|   198|   12.16.98.222|
+------+---------------+
only showing top 20 rows

solucion del manual: /home/scelisdev02/training_materials/devsh/exercises/datasets
-----------------------------------------------

Bonus Exercise
1. Create a view on the AccountIPDS Dataset, and perform a SQL query on the view.
What is the return type of the SQL query? Were column types preserved?

accountIPDS.createOrReplaceTempView("vaccount_ips")
val qryDF = spark.sql("SELECT *  FROM vaccount_ips")
qryDF.getClass
qryDF.printSchema

scala> spark.sql("SELECT *  FROM vaccount_ips")
res21: org.apache.spark.sql.DataFrame = [id: int, ip: string]

scala> val qryDF = spark.sql("SELECT *  FROM vaccount_ips")
qryDF: org.apache.spark.sql.DataFrame = [id: int, ip: string]

scala> qryDF.getClass
res22: Class[_ <: org.apache.spark.sql.DataFrame] = class org.apache.spark.sql.Dataset

scala> qryDF.printSchema
root
 |-- id: integer (nullable = false)
 |-- ip: string (nullable = true)

scala> qryDF.show
+-----+---------------+
|   id|             ip|
+-----+---------------+
|69827|      3.94.78.5|
|69827|      3.94.78.5|
|21475|   19.38.140.62|
|21475|   19.38.140.62|
| 2489| 129.133.56.105|
| 2489| 129.133.56.105|
| 4712|217.150.149.167|
| 4712|217.150.149.167|
| 4712|217.150.149.167|
| 4712|217.150.149.167|
|45922|  209.151.12.34|
|45922|  209.151.12.34|
|  144|  184.97.84.245|
|  144|  184.97.84.245|
|33908|   233.60.251.2|
|33908|   233.60.251.2|
|51340|160.134.139.204|
|51340|160.134.139.204|
|13392|  19.209.18.222|
|13392|  19.209.18.222|
+-----+---------------+
only showing top 20 rows

scala> val qryDF = spark.sql("SELECT count(*)  FROM vaccount_ips")
qryDF: org.apache.spark.sql.DataFrame = [count(1): bigint]

scala> qryDF.show
+--------+                                                                      
|count(1)|
+--------+
| 1079891|
+--------+

scala> val qryDF = spark.sql("SELECT DISTINCT id, ip FROM vaccount_ips")
qryDF: org.apache.spark.sql.DataFrame = [id: int, ip: string]

scala> qryDF.show
+-----+---------------+                                                         
|   id|             ip|
+-----+---------------+
|59751|100.249.249.195|
|48040|   239.133.7.31|
|19940|   108.70.38.24|
| 5008|117.160.239.165|
|51366| 208.105.130.72|
|95580|    60.6.251.40|
|27216|   216.89.61.81|
|   80| 37.133.154.226|
|49330|   3.128.97.218|
|42259|  12.69.221.179|
|35186|207.168.188.124|
|58344|     20.7.9.198|
|91682|   73.240.165.1|
|44794|   6.200.40.248|
|99108| 16.115.252.127|
|26740|   79.71.179.66|
|  120| 129.253.238.61|
|   58|  178.1.172.126|
| 5950|  71.101.91.243|
|   36|   48.111.220.5|
+-----+---------------+
only showing top 20 rows

scala> val qryDF = spark.sql("SELECT DISTINCT * FROM vaccount_ips")
qryDF: org.apache.spark.sql.DataFrame = [id: int, ip: string]

scala> qryDF.show
+-----+---------------+                                                         
|   id|             ip|
+-----+---------------+
|  193|  24.236.212.29|
|10970|    93.144.98.8|
|70280|  93.246.33.247|
|28703| 130.170.128.91|
|11288| 131.164.41.215|
| 3422|  218.62.175.59|
|65036|150.157.234.201|
|26561| 232.24.198.135|
|   83|  208.232.174.8|
|   76| 213.125.36.207|
|  189| 177.32.123.243|
|16105|   219.53.222.2|
|69670|   58.18.87.181|
|  132|  91.209.61.124|
|69464|  9.244.227.190|
|21294| 167.174.157.24|
|  203| 245.249.107.36|
|16401| 147.172.225.10|
|  102|  90.184.36.101|
|   79| 50.251.157.101|
+-----+---------------+
only showing top 20 rows

solucion del manual: /home/scelisdev02/training_materials/devsh/exercises/datasets/bonus/datasets-bonus.scalaspark
-----------------------------------------------
------------------
Hands-On Exercise: Writing, Configuring, and Running a Spark Application
------------------
The Spark application will take a single argument—a state code (such as CA). 
The program should read the data from the devsh.accounts Hive table and save the rows whose state column value matches the specified state code. 
Write the results to /devsh_loudacre/accounts_by_state/state-code (such as accounts_by_state/CA).

-- Write and Run a Spark Application in Python

2. A simple stub file to get you started has been provided in the exercise directory:
$DEVSH/exercises/spark-application/python-stubs/accounts-by-state.py. 
This stub imports the required Spark classes and sets up your main code block. Open the stub file in an editor.

# Python application stub
import sys

from pyspark.sql import SparkSession

if __name__ == "__main__":
  if len(sys.argv) < 2:
    print(sys.stderr, "Usage: accounts-by-state.py <state-code>")
    sys.exit()
    
  stateCode = sys.argv[1]

  print("TODO: Solution not yet implemented")


3. Create a SparkSession object using the following code:

  spark = SparkSession.builder.getOrCreate()

4. Optional: Change the application log level from INFO (the default) to WARN to reduce distracting output.

  spark.sparkContext.setLogLevel("WARN")

5. In the body of the program, load the devsh.accounts Hive table into a DataFrame. 
Select accounts where the state column value matches the string provided as the first argument to the application. 
Save the results to a directory called (where state-code is a string such as CA.) 
Use overwrite mode when saving the file so that you can re-run the application without needing to delete the directory.

  accountsDF = spark.read.table("devsh.accounts")

  accountsDF.where("state==stateCode").write.mode("overwrite").save("devsh_loudacre/accounts_by_state/" + stateCode)

  otra forma: 
  stateAccountsDF = accountsDF.where(accountsDF.state == stateCode)
  stateAccountsDF.write.mode("overwrite").save("/devsh_loudacre/accounts_by_state/" + stateCode)


6. At the end of the application, be sure to stop the Spark session:

  spark.stop()

7. If you have a Spark shell running in any terminal, exit the shell before running your application.

8. Run your application. In a terminal window, change to the exercise working directory, 
then run the program, passing the state code to select. 
cd $DEVSH/exercises/spark-application/
$ spark-submit python-stubs/accounts-by-state.py CA

For example, to select accounts in California, use the following command:
$ spark-submit  app-accounts-by-state.py CA

9. After the program completes, confirm the files were

hdfs dfs -ls /devsh_loudacre/accounts_by_stateCA/

-- Write and Run a Spark Application in Scala below.

A Maven project to get you started has been provided in the exercise directory:
$DEVSH/exercises/spark-application/accounts-by-state_project.
The first time you build a Scala Spark application in your environment, 
Maven must first download the Spark libraries from the Maven central repository. 
This can take several minutes, so before starting work on the exercise steps, 
compile the exercise project following the steps below. 
(Maven only needs to download the libraries the first time. It 

11. In a terminal window, change to the project directory. 
Be sure to enter the command line shown below as a single line.
$ cd $DEVSH/exercises/spark-application/accounts-by-state_project

12. Build the exercise project using Maven.
$ mvn package caches the libraries locally, and uses the cache for subsequent builds.)

scelisdev03@cluster-cca175-m:~/training_materials/devsh/exercises/spark-application/accounts-by-state_project$ sudo apt install maven 
scelisdev03@cluster-cca175-m:~/training_materials/devsh/exercises/spark-application/accounts-by-state_project$ mvn package

Maven will begin to download the required Spark libraries. 
Next time you build the project, Maven will used the libraries in its local cache. 
While Maven downloads the libraries, continue with the exercise steps below.

13. Edit the Scala class defined in stubs package in src/main/scala/stubs/AccountsByState.scala.
package stubs

import org.apache.spark.sql.SparkSession

object AccountsByState {
  def main(args: Array[String]) {
    if (args.length < 1) {
      System.err.println("Usage: stubs.AccountByState <state-code>")
      System.exit(1)
    }
 
    val stateCode = args(0)

    println("TODO: Solution not yet implemented")

  }
}

14. Create a SparkSession object using the following code:

    val spark = SparkSession.builder.getOrCreate()

15. Change the application log level from INFO (the default) to WARN to reduce distracting output.

    spark.sparkContext.setLogLevel("WARN")

16. In the body of the program, load the accounts Hive table into a DataFrame.
Select accounts where the state column value matches the string provided as the first argument to the application. 

Save the results to a Parquet file called /devsh_loudacre/accounts_by_state/state-code 
(where state-code is a string such as CA). 
Use overwrite mode when saving the file so that you can re-run the application without needing to delete the save directory.

    // Leer la tabla Hive
    val accountsDF = spark.read.table("devsh.accounts")

    // seleccionar los registros que coincidan con el argumento pasado
    // save el resultado
    // hdfs dfs -rm -R /devsh_loudacre/accounts_by_stateCA/

    accountsDF.where(accountsDF("state") === stateCode).write.mode("overwrite").save("/devsh_loudacre/accounts_by_state" + stateCode)

    // Otra forma
    /*
    val stateAccountsDF = accountsDF.where(accountsDF("state") === stateCode)
    stateAccountsDF.write.mode("overwrite").save("/devsh_loudacre/accounts_by_state/" + stateCode)
    */

17. At the end of the application, be sure to stop the Spark session:
  
  spark.stop

18. Return to the terminal in which you ran Maven earlier in order to cache Spark libraries locally. 
If the Maven command is still running, wait for it to finish. 
When it finishes, rebuild the project, this time including the code you added above. 
This time, the Maven command should take much less time.

  $ mvn package

If the build is successful, Maven will generate a JAR file called accounts-by-state-1.0.jar in the target directory.

19. If you have a Spark shell running in any terminal, exit the shell before running your application.

20. Run the program in the compiled JAR file, passing the state code to select. 
For example, to select accounts in California, use the following command:

cd $DEVSH/exercises/spark-application/accounts-by-state_project
mvn clean
mvn package
spark-submit --class stubs.AccountsByState target/accounts-by-state-1.0.jar CA

# For solution use 
# spark-submit --class solution.AccountsByState target/accounts-by-state-1.0.jar CA

hdfs dfs -ls /devsh_loudacre/accounts_by_state/CA/
hdfs dfs -get /devsh_loudacre/accounts_by_state/CA/ /tmp/accounts_CA
parquet-tools head /tmp/accounts_CA


Esta es la ejecución desde el cluster
scelisdev03@cluster-cca175-m:~/training_materials/devsh/exercises/spark-application/accounts-by-state_project$ spark-submit --class stubs.AccountsByState target/accounts-by-state-1.0.jar CA
21/10/20 16:20:21 INFO org.sparkproject.jetty.util.log: Logging initialized @2814ms to org.sparkproject.jetty.util.log.Slf4jLog
21/10/20 16:20:21 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_292-b10
21/10/20 16:20:21 INFO org.sparkproject.jetty.server.Server: Started @2929ms
21/10/20 16:20:21 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@36b6964d{HTTP/1.1, (http/1.1)}{0.0.0.0:39619}
21/10/20 16:20:22 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-cca175-m/10.128.0.2:8032
21/10/20 16:20:22 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at cluster-cca175-m/10.128.0.2:10200
21/10/20 16:20:23 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
21/10/20 16:20:23 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
21/10/20 16:20:24 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1634723533274_0010
21/10/20 16:20:25 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-cca175-m/10.128.0.2:8030
21/10/20 16:20:27 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used

Note: To run the solution application, use --class solution.AccountsByState.

$ spark-submit --class solution.AccountsByState target/accounts-by-state-1.0.jar CA

21. After the program completes, confirm the files were saved, then use parquet-tools to verify that the file contents are correct.

scelisdev02@cca175-m:~$ hdfs dfs -get /devsh_loudacre/accounts_by_stateCA) /tmp/

parquet-tools head /tmp/accounts_by_stateCA
parquet-tools schema /tmp/accounts_by_stateCA/ 

View the Spark Application UI - lo hice en los laboratorios
In the previous section, you ran a Python or Scala Spark application using spark-submit. 
Now view that application’s Spark UI (or history server UI if the application is complete).

22. Open Firefox on your remote desktop and visit the YARN Resource Manager UI (or go to URI http://localhost:8088/). 

While the application is running:
State: ACCEPTED
Final Status: UNDEFINED
Tracking UI: ApplicationMaster

After the application has completed
State: FINISHED
Final Status: SUCCEEDED
Tracking UI: History

Set Configuration Options Using Submit Script Flags
23. Change to the correct directory (if necessary) and re-run the Python or Scala program you wrote in the previous section, 
this time specifying an application name. 

ejecución en local sin nombre de aplicación - py
cd $DEVSH/exercises/spark-application/
$ spark-submit python-stubs/accounts-by-state.py CA

ejecución en local con nombre de aplicación - py
$ spark-submit --name "Accounts by State 1" python-stubs/accounts-by-state.py CA
Name: Accounts by State 1

ejecución en local sin nombre de aplicación - scala
cd $DEVSH/exercises/spark-application/accounts-by-state_project
$ spark-submit --class stubs.AccountsByState target/accounts-by-state-1.0.jar CA

ejecución en local con nombre de aplicación - scala
$ spark-submit --name "Accounts by State 1" --class stubs.AccountsByState target/accounts-by-state-1.0.jar CA
Name: Accounts by State 1

25. Follow the ApplicationMaster or History link. View the Environment tab. 
Take note of the spark.* properties such as master and app.name.

spark.app.name: Accounts by State 1
spark.submit.deployMode: client

26. You can set most of the common application properties using submit script flags such as name, but for others you need to use conf. 
Use conf to set the spark.default.parallelism property, which controls how many partitions result after a "wide" RDD operation like reduceByKey.

ejecución en local - py
$ spark-submit --name "Accounts by State 2" --conf spark.default.parallelism=4 python-stubs/accounts-by-state.py CA

ejecución en local - scala
$ spark-submit --name "Accounts by State 2" --conf spark.default.parallelism=4 --class stubs.AccountsByState target/accounts-by-state-1.0.jar CA

27. View the application history for this application to confirm that the spark.default.parallelism property was set correctly.

spark.default.parallelism: 4

Optional Review Property Setting Overrides

28. Rerun the previous submit command with the verbose option. This will display your application property default and override values.

ejecución en local - py
$ spark-submit --verbose --name "Accounts by State 3" --conf spark.default.parallelism=4 python-stubs/accounts-by-state.py CA

ejecución en local - scala
$ spark-submit --verbose --name "Accounts by State 3" --conf spark.default.parallelism=4 --class stubs.AccountsByState target/accounts-by-state-1.0.jar CA

29. Examine the extra output displayed when the application starts up.  

a. The first section starts with Using properties file, and shows the file name and the default property settings the application loaded from that properties file.

• What is the system properties file?
  /opt/cloudera/parcels/CDH-7.1.1-1.cdh7.1.1.p0.3266817/lib/spark/conf/spark-defaults.conf

• What properties are being set in the file?
  cat /opt/cloudera/parcels/CDH-7.1.1-1.cdh7.1.1.p0.3266817/lib/spark/conf/spark-defaults.conf

b. The second section starts with Parsed arguments. 

This lists the arguments —that is, the flags and settings—you set when running the submit script (except for conf). 
Submit script flags that you did not pass use their default values, if defined by the script, or are shown as null.

• Does the list correctly include the value you set with --name?
  mainClass: stubs.AccountsByState 
  primaryResource: file: /home/training_materials/devsh/exercises/spark-application/accounts-by-state_project/target/accounts-by-state-1.0.jar CA
  name: Accounts by State 3
  childArgs: [CA]
  verbose: true

• Which arguments (flags) have defaults set in the script and which do not?
  master: yarn
  deployMode: client
  supervise: false

c. Scroll down to the section that starts with System properties. 
This list shows all the properties set—those loaded from the system properties file, those you set using submit script arguments, 
and those you set using the conf flag.

• Is spark.default.parallelism included and set correctly? yes
(spark.default.parallelism,4)

solucion del manual: /home/scelisdev02/training_materials/devsh/exercises/spark-application
-----------------------------------------------

Bonus Exercise: Set Configuration Properties Programmatically

If you have more time, attempt the following extra bonus steps:

1. Edit the Python or Scala application you wrote above, and use the builder function appName to set the application name.

   spark = SparkSession.builder.getOrCreate()
   spark = SparkSession.builder.appName("Accounts by State 4").getOrCreate()

   val spark = SparkSession.builder.getOrCreate()
   val spark = SparkSession.builder.appName("Accounts by State 4").getOrCreate()

2. Re-run the application without using script options to set properties.
ejecución en local - py
$ spark-submit --verbose --conf spark.default.parallelism=4 python-stubs/accounts-by-state.py CA

ejecución en local - scala
$ spark-submit --verbose --conf spark.default.parallelism=4 --class stubs.AccountsByState target/accounts-by-state-1.0.jar CA

3. View the YARN UI to confirm that the application name was correctly set.

You can find the Python bonus solution in $DEVSH/exercises/spark-application/python-bonus. 

solucion del manual: /home/scelisdev02/training_materials/devsh/exercises/datasets/bonus/datasets-bonus.scalaspark
-----------------------------------------------




GCP - Cluster
Instance "cluster-cca175-w-1" is underutilized. 
You can save an estimated $23 per month by switching to the machine type: custom (2 vCPUs, 7.75 GB memory)


GCP - Martes 19/10/2021 Free trial status: €248.70 credit and 86 days remaining
GCP - Martes 19/10/2021 Free trial status: €246.60 credit and 86 days remaining
GCP - Miercoles 20/10/2021 Free trial status: €239.53 credit and 85 days remaining
GCP - Jueves 21/10/2021 Free trial status: €233.77 credit and 84 days remaining
GCP - Viernes 22/10/2021 Free trial status: €223.57 credit and 84 days remaining
                         Free trial status: €219.22 credit and 83 days remaining
GCP - Lunes 25/10/2021 Free trial status: €174.81 credit and 81 days remaining
                       Free trial status: €173.82 credit and 81 days remaining 
                       Free trial status: €171.62 credit and 80 days remaining
GCP - Martes 26/10/2021 Free trial status: €163.05 credit and 80 days remaining
GCP - Miercoles 03/11/2021 Free trial status: €145.96 credit and 71 days remaining
GCP - Jueves 04/11/2021 Free trial status: €138.54 credit and 70 days remaining
GCP - Viernes 05/11/2021 Free trial status: €132.03 credit and 69 days remaining

solucion del manual: /home/scelisdev02/training_materials/devsh/exercises/spark-application
-----------------------------------------------
OJO - ESTO ES DEL CAPITULO 14 QUE ESTA MAL ESCRITO Y DEBE SER: print( myRDD. todebugString(). decode() )

