========================================================================================================================================================================
https://cloud.google.com/
========================================================================================================================================================================
---------
concepts
---------

------------------
Starting the Environment - GCP - https://console.cloud.google.com/dataproc/clusters?project=cca175-325912
------------------

Crear Cluster
/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/CCA175/Crear clúster Spark en Google Cloud v2.pdf
/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/CCA175/Crear cluster Spark en google.pdf

New Project
Project: CCA175
ID: cca175-325912

Dataproc > Clusters 
Name: cca175 
Cluster UUID: 0a5e8c55-82bb-43d6-83f7-ef3d412fa54d 
Type: Dataproc Cluster 
Cloud Storage staging bucket: dataproc-staging-europe-west1-615171933800-1hr3y2vt 

/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/CCA175/Crear cluster Spark en google.pdf

instances:
https://console.cloud.google.com/compute/instances?project=cca175-325912
https://console.cloud.google.com/compute/instances?project=cca175-325912&cloudshell=true

Google Cloud Shell machine...shell 
https://shell.cloud.google.com/?hl=en_US&fromcloudshell=true&show=ide%2Cterminal
Welcome to Cloud Shell! Type "help" to get started.
Your Cloud Platform project in this session is set to cca175-325912.
Use “gcloud config set project [PROJECT_ID]” to change to a different project.
scelisdev02@cloudshell:~ (cca175-325912)$ $ sudo apt update && sudo apte upgrade


Compute Engine > VM instances > cca175-m > 10.132.0.3 > 34.77.42.105 > SSH

ssh in new browser
https://ssh.cloud.google.com/projects/cca175-325912/zones/europe-west1-b/instances/cca175-m?authuser=0&hl=en_US&projectNumber=615171933800&useAdminProxy=true&troubleshoot4005Enabled=true&troubleshoot255Enabled=true
Connected, host fingerprint: ssh-rsa 0 75:69:F5:20:FC:17:28:AD:FD:2E:6B:91:DB:90
:43:FD:DF:A6:00:BE:BA:00:D2:2B:70:2B:D0:38:80:38:3F:B9
Last login: Wed Sep 15 10:05:27 2021 from 35.235.243.225


--------------------------------------------------------------
Upload scelisdev.txt training_materials
-------------------------------------------------------------

scelisdev02@cca175-m:~$ pwd
/home/scelisdev02

scelisdev02@cca175-m:~$ cd.. 

scelisdev02@cca175-m:/home$ ls
dataproc  scelisdev02

scelisdev02@cca175-m:/home$ ls scelisdev02/
scelisdev.txt  training_materials

documentación:

CCA 159: https://www.cloudera.com/about/training/cdhhdp-certification/cca-data-analyst.html
CCA 175: https://www.cloudera.com/about/training/cdhhdp-certification/cca-spark.html/

Hadoop includes two general data processing engines: MapReduce & Spark

https://learning.oreilly.com/library/view/hadoop-application-architectures/9781491910313/

******************************************************************************
* -O-J-O- YARN (Yet Another Resource Negotiator) allocates and monitors cluster resources
******************************************************************************
YARN gestiona los recursos de un clúster Hadoop y programa las aplicaciones
Like HDFS, YARN runs on a master node and worker nodes

Worker nodes, ejecutan demonios NodeManager, gestionados por un ResourceManager en un nodo maestro
Las aplicaciones que se ejecutan en YARN constan de un ApplicationMaster y uno o más ejecutores

YARN = Yet Another Resource Negotiator
▪ YARN is the Hadoop processing layer that contains
─ A resource manager
─ A job scheduler

YARN Daemons
------------
▪ ResourceManager (RM)
─ Runs on master node
─ Global resource scheduler
─ Arbitrates system resources between competing applications
─ Has a pluggable scheduler to support different algorithms, such as Capacity or Fair Scheduler

▪ NodeManager (NM)
─ Runs on worker nodes
─ Communicates with RM
─ Manages node resources
─ Launches containers

Applications on YARN
▪ Containers
─ Containers allocate a certain amount of resources (memory, CPU cores) on a worker node
─ Applications run in one or more containers
─ Applications request containers from RM

▪ ApplicationMaster (AM)
─ One per application
─ Framework/application specific
─ Runs in a container
─ Requests more containers to run application tasks

▪ Each application consists of one or more containers
─ The ApplicationMaster runs in one container
─ The application’s distributed processes (JVMs) run in other containers
  ─ The processes run in parallel, and are managed by the AM
  ─ The processes are called executors in Apache Spark and tasks in Hadoop MapReduce
▪ Applications are typically submitted to the cluster from an edge or gateway node

▪ Developers need to be able to
─ Submit jobs (applications) to run on the YARN cluster
─ Monitor and manage jobs
▪ YARN tools for developers
─ The YARN web UI
─ The YARN command line interface

The ResourceManager web UI is the main entry point to the YARN UI
─ Runs on the RM host on port 8088 by default
▪ About
─ View status, metrics, and cluster details
▪ Node Labels
─ Groups of similarly configured nodes
▪ Nodes
─ List NodeManagers and statuses
─ Link to details such as applications and containers
▪ Applications
─ List and filter applications
─ Link to application details
▪ Scheduler
─ Details about resource distribution
▪ Tools
─ Configuration, YARN logs, server details

The YARN command line interface
scelisdev02@cca175-m:~$ yarn
scelisdev02@cca175-m:~$ yarn -help
scelisdev02@cca175-m:~$ yarn version
scelisdev02@cca175-m:~$ yarn application -list
scelisdev02@cca175-m:~$ yarn logs -applicationId application_1632225409574_0001
scelisdev02@cca175-m:~$ yarn application -list -appStates ALL
scelisdev02@cca175-m:~$ yarn application -status application_1632225409574_0006

The YARN command line interface
Command to configure and view information about the YARN cluster - View the full list of command options
scelisdev02@cca175-m:~$ yarn
ó
scelisdev02@cca175-m:~$ yarn -help

scelisdev02@cca175-m:~$ yarn version
Hadoop 2.10.1
Subversion https://bigdataoss-internal.googlesource.com/third_party/apache/hadoop -r f15c90a6ce7ee489e9e142ac72997d1e1e69b158
Compiled by bigtop on 2021-08-18T05:19Z
Compiled with protoc 2.5.0
From source with checksum 2550369dbba33b519ba32e45aa95cef
This command was run using /usr/lib/hadoop/hadoop-common-2.10.1.jar

List running applications
scelisdev02@cca175-m:~$ yarn application -list
21/09/21 13:17:46 INFO client.RMProxy: Connecting to ResourceManager at cca175-m/10.132.0.3:8032
21/09/21 13:17:46 INFO client.AHSProxy: Connecting to Application History server at cca175-m/10.132.0.3:10200
Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):1
                Application-Id      Application-Name        Application-Type          User       Queue                    State             Final-State             Progress                        Tracking-URL
application_1632225409574_0001          PySparkShell                   SPARK    scelisdev02    default                  RUNNING               UNDEFINED                  10% http://cca175-m.europe-west1-b.c.cca175-325912.internal:33441

Kill a running application
$ yarn application -kill app-id

View the logs of the specified application
$ yarn logs -applicationId app-id

scelisdev02@cca175-m:~$ yarn logs -applicationId application_1632225409574_0001
21/09/21 13:19:51 INFO client.RMProxy: Connecting to ResourceManager at cca175-m/10.132.0.3:8032
21/09/21 13:19:51 INFO client.AHSProxy: Connecting to Application History server at cca175-m/10.132.0.3:10200
Can not find any log file matching the pattern: [ALL] for the container: container_1632225409574_0001_01_000004 within the application: application_1632225409574_0001
End of LogType:prelaunch.err.
******************************************************************************

End of LogType:stdout.
***********************************************************************

Container: container_1632225409574_0001_01_000003 on cca175-w-0.europe-west1-b.c.cca175-325912.internal:8026
LogAggregationType: LOCAL
============================================================================================================
LogType:stderr
LogLastModifiedTime:Tue Sep 21 12:00:41 +0000 2021
LogLength:545
LogContents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
21/09/21 12:00:41 ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
End of LogType:stderr.
***********************************************************************


Container: container_1632225409574_0001_01_000003 on cca175-w-0.europe-west1-b.c.cca175-325912.internal:8026
LogAggregationType: LOCAL
============================================================================================================
LogType:prelaunch.out
LogLastModifiedTime:Tue Sep 21 11:59:34 +0000 2021
LogLength:70
LogContents:
Setting up env variables
Setting up job resources
Launching container
End of LogType:prelaunch.out.
******************************************************************************

End of LogType:prelaunch.err.This log file belongs to a running container (container_1632225409574_0001_01_000002) and so may not be complete.
******************************************************************************

End of LogType:stdout.This log file belongs to a running container (container_1632225409574_0001_01_000002) and so may not be complete.
***********************************************************************

Container: container_1632225409574_0001_01_000002 on cca175-w-1.europe-west1-b.c.cca175-325912.internal:8026
LogAggregationType: LOCAL
============================================================================================================
LogType:stderr
LogLastModifiedTime:Tue Sep 21 11:59:38 +0000 2021
LogLength:444
LogContents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
End of LogType:stderr.This log file belongs to a running container (container_1632225409574_0001_01_000002) and so may not be complete.
***********************************************************************

Container: container_1632225409574_0001_01_000002 on cca175-w-1.europe-west1-b.c.cca175-325912.internal:8026
LogAggregationType: LOCAL
============================================================================================================
LogType:prelaunch.out
LogLastModifiedTime:Tue Sep 21 11:59:36 +0000 2021
LogLength:70
LogContents:
Setting up env variables
Setting up job resources
Launching container
End of LogType:prelaunch.out.This log file belongs to a running container (container_1632225409574_0001_01_000002) and so may not be complete.
******************************************************************************

End of LogType:prelaunch.err.This log file belongs to a running container (container_1632225409574_0001_01_000001) and so may not be complete.
******************************************************************************

End of LogType:stdout.This log file belongs to a running container (container_1632225409574_0001_01_000001) and so may not be complete.
***********************************************************************

Container: container_1632225409574_0001_01_000001 on cca175-w-0.europe-west1-b.c.cca175-325912.internal:8026
LogAggregationType: LOCAL
============================================================================================================
LogType:stderr
LogLastModifiedTime:Tue Sep 21 11:59:33 +0000 2021
LogLength:1051
LogContents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
21/09/21 11:59:32 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cca175-m/10.132.0.3:8030
21/09/21 11:59:33 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
21/09/21 11:59:33 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
21/09/21 11:59:33 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE
21/09/21 11:59:33 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE
End of LogType:stderr.This log file belongs to a running container (container_1632225409574_0001_01_000001) and so may not be complete.
***********************************************************************

Container: container_1632225409574_0001_01_000001 on cca175-w-0.europe-west1-b.c.cca175-325912.internal:8026
LogAggregationType: LOCAL
============================================================================================================
LogType:prelaunch.out
LogLastModifiedTime:Tue Sep 21 11:59:28 +0000 2021
LogLength:70
LogContents:
Setting up env variables
Setting up job resources
Launching container
End of LogType:prelaunch.out.This log file belongs to a running container (container_1632225409574_0001_01_000001) and so may not be complete.


spark-submit \
$DEVSH/exercises/yarn/wordcount.py /devsh_loudacre/kb/*
------------------
Hands-On Exercise: Running and Monitoring a YARN Job
-----------------

******************************************************************************
* -O-J-O- Apache Spark
******************************************************************************

UDFs — User-Defined Functions

https://spark.apache.org/docs/3.2.0/sql-ref-functions.html#udfs-user-defined-functions

https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.udf.html?highlight=udf

https://spark.apache.org/docs/3.2.0/sql-ref-functions-udf-scalar.html#content

https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-udfs.html

User-Defined Functions (aka UDF) is a feature of Spark SQL to define new Column-based functions that extend the vocabulary of Spark SQL’s DSL for transforming Datasets.

PySpark Documentation
https://spark.apache.org/docs/latest/api/python/index.html


▪ Apache Spark es un marco de trabajo para analizar y procesar big data
Spark data processing engine Builds on the same “map-reduce” programming model as MapReduce developed at AMPLab Supports Scala, Java, Python, and R Has the same benefits as MapReduce, plus…
Improved performance using in-memory processing ─ Higher-level programming model to speed development
Spark built-in direct metastore access using HCatalog is a Hive sub-project that provides access to the metastore
─ Accessible through command line and REST API
─ Allows you to define and describe tables using Hive syntax
─ Enables integration with applications that lack direct metastore access

▪ Apache Spark es un motor rápido y de propósito general para el procesamiento de datos a gran escala
▪ Written in Scala - Escrito en Scala
─ Lenguaje de programación funcional que se ejecuta en una JVM
▪ Spark shell
─ Interactivo-para el aprendizaje, la exploración de datos o la analítica ad hoc
─ Python and Scala
▪ Spark applications
─ For large scale data processing
─ Python, Scala, and Java

The Spark Stack(pila)
▪ Spark provides a stack of libraries built on core Spark
─ Core Spark provides the fundamental Spark abstraction: Resilient Distributed Datasets (RDDs)
─ Spark SQL works with structured data
─ MLlib supports scalable machine learning
─ Spark Streaming applications process data in real time
─ GraphX works with graphs and graph-parallel computation

Spark SQL
▪ Spark SQL is a Spark library for working with structured data
Spark SQL provide
The DataFrame and Dataset API
─ The primary entry point for developing Spark applications
─ DataFrames and Datasets are abstractions for representing structured data
─ Catalyst Optimizer—an extensible optimization framework
─ A SQL engine and command line interface

The Spark Shell
▪ The Spark shell provides an interactive Spark environment
▪ Los shells Spark de Python y Scala son REPLs de línea de comandos para ejecutar Spark de forma interactiva
─ Often called a REPL, or (Read-Evaluate-Print Loop)
─ For learning, testing, data exploration, or ad hoc analytics
─ You can run the Spark shell using either Python or Scala
▪ You typically run the Spark shell on a gateway node
─ Las aplicaciones de Spark se ejecutan en modo batch fuera de la shell

Starting the Spark Shell
▪ On a Cloudera cluster y en GCP, the command to start the Spark shell is
─ pyspark for Python
─ spark-shell for Scala
▪ The Spark shell has a number of different start-up options, including
─ master: specify the cluster to connect to
─ jars: Additional JAR files
─ py-files: Additional Python files (Python only)
─ name: the name the Spark Application UI uses for this application
  ─ Defaults to PySparkShell (Python) or Spark shell (Scala)
─ help: Show all the available shell options

$ pyspark --name "My Application"

Spark Cluster Options
▪ Spark applications can run on these types of clusters
  ─ Apache Hadoop YARN
  ─ Kubernetes
  ─ Apache Mesos
  ─ Spark Standalone
▪ They can also run locally instead of on a cluster - También pueden ejecutarse localmente en lugar de en un clúster
▪ CDH, HDP, and CDP Data Center use YARN
▪ CDP Private Cloud and CDP Public Cloud use Kubernetes
▪ Specify the type or URL of the cluster using the master option

▪ Set the master option to specify cluster type
  ─ yarn
  ─ local[*] runs locally with as many threads as cores (default) - tantos hilos como nucleos
  ─ local[n] runs locally with n threads - se ejecuta localmente con n hilos
  ─ local runs locally with a single thread - se ejecuta localmente con un solo hilo

----Language: Python
─ pyspark for Python
$ pyspark 
$ pyspark --master yarn

Spark Session
▪ The main entry point for the Spark API is a Spark session
▪ The Spark shell provides a preconfigured SparkSession object called spark

In [1]: spark
Out [1]: <pyspark.sql.session.SparkSession at 0x1928b90>

Working with the Spark Session
▪ The SparkSession class provides functions and attributes to access all of Spark functionality
▪ Examples include
─ sql: execute a Spark SQL query
─ catalog: entry point for the Catalog API for managing tables
─ read: function to read data from a file or other data source
─ conf: object to manage Spark configuration settings
─ sparkContext: entry point for core Spark API

Log Levels

▪ Spark logs messages using Apache Log4J
▪ Messages are tagged (etiquetados), with their level

Available log levels are
─ TRACE
─ DEBUG
─ INFO (default level in Spark applications)
─ WARN (default level in Spark shell)
─ ERROR
─ FATAL
─ OFF

Setting the Log Level
▪ Set the log level for the current Spark shell using the Spark context setLogLevel method

> spark.sparkContext.setLogLevel("INFO")

DataFrames and Datasets
▪ Los DataFrames representan datos estructurados en forma tabular aplicando un esquema
▪ DataFrames and Datasets are the primary representation of data in Spark
▪ DataFrames represent structured data in a tabular (cuadros) form
  ─ DataFrames model data similar to tables in an RDBMS
  ─ DataFrames consist of a collection of loosely (suelto, flojo) typed Row objects
  ─ Rows are organized into columns described by a schema, se organizan por un esquema
▪ Datasets represent data as a collection of objects of a specified type
  ─ Datasets are strongly-typed—type checking is enforced at compile time rather than run time(fuertemente tipados)
    se comprueban en tiempo de compilación 
  ─ An associated schema maps object properties to a table-like structure of rows and columns
    Un esquema asociado asigna las propiedades de los objetos a una estructura de filas y columnas similar a una tabla
  ─ Datasets are only defined in Scala and Java
  ─ DataFrame is an alias for Dataset[Row]—Datasets containing Row objects
    DataFrame es un alias para Dataset[Row]-Conjuntos de datos que contienen objetos Row

DataFrames contain a collection of Row objects - contienen una colección de objetos Row
─ Rows contain an ordered collection of values - contienen una colección ordenada de valores
─  Los valores de las filas pueden ser tipos básicos(such as integers, strings, and floats)
  or colecciones de esos tipos (such as arrays and lists) 
─ A schema asigna los nombres y tipos de las columnas a los valores de una fila 

The users.json text file contains sample data
─ Each line contains a single JSON record that can include a name, age, and postal code field

scelisdev02@cca175-m:~$ cat users.json 
{"name":"Alice", "pcode":"94304"} 
{"name":"Brayden", "age":30, "pcode":"94304"} 
{"name":"Carla", "age":19, "pcode":"10036"} 
{"name":"Diana", "age":46}
{"name":"Étienne", "pcode":"94104"}

Create DataFrame

▪ Create a DataFrame using spark.read
▪ Returns the Spark session’s DataFrameReader
▪ Call json function to create a new DataFrame

> usersDF = \ spark.read.json("users.json")

▪ DataFrames always have an associated schema
▪ DataFrameReader can infer the schema from the data
▪ Use printSchema to show the DataFrame’s schema

> usersDF.printSchema()
root
|-- age: long (nullable = true)
|-- name: string (nullable = true)
|-- pcode: string (nullable = true)

▪ The show method displays the first few rows in a tabular format

> usersDF.show()
+----+-------+-----+
| age| name|pcode|
+----+-------+-----+
|null| Alice|94304|
| 30|Brayden|94304|
| 19| Carla|10036|
| 46| Diana| null|
|null|Etienne|94104|
+----+-------+-----+

DataFrame Operations
▪ There are two main types of DataFrame operations
  ─ Transformations create a new DataFrame based on existing one(s)
    ─ Transformations are executed in parallel by the application’s executors
  ─ Actions output data values from the DataFrame
    ─ Output is typically returned from the executors to the main Spark program (called the driver) or saved to a file

DataFrame Operations: Actions
▪ Some common DataFrame actions include
  ─ count: returns the number of rows
  ─ first: returns the first row (synonym for head())
  ─ take(n): returns the first n rows as an array (synonym for head(n))
  ─ show(n): display the first n rows in tabular form (default is 20 rows)
  ─ collect: returns all the rows in the DataFrame as an array
  ─ write: save the data to a file or other data source

> usersDF = spark.read.json("users.json")
> users = usersDF.take(3)
[Row(age=None, name=u'Alice', pcode=u'94304'),
 Row(age=30, name=u'Brayden', pcode=u'94304'),
 Row(age=19, name=u'Carla', pcode=u'10036')]

DataFrame Operations: Transformations
▪ Tipos de operaciones de los DataFrame
  ─ Las transformaciones crean nuevos DataFrames transformando los datos de los existentes
▪ Transformations create a new DataFrame based on an existing one
  ─ The new DataFrame may have the same schema or a different one
▪ Transformations do not return any values or data to the driver
  ─ Las acciones recogen valores en un DataFrame y los guardan o los devuelven a el controlador de Spark
  ─ Data remains distributed across the application’s executors
▪ DataFrames are immutable
  ─ Data in a DataFrame is never modified
  ─ Use transformations to create a new DataFrame with the data you need

▪ Common transformations include
  ─ select: only the specified columns are included
  ─ where: only rows where the specified expression is true are included (synonym for filter)
  ─ orderBy: rows are sorted by the specified column(s) (synonym for sort)
  ─ join: joins two DataFrames on the specified column(s)
  ─ limit(n): creates a new DataFrame with only the first n rows

Select and where Transformations
> nameAgeDF = usersDF.select("name","age")
> nameAgeDF.show()
> over20DF = usersDF.where("age > 20")
> over20DF.show()

Defining Queries
▪ Una consulta consiste en una secuencia de transformaciones seguida de una acción
▪ A sequence of transformations followed by an action is a query
> nameAgeDF = usersDF.select("name","age")
> nameAgeOver20DF = nameAgeDF.where("age > 20")
> nameAgeOver20DF.show()

Solution and example files
─ .pyspark: Python commands that can be copied into the PySpark shell
─ .scalaspark: Scala commands that can be copied into the Scala Spark shell
─ .py: Complete Python Spark applications
─ .scala: Complete Scala Spark applications

Working with DataFrames and Schemas

▪ Define a DataFrame schema through inference or programmatically
▪ Explain the difference between lazy(perezosa) and eager(ansiosa) query execution

Spark SQL supports a wide range of data source types and formats
─ Text files
  ─ CSV, JSON, plain text
─ Binary format files
  ─ Apache Parquet, Apache ORC, Apache Avro data format
─ Tables
  ─ Hive metastore, JDBC
─ Cloud
  ─ Such as Amazon S2 and Microsoft ADLS
▪ You can also use custom or third-party data source types

Creating a DataFrame from a Data Source
▪ spark.read returns a DataFrameReader object
▪ Use DataFrameReader settings to specify how to load data from the data source
  ─ format indicates the data source type, such as csv, json, or parquet (the default is parquet)
  ─ option specifies a key/value setting for the underlying data source
  ─ schema specifies a schema to use instead of inferring one from the data source
▪ Create the DataFrame based on the data source
  ─ load loads data from a file or files

Creating a DataFrame from a Data Source
▪ Example: Read a CSV text file
  ─ Treat the first line in the file as a header instead of data

https://spark.apache.org/docs/2.4.8/

Read a CSV text file
─ Treat the first line in the file as a header instead of data
myDF = spark.read. \
format("csv"). \
option("header","true"). \
load("/loudacre/myFile.csv")

Read an Avro file
myDF = spark.read. \
format("avro"). \
load("/loudacre/myData.avro")

DataFrameReader Convenience Functions
▪ You can call a format-specific load function for some formats
  ─ A shortcut instead of setting the format and using load
▪ The following two code examples are equivalent

spark.read.format("csv").load("/loudacre/myFile.csv")
spark.read.csv("/loudacre/myFile.csv")

Specifying Data Source File Locations
▪ You must specify a location when reading from a file data source
  ─ The location can be a single file, a list of files, a directory, or a wildcard
  ─ spark.read.json("myfile.json")
  ─ spark.read.json("mydata/")
  ─ spark.read.json("mydata/*.json")
  ─ spark.read.json("myfile1.json","myfile2.json")

▪ Files and directories are referenced by absolute or relative URI
  ─ Relative URI (uses default file system)
  ─ myfile.json
─ Absolute URI
  ─ hdfs://nnhost/loudacre/myfile.json
  ─ file:/home/training/myfile.json

Creating DataFrames from Hive Tables
▪ Apache Hive provides database-like access to data in HDFS
  ─ Applies schemas to HDFS files
  ─ Metadata is stored in the Hive metastore
▪ Spark can read from and write to Hive tables
  Infers the DataFrame schema from the Hive metadata
▪ Spark Hive support must be enabled and configured with location of the Hive warehouse in HDFS

usersDF = spark.read.table("users")

Creating DataFrames from Data in Memory

▪ You can also create DataFrames from a collection of in-memory data
  ─ Useful for testing and integrations

val mydata = List(("Josiah","Bartlet"),
                  ("Harry","Potter"))
val myDF = spark.createDataFrame(mydata)
myDF.show
+------+-------+
| _1| _2|
+------+-------+
|Josiah|Bartlet|
| Harry| Potter|
+------+-------+

Key DataFrameWriter Functions
▪ The DataFrame write function returns a DataFrameWriter
  ─ Saves data to a data source such as a table or set of files
  ─ Works similarly to DataFrameReader
▪ DataFrameWriter methods
  ─ format specifies a data source type
  ─ mode determines the behavior if the directory or table already exists
    ─ error, overwrite, append, or ignore (default is error)
  ─ partitionBy stores data in partitioned directories in the form column=value (as with Hive partitioning)
  ─ option specifies properties for the target data source
  ─ save saves the data as files in the specified directory
    ─ Or use json, csv, parquet, and so on 
  ─ saveAsTable saves the data to a Hive metastore table
    ─ Data location based on Hive warehouse default
    ─ Set path option to override location

Write data to a Hive metastore table called my_table
  ─ Append the data if the table already exists
  ─ Use an alternate location
    myDF.write. \
    mode("append"). \
    option("path","/loudacre/mydata"). \
    saveAsTable("my_table")

Write data as Parquet files in the mydata directory
    myDF.write.save("mydata")

Saving Data to Files
▪ When you save data from a DataFrame, you must specify a directory
  ─ Spark saves the data to one or more part- files in the directory
    devDF.write.csv("devices")

$ hdfs dfs -ls
devices
Found 4 items
-rw-r--r-- 3 training training
0 … devices/_SUCCESS
-rw-r--r-- 3 training training 2119 … devices/part-00000-e0fa6381-….csv
-rw-r--r-- 3 training training 2202 … devices/part-00001-e0fa6381-….csv
-rw-r--r-- 3 training training 2333 … devices/part-00002-e0fa6381-….csv

DataFrame Schemas
▪ Every DataFrame has an associated schema
  ─ Defines the names and types of columns
  ─ Immutable and defined when the DataFrame is created

myDF.printSchema()
root
|-- lastName: string (nullable = true)
|-- firstName: string (nullable = true)
|-- age: integer (nullable = true)

When creating a new DataFrame from a data source, the schema can be
─ Automatically inferred from the data source
─ Specified programmatically
When a DataFrame is created by a transformation, Spark calculates the new schema based on the query

Inferred Schemas
▪ Spark can load schemas from structured data, such as
─ Parquet, ORC, and Avro data files—schema is embedded in the file
─ Hive tables—schema is defined in the Hive metastore
─ Parent DataFrames
▪ Spark can also attempt to infer a schema from semi-structured data sources
─ For example, JSON and CSV

Inferring the Schema of a CSV File (No Header)
02134,Hopper,Grace,52
94020,Turing,Alan,32
94020,Lovelace,Ada,28
87501,Babbage,Charles,49
02134,Wirth,Niklaus,48

spark.read.option("inferSchema","true").csv("people.csv").printSchema()
root
|-- _c0: integer (nullable = true)
|-- _c1: string (nullable = true)
|-- _c2: string (nullable = true)
|-- _c3: integer (nullable = true)

Inferring the Schema of a CSV File (with Header)
pcode,lastName,firstName,age
02134,Hopper,Grace,52
94020,Turing,Alan,32
94020,Lovelace,Ada,28
87501,Babbage,Charles,49
02134,Wirth,Niklaus,48

spark.read.option("inferSchema","true").option("header","true").csv("people.csv").printSchema()
root
|-- pcode: integer (nullable = true)
|-- lastName: string (nullable = true)
|-- firstName: string (nullable = true)
|-- age: integer (nullable = true)

Inferred Schemas versus Manual Schemas
▪ Drawbacks to relying on Spark’s automatic schema inference
─ Inference requires an initial file scan, which may take a long time
─ The schema may not be correct for your use case
▪ You can define the schema manually instead
─ A schema is a StructType object containing a list of StructField objects
─ Each StructField represents a column in the schema, specifying
  ─ Column name
  ─ Column data type
  ─ Whether the data can be null (optional—the default is true)

Incorrect Schema Inference
This inferred schema for the CSV file is incorrect
─ The pcode column should be a string

spark.read.option("inferSchema","true"). \
option("header","true").csv("people.csv"). \
printSchema()
root
|-- pcode: integer (nullable = true)
|-- lastName: string (nullable = true)
|-- firstName: string (nullable = true)
|-- age: integer (nullable = true)

Defining a Schema Programmatically (Python)
Defining a Schema Programmatically (Scala)

Applying a Schema Manually
spark.read.option("header","true").
schema(peopleSchema).csv("people.csv").printSchema()
root
|-- pcode: string (nullable = true)
|-- lastName: string (nullable = true)
|-- firstName: string (nullable = true)
|-- age: integer (nullable = true)

Eager and Lazy Execution
▪ Operations are eager when they are executed as soon as the statement is reached in the code
  (se ejecutan tan pronto como la sentencia se se alcanza en el código)
▪ Operations are lazy when the execution occurs only when the result is referenced
  (la ejecución se produce sólo cuando el resultado es referenciado)
▪ Spark queries execute both lazily and eagerly
─ DataFrame schemas are determined eagerly (ansiosa)
─ Data transformations are executed lazily (perezosa)
▪ Lazy execution is triggered when an action is called on a series of transformations
  (se activa cuando se llama a una acción sobre una serie de transformaciones)

▪DataFrames puede cargarse desde y guardarse en varios tipos diferentes de fuentes de datos
─ Archivos de texto semiestructurados como CSV y JSON
─ Formatos binarios estructurados como Parquet, Avro y ORC
─ Tablas Hive y JDBC
▪ DataFrames puede inferir un esquema a partir de una fuente de datos, o puede definir uno manualmente
▪ Los esquemas de DataFrame se determinan de forma ansiosa (en el momento de la creación) pero las consultas se ejecutan de forma perezosa (cuando se llama a una acción)

https://spark.apache.org/docs/2.4.8/sql-programming-guide.html

https://spark.apache.org/docs/2.4.8/api/python/index.html

http://spark.apache.org/docs/2.4.0/api/scala/index.html#org.apache.spark.package

http://spark.apache.org/docs/2.4.0/api/scala/index.html#org.apache.spark.sql.package

http://spark.apache.org/docs/2.4.0/api/scala/index.html#org.apache.spark.sql.Dataset

Columns, Column Names, and Column Expressions
▪ Most DataFrame transformations require you to specify a column or columns
─ select(column1, column2, …)
─ orderBy(column1, column2, …)
▪ For many simple queries, you can just specify the column name as a string
─ peopleDF.select("firstName","lastName")
▪ Some types of transformations use column references or column expressions instead of column name strings

def select(col: String, cols:String*): DataFrame
    Select a set of columns.

def select(cols: Column*): DataFrame
    Select a set of columns based expressions.

Column References (Python)
Las columnas de un DataFrame se pueden especificar por nombre o por objetos de columna
  ─ Se pueden definir expresiones de columna utilizando operadores de columna
▪ In Python, there are two equivalent ways to refer to a column 
peopleDF = spark.read.option("header","true").csv("people.csv")

peopleDF['age']
Column<age>

peopleDF.age
Column<age>

peopleDF.select(peopleDF.age).show()
+---+
|age|
+---+
| 52|
| 32|
| 28|

Column Expressions
▪ Using column references instead of simple strings allows you to create column
expressions
▪ Column operations include
  ─ Arithmetic operators such as +, -, %, /, and *
  ─ Comparative and logical operators such as >, <, && and ||
    ─ The equality comparator is === in Scala, and == in Python
  ─ String functions such as contains, like, and substring
  ─ Data testing functions such as isNull, isNotNull, and NaN (not a number)
  ─ Sorting functions such as asc and desc
    ─ Work only when used in sort/orderBy
▪ For the full list of operators and functions, see the API documentation for Column

https://spark.apache.org/docs/2.4.8/api/python/pyspark.sql.html#pyspark.sql.Column

http://spark.apache.org/docs/2.4.0/api/scala/index.html#org.apache.spark.sql.Column

Column Expressions (Python)
peopleDF.select("lastName", peopleDF.age * 10).show()
+--------+----------+
|lastName|(age * 10)|
+--------+----------+
| Hopper| 520|
| Turing| 320|

>>> devDF.select("model", devDF.dev_type, devDF.dev_type*10)
>>> devDF.select("model", devDF.dev_type, devDF.dev_type*10).show()
>>> devDF.where(devDF.model.startswith("A")).show()
>>> devDF.where(devDF.model.startswith("")).show()

scala> devDF.select($"model", $"dev_type", $"dev_type"*10).show
scala> devDF.where(devDF("model").startsWith("")).show

Column Aliases 
▪ Use the column alias function to rename a column in a result set
  ─ name is a synonym for alias
>>> devDF.select("model", (devDF.model * 10).alias("model_10")).show()
scala> devDF.select($"model", ($"model" * 10).alias("model_10")).show

Aggregation Queries
▪ Aggregation queries perform a calculation on a set of values and return a single value
▪ To execute an aggregation on a set of grouped values, use groupBy combined with an aggregation function

>>> devDF.groupBy("dev_type").count().show()

The groupBy Transformation
▪ groupBy takes one or more column names or references
  ─ In Scala, returns a RelationalGroupedDataset object
  ─ In Python, returns a GroupedData object
▪ Returned objects provide aggregation functions, including
  ─ count
  ─ max and min
  ─ mean (and its alias avg)
  ─ sum
  ─ pivot
  ─ agg (aggregates using additional aggregation functions)

Additional Aggregation Functions
▪ The functions object provides several additional aggregation functions
▪ Aggregate functions include
  ─ first/last returns the first or last items in a group
  ─ countDistinct returns the number of unique items in a group
  ─ approx_count_distinct returns an approximate counts of unique items
    ─ Much faster than a full count
─ stddev calculates the standard deviation for a group of values
─ var_sample/var_pop calculates the variance for a group of values
─ covar_samp/covar_pop calculates the sample and population covariance of a group of values
─ corr returns the correlation of a group of values

Using the functions Object
▪ Calculate aggregate values on groups of rows using groupBy and an aggregation function
from pyspark.sql.functions import stddev
DF.groupBy("model").agg(stddev("devnum")).show()

Joining DataFrames
▪ Utilizar la operación de unión para unir dos DataFrames
▪ Use the join transformation to join two DataFrames
▪ DataFrames support several types of joins
  ─ inner (default)
  ─ outer
  ─ left_outer
  ─ right_outer
  ─ leftsemi
▪ The crossJoin transformation joins every element of one DataFrame with every element of the other

peopleDF.join(pcodesDF, "pcode").show()

En pyspark
from pyspark.sql.types import *
devColumns = [StructField("pcode",LongType()), StructField("lastName",StringType()), StructField("firstName",StringType()), StructField("age",LongType())]
devSchema = StructType(devColumns)
peopleDF = spark.read.schema(devSchema).csv("/devsh_loudacre/people-no-pcode.csv")
peopleDF.printSchema()
devColumns = [ StructField("pcode",LongType()), StructField("city",StringType()), StructField("state",LongType())]
devSchema = StructType(devColumns)
pcodesDF = spark.read.schema(devSchema).csv("/devsh_loudacre/pcodes.csv")
pcodesDF.printSchema()

En scala
import org.apache.spark.sql.types._
val devColumns = List(StructField("pcode",LongType), StructField("lastName",StringType), StructField("firstName",StringType), StructField("age",LongType))
val devSchema = StructType(devColumns)
val peopleDF = spark.read.schema(devSchema).csv("/devsh_loudacre/people-no-pcode.csv")
peopleDF.printSchema
val devColumns = List(StructField("pcode",LongType), StructField("city",StringType), StructField("state",LongType))
val devSchema = StructType(devColumns)
val pcodesDF = spark.read.schema(devSchema).csv("/devsh_loudacre/pcodes.csv")
pcodesDF.printSchema

Este join no me funciono
peopleDF.join(pcodesDF, "pcode")

A Left Outer Join 
▪ Specify type of join as inner (default), outer, left_outer, right_outer, or leftsemi

peopleDF.join(pcodesDF, "pcode", "left_outer").show()

Example: A Left Outer Join 
▪ Specify type of join as inner (default), outer, left_outer, right_outer, or leftsemi

peopleDF.join(pcodesDF, peopleDF("pcode") === pcodesDF("pcode"), "left_outer").show

Joining on Columns with Different Names
Use column expressions when the names of the join columns are different
  ─ The result includes both of the join columns

peopleDF.join(zcodesDF, $"pcode" === $"zip").show

Joining on Columns with Different Names 
peopleDF.join(zcodesDF, peopleDF.pcode == zcodesDF.zip).show()

Columns in a DataFrame can be specified by name or Column objects
  ─ You can define column expressions using column operators
▪ Use the join operation to join two DataFrames
  ─ Supports inner, left outer, right outer and semi joins

Los RDDs (Resilient Distributed Datasets) son un concepto clave en Spark
─ Representan una colección distribuida de elementos
─ Los elementos pueden ser de cualquier tipo

Resilient Distributed Datasets (RDDs)
▪ RDDs are part of core Spark
▪ Resilient Distributed Dataset (RDD)
  ─ Resilient: If data in memory is lost, it can be recreated
  ─ Distributed: Processed across the cluster
  ─ Dataset: Initial data can come from a source such as a file, or it can be created programmatically
▪ Despite the name, RDDs are not Spark SQL Dataset objects - A pesar del nombre, los RDDs no son objetos Spark SQL Dataset
  ─ RDDs predate Spark SQL and the DataFrame/Dataset API ─ Los RDDs son anteriores a Spark SQL y al API de DataFrame/Dataset

Comparing RDDs to DataFrames and Datasets

RDDs are unstructured
─ No schema defining columns and rows - No tienen un esquema asociado como los DataFrames y los Datasets
─ Not table-like; cannot be queried using SQL-like transformations such as where and select
─ RDD transformations use lambda functions
RDDs can contain any type of object
─ DataFrames are limited to Row objects
─ Datasets are limited to Row objects, case class objects (products), and primitive types

▪ RDDs are used in all Spark languages (Python, Scala, Java)
  ─ Strongly-typed in Scala and Java, like Datasets
▪ RDDs are not optimized by the Catalyst optimizer
  ─ Manually coded RDDs are typically less efficient than DataFrames
▪ You can use RDDs to create DataFrames and Datasets
  ─ RDDs are often used to convert unstructured or semi-structured data into structured form
  ─ You can also work directly with the RDDs that underlie DataFrames and Datasets ─ También se puede trabajar directamente con los RDDs que subyacen a los DataFrames y a los Datasets

  RDD Data Types
▪ RDDs can hold any serializable type of element
  ─ Primitive types such as integers, characters, and booleans
  ─ Collections such as strings, lists, arrays, tuples, and dictionaries (including nested collection types)
  ─ Scala/Java Objects (if serializable)
  ─ Mixed types
▪ Some RDDs are specialized and have additional functionality
  ─ Pair RDDs
    ─ RDDs consisting of key-value pairs
  ─ Double RDDs
    ─ RDDs consisting of numeric data

RDD Data Sources
Los RDDs se crean a partir de fuentes de datos
  ─ Archivos de texto y otros formatos de archivos de datos
  ─ Datos en otros RDDs, Datos en la memoria ─ DataFrames y Datasets
▪ There are several types of data sources for RDDs
  ─ Files, including text files and other formats
  ─ Data in memory
  ─ Other RDDs
  ─ Datasets or DataFrames

Creating RDDs from Files
▪ Use SparkContext object, not SparkSession
  ─ SparkContext is part of the core Spark library
  ─ SparkSession is part of the Spark SQL library
  ─ One Spark context per application
  ─ Use SparkSession.sparkContext to access the Spark context
  ─ Called sc in the Spark shell

--O-J-O--
SparkSession = DataFrames
sparkContext = RDD
sc = SparkSession.sparkContext

RDD no tiene modo "overwrite" para los archivos SIEMPRE se tienen que volver a crear... (es decir revisar si existen y borrar previamente)
DF tiene modo "overwrite"
DF tiene que comprobar el schema

  ─ Use SparkSession.sparkContext to access the Spark context
▪ Create file-based RDDs using the Spark context
  ─ Use textFile or wholeTextFiles to read text files
  ─ Use hadoopFile or newAPIHadoopFile to read other formats
    ─ The Hadoop “new API” was introduced in Hadoop .20
    ─ Spark supports both for backward compatibility
    

▪ SparkContext.textFile reads newline-terminated text files
  ─ Accepts a single file, a directory of files, a wildcard list of files, or a comma-separated list of files
    ─ Examples
      ─ textFile("myfile.txt")
      ─ textFile("mydata/")
      ─ textFile("mydata/*.log")
      ─ textFile("myfile1.txt,myfile2.txt")

myRDD = spark.sparkContext.textFile("mydata/")

▪ textFile maps each line in a file to a separate RDD element - asigna cada línea de un archivo a un elemento RDD independiente
     ─ Only supports newline-terminated text

Multi-line Text Elements
▪ textFile maps each line in a file to a separate RDD element
  ─ What about files with a multi-line input format, such as XML or JSON?
▪ Use wholeTextFiles
  ─ Maps entire contents of each file in a directory to a single RDD element ─ Asigna todo el contenido de cada archivo de un directorio a un único elemento RDD
  ─ Works only for small files (element must fit in memory) ─ Sólo funciona con archivos pequeños (el elemento debe caber en la memoria)

Using wholeTextFiles
userRDD = spark.sparkContext.wholeTextFiles("userFiles")

Creating RDDs from Collections
▪ You can create RDDs from collections instead of files
  ─ SparkContext.parallelize(collection) 
▪ Useful when
  ─ Testing
  ─ Generating data programmatically
  ─ Integrating with other systems or libraries
  ─ Learning

myData = ["Alice","Carlos","Frank","Barbara"]
myRDD = sc.parallelize(myData)

Saving RDDs
▪ You can save RDDs to the same data source types supported for reading RDDs
  ─ Use RDD.saveAsTextFile to save as plain text files in the specified directory
  ─ Use RDD.saveAsHadoopFile or saveAsNewAPIHadoopFile with a specified Hadoop OutputFormat to save using other formats
▪ The specified save directory cannot already exist
myRDD.saveAsTextFile("mydata/")

RDD Operations
▪ Two general types of RDD operations
  ─ Actions return a value to the Spark driver or save data to a data source - devuelven un valor de un RDD al controlador de Spark
  ─ Transformations define a new RDD based on the current one(s) - definen un nuevo RDD a partir de uno existente
▪ RDDs operations are performed lazily
  ─ Actions trigger execution of the base RDD transformations - acciones desencadenan la ejecución de las transformaciones 
  Las transformaciones de RDD utilizan la programación funcional

RDD Action Operations
▪ Some common actions
  ─ count returns the number of elements
  ─ first returns the first element
  ─ take(n) returns an array (Scala) or list (Python) of the first n elements
  ─ collect returns an array (Scala) or list (Python) of all elements
  ─ saveAsTextFile(dir) saves to text files

myRDD = sc.textFile("purplecow.txt")
for line in myRDD.take(2):
  print(line)

RDD Transformation Operations - Las transformaciones de RDD utilizan la programación funcional

▪ Transformations create a new RDD from an existing one
▪ RDDs are immutable
  ─ Data in an RDD is never changed
  ─ Transform data to create a new RDD
▪ A transformation operation executes a transformation function
  ─ The function transforms elements of an RDD into new elements
  ─ Some transformations implement their own transformation logic
  ─ For many, you must provide the function to perform the transformation
▪ Transformation operations include
  ─ distinct creates a new RDD with duplicate elements in the base RDD removed
  ─ union(rdd) creates a new RDD by appending the data in one RDD to another
  ─ map(function) creates a new RDD by performing a function on each record in the base RDD
  ─ filter(function) creates a new RDD by including or excluding each record in the base RDD according to a Boolean function

distinctRDD = sc.textFile("cities1.csv").distinct()
for city in distinctRDD.collect():
    print(city)

Boston,MA
Palo Alto,CA
Santa Fe,NM

unionRDD = sc.textFile("cities2.csv").union(distinctRDD)
for city in unionRDD.collect():
    print(city)

Calgary,AB
Chicago,IL
Palo Alto,CA
Boston,MA
Palo Alto,CA
Santa Fe,NM

Transforming Data with RDDs
Functional Programming in Spark
▪ Key concepts of functional programming
  ─ Functions are the fundamental unit of programming
  ─ Functions have input and output only
    ─ No state or side effects
  ─ Functions can be passed as arguments to other functions
    ─ Called procedural parameters
▪ Spark’s architecture is based on functional programming
  ─ Passed functions can be executed by multiple executors in parallel
La arquitectura de Spark se basa en la programación funcional
  ─ Las funciones pasadas pueden ser ejecutadas por múltiples ejecutores en paralelo

RDD Transformation Procedures
▪ RDD transformations execute a transformation procedure
  ─ Transforms elements of an RDD into new elements
  ─ Runs on executors
▪ A few transformation operations implement their own transformation logic
  ─ Examples: distinct and union
▪ Most transformation operations require you to pass a function
  ─ Function implements your own transformation procedure
  ─ Examples: map and filter
▪ This is a key difference between RDDs and DataFrame/Datasets

Passing Functions
▪ Passed functions can be named or anonymous - pueden ser nombradas o anónimas
▪ Anonymous functions are defined inline without an identifier - Las anónimas se definen en línea sin un identificador
─ Best for short, one-off functions
─ Supported in many programming languages
─ Python: lambda x: …
─ Scala: x => …
─ Java 8: x -> …

Passing Named Functions (Python)

def toUpper(s):
  return s.upper()

myRDD = sc.textFile("purplecow.txt")

myUpperRDD = myRDD.map(toUpper)

for line in myUpperRDD.take(2):
  print(line)

I'VE NEVER SEEN A PURPLE COW.
I NEVER HOPE TO SEE ONE;

Passing Named Functions (Scala)

def toUpper(s: String):
    String = { s.toUpperCase }
val myRDD = sc.textFile("purplecow.txt")
val myUpperRDD = myRDD.map(toUpper)
myUpperRDD.take(2).foreach(println)

I'VE NEVER SEEN A PURPLE COW.
I NEVER HOPE TO SEE ONE;

El resultado visual sería:

myRDD                                   myUpperRDD
+-------------------------------+       +-------------------------------+
| I've never seen a purple cow. |       | I'VE NEVER SEEN A PURPLE COW. |
+-------------------------------+       +-------------------------------+
| I never hope to see one;      |       | I NEVER HOPE TO SEE ONE;      |
+-------------------------------+       +-------------------------------+
| But I can tell you, anyhow,   |       | But I can tell you, anyhow,   |
+-------------------------------+       +-------------------------------+
| I'd rather see than be one.   |       | I'd rather see than be one.   |
+-------------------------------+       +-------------------------------+

Passing Anonymous Functions (Python)
▪ Python: Use the lambda keyword to specify the name of the input
  parameter(s) and the function that returns the output

myUpperRDD = myRDD.map(lambda line: line.upper())

Passing Anonymous Functions (Scala)
Use the => operator to specify the name of the input parameter(s) and the function that returns the output
val myUpperRDD = myRDD.map(line => line.toUpperCase)

Use underscore (_) to stand for anonymous input parameters
val myUpperRDD = myRDD.map(_.toUpperCase)

map and filter Transformations (Python)
myFilteredRDD = myRDD.map(lambda line: line.upper()).filter(lambda line: line.startswith('I'))

map and filter Transformations (Scala)
val myFilteredRDD = myRDD.map(line => line.toUpperCase).filter(line => line.startsWith("I"))

my????????                              myFilteredRDD
+-------------------------------+       +-------------------------------+
| I'VE NEVER SEEN A PURPLE COW. |       | I'VE NEVER SEEN A PURPLE COW. |
+-------------------------------+       +-------------------------------+
| I NEVER HOPE TO SEE ONE;      |       | I NEVER HOPE TO SEE ONE;      |
+-------------------------------+       +-------------------------------+
| BUT I CAN TELL YOU, ANYHOW,   |       | I'D RATHER SEE THAN BE ONE.   |
+-------------------------------+       +-------------------------------+
| I'D RATHER SEE THAN BE ONE.   |       
+-------------------------------+       

RDD Execution
▪ An RDD query consists of a sequence of one or more transformations completed by an action
  las transformaciones no se ejecutan hasta que son desencadenadas por una acción
▪ RDD queries are executed lazily
  ─ When the action is called
▪ RDD queries are executed differently than DataFrame and Dataset queries
  ─ DataFrames and Datasets scan their sources to determine the schema eagerly (when created)
  ─ RDDs do not have schemas and do not scan their sources before loading

RDD Lineage
▪ Transformations create a new RDD based on one or more existing RDDs
  ─ Result RDDs are considered children of the base (parent) RDD
  ─ Child RDDs depend on their parent RDD
▪ An RDD’s lineage is the sequence of ancestor RDDs that it depends on
  ─ When an RDD executes, it executes its lineage starting from the source
▪ Spark maintains each RDD’s lineage
  ─ Use toDebugString to view the lineage

RDD Lineage and toDebugString (Scala)

val myFilteredRDD = sc.textFile("purplecow.txt").map(line => line.toUpperCase).filter(line => line.startsWith("I"))

myFilteredRDD.toDebugString
(2) MapPartitionsRDD[7] at filter …
 |  MapPartitionsRDD[6] at map …
 |  purplecow.txt
    MapPartitionsRDD[5]
    at textFile …
|   purplecow.txt HadoopRDD[4]
    at textFile …


RDD Lineage and toDebugString (Python)
▪ toDebugString output is not displayed as nicely in Python shell
myFilteredRDD.toDebugString()
(2) PythonRDD[7] at RDD at PythonRDD.scala:48 []\n |
 purplecow.txt MapPartitionsRDD[6] … []\n | purplecow.txt
 HadoopRDD[5] at textFile … []

▪ Use print for prettier output
print myFilteredRDD.toDebugString()
(2) PythonRDD[7] at RDD at PythonRDD.scala:48 []
 |  purplecow.txt MapPartitionsRDD[6] at textFile …
 |  purplecow.txt HadoopRDD[5] at textFile …

Pipelining (tubo - encadenadas - una tras otra)
▪ Las operaciones sobre el mismo elemento RDD se canalizan juntas si es posible
▪ When possible, Spark will perform sequences of transformations by element so no data is stored
(Scala)
val myFilteredRDD = sc.textFile("purplecow.txt").map(line => line.toUpperCase).filter(line => line.startsWith("I"))
myFilteredRDD.take(2)

I'VE NEVER SEEN A PURPLE COW.
I NEVER HOPE TO SEE ONE;

Converting RDDs to DataFrames
▪ You can create a DataFrame from an RDD
  ─ Useful with unstructured or semi-structured data such as text
  ─ Define a schema
  ─ Transform the base RDD to an RDD of Row objects (Scala) or lists (Python)
  ─ Use SparkSession.createDataFrame
▪ You can also return the underlying RDD of a DataFrame
  ─ Use the DataFrame.rdd attribute to return an RDD of Row objects

Create a DataFrame from an RDD
▪ Example data: semi-structured text data source

02134,Hopper,Grace,52
94020,Turing,Alan,32
94020,Lovelace,Ada,28
87501,Babbage,Charles,49
02134,Wirth,Niklaus,48

(Scala)
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
val mySchema = StructType(Array( StructField("pcode", StringType), StructField("lastName", StringType), StructField("firstName", StringType), StructField("age", IntegerType)))
val rowRDD = sc.textFile("people.txt").map(line => line.split(",")).map(values => Row(values(0),values(1),values(2),values(3).toInt))
val myDF = spark.createDataFrame(rowRDD,mySchema)
myDF.show(2)

(Python) 
from pyspark.sql.types import *
mySchema = StructType([ StructField("pcode",StringType()), StructField("lastName",StringType()), StructField("firstName",StringType()), StructField("age",IntegerType())])
myRDD = sc.textFile("people.txt").map(lambda line: line.split(",")).map(lambda values: [values[0],values[1],values[2],int(values[3])])
myDF = spark.createDataFrame(myRDD,mySchema)
myDF.show(2)

Return a DataFrame’s Underlying RDD (Python)

myRDD2 = myDF.rdd
for row in myRDD2.take(2): print(row)
    Row(pcode=u'02134', lastName=u'Hopper', firstName=u'Grace', age=52)
    Row(pcode=u'94020', lastName=u'Turing', firstName=u'Alan', age=32)


Return a DataFrame’s Underlying RDD (Scala)

val myRDD2 = myDF.rdd
myRDD2.take(2).foreach(println)

[02134,Hopper,Grace,52]
[94020,Turing,Alan,32]

Aggregating Data with Pair RDDs

▪ Pair RDDs are a special form of RDD consisting of key-value pairs (tuples) - pares clave-valor (tuplas)
▪ Pair RDDs are a special form of RDD
  ─ Each element must be a key/value pair (a two-element tuple)
  ─ Keys and values can be any type
▪ Why?
  ─ Use with map-reduce algorithms
  ─ Many additional functions are available for common data processing needs: Such as sorting, joining, grouping, and counting

+-------------------------------+
|          (key1,value1)        |
+-------------------------------+
|      (key1,value1)            |
+-------------------------------+
|      (key1,value1)            |
+-------------------------------+
|         (key1,value1)         |
+-------------------------------+

Creating Pair RDDs
▪ The first step in most workflows is to get the data into key/value form
  ─ What should the RDD should be keyed on?
  ─ What is the value?
▪ Commonly used functions to create pair RDDs
  ─ map
  ─ flatMap/flatMapValues
  ─ keyBy

A Simple Pair RDD (Python)
▪ Example: Create a pair RDD from a tab-separated file
usersRDD = sc.textFile("userlist.tsv").map(lambda line: line.split('\t')). map(lambda fields: (fields[0],fields[1]))

user001\tFred Flintstone
user090\tBugs Bunny
user111\tHarry Potter

+-------------------------------+
|("user001","Fred Flintstone")  |
+-------------------------------+
| ("user090","Bugs Bunny")      |
+-------------------------------+
| ("user111","Harry Potter")    |
+-------------------------------+

A Simple Pair RDD (Scala)
▪ Example: Create a pair RDD from a tab-separated file

val usersRDD = sc.textFile("userlist.tsv").map(line => line.split('\t').map(fields => (fields(0),fields(1)))

Keying Web Logs by User ID (Python)
sc.textFile("weblogs/").keyBy(lambda line: line.split(' ')[2])

56.38.234.188 - 99788 "GET /KBDOC-00157.html http/1.0" …
56.38.234.188 - 99788 "GET /theme.css http/1.0" …
203.146.17.59 - 25254 "GET /KBDOC-00230.html http/1.0" …

+-----------------------------------------------------+
|(99788,56.38.234.188 - 99788 "GET /KBDOC-00157.html…)|
+-----------------------------------------------------+
|(99788,56.38.234.188 - 99788 "GET /theme.css…)       |
+-----------------------------------------------------+
|(25254,203.146.17.59 - 25254 "GET /KBDOC-00230.html…)|
+-----------------------------------------------------+

Pairs with Complex Values
▪ How would you do this?
─ Input: a tab-delimited list of postal codes with latitude and longitude
─ Output: postal code (key) and lat/long pair (value)

00210\t43.00589\t-71.01320
01014\t42.17073\t-72.60484
01062\t42.32423\t-72.67915
01263\t42.3929\t-73.22848

+-------------------------------+
|("00210",(43.00589,-71.01320)) |
+-------------------------------+
|("01014",(42.17073,-72.60484)) |
+-------------------------------+
|("01062",(42.32423,-72.67915)) |
+-------------------------------+
|("01263",(42.3929,-73.22848))  |
+-------------------------------+

Pairs with Complex Values (Python)

sc.textFile("latlon.tsv").map(lambda line: line.split('\t')).map(lambda fields: (fields[0],(float(fields[1]),float(fields[2]))))

Mapping Single Elements to Multiple Pairs
▪ How would you do this?
─ Input: order numbers with a list of SKUs in the order
─ Output: order (key) and sku (value)

00001 sku010:sku933:sku022
00002 sku912:sku331
00003 sku888:sku022:sku010:sku594
00004 sku411

+--------------------+
| ("00001","sku010") |
+--------------------+
| ("00001","sku933") |
+--------------------+
| ("00001","sku022") |
+--------------------+
| ("00002","sku912") |
+--------------------+
| ("00002","sku331") |
+--------------------+
| ("00003","sku888") |
+--------------------+
| .....              |
+--------------------+

Mapping Single Elements to Multiple Pairs (Scala)

val ordersRDD = sc.textFile("orderskus.txt").map(line => line.split(' ')).map(fields => (fields(0),fields(1))).flatMapValues(skus => skus.split(':'))

▪ Map-reduce is a generic programming model for distributed processing - modelo de programación genérico para el procesamiento distribuido
Map-Reduce
▪ Map-reduce is a common programming model
  ─ Spark implements map-reduce with pair RDDs
  ─ Easily applicable to distributed processing of large data sets
▪ Hadoop MapReduce was the first major distributed implementation
  ─ Hadoop MapReduce and other implementations are limited to a single map and single reduce phase per job
  ─ Somewhat limited
  ─ Each job has one map phase, one reduce phase
  ─ Job output is saved to files
▪ Spark implements map-reduce with much greater flexibility
  ─ Spark allows flexible chaining of map and reduce operations - encadenamiento flexible de operaciones de mapa y reducción
  ─ Map and reduce functions can be interspersed
    ─ Results can be stored in memory 
      ─ Operations can easily be chained

Map-Reduce in Spark
▪ Map-reduce in Spark works on pair RDDs
▪ Map phase
  ─ Operates on one record at a time
  ─ “Maps” each record to zero or more new records
  ─ Examples: map, flatMap, filter, keyBy
▪ Reduce phase
  ─ Works on map output
  ─ Consolidates multiple records
  ─ Examples: reduceByKey, sortByKey, mean

Map-Reduce Example: Word Count
Input Data
the cat sat on the mat the aardvark sat on the sofa
Result

catsat.txt
+--------------------+
|  (on, 2)           |
+--------------------+
|  (sofa, 1)         |
+--------------------+
|  (mat, 1)          |
+--------------------+
|  (aardvark, 1)     |
+--------------------+
|  (the, 4)          |
+--------------------+
|  (cat, 1)          |
+--------------------+
|  (sat, 2)          |
+--------------------+
(Python)
countsRDD = sc.textFile("catsat.txt").flatMap(lambda line: line.split(' ')).map(lambda word: (word,1)).reduceByKey(lambda v1,v2: v1+v2)

The function passed to reduceByKey combines values from two keys
─ Function must be binary

The function might be called in any order, therefore must be
─ Commutative: x + y = y + x
─ Associative: (x + y) + z = x + (y + z)

reduceByKey(lambda v1,v2: v1+v2)

(Scale)
val counts = sc.textFile("catsat.txt").flatMap(line => line.split(' ')). map(word => (word,1)).reduceByKey((v1,v2) => v1+v2)
OR
val counts = sc.textFile("catsat.txt").flatMap(_.split(' ')).map((_,1)).reduceByKey(_+_)

▪ Word count is challenging with massive amounts of data
  ─ Using a single compute node would be too time-consuming
▪ Statistics are often simple aggregate functions
  ─ Distributive in nature
  ─ For example: max, min, sum, and count
▪ Map-reduce breaks complex tasks down into smaller elements which can be executed in parallel
  ─ RDD transformations are implemented using the map-reduce paradigm
▪ Many common tasks are very similar to word count
  ─ Such as log file analysis

Pair RDD Operations ▪ Spark provides several operations for working with pair RDDs
▪ In addition to map and reduceByKey operations, Spark has several operations specific to pair RDDs
▪ Examples
─ countByKey returns a map(dict en python) with the count of occurrences of each key - ineficiente 
─ groupByKey groups all the values for each key in an RDD - transformación
─ sortByKey sorts in ascending or descending order
─ join returns an RDD containing all pairs with matching keys from two RDDs
─ leftOuterJoin, rightOuterJoin , fullOuterJoin join two RDDs, including keys defined in the left, right, or both RDDs respectively
─ mapValues, flatMapValues execute a function on just the values, keeping the key the same ( key , resultado de la funcion)
  flatMapValues sólo se puede usar con pair RDD
─ lookup(key) returns the value(s) for a key as a list

sortByKey Transformation

ordersRDD.sortByKey(ascending=false)
                    →
+----------------+     +----------------+
| (00001,sku010) |     | (00004,sku411) |
+----------------+     +----------------+
| (00001,sku933) |     | (00003,sku888) |
+----------------+     +----------------+
| (00001,sku022) |     | (00003,sku022) |
+----------------+     +----------------+
| (00002,sku912) |     | (00003,sku010) |
+----------------+     +----------------+
| (00002,sku331) |     | (00003,sku594) |
+----------------+     +----------------+
| (00003,sku888) |     | (00002,sku912) |
+----------------+     +----------------+

groupByKey Transformation

ordersRDD.groupByKey()
                    →
+----------------+     +---------------------------------------+
| (00001,sku010) |     | (00002,[sku912,sku331])               |
+----------------+     +---------------------------------------+
| (00001,sku933) |     | (00001,[sku022,sku010,sku933])        |
+----------------+     +---------------------------------------+
| (00001,sku022) |     | (00003,[sku888,sku022,sku010,sku594]) |
+----------------+     +---------------------------------------+
| (00002,sku912) |     | (00004,[sku411])                      |
+----------------+     +---------------------------------------+
| (00002,sku331) |
+----------------+
| (00003,sku888) |
+----------------+
Use reduceByKey in place of groupByKey where possible, as it can provide better performance

Joining by Key (producto cartesiano de las keys)
movieGrossRDD                 movieYearRDD
                    →
+--------------------+     +-------------------+
| (Casablanca,$3.7M) |     | (Casablanca,1942) |
+--------------------+     +-------------------+
| (Star Wars,$775M)  |     | (Star Wars,1977)  |
+--------------------+     +-------------------+
|(Annie Hall,$38M)   |     | (Annie Hall,1977) |
+--------------------+     +-------------------+
| (Argo,$232M)       | ↓ ↓ | (Argo,2012)       |
+--------------------+     +-------------------+

joinedRDD = movieGrossRDD.join(movieYearRDD) →

+--------------------------+
| (Casablanca,($3.7M,1942))|
+--------------------------+
| (Star Wars,($775M,1977)) |
+--------------------------+
| (Annie Hall,($38M,1977)) |
+--------------------------+
| (Argo,($232M,2012))      |
+--------------------------+

Querying Tables and Views with SQL
Spark SQL Queries
▪ You can query data in Spark SQL using SQL commands
  ─ Similar to queries in a relational database
  ─ Spark SQL includes a native SQL 2003 parser
▪ You can query Hive tables or DataFrame/Dataset views
▪ Spark SQL queries are particularly useful for
  ─ Developers or analysts who are comfortable with SQL
  ─ Doing ad hoc analysis
▪ Use the SparkSession.sql function to execute a SQL query on a table
  ─ Returns a DataFrame

Spark SQL Query
For Spark installations integrated with Hive, Spark can query tables defined in the Hive metastore

Hive Table: people
+-----+------------+-----------+--------+
| age | first_name | last_name | pcode  |
+-----+------------+-----------+--------+
| 52  | Grace      | Hopper    | 02134  |
+-----+------------+-----------+--------+
| null| Alan       | Turing    | 94020  |
+-----+------------+-----------+--------+
| 28  | Ada        | Lovelace  | 94020  |
+-----+------------+-----------+--------+

(Python)
peopleDF.write.mode("overwrite").save("/devsh_loudacre/people")
myDF = spark.sql("SELECT * FROM people WHERE pcode = 94020")
myDF.printSchema()

root
|-- pcode: integer (nullable = true)
|-- lastname: string (nullable = true)
|-- firstname: string (nullable = true)
|-- age: integer (nullable = true)

myDF.show()
+----+----------+---------+-----+
| age|first_name|last_name|pcode|
+----+----------+---------+-----+
|null| Alan     | Turing  |94020| 
| 28 | Ada      | Lovelace|94020|
+----+----------+---------+-----+

(Python)
maAgeDF = spark.sql("SELECT MEAN(age) AS mean_age, STDDEV(age) AS sdev_age 
FROM people WHERE pcode IN (SELECT pcode FROM pcodes WHERE state='MA')")
maAgeDF.printSchema()
root
|-- mean_age: double (nullable = true)
|-- sdev_age: double (nullable = true)

maAgeDF.show()
+--------+------------------+
|mean_age| sdev_age         |
+--------+------------------+
| 50.0   |2.8284271247461903|
+--------+------------------+

SQL Queries and DataFrame Queries
▪ You can query using SQL in addition to DataFrame and Dataset operations
  ─ Incorporates SQL queries into procedural development
▪ The Catalog API lets you list and describe tables, views, and columns, choose a database, or delete a view
▪ SQL queries and DataFrame transformations provide equivalent functionality
▪ Both are executed as series of transformations
  ─ Optimized by the Catalyst optimizer
▪ The following Python examples are equivalent

myDF = spark.sql("SELECT * FROM people WHERE pcode = 94020")
myDF = spark.read.table("people").where("pcode=94020")

SQL Queries on Files
▪ You can query directly from Parquet or JSON files that are not Hive tables 

spark.sql("SELECT * FROM parquet.`/loudacre/people.parquet` WHERE firstName LIKE 'A%' ").
show()

▪ You can use SQL with Hive tables and temporary views
  ─ Temporary views let you use SQL on data in DataFrames and Datasets
▪ SQL Queries on Views
  ─ Views provide the ability to perform SQL queries on a DataFrame or Dataset
▪ Views are temporary
  ─ Regular views can only be used within a single Spark session
  ─ Global views can be shared between multiple Spark sessions within a single Spark application
▪ Creating a view
  ─ DataFrame.createTempView(view-name)
  ─ DataFrame.createOrReplaceTempView(view-name)
  ─ DataFrame.createGlobalTempView(view-name)
  ─ DataFrame.createOrReplaceGlobalTempView(view-name)

Creating and Querying a View
▪ After defining a DataFrame view, you can query with SQL just as with a table

spark.read.load("/loudacre/people.parquet").select("firstName", "lastName").createTempView("user_names")
spark.sql( "SELECT * FROM user_names WHERE firstName LIKE 'A%'" ).show()

The Catalog API
▪ SQL queries and DataFrame/Dataset queries are equivalent
  ─ Both are optimized by Catalyst
▪ Use the Catalog API to explore tables and manage views
▪ The entry point for the Catalog API is spark.catalog
▪ Functions include
  ─ listDatabases returns a Dataset (Scala) or list (Python) of existing databases
  ─ setCurrentDatabase(dbname) sets the current database for the session
    ─ Default database is default
    ─ Subsequent commands refer to current database unless otherwise specified
    ─ Equivalent to the USE statement in SQL

▪ Functions include (continued)
  ─ listTables(database) returns a Dataset (Scala) or list (Python) of tables and views
    ─ Specified database (or current database if not specified)
  ─ listColumns(tablename) returns a Dataset (Scala) or list (Python) of the columns in the specified table or view in the current database
    ─ Use listColumns(tablename,database) in Python or 
    ─ listColumns(database,tablename) in Scala to specify a database

  ─ dropTempView(viewname) removes a temporary view

Listing Tables and Views (Scala)
    spark.catalog.listTables.show

Listing Tables and Views (Python)
    for table in spark.catalog.listTables(): print(table)
    Table(name=u'people', database=u'default', description=None, tableType=u'EXTERNAL', isTemporary=False)
    Table(name=u'user_names', database=u'default’, description=None, tableType=u'TEMPORARY', isTemporary=True)


Dataset - DataSet
▪ Datasets represent data consisting of strongly-typed objects
▪ A distributed collection of strongly-typed objects
  ─ Primitive types such as Int or String
  ─ Complex types such as arrays and lists containing supported types
  ─ Product objects based on Scala case classes (or JavaBean objects in Java)
  ─ Row objects
  ─ Primitive types, complex types, and Product and Row objects
  ─ Encoders map the Dataset’s data type to a table-like schema
▪ Mapped to a relational schema
  ─ The schema is defined by an encoder
  ─ The schema maps object properties to typed columns
▪ Implemented only in Scala and Java ▪ Datasets are defined in Scala and Java
  ─ Python is not a statically-typed language—no benefit from Dataset strong typing
  ─ Python is a dynamically-typed language, no need for strongly-typed data representation

Datasets and DataFrames
▪ In Scala and Java, DataFrame is just an alias for Dataset[Row]
▪ In Scala, DataFrame is an alias for a Dataset containing Row objects
  ─ There is no distinct class for DataFrame
▪ DataFrames and Datasets represent different types of data
  ─ DataFrames (Datasets of Row objects) represent tabular data
  ─ Datasets represent typed, object-oriented data
▪ DataFrame transformations are referred to as untyped
  ─ Rows can hold elements of any type
  ─ Schemas defining column types are not applied until runtime
▪ Dataset transformations are typed
  ─ Object properties are inherently typed at compile time

▪ Datasets can be created from in-memory data, DataFrames, and RDDs

Creating Datasets: A Simple Example
▪ Use SparkSession.createDataset(Seq) to create a Dataset from in-memory data (experimental)

--O-J-O--
SparkSession = DataFrames
sparkContext = RDD
sc = SparkSession.sparkContext

RDD no tiene modo "overwrite" para los archivos SIEMPRE se tienen que volver a crear... (es decir revisar si existen y borrar previamente)
DF tiene modo "overwrite"
DF tiene que comprobar el schema

▪ Example: Create a Dataset of strings (Dataset[String])

val strings = Seq("a string","another string")
val stringDS = spark.createDataset(strings)
scala> stringDS.show
+--------------+
|         value|
+--------------+
|      a string|
|another string|
+--------------+

Datasets and Case Classes
▪ Scala case classes are a useful way to represent data in a Dataset
  ─ They are often used to create simple data-holding objects in Scala
  ─ Instances of case classes are called products

case class Name(firstName: String, lastName: String)
val names = Seq(Name("Fred","Flintstone"),
                Name("Barney","Rubble"))
names.foreach(name => println(name.firstName))
names.foreach(i => println(i.firstName))

scala> case class Name(firstName: String, lastName: String)
defined class Name

scala> val names = Seq(Name("Fred","Flintstone"),
     |                 Name("Barney","Rubble"))
names: Seq[Name] = List(Name(Fred,Flintstone), Name(Barney,Rubble))

scala> names.foreach(name => println(name.firstName))
Fred
Barney

scala> names.foreach(name => println(name.firstName, name.lastName))
(Fred,Flintstone)
(Barney,Rubble)

▪ Encoders define a Dataset’s schema using reflection on the object type
  ─ Case class arguments are treated as columns

import spark.implicits._ // required if not running in shell

scala> val namesDS = spark.createDataset(names)
namesDS: org.apache.spark.sql.Dataset[Name] = [firstName: string, lastName: string]

scala> namesDS.show
+---------+----------+
|firstName|  lastName|
+---------+----------+
|     Fred|Flintstone|
|   Barney|    Rubble|
+---------+----------+

Type Safety in Datasets and DataFrames
▪ Type safety means that type errors are found at compile time rather than runtime
▪ Example: Assigning a String value to an Int variable

scala> val i:Int = namesDS.first.lastName
<console>:26: error: type mismatch;
 found   : String
 required: Int
       val i:Int = namesDS.first.lastName
   
scala> namesDS.first.lastName
res10: String = Flintstone

scala> print(res10)
Flintstone

scala> val row = namesDS.first
row: Name = Name(Fred,Flintstone)

scala> val i:Int = row.getInt(row.fieldIndex("lastName"))
<console>:26: error: value getInt is not a member of Name
       val i:Int = row.getInt(row.fieldIndex("lastName"))
                       ^
<console>:26: error: value fieldIndex is not a member of Name
       val i:Int = row.getInt(row.fieldIndex("lastName"))
  
Loading and Saving Datasets
▪ You cannot load a Dataset directly from a structured source
  ─ Datasets are saved as DataFrames
▪ Create a Dataset by loading a DataFrame or RDD and converting to a Dataset
  ─ Save using Dataset.write (returns a DataFrameWriter)
  ─ The type of object in the Dataset is not saved
  
Creating a Dataset from a DataFrame

▪ Use Dataset.as[type] to create a Dataset from a DataFrame
  ─ Encoders convert Row elements to the Dataset’s type
  ─ The Dataset.as function is experimental
▪ a Dataset of type Name based a JSON file

names.json
{"firstName":"Grace","lastName":"Hopper"}
{"firstName":"Alan","lastName":"Turing"}
{"firstName":"Ada","lastName":"Lovelace"}
{"firstName":"Charles","lastName":"Babbage"}

Creating a Dataset from a DataFrame

scala> val namesDF = spark.read.json("/user/scelisdev03/names.json")
namesDF: org.apache.spark.sql.DataFrame = [firstName: string, lastName: string]

scala> namesDF.show
+---------+--------+
|firstName|lastName|
+---------+--------+
|    Grace|  Hopper|
|     Alan|  Turing|
|      Ada|Lovelace|
|  Charles| Babbage|
+---------+--------+

Creating a Dataset from a DataFrame

scala> case class Name(firstName: String, lastName: String)
defined class Name

scala> val namesDS = namesDF.as[Name]
namesDS: org.apache.spark.sql.Dataset[Name] = [firstName: string, lastName: string]

scala> namesDS.show
+---------+--------+
|firstName|lastName|
+---------+--------+
|    Grace|  Hopper|
|     Alan|  Turing|
|      Ada|Lovelace|
|  Charles| Babbage|
+---------+--------+

Creating Datasets from RDDs

▪ Datasets can be created based on RDDs
  ─ Useful with unstructured or semi-structured data such as text
    ─ Tab-separated text file with postal code, latitude, and longitude
In:

latlon.tsv
00210\t43.005895\t71.013202
01014\t42.170731\t-72.604842
01062\t42.324232\t-72.67915

Out: PcodeLatLon(pcode,(lat,lon)) objects

scala> case class PcodeLatLon(pcode: String,
     |                        latlon: Tuple2[Double,Double])
defined class PcodeLatLon

▪ Parse input to structure the data
▪ Create RDD of PcodeLatLon case class instances

scala> val pLatLonRDD = sc.textFile("/user/scelisdev03/latlon.tsv").
     |     map(line => line.split('\t')).
     |     map(fields =>
     |       (PcodeLatLon(fields(0), (fields(1).toFloat, fields(2).toFloat))))
pLatLonRDD: org.apache.spark.rdd.RDD[PcodeLatLon] = MapPartitionsRDD[29] at map at <console>:30

▪ Convert RDD to a Dataset of PcodeLatLon objects

scala> val pLatLonDS = spark.createDataset(pLatLonRDD)
pLatLonDS: org.apache.spark.sql.Dataset[PcodeLatLon] = [pcode: string, latlon: struct<_1: double, _2: double>]

este es el esquema con mi fichero..
scala> pLatLonDS.printSchema
root
 |-- pcode: string (nullable = true)
 |-- latlon: struct (nullable = true)
 |    |-- _1: double (nullable = false)
 |    |-- _2: double (nullable = false)

este es el schema del manual que es diferente a la ejecucion...  0_0
root
|-- pcode: string (nullable = true)
|-- latlon: struct (nullable = true)
|    |-- _1: double (nullable = true)
|    |-- _2: double (nullable = true)

scala> println(pLatLonDS.first)
21/10/19 11:56:51 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 9.0 (TID 14) (cluster-cca175-w-0.us-central1-b.c.cca175-329021.internal executor 1): java.lang.ArrayIndexOutOfBoundsException: 1

según el manual deberia dar esto .. pero no lo hizo : PcodeLatLon(00210,(43.00589370727539,-71.01319885253906))

DataSet Operations
▪ Datasets have typed and untyped operations
Typed and Untyped Transformations
▪ Typed transformations create a new Dataset based on an existing Dataset
  ─ Typed operations return Datasets based on the original type
  ─ Typed transformations can be used on Datasets of any type (including Row)
    Typed operations (operations that return typed Datasets) include
    ─ filter (and its alias, where)
    ─ distinct
    ─ limit
    ─ sort (and its alias, orderBy)
    ─ union
▪ Untyped transformations return DataFrames (Datasets containing Row objects) or untyped Columns
  ─ Untyped operations return DataFrames (Datasets of rows)
  ─ Do not preserve type of the data in the parent Dataset
    ▪ Untyped operations (those that return Row Datasets) include
    ─ join
    ─ groupBy (with aggregation function)
    ─ drop
    ─ select
    ─ withColumn

Typed and Untyped Transformations

case class Person(pcode:String, lastName:String, firstName:String, age:Int)

val people = Seq(Person("02134","Hopper","Grace",48),…)

val peopleDS = spark.createDataset(people)
peopleDS: org.apache.spark.sql.Dataset[Person] =
[pcode: string, firstName: string ... 2 more fields]


▪ Typed operations return Datasets based on the starting Dataset
▪ Untyped operations return DataFrames (Datasets of Rows)

val sortedDS = peopleDS.sort("age")
sortedDS: org.apache.spark.sql.Dataset[Person] =
[pcode: string, lastName: string ... 2 more fields]

val firstLastDF = peopleDS.select("firstName","lastName")
firstLastDF: org.apache.spark.sql.DataFrame =
[firstName: string, lastName: string]


Combining Typed and Untyped Operations

val combineDF = peopleDS.sort("lastName"). where("age > 40").select("firstName","lastName")
combineDF: org.apache.spark.sql.DataFrame =
[firstName: string, lastName: string]

combineDF.show
+---------+--------+
|firstName|lastName|
+---------+--------+
| Charles| Babbage |
| Grace  | Hopper  |
+---------+--------+

Writing, Configuring, and Running Spark Applications

▪ The Spark shell allows interactive exploration and manipulation of data
  ─ REPL using Python or Scala
▪ Spark applications run as independent programs
  ─ For jobs such as ETL processing, streaming, and so on
  ─ Python, Scala, or Java

The Spark Session and Spark Context
▪ Every Spark program needs
  ─ One SparkContext object
  ─ One or more SparkSession objects
    ─ If you are using Spark SQL
▪ The interactive shell creates these for you
  ─ A SparkSession object called spark
  ─ A SparkContext object called sc
▪ In a standalone Spark application you must create these yourself 
  ─ Use a Spark session builder to create a new session
    ─ The builder automatically creates a new Spark context as well
  ─ Call stop on the session or context when program terminates

Creating a SparkSession Object
▪ SparkSession.builder points to a Builder object
  ─ Use the builder to create and configure a SparkSession object
▪ The getOrCreate builder function returns the existing SparkSession object if it exists
  ─ Creates a new Spark session if none exists
  ─ Automatically creates a new SparkContext object as sparkContext on the SparkSession object

Examples: NameList in Python and Scala

Building an Application: Scala or Java
▪ Scala or Java Spark applications must be compiled and assembled into JAR files
  ─ JAR file will be passed to worker nodes
▪ Apache Maven is a popular build tool
  ─ For specific setting recommendations, see the Spark Programming Guide
▪ Build details will differ depending on
  ─ Version of Hadoop and CDP
  ─ Deployment platform (YARN, Mesos, Spark Standalone)
▪ Consider using an Integrated Development Environment (IDE)
  ─ IntelliJ or Eclipse are two popular examples
  ─ Can run Spark locally in a debugger

Building Scala Applications
Basic Apache Maven projects are provided in the exercise directory
─ stubs: starter Scala files—do exercises here
─ solution: exercise solutions
─ bonus: bonus solutions
Build command: mvn package

Running a Spark Application
▪ The easiest way to run a Spark application is to use the submit script
   ─ Python
           $ spark-submit NameList.py people.json namelist/
   ─ Scala or Java
           $ spark-submit --class NameList MyJarFile.jar people.json namelist/

Submit Script Options
▪ The Spark submit script provides many options to specify how the application should run
  ─ Most are the same as for pyspark and spark-shell
▪ General submit flags include
  ─ master: local, yarn, or a Mesos or Spark Standalone cluster manager URI
  ─ jars: Additional JAR files
  ─ pyfiles: Additional Python files (Python only)
  ─ driver-java-options: Parameters to pass to the driver JVM
▪ YARN-specific flags include
  ─ num-executors: Number of executors to start application with
  ─ driver-cores: Number cores to allocate for the Spark driver
  ─ queue: YARN queue to run in
▪ Show all available options
  ─ help

Application Deployment Mode
▪ Spark applications can run
  ─ Locally with one or more threads
    ─ On a cluster
      ─ In client mode (default), the driver runs locally on a gateway node
        ▪ Requires direct communication between driver and cluster worker nodes

      ─ In cluster mode, the driver runs in the application master on the cluster
        ▪ Common in production systems

        
▪ Specify the deployment mode when submitting the application

Spark Deployment Mode on YARN: Cluster Mode - En el Application Master esta el Driver Program y se ejecuta el Spark Context 
$ spark-submit --master yarn --deploy-mode cluster NameList.py people.json namelist/

Spark Deployment Mode on YARN: Client Mode - En el Gateway Node esta el Driver Program y se ejecuta el Spark Context 
$ spark-submit --master yarn NameList.py people.json namelist/

Spark Deployment Mode on YARN - Client Mode ---------------------------------------------------- Cliente Mode

+-----------------------+                            +---------------------------+
|     Gateway Node      |                            |     Worker Node A         |
|                       |            +-------------->|                           |
| $ spark-submit        |            |               |                           |
|   MyApp               |            |               |                           |
+-----------------------+            |               +---------------------------+
                                     |
+--------------+                     |               +---------------------------+
|              |                     |               |     Worker Node B         |
|   RESOURCE   |---------------------|-------------->|                           |
|   MANAGER    |                     |               |                           |
+--------------+                     |               |                           |
                                     |               +---------------------------+
                                     |
                                     |               +---------------------------+
                                     |               |     Worker Node C         |
                                     --------------->|                           |
                                                     |                           |
                                                     |                           |
                                                     +---------------------------+

El SparkContext esta en el Gateway Node

+-----------------------+                            +---------------------------+
|     Gateway Node      |                            |     Worker Node A         |
|    Driver Program     |            +-------------->|                           |
|        Spark          |            |               |  Executor     Executor <<-|------------
|       Context         |            |               |                           |            |
+-----------------------+            |               +---------------------------+            |
          |                          |                     ^                                  |
          |                          |                     |__________________________        |
          V                          |                                                |       |
+--------------+                     |               +---------------------------+    |       |
|              |                     |               |     Worker Node B         |    |       |
|   RESOURCE   |---------------------|-------------->|                           |    |       |
|   MANAGER    |                     |               |  Application ----------->>|------------- 
+--------------+                     |               |  Master                   |    |
                                     |               +---------------------------+    |
                                     |                                                |
                                     |               +---------------------------+    |
                                     |               |     Worker Node C         |    |
                                     --------------->|                           |    |
                                                     |               Executor <<-|-----
                                                     |                           |
                                                     +---------------------------+


Spark Deployment Mode on YARN - Cluster Mode ---------------------------------------------------- Cluster Mode

+-----------------------+                            +---------------------------+
|     Gateway Node      |                            |     Worker Node A         |
|                       |            +-------------->|                           |
| $ spark-submit        |            |               |                           |
|   MyApp               |            |               |                           |
+-----------------------+            |               +---------------------------+
                                     |
+--------------+                     |               +---------------------------+
|              |                     |               |     Worker Node B         |
|   RESOURCE   |---------------------|-------------->|                           |
|   MANAGER    |                     |               |                           |
+--------------+                     |               |                           |
                                     |               +---------------------------+
                                     |
                                     |               +---------------------------+
                                     |               |     Worker Node C         |
                                     --------------->|                           |
                                                     |                           |
                                                     |                           |
                                                     +---------------------------+

El SparkContext esta en el Application Master

+-----------------------+                            +---------------------------+
|     Gateway Node      |                            |     Worker Node A         |
|                       |            +-------------->|                           |
| $ spark-submit        |            |               |   Executor     Executor   |
|   MyApp               |            |               |                           |
+-----------------------+            |               +---------------------------+
          |                          |                                                                                            
          |                          |                                                         
          V                          |
+--------------+                     |               +---------------------------+
|              |                     |               |          Driver Program   |
|              |                     |               |               Spark       |
|   RESOURCE   |---------------------|-------------->|               Context     |
|   MANAGER    |                     |               |  Application ----------->>|
+--------------+                     |               |  Master                   |
                                     |               +---------------------------+
                                     |                                            
                                     |               +---------------------------+
                                     |               |     Worker Node C         |
                                     --------------->|                           |
                                                     |               Executor <<-|
                                                     |                           |
                                                     +---------------------------+



The Spark Application Web UI
▪ The Spark UI lets you monitor running jobs, and view statistics and configuration

Accessing the Spark UI
▪ The web UI is run by the Spark driver
  ─ When running locally: http://localhost:4040
  ─ When running in client mode: http://gateway:4040
  ─ When running in cluster mode, access via the YARN UI

Spark Application History UI
▪ The Spark UI is only available while the application is running
▪ Use the Spark application history server to view metrics for a completed application
  ─ Optional Spark component

Viewing the Application History UI
▪ You can access the history server UI by
  ─ Using a URL with host and port configured by a system administrator
  ─ Following the History link in the YARN UI

Spark Application Configuration Properties
▪ Spark provides numerous properties to configure your application
▪ Some example properties
  ─ spark.master: Cluster type or URI to submit application to
  ─ spark.app.name: Application name displayed in the Spark UI
  ─ spark.submit.deployMode: Whether to run application in client or cluster mode (default: client)
  ─ spark.ui.port: Port to run the Spark Application UI (default 4040)
  ─ spark.executor.memory: How much memory to allocate to each Executor (default 1g)
  ─ spark.pyspark.python: Which Python executable to use for Pyspark applications
  ─ And many more…
     ─ See the Spark Configuration page in the Spark documentation for more details 
      https://spark.apache.org/docs/latest/configuration.html

Setting Configuration Properties
▪ Most properties are set by system administrators
  ─ Managed manually or using Cloudera Manager
  ─ Stored in a properties file
▪ Developers can override system settings when submitting applications by
  ─ Using submit script flags
  ─ Loading settings from a custom properties file instead of the system file
  ─ Setting properties programmatically in the application
▪ Properties that are not set explicitly use Spark default values

Overriding Properties Using Submit Script
▪ Some Spark submit script flags set application properties
  ─ For example
    ─ Use --master to set spark.master
    ─ Use --name to set spark.app.name
    ─ Use --deploy-mode to set spark.submit.deployMode
▪ Not every property has a corresponding script flag
    ─ Use --conf to set any property

    $ spark-submit --conf spark.pyspark.python=/alt/path/to/python

Setting Properties in a Properties File

▪ System administrators set system properties in properties files
  ─ You can use your own custom properties file instead
    spark.master              local[*]
    spark.executor.memory     512k
    spark.pyspark.python      /alt/path/to/python

▪ Specify your properties file using the properties-file option

  $ spark-submit --properties-file=dir/my-properties.conf

▪ Note that Spark will load only your custom properties file
  ─ System properties file is ignored
  ─ Copy important system settings into your custom properties file
  ─ Custom file will not reflect future changes to system settings

Setting Configuration Properties Programmatically
▪ Spark configuration settings are part of the Spark session or Spark context
▪ Set using the Spark session builder functions
  ─ appName sets spark.app.name
  ─ master sets spark.master
  ─ config can set any property

Scala

import org.apache.spark.sql.SparkSession
…
val spark = SparkSession.builder.
    appName("my-spark-app").
    config("spark.ui.port","5050").
    getOrCreate()
…

Priority of Spark Property Settings
▪ Properties set with higher priority methods override lower priority methods
1. Programmatic settings
2. Submit script (command line) settings
3. Properties file settings
   ─ Either administrator site-wide file or custom properties file
4. Spark default settings
   ─ See the Spark Configuration guide: http://spark.apache.org/docs/latest/configuration.html

Viewing Spark Properties
▪ You can view the Spark property settings two ways
  ─ Using --verbose with the submit script
  ─ In the Spark Application UI Environment tab

▪ Use the Spark shell for interactive data exploration
▪ Write a Spark application to run independently
▪ Spark applications require a SparkContext object and usually a SparkSession object
▪ Use Maven or a similar build tool to compile and package Scala and Java applications
  ─ Not required for Python
▪ Deployment mode determines where the application driver runs—on the gateway or on a worker node
▪ Use the spark-submit script to run Spark applications locally or on a cluster
▪ Application properties can be set on the command line, in a properties file, or in the application code

OJO - ESTO ES DEL CAPITULO 14 QUE ESTA MAL ESCRITO Y DEBE SER: print( myRDD. todebugString(). decode() )

--nowpyspark
******************************************************************************
* -O-J-O- Apache Zeppelin
******************************************************************************

▪ Apache Zeppelin is a web-based notebook approach(enfoque) to interactive data analytics.
  Provides collaborative environment with Python, Scala, SQL, and more.

******************************************************************************
* -O-J-O- 
******************************************************************************

Apache Pig builds on Hadoop to offer high level data processing 
This is an alternaDve to wriDng low/level MapReduce code 
Pig is especially good at joining and transforming data 

Hive is an abstraction on top of Hadoop
Reduces development time Uses a SQL-like language called HiveQL

HBase is "The Hadoop database"
Tables can have many thousands of columns
NoSQL : There is no high/level query language
Use API to scan / get / put values based on keys 

Cloudera Impala
Massively parallel SQL query data stored in HDFS or HBase tables
Impala SQL similar HiveQL

Apache Sqoop = "SQL to Hadoop"
Sqoop exchanges data between an RDBMS and Hadoop
It can import all tables, a single table, or a portion of a table into HDFS
Sqoop can also export data FROM HDFS back to the database

Apache Kudu 
Distributed columnar (key-value) storage for structured data
Allows random access and updating data (unlike HDFS) 
Supports SQL-based analytics 
Works directly on native file system; is not built on HDFS 
Integrates with Spark, MapReduce, and Apache Impala

Apache Flume
Imports data into HDFS as it is being generated by various sources

Apache Oozie 
Oozie allows developers to manage processing workﬂows 
It coordinates execution and control of individual jobs 

**********************************************************************************************
***********************
*   -O-J-O- CCA 175   *   INICIO - CCA Spark and Hadoop Developer
***********************
**********************************************************************************************
----------------------------------------------------------------------------------------------
| entorno - https://labs.pue.es - CTEC-347_Dev_Cloudera-03-Developer 	Running 	172.16.13.9 		s20.labs.pue.es 
----------------------------------------------------------------------------------------------

eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 172.16.13.9  netmask 255.255.255.0  broadcast 172.16.13.255
        inet6 fe80::f7c5:99f5:d79e:b7b7  prefixlen 64  scopeid 0x20<link>
        inet7 fe80::e0d:a8e3:bcd4:379d  prefixlen 64  scopeid 0x20<link>
        inet6 fe80::2ae6:55d2:55e4:56ef  prefixlen 64  scopeid 0x20<link>
        ether 52:54:00:b3:03:09  txqueuelen 1000  (Ethernet)
        RX packets 8945  bytes 3119827 (2.9 MiB)
        RX errors 0  dropped 6  overruns 0  frame 0
        TX packets 4143  bytes 7986209 (7.6 MiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 1  (Local Loopback)
        RX packets 178431  bytes 259810194 (247.7 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 178431  bytes 259810194 (247.7 MiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

[training@dev ~]$ uname -n
dev.lab.local

[training@dev ~]$ uname -i
x86_64

[training@dev ~]$ who am i
training pts/0        2021-09-17 09:39 (:10.0)

[training@dev ~]$ hostname
dev.lab.local

[training@dev ~]$ ping dev.lab.local
PING dev.lab.local (127.0.0.1) 56(84) bytes of data.

[training@dev ~]$ pwd
/home/training

[training@dev ~]$ echo $DEVSH
/home/training/training_materials/devsh

[training@dev ~]$ echo $DEVDATA
/home/training/training_materials/devsh/data

echo $SCA
/home/training/training_materials/jes

echo $SCADATA
/home/training/training_materials/jes/data


[training@dev ~]$ ls
bin            CMhostInfo.txt  Desktop     Escritorio           Imágenes  Plantillas                  PycharmProjects     Vídeos
cloudera-rpms  config          Documentos  firewall-cmd         kk.txt    Público                     thinclient_drives
cluster.log    Descargas       Downloads   idea-IC-212.5080.55  Música    pycharm-community-2021.2.1  training_materials

[training@dev ~]$ ls training_materials
devsh

[training@dev ~]$ ls training_materials/devsh
data  examples  exercises  scripts

[training@dev ~]$ ls training_materials/devsh/data
accountdevice       base_stations.parquet  devicestatus_stream      frostroad.txt  makes1.txt   weblogs
activations         devices.csv            devicestatus_stream.txt  kb             makes2.txt   writedevicestatus-devsh.py
activations_stream  devices.json           devicestatus.txt         loudacre.sql   static_data

[training@dev ~]$ ls training_materials/devsh/examples
AggregatingDataPairRDDs.pyspark               kafka                                     StreamingIntro.pyspark
AggregatingDataPairRDDs.scalaspark            kmeans-dataframe.pyspark                  StreamingIntro.scalaspark
AnalyzingDataWithDataFrameQueries.pyspark     kmeans-dataframe.scalaspark               StreamingJoins.pyspark
AnalyzingDataWithDataFrameQueries.scalaspark  other-topics                              StreamingJoins.scalaspark
CommonPatterns.pyspark                        QueryingTablesAndViewsWithSQL.pyspark     StreamingKafka-in.pyspark
CommonPatterns.scalaspark                     QueryingTablesAndViewsWithSQL.scalaspark  StreamingKafka-out.pyspark
DataFramesAndRDDs.pyspark                     RDDOverview.pyspark                       StreamingKafka-out.scalaspark
DataFramesAndRDDs.scalaspark                  RDDOverview.scalaspark                    StreamkingKafka-in.scalaspark
DistributedDataPersistence.pyspark            spark-applications                        TransformingDataRDDs.pyspark
DistributedDataPersistence.scalaspark         SparkBasics.pyspark                       TransformingDataRDDs.scalaspark
DistributedProcessing.pyspark                 SparkBasics.scalaspark                    WorkingWithDataFrames.pyspark
DistributedProcessing.scalaspark              StreamingAggregation.pyspark              WorkingWithDataFrames.scalaspark
example-data                                  StreamingAggregation.scalaspark           WorkingWithDatasets.scalaspark

[training@dev ~]$ ls training_materials/devsh/exercises/
analyze     datasets  iterative  pair-rdds  query-execution  spark-application  spark-sql  streaming-aggregation  transform-rdds
dataframes  hdfs      kafka      persist    rdds             spark-shell        streaming  streaming-kafka        yarn

[training@dev ~]$ ls training_materials/devsh/scripts
advance_exercises.sh  create-accounts.sql       setup.sh            streamtest-kafka.sh
catchup.sh            firefox-training.default  streamtest-file.sh  streamtest-network.py

[training@dev ~]$ hdfs dfs -ls
Found 2 items
drwx------   - training training          0 2020-04-29 14:00 .Trash
drwxr-xr-x   - training training          0 2021-09-15 10:19 .sparkStaging

[training@dev ~]$ hdfs dfs -ls /
Found 3 items
drwxr-xr-x   - hbase hbase               0 2021-09-17 09:52 /hbase
drwxrwxrwt   - hdfs  supergroup          0 2020-01-22 15:30 /tmp
drwxr-xr-x   - hdfs  supergroup          0 2020-01-16 11:51 /user

[training@dev ~]$ hdfs dfs -ls /hbase
Found 13 items
drwxr-xr-x   - hbase hbase          0 2020-01-16 11:38 /hbase/.hbck
drwxr-xr-x   - hbase hbase          0 2021-09-17 09:52 /hbase/.tmp
drwxr-xr-x   - hbase hbase          0 2021-09-17 09:52 /hbase/MasterProcWALs
drwxr-xr-x   - hbase hbase          0 2021-09-17 09:52 /hbase/WALs
drwxr-xr-x   - hbase hbase          0 2021-09-15 13:22 /hbase/archive
drwxr-xr-x   - hbase hbase          0 2020-01-16 11:38 /hbase/corrupt
drwxr-xr-x   - hbase hbase          0 2020-01-16 11:38 /hbase/data
drwxr-xr-x   - hbase hbase          0 2020-01-16 11:38 /hbase/hbase
-rw-r--r--   1 hbase hbase         42 2020-01-16 11:37 /hbase/hbase.id
-rw-r--r--   1 hbase hbase          7 2020-01-16 11:37 /hbase/hbase.version
drwxr-xr-x   - hbase hbase          0 2020-01-16 11:38 /hbase/mobdir
drwxr-xr-x   - hbase hbase          0 2021-09-17 09:52 /hbase/oldWALs
drwx--x--x   - hbase hbase          0 2020-01-16 11:38 /hbase/staging

[training@dev ~]$ hdfs dfs -ls /tmp
Found 4 items
d---------   - hdfs     supergroup          0 2021-09-17 09:55 /tmp/.cloudera_health_monitoring_canary_files
drwx-wx-wx   - hive     supergroup          0 2020-01-21 15:52 /tmp/hive
drwxrwxrwt   - mapred   hadoop              0 2020-01-20 15:49 /tmp/logs
drwxrwxrwx   - training supergroup          0 2020-01-21 20:12 /tmp/streaming-checkpoint

[training@dev ~]$ hdfs dfs -ls /user
Found 8 items
drwxrwxrwx   - mapred   hadoop              0 2020-01-16 11:37 /user/history
drwxrwxr-t   - hive     hive                0 2020-01-16 11:37 /user/hive
drwxrwxr-x   - hue      hue                 0 2020-01-16 11:38 /user/hue
drwxrwxr-x   - impala   impala              0 2020-01-30 17:39 /user/impala
drwxrwxr-x   - oozie    oozie               0 2020-01-16 11:36 /user/oozie
drwxr-x--x   - spark    spark               0 2020-01-16 11:36 /user/spark
drwxr-xr-x   - training training            0 2020-06-16 21:26 /user/training
drwxr-xr-x   - hdfs     supergroup          0 2020-01-16 11:36 /user/yarn

[training@dev ~]$ hdfs dfs -ls /user/training
Found 2 items
drwx------   - training training          0 2020-04-29 14:00 /user/training/.Trash
drwxr-xr-x   - training training          0 2021-09-15 10:19 /user/training/.sparkStaging

----------------------------------------------------------------------------------------------
| entorno - https://labs.pue.es - CTEC-347_Dev_Cloudera-03-Developer 	Running 	172.16.13.9 		s20.labs.pue.es 
----------------------------------------------------------------------------------------------

---------------------------------------

---------------------------------------------------------------
| Google cloud shell --> Upload Files --> Cloud Storage --> Bucket details / cftic_cca175_bucket 
---------------------------------------------------------------
scelisdev02@cloudshell:~ (cca175-325912)$
scelisdev02@cloudshell:~ (cca175-325912)$ ls
datos_banco.sql  Exercises.SQL  LearningSQLExample.sql  README-cloudshell.txt
scelisdev02@cloudshell:~ (cca175-325912)$ echo $SHELL
/bin/bash
scelisdev02@cloudshell:~ (cca175-325912)$ pwd
/home/scelisdev02
scelisdev02@cloudshell:~ (cca175-325912)$ uname -n
cs-259003285496-default-boost-mjbv6
scelisdev02@cloudshell:~ (cca175-325912)$ hostname
cs-259003285496-default-boost-mjbv6
scelisdev02@cloudshell:~ (cca175-325912)$

--------------------------------------------------------------
| INSTANCE de VM - ssh in new browser -> Upload dataset.zip, devsh.zip, training_materials.zip
--------------------------------------------------------------

scelisdev02@cca175-m:/$ pwd
/
scelisdev02@cca175-m:/$ cd $HOME
scelisdev02@cca175-m:~$ pwd
/home/scelisdev02
scelisdev02@cca175-m:~$ uname -n
cca175-m
scelisdev02@cca175-m:~$ who i am
scelisdev02 pts/0        2021-09-16 08:09 (35.235.243.225)

scelisdev02@cca175-m:~$ hostname
cca175-m

scelisdev02@cca175-m:~$ echo $SHELL
/bin/bash

scelisdev02@cca175-m:~$ ls
scelisdev.txt  training_materials

scelisdev02@cca175-m:~$ ls -lF
total 344
-rw-r--r--. 1 scelisdev02 scelisdev02 352112 Sep 14 16:07 scelisdev.txt
drwxr-xr-x. 3 scelisdev02 scelisdev02     19 Sep 14 16:10 training_materials/

/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/CCA175/Cargar Tablas hive.pdf

scelisdev02@cca175-m:~$ cd /tmp
scelisdev02@cca175-m:/tmp$ rm -rf dataset

scelisdev02@cca175-m:/tmp$ mv $HOME/dataset.zip .
scelisdev02@cca175-m:/tmp$ unzip dataset.zip
scelisdev02@cca175-m:/tmp$ mv $HOME/devsh.zip .
scelisdev02@cca175-m:/tmp$ unzip devsh.zip 
scelisdev02@cca175-m:/tmp$ mkdir dataset
scelisdev02@cca175-m:/tmp/dataset$ mv $HOME/*.sql .

scelisdev02@cca175-m:/tmp/dataset$ ls -lF $HOME
total 344
-rw-r--r--. 1 scelisdev02 scelisdev02 352112 Sep 14 16:07 scelisdev.txt
drwxr-xr-x. 3 scelisdev02 scelisdev02     19 Sep 14 16:10 training_materials/

scelisdev02@cca175-m:/tmp$ ls -lsha
   0 drwxr-xr-x.   3 scelisdev02 scelisdev02   57 Sep 16 12:15 dataset
3.6M -rw-r--r--.   1 scelisdev02 scelisdev02 3.6M Sep 16 08:23 dataset.zip
   0 drwxr-xr-x.   6 scelisdev02 scelisdev02   66 Sep 16 08:35 devsh
 84M -rw-r--r--.   1 scelisdev02 scelisdev02  84M Sep 14 07:52 devsh.zip

scelisdev02@cca175-m:/tmp$ ls -lsha dataset
total 32K
   0 drwxr-xr-x.   3 scelisdev02 scelisdev02   57 Sep 16 12:15 .
 20K drwxrwxrwt. 112 root        root         16K Sep 16 12:23 ..
4.0K -rw-r--r--.   1 scelisdev02 scelisdev02  409 Sep 16 12:15 data.sql
4.0K drwxr-xr-x.  18 scelisdev02 scelisdev02 4.0K Aug  7  2020 retail_db
4.0K -rw-r--r--.   1 scelisdev02 scelisdev02 1.3K Sep 16 12:15 tables.sql

scelisdev02@cca175-m:/tmp$ ls datas*
dataset.zip

dataset:
data.sql  retail_db  tables.sql

scelisdev02@cca175-m:/tmp$ ls -lF datas*
-rw-r--r--. 1 scelisdev02 scelisdev02 3720154 Sep 16 08:23 dataset.zip

dataset:
total 12
-rw-r--r--.  1 scelisdev02 scelisdev02  409 Sep 16 12:15 data.sql
drwxr-xr-x. 18 scelisdev02 scelisdev02 4096 Aug  7  2020 retail_db/
-rw-r--r--.  1 scelisdev02 scelisdev02 1231 Sep 16 12:15 tables.sql

scelisdev02@cca175-m:/tmp$ ls -lF dataset/retail_db/
total 4
drwxr-xr-x. 2 scelisdev02 scelisdev02   26 Aug  7  2020 categories/
drwxr-xr-x. 2 scelisdev02 scelisdev02   46 Aug  7  2020 categories-header/
drwxr-xr-x. 2 scelisdev02 scelisdev02   26 Aug  7  2020 customers/
drwxr-xr-x. 2 scelisdev02 scelisdev02  106 Aug  7  2020 customers-avro/
drwxr-xr-x. 2 scelisdev02 scelisdev02   31 Aug  7  2020 customers-json/
drwxr-xr-x. 2 scelisdev02 scelisdev02   31 Aug  7  2020 customers-multiline-json/
drwxr-xr-x. 2 scelisdev02 scelisdev02   26 Aug  7  2020 customers-tab-delimited/
drwxr-xr-x. 2 scelisdev02 scelisdev02   26 Aug  7  2020 departments/
drwxr-xr-x. 2 scelisdev02 scelisdev02   26 Aug  7  2020 order_items/
drwxr-xr-x. 2 scelisdev02 scelisdev02   26 Aug  7  2020 orders/
drwxr-xr-x. 2 scelisdev02 scelisdev02   58 Aug  7  2020 orders_parquet/
drwxr-xr-x. 2 scelisdev02 scelisdev02   86 Aug  7  2020 orders_permissions/
drwxr-xr-x. 2 scelisdev02 scelisdev02   26 Aug  7  2020 products/
drwxr-xr-x. 2 scelisdev02 scelisdev02  106 Aug  7  2020 products_avro/
drwxr-xr-x. 2 scelisdev02 scelisdev02 4096 Aug  7  2020 products_orc/
drwxr-xr-x. 2 scelisdev02 scelisdev02   42 Aug  7  2020 products_sequencefile/

scelisdev02@cca175-m:/tmp$ cd $HOME

scelisdev02@cca175-m:~$ ls -lF training_materials/
total 0
drwxr-xr-x. 7 scelisdev02 scelisdev02 92 Sep 14 16:12 devsh/

scelisdev02@cca175-m:~$ ls -lF training_materials/devsh
total 80
drwxr-xr-x.  9 scelisdev02 scelisdev02  4096 Sep 14 16:04 data/
drwxr-xr-x.  6 scelisdev02 scelisdev02  4096 Sep 14 16:04 examples/
drwxr-xr-x. 20 scelisdev02 scelisdev02  4096 Sep 14 16:04 exercises/
drwxr-xr-x.  4 scelisdev02 scelisdev02    55 Jul 26  2020 jep/
-rw-r--r--.  1 scelisdev02 scelisdev02 66361 Sep 14 16:11 jep.zip
drwxr-xr-x.  3 scelisdev02 scelisdev02   209 Sep 15 07:20 scripts/

----------------------------------------------------------------------------------------------
| creación de variables de entorno - repetir después de iniciar
----------------------------------------------------------------------------------------------
| 
| scelisdev02@cca175-m:~$ echo $DEVSH
| scelisdev02@cca175-m:~$ cd training_materials/devsh
| scelisdev02@cca175-m:~/training_materials/devsh$ pwd
| /home/scelisdev02/training_materials/devsh
| 
| scelisdev02@cca175-m:~/training_materials/devsh$ ls
| data  examples  exercises  jep  jep.zip  scripts
| 
| scelisdev02@cca175-m:~/training_materials/devsh$ DEVSH=/home/scelisdev02/training_materials/devsh
| scelisdev02@cca175-m:~/training_materials/devsh$ echo $DEVSH
| /home/scelisdev02/training_materials/devsh
| 
| scelisdev02@cca175-m:~/training_materials/devsh$ cd $HOME
| 
| scelisdev02@cca175-m:~$ echo $DEVSH
| /home/scelisdev02/training_materials/devsh
| 
| scelisdev02@cca175-m:~$ ls $DEVSH
| data  examples  exercises  jep  jep.zip  scripts
| 
| scelisdev02@cca175-m:~$ echo $DEVDATA
| scelisdev02@cca175-m:~$ cd training_materials/
| scelisdev02@cca175-m:~/training_materials$ cd devsh
| scelisdev02@cca175-m:~/training_materials/devsh$ cd data
| scelisdev02@cca175-m:~/training_materials/devsh/data$ pwd
| /home/scelisdev02/training_materials/devsh/data
| 
| scelisdev02@cca175-m:~/training_materials/devsh/data$ DEVDATA=/home/scelisdev02/training_materials/devsh/data
| 
| scelisdev02@cca175-m:~/training_materials/devsh/data$ echo $DEVDATA
| /home/scelisdev02/training_materials/devsh/data
| 
| scelisdev02@cca175-m:~/training_materials/devsh/data$ ls $DEVDATA
| accountdevice       base_stations.parquet  devicestatus_stream      frostroad.txt  makes1.txt   weblogs
| activations         devices.csv            devicestatus_stream.txt  kb             makes2.txt   writedevicestatus-devsh.py
| activations_stream  devices.json           devicestatus.txt         loudacre.sql   static_data
| 
| scelisdev02@cca175-m:~/training_materials/devsh/data$ cd $HOME
| scelisdev02@cca175-m:~$ echo $DEVDATA
| /home/scelisdev02/training_materials/devsh/data
| 
| scelisdev02@cca175-m:~$ ls $DEVDATA
| accountdevice       base_stations.parquet  devicestatus_stream      frostroad.txt  makes1.txt   weblogs
| activations         devices.csv            devicestatus_stream.txt  kb             makes2.txt   writedevicestatus-devsh.py
| activations_stream  devices.json           devicestatus.txt         loudacre.sql   static_data
| scelisdev02@cca175-m:~$ 
| 
----------------------------------------------------------------------------------------------
| creación de variables de entorno - repetir después de iniciar
----------------------------------------------------------------------------------------------

scelisdev02@cca175-m:~$ hdfs dfs -ls /
Found 2 items
drwxrwxrwt   - hdfs hadoop          0 2021-09-13 12:36 /tmp
drwxrwxrwt   - hdfs hadoop          0 2021-09-13 12:35 /user

scelisdev02@cca175-m:~$ hdfs dfs -ls /tmp
Found 3 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-16 13:04 /tmp/dataset
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /tmp/hadoop-yarn
drwx-wx-wx   - hive        hadoop          0 2021-09-14 16:20 /tmp/hive

scelisdev02@cca175-m:~$ hdfs dfs -ls /user
Found 9 items
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/hbase
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/hdfs
drwxrwxrwt   - hdfs        hadoop          0 2021-09-16 13:13 /user/hive
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/mapred
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/pig
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/spark
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/yarn
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/zookeeper

scelisdev02@cca175-m:~$ hdfs dfs -mkdir /user/training

scelisdev02@cca175-m:~$ hdfs dfs -ls /user
Found 9 items
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/hbase
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/hdfs
drwxrwxrwt   - hdfs        hadoop          0 2021-09-16 13:13 /user/hive
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/mapred
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/pig
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/spark
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-17 09:30 /user/training
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/yarn
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/zookeeper

scelisdev02@cca175-m:~$ hdfs dfs -ls /user/training


----------------------------------------------------------------------------------------------
-----NOOOO por ahora

esto no lo he hecho:
hdfs dfs -mkdir /user/training/dataset
hdfs dfs -put ~/spark-data/* /user/training/dataset


scelisdev02@cca175-m:~$ hdfs dfs -put /tmp/dataset /tmp/dataset

scelisdev02@cca175-m:~$ hdfs dfs -ls /tmp/dataset


OJO - no tengo claro que sea necesario hacer esto ahora: GCS plugin y montar el segmento (bucket)
por ahora no lo he instalado
https://medium.com/analytics-vidhya/improve-workflow-with-cloud-storage-fuse-89b8d76d0886
echo "deb http://packages.cloud.google.com/apt $GCSFUSE_REPO main" | sudo tee /etc/apt/sources.list.d/gcsfuse.list
curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo apt-get update
sudo apt-get install gcsfuse
export GCSFUSE_REPO=gcsfuse-`lsb_release -c -s`
OJO - no tengo claro que sea necesario hacer esto ahora: GCS plugin y montar el segmento (bucket)

scelisdev02@cca175-m:/$ cd $HOME
scelisdev02@cca175-m:~$ mkdir spark-data
scelisdev02@cca175-m:~$ gcsfuse --implicit-dirs cftic_cca175_bucket spark-data

set -o vi
-----NOOOO por ahora
----------------------------------------------------------------------------------------------

scelisdev02@cca175-m:~$ ls /tmp/hive
a94c1710-5215-4814-9f46-3868e4ca9090                             a94c1710-5215-4814-9f46-3868e4ca90902589062162393934065.pipeout  stderr
a94c1710-5215-4814-9f46-3868e4ca90901880298804060723487.pipeout  operation_logs

scelisdev02@cca175-m:~$ cd /tmp/dataset/

scelisdev02@cca175-m:/tmp/dataset$ ls
data.sql  retail_db  tables.sql

scelisdev02@cca175-m:/tmp/dataset$ hive -f tables.sql 
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin)

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true
OK
Time taken: 0.885 seconds
OK
Time taken: 2.45 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.089 seconds
OK
Time taken: 0.199 seconds
OK
Time taken: 0.085 seconds
OK
Time taken: 0.149 seconds

scelisdev02@cca175-m:/tmp/dataset$  hive -f data.sql
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr
/bin:/usr/local/sbin:/usr/sbin)
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true
Loading data to table default.orders
OK
Time taken: 2.004 seconds
Loading data to table default.customers
OK
Time taken: 0.534 seconds
Loading data to table default.products
OK
Time taken: 0.519 seconds
Loading data to table default.categories
OK
Time taken: 0.446 seconds

scelisdev02@cca175-m:~$ cd /tmp/dataset
scelisdev02@cca175-m:/tmp/dataset$ pwd
/tmp/dataset

scelisdev02@cca175-m:/tmp/dataset$ hive
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin)

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true
hive> quit;

scelisdev02@cca175-m:/tmp/dataset$ hive -e "SELECT * FROM customers LIMIT 10"
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin)

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true
Query ID = scelisdev02_20210916203902_9f07b7d4-04e8-402e-9fce-cf7dfecdd5f2
Total jobs = 1
Launching Job 1 out of 1
Status: Running (Executing on YARN cluster with App id application_1631779656195_0008)

----------------------------------------------------------------------------------------------
        VERTICES      MODE        STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED  
----------------------------------------------------------------------------------------------
Map 1 .......... container     SUCCEEDED      1          1        0        0       0       0  
----------------------------------------------------------------------------------------------
VERTICES: 01/01  [==========================>>] 100%  ELAPSED TIME: 4.95 s     
----------------------------------------------------------------------------------------------
OK
1       Richard Hernandez       XXXXXXXXX       XXXXXXXXX       6303 Heather Plaza      Brownsville     TX      78521
2       Mary    Barrett XXXXXXXXX       XXXXXXXXX       9526 Noble Embers Ridge Littleton       CO      80126
3       Ann     Smith   XXXXXXXXX       XXXXXXXXX       3422 Blue Pioneer Bend  Caguas  PR      00725
4       Mary    Jones   XXXXXXXXX       XXXXXXXXX       8324 Little Common      San Marcos      CA      92069
5       Robert  Hudson  XXXXXXXXX       XXXXXXXXX       10 Crystal River Mall   Caguas  PR      00725
6       Mary    Smith   XXXXXXXXX       XXXXXXXXX       3151 Sleepy Quail Promenade     Passaic NJ      07055
7       Melissa Wilcox  XXXXXXXXX       XXXXXXXXX       9453 High Concession    Caguas  PR      00725
8       Megan   Smith   XXXXXXXXX       XXXXXXXXX       3047 Foggy Forest Plaza Lawrence        MA      01841
9       Mary    Perez   XXXXXXXXX       XXXXXXXXX       3616 Quaking Street     Caguas  PR      00725
10      Melissa Smith   XXXXXXXXX       XXXXXXXXX       8598 Harvest Beacon Plaza       Stafford        VA      22554
Time taken: 16.541 seconds, Fetched: 10 row(s)

scelisdev02@cca175-m:~$ hive -e "describe customers"
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr
/bin:/usr/local/sbin:/usr/sbin)
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true
OK
customer_id             int                                         
customer_fname          string                                      
customer_lname          string                                      
customer_email          string                                      
customer_password       string                                      
customer_street         string                                      
customer_city           string                                      
customer_state          string                                      
customer_zipcode        string                                      
Time taken: 1.192 seconds, Fetched: 9 row(s)

Para descargar un fichero desde GCP se debe dar la ruta exacta (origen) del fichero:
/home/scelisdev02/scelisdev.txt

scelisdev02@cca175-m:~$ hive -e "describe orders"
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr
/bin:/usr/local/sbin:/usr/sbin)
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true
OK
order_id                int                                         
order_date              string                                      
order_customer_id       int                                         
order_status            string                                      
Time taken: 1.058 seconds, Fetched: 4 row(s)

scelisdev02@cca175-m:~$ hive -e "SELECT * FROM orders LIMIT 10"
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr
/bin:/usr/local/sbin:/usr/sbin)
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true
Query ID = scelisdev02_20210917100040_8ce11b56-3d00-4970-82c3-4a3a59cce714
Total jobs = 1
Launching Job 1 out of 1
Status: Running (Executing on YARN cluster with App id application_1631869916391_0009)
----------------------------------------------------------------------------------------------
        VERTICES      MODE        STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED  
----------------------------------------------------------------------------------------------
Map 1 .......... container     SUCCEEDED      1          1        0        0       0       0  
----------------------------------------------------------------------------------------------
VERTICES: 01/01  [==========================>>] 100%  ELAPSED TIME: 4.59 s     
----------------------------------------------------------------------------------------------
OK
1       2013-07-25 00:00:00.0   11599   CLOSED
2       2013-07-25 00:00:00.0   256     PENDING_PAYMENT
3       2013-07-25 00:00:00.0   12111   COMPLETE
4       2013-07-25 00:00:00.0   8827    CLOSED
5       2013-07-25 00:00:00.0   11318   COMPLETE
6       2013-07-25 00:00:00.0   7130    COMPLETE
7       2013-07-25 00:00:00.0   4530    COMPLETE
8       2013-07-25 00:00:00.0   2911    PROCESSING
9       2013-07-25 00:00:00.0   5657    PENDING_PAYMENT
10      2013-07-25 00:00:00.0   5648    PENDING_PAYMENT
Time taken: 17.861 seconds, Fetched: 10 row(s)

scelisdev02@cca175-m:~$ cd $DEVSH/scripts
scelisdev02@cca175-m:~/training_materials/devsh/scripts$ ls
advance_exercises.sh  create-accounts.sql       setup.sh            streamtest-kafka.sh
catchup.sh            firefox-training.default  streamtest-file.sh  streamtest-network.py

scelisdev02@cca175-m:~/training_materials/devsh/scripts$ hive
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr
/bin:/usr/local/sbin:/usr/sbin)
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true
hive> 

hive> show databases
    > ;
OK
default
Time taken: 0.58 seconds, Fetched: 1 row(s)

hive> create database devsh
    > ;
OK
Time taken: 1.541 seconds

hive> show databases
    > ;
OK
default
devsh
Time taken: 0.072 seconds, Fetched: 2 row(s)

hive> exit
    > ;

scelisdev02@cca175-m:~/training_materials/devsh/scripts$ vi create-accounts.sql 

scelisdev02@cca175-m:~/training_materials/devsh/scripts$ hive -f create-accounts.sql 
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr
/bin:/usr/local/sbin:/usr/sbin)
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true
OK
Time taken: 1.12 seconds
OK
Time taken: 0.463 seconds

scelisdev02@cca175-m:~/training_materials/devsh/scripts$ 

scelisdev02@cca175-m:~/training_materials/devsh/scripts$ hive -e "describe devsh.accounts"
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr
/bin:/usr/local/sbin:/usr/sbin)
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true
OK
acct_num                int                                         
acct_create_dt          timestamp                                   
acct_close_dt           timestamp                                   
first_name              varchar(255)                                
last_name               varchar(255)                                
address                 varchar(255)                                
city                    varchar(255)                                
state                   varchar(255)                                
zipcode                 varchar(255)                                
phone_number            varchar(255)                                
created                 timestamp                                   
modified                timestamp                                   
Time taken: 1.085 seconds, Fetched: 12 row(s)
scelisdev02@cca175-m:~/training_materials/devsh/scripts$ 

scelisdev02@cca175-m:~/training_materials/devsh/scripts$ cd ~

scelisdev02@cca175-m:~$ pwd
/home/scelisdev02

scelisdev02@cca175-m:~$ hdfs dfs -ls /
Found 3 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-16 19:00 /devsh_loudacre
drwxrwxrwt   - hdfs        hadoop          0 2021-09-17 09:28 /tmp
drwxrwxrwt   - hdfs        hadoop          0 2021-09-17 09:30 /user

scelisdev02@cca175-m:~$ hdfs dfs -ls /user
Found 9 items
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/hbase
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/hdfs
drwxrwxrwt   - hdfs        hadoop          0 2021-09-16 13:13 /user/hive
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/mapred
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/pig
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/spark
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-17 09:30 /user/training
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/yarn
drwxrwxrwt   - hdfs        hadoop          0 2021-09-13 12:35 /user/zookeeper

scelisdev02@cca175-m:~$ hdfs dfs -ls /user/hive/
Found 1 items
drwxrwxrwt   - scelisdev02 hadoop          0 2021-09-17 14:37 /user/hive/warehouse
scelisdev02@cca175-m:~$ hdfs dfs -ls /user/hive/warehouse
Found 5 items
drwxrwxrwt   - scelisdev02 hadoop          0 2021-09-16 13:15 /user/hive/warehouse/categories
drwxrwxrwt   - scelisdev02 hadoop          0 2021-09-16 13:15 /user/hive/warehouse/customers
drwxrwxrwt   - scelisdev02 hadoop          0 2021-09-17 14:38 /user/hive/warehouse/devsh.db
drwxrwxrwt   - scelisdev02 hadoop          0 2021-09-16 13:15 /user/hive/warehouse/orders
drwxrwxrwt   - scelisdev02 hadoop          0 2021-09-16 13:15 /user/hive/warehouse/products

scelisdev02@cca175-m:~$ hdfs dfs -ls /user/hive/warehouse/devsh.db/
Found 1 items
drwxrwxrwt   - scelisdev02 hadoop          0 2021-09-17 14:38 /user/hive/warehouse/devsh.db/accounts

scelisdev02@cca175-m:~/training_materials/devsh/scripts$ hive -e "describe devsh.accounts"
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin)

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true
OK
acct_num                int                                         
acct_create_dt          timestamp                                   
acct_close_dt           timestamp                                   
first_name              varchar(255)                                
last_name               varchar(255)                                
address                 varchar(255)                                
city                    varchar(255)                                
state                   varchar(255)                                
zipcode                 varchar(255)                                
phone_number            varchar(255)                                
created                 timestamp                                   
modified                timestamp                                   
Time taken: 1.049 seconds, Fetched: 12 row(s)

/*
salir de hive quit;
hive> quit;
*/

scelisdev02@cca175-m:~/training_materials/devsh/scripts$ hdfs dfs -put ~/training_materials/devsh/data/static_data/accounts/* \/user/hive/warehouse/devsh.db/accounts/


hdfs dfs -put ~/training_materials/devsh/data/static_data/accounts/* /user/hive/warehouse/devsh.db/accounts/

scelisdev02@cca175-m:~/training_materials/devsh/scripts$ hive -e "SELECT first_name, last_name FROM devsh.accounts LIMIT 5"
which: no hbase in (/home/scelisdev02/.local/bin:/home/scelisdev02/bin:/opt/conda/default/bin:/opt/conda/anaconda/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin)

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true
Query ID = scelisdev02_20210921104257_f5c0116f-4b96-4086-8d69-53bc9d456624
Total jobs = 1
Launching Job 1 out of 1
Status: Running (Executing on YARN cluster with App id application_1632219953868_0008)

----------------------------------------------------------------------------------------------
        VERTICES      MODE        STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED  
----------------------------------------------------------------------------------------------
Map 1 .......... container     SUCCEEDED      1          1        0        0       0       0  
----------------------------------------------------------------------------------------------
VERTICES: 01/01  [==========================>>] 100%  ELAPSED TIME: 5.09 s     
----------------------------------------------------------------------------------------------
OK
Donald  Becton
Donna   Jones
Dorthy  Chalmers
Leila   Spencer
Anita   Laughlin
Time taken: 17.412 seconds, Fetched: 5 row(s)

Algunos de los pasos anteriores estan en: 
------------------
Hands-On Exercise: Starting the Exercise Environment
-----------------

Continuar con:
------------------
Hands-On Exercise: Working with HDFS
------------------

..... Now ...... Aún no lo he hecho...

scelisdev02@cca175-m:~$ hdfs dfs -mkdir /user/scelisdev02

scelisdev02@cca175-m:~$ hdfs dfs -ls /user/scelisdev02
Found 2 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 10:43 /user/scelisdev02/.sparkStaging
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset

scelisdev02@cca175-m:~$ hdfs dfs -ls /user/scelisdev02/dataset
Found 1 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db

scelisdev02@cca175-m:~$ hdfs dfs -ls /user/scelisdev02/dataset/retail_db
Found 16 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/categories
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/categories-header
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/customers
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/customers-avro
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/customers-json
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/customers-multiline-json
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/customers-tab-delimited
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/departments
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/order_items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/orders
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/orders_parquet
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/orders_permissions
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/products
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/products_avro
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/products_orc
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/products_sequencefile

scelisdev02@cca175-m:~$ 

scelisdev02@cca175-m:~$ vi $HOME/scelisdev.txt

scelisdev02@cca175-m:~$ hdfs dfs -ls /user/scelisdev02
Found 2 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 10:43 /user/scelisdev02/.sparkStaging
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset

scelisdev02@cca175-m:~$ hdfs dfs -ls /user/scelisdev02/dataset
Found 1 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db

scelisdev02@cca175-m:~$ hdfs dfs -ls /user/scelisdev02/dataset/retail_db
Found 16 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/categories
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/categories-header
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/customers
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/customers-avro
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/customers-json
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/customers-multiline-json
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/customers-tab-delimited
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/departments
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/order_items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/orders
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/orders_parquet
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/orders_permissions
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/products
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/products_avro
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/products_orc
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset/retail_db/products_sequencefile

scelisdev02@cca175-m:~$ find -name catchup.sh
./training_materials/devsh/scripts/catchup.sh

scelisdev02@cca175-m:~$ vi ~/training_materials/devsh/scripts/catchup.sh 

scelisdev02@cca175-m:~$ vi ~/training_materials/devsh/scripts/advance_exercises.sh

..... Now ...... Aún no lo he hecho...

----------------------------------------------------------------------------------------------
| creación de variables de entorno - repetir después de iniciar
----------------------------------------------------------------------------------------------

scelisdev02@cca175-m:~$ DEVSH=~/training_materials/devsh
scelisdev02@cca175-m:~$ echo $DEVSH
/home/scelisdev02/training_materials/devsh

scelisdev02@cca175-m:~$ DEVDATA=~/training_materials/devsh/data
scelisdev02@cca175-m:~$ echo $DEVDATA
/home/scelisdev02/training_materials/devsh/data

scelisdev02@cca175-m:~$ hdfs dfs -put $HOME/training_materials /user/scelisdev02/training

scelisdev02@cca175-m:~$ hdfs dfs -mkdir /devsh_loudacre

scelisdev02@cca175-m:~$ hdfs dfs -ls /
Found 3 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 20:06 /devsh_loudacre
drwxrwxrwt   - hdfs      hadoop          0 2021-02-21 10:34 /tmp
drwxrwxrwt   - hdfs      hadoop          0 2021-02-21 10:43 /user

scelisdev02@cca175-m:~$ hdfs dfs -ls /user/scelisdev02/           <-- en GCP
Found 3 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 18:59 /user/scelisdev02/.sparkStaging
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 16:12 /user/scelisdev02/dataset
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 20:09 /user/scelisdev02/training

scelisdev02@cca175-m:~$ hdfs dfs -ls /user/scelisdev02/training
Found 4 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 20:08 /user/scelisdev02/training/devsh
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 20:08 /user/scelisdev02/training/jep
-rw-r--r--   1 scelisdev02 hadoop      66361 2021-02-21 20:09 /user/scelisdev02/training/jep.zip
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-21 20:09 /user/scelisdev02/training/jes

scelisdev02@cca175-m:~$ hadoop version --> Hadoop 2.10.1

scelisdev02@cca175-m:~$ spark-shell --version --> Using Scala version 2.12.10, OpenJDK 64-Bit Server VM, 1.8.0_275

scelisdev02@cca175-m:~$ hive --version --> Hive 2.3.7

Cloud Storage > browser >
Crear Datos - crear el Bucket - cftic_cca175_bucket

https://console.cloud.google.com/storage/browser/cftic_cca175_bucket

cftic_cca175_bucket/test/in

******************************************************************************
* -O-J-O- <scala> ----Language: Scala
******************************************************************************
youtube - Apache Spark Tutorial - Scala - From Novice to Expert
https://www.youtube.com/watch?v=2Hx6zKlxGlc&list=PLlL9SaZVnVgizWn2Gr_ssHExaQUYik2vp&index=2

ctrl+l => clear 

----Language: Scala
─ spark-shell for Scala
$ spark-shell --master yarn

scelisdev02@cca175-m:~$ spark-shell
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://cca175-m.europe-west1-b.c.cca175-325912.internal:40713
Spark context available as 'sc' (master = yarn, app id = application_1632394097018_0003).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.8
      /_/
         
Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_275)
Type in expressions to have them evaluated.
Type :help for more information.

scala> spark
res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@c76c029

requeriments for any spark job:

Spark context Web UI available at http://cca175-325912-m.europe-west1-d.c.cca175-325912.internal:37055
Spark context available as 'sc' (master = yarn, app id = application_1614035876509_0001).
Spark session available as 'spark'.

scala> val devDF = spark.read.json("/devsh_loudacre/devices.json")
devDF: org.apache.spark.sql.DataFrame = [dev_type: string, devnum: bigint ... 3 more fields]

The printSchema operations is an actions
scala> devDF.printSchema
root
 |-- dev_type: string (nullable = true)
 |-- devnum: long (nullable = true)
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)
 |-- release_dt: string (nullable = true)

The show operations is an actions
scala> devDF.show(5)
+--------+------+--------+-----+--------------------+
|dev_type|devnum|    make|model|          release_dt|
+--------+------+--------+-----+--------------------+
|   phone|     1|Sorrento| F00L|2008-10-21T00:00:...|
|   phone|     2| Titanic| 2100|2010-04-19T00:00:...|
|   phone|     3|  MeeToo|  3.0|2011-02-18T00:00:...|
|   phone|     4|  MeeToo|  3.1|2011-09-21T00:00:...|
|   phone|     5|  iFruit|    1|2008-10-21T00:00:...|
+--------+------+--------+-----+--------------------+
only showing top 5 rows

scala> devDF.count
res7: Long = 50

scala> val makeModelDF = devDF.select("make","model")
makeModelDF: org.apache.spark.sql.DataFrame = [make: string, model: string]

scala> makeModelDF.printSchema
root
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)

scala> makeModelDF.show
+--------+--------------+
|    make|         model|
+--------+--------------+
|Sorrento|          F00L|
| Titanic|          2100|
|  MeeToo|           3.0|
|  MeeToo|           3.1|
|  iFruit|             1|
|  iFruit|             3|
|  iFruit|             2|
|  iFruit|             5|
| Titanic|          1000|
|  MeeToo|           1.0|
|Sorrento|          F21L|
|  iFruit|             4|
|Sorrento|          F23L|
| Titanic|          2200|
|   Ronin|Novelty Note 1|
| Titanic|          2500|
|   Ronin|Novelty Note 3|
|   Ronin|Novelty Note 2|
|   Ronin|Novelty Note 4|
|  iFruit|            3A|
+--------+--------------+
only showing top 20 rows


scala> makeModelDF.count
res10: Long = 50

scala> devDF.select("devnum","make","model").where("make = 'Ronin'").show()
+------+-----+--------------+
|devnum| make|         model|
+------+-----+--------------+
|    15|Ronin|Novelty Note 1|
|    17|Ronin|Novelty Note 3|
|    18|Ronin|Novelty Note 2|
|    19|Ronin|Novelty Note 4|
|    46|Ronin|            S4|
|    47|Ronin|            S1|
|    48|Ronin|            S3|
|    49|Ronin|            S2|
+------+-----+--------------+

scala> val accountsDF = spark.read.table("devsh.accounts")
accountsDF: org.apache.spark.sql.DataFrame = [acct_num: int, acct_create_dt: timestamp ... 10 more fields]

scala> accountsDF.where("zipcode = 94913").write.option("header", "true").csv("/devsh_loudacre/accounts_zip94913")

scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/
Found 3 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-23 21:22 /devsh_loudacre/accounts_zip94913
-rw-r--r--   2 scelisdev02 hadoop       5483 2021-09-23 11:25 /devsh_loudacre/devices.json
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-21 13:42 /devsh_loudacre/kb

scala> accountsDF.printSchema
root
 |-- acct_num: integer (nullable = true)
 |-- acct_create_dt: timestamp (nullable = true)
 |-- acct_close_dt: timestamp (nullable = true)
 |-- first_name: string (nullable = true)
 |-- last_name: string (nullable = true)
 |-- address: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- zipcode: string (nullable = true)
 |-- phone_number: string (nullable = true)
 |-- created: timestamp (nullable = true)
 |-- modified: timestamp (nullable = true)

scala> val acczip94913DF = spark.read.csv("/devsh_loudacre/accounts_zip94913")
acczip94913DF: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 10 more fields]

scala> acczip94913DF.printSchema
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)
 |-- _c9: string (nullable = true)
 |-- _c10: string (nullable = true)
 |-- _c11: string (nullable = true)

scala> val acczip94913DF = spark.read.options(Map("inferSchema"->"true", "header"->"true")).csv("/devsh_loudacre/accounts_zip94913")
acczip94913DF: org.apache.spark.sql.DataFrame = [acct_num: int, acct_create_dt: timestamp ... 10 more fields]

scala> acczip94913DF.printSchema
root
 |-- acct_num: integer (nullable = true)
 |-- acct_create_dt: timestamp (nullable = true)
 |-- acct_close_dt: timestamp (nullable = true)
 |-- first_name: string (nullable = true)
 |-- last_name: string (nullable = true)
 |-- address: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- zipcode: integer (nullable = true)
 |-- phone_number: long (nullable = true)
 |-- created: timestamp (nullable = true)
 |-- modified: timestamp (nullable = true)

scala> val devDF = spark.read.json("/devsh_loudacre/devices.json")
devDF: org.apache.spark.sql.DataFrame = [dev_type: string, devnum: bigint ... 3 more fields]

scala> import org.apache.spark.sql.types._
import org.apache.spark.sql.types._

scala> val devColumns = List(
     | StructField("devnum",LongType),
     | StructField("make",StringType),
     | StructField("model",StringType),
     | StructField("release_dt",TimestampType),
     | StructField("dev_type",StringType))
devColumns: List[org.apache.spark.sql.types.StructField] = List(StructField(devnum,LongType,true), StructField(make,StringType,true), StructField(model,StringType,true), StructField(release_dt,TimestampType,true), StructField(dev_type,StringType,true))

scala> val devSchema = StructType(devColumns)
devSchema: org.apache.spark.sql.types.StructType = StructType(StructField(devnum,LongType,true), StructField(make,StringType,true), StructField(model,StringType,true), StructField(release_dt,TimestampType,true), StructField(dev_type,StringType,true))

scala> val devDF = spark.read.schema(devSchema).json("/devsh_loudacre/devices.json")
devDF: org.apache.spark.sql.DataFrame = [devnum: bigint, make: string ... 3 more fields]

scala> devDF.printSchema
root
 |-- devnum: long (nullable = true)
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)
 |-- release_dt: timestamp (nullable = true)
 |-- dev_type: string (nullable = true)

scala> $"devnum"
res26: org.apache.spark.sql.ColumnName = devnum

scala> "devnum"
res27: String = devnum

scala> devDF.select(devDF("model")).show()
+-----------------+
|            model|
+-----------------+
|"make":"Sorrento"|
| "make":"Titanic"|
|  "make":"MeeToo"|
|  "make":"MeeToo"|
|  "make":"iFruit"|
|  "make":"iFruit"|
|  "make":"iFruit"|
|  "make":"iFruit"|
| "make":"Titanic"|
|  "make":"MeeToo"|
|"make":"Sorrento"|
|  "make":"iFruit"|
|"make":"Sorrento"|
| "make":"Titanic"|
|   "make":"Ronin"|
| "make":"Titanic"|
|   "make":"Ronin"|
|   "make":"Ronin"|
|   "make":"Ronin"|
|  "make":"iFruit"|
+-----------------+
only showing top 20 rows

scala> val devDF = spark.read.json("/devsh_loudacre/devices.json")
devDF: org.apache.spark.sql.DataFrame = [dev_type: string, devnum: bigint ... 3 more fields]

scala> devDF.select($"model", $"dev_type", $"dev_type"*10).show
+--------------+--------+---------------+
|         model|dev_type|(dev_type * 10)|
+--------------+--------+---------------+
|          F00L|   phone|           null|
|          2100|   phone|           null|
|           3.0|   phone|           null|
|           3.1|   phone|           null|
|             1|   phone|           null|
|             3|   phone|           null|
|             2|   phone|           null|
|             5|   phone|           null|
|          1000|   phone|           null|
|           1.0|   phone|           null|
|          F21L|   phone|           null|
|             4|   phone|           null|
|          F23L|   phone|           null|
|          2200|   phone|           null|
|Novelty Note 1|   phone|           null|
|          2500|   phone|           null|
|Novelty Note 3|   phone|           null|
|Novelty Note 2|   phone|           null|
|Novelty Note 4|   phone|           null|
|            3A|   phone|           null|
+--------------+--------+---------------+
only showing top 20 rows

scala> devDF.where(devDF("model").startsWith("A")).show
+--------+------+----+-----+----------+
|dev_type|devnum|make|model|release_dt|
+--------+------+----+-----+----------+
+--------+------+----+-----+----------+


scala> devDF.where(devDF("model").startsWith("")).show
+--------+------+--------+--------------+--------------------+
|dev_type|devnum|    make|         model|          release_dt|
+--------+------+--------+--------------+--------------------+
|   phone|     1|Sorrento|          F00L|2008-10-21T00:00:...|
|   phone|     2| Titanic|          2100|2010-04-19T00:00:...|
|   phone|     3|  MeeToo|           3.0|2011-02-18T00:00:...|
|   phone|     4|  MeeToo|           3.1|2011-09-21T00:00:...|
|   phone|     5|  iFruit|             1|2008-10-21T00:00:...|
|   phone|     6|  iFruit|             3|2011-11-02T00:00:...|
|   phone|     7|  iFruit|             2|2010-05-20T00:00:...|
|   phone|     8|  iFruit|             5|2013-07-02T00:00:...|
|   phone|     9| Titanic|          1000|2008-10-21T00:00:...|
|   phone|    10|  MeeToo|           1.0|2008-10-21T00:00:...|
|   phone|    11|Sorrento|          F21L|2011-02-28T00:00:...|
|   phone|    12|  iFruit|             4|2012-10-25T00:00:...|
|   phone|    13|Sorrento|          F23L|2011-11-21T00:00:...|
|   phone|    14| Titanic|          2200|2010-05-25T00:00:...|
|   phone|    15|   Ronin|Novelty Note 1|2010-06-20T00:00:...|
|   phone|    16| Titanic|          2500|2012-07-21T00:00:...|
|   phone|    17|   Ronin|Novelty Note 3|2013-04-11T00:00:...|
|   phone|    18|   Ronin|Novelty Note 2|2011-10-02T00:00:...|
|   phone|    19|   Ronin|Novelty Note 4|2013-07-02T00:00:...|
|   phone|    20|  iFruit|            3A|2012-07-21T00:00:...|
+--------+------+--------+--------------+--------------------+
only showing top 20 rows


scala> devDF.select($"model", ($"model" * 10).alias("model_10")).show
scala> devDF.select($"model", ($"model" * 10).as("model_10")).show

+--------------+--------+
|         model|model_10|
+--------------+--------+
|          F00L|    null|
|          2100| 21000.0|
|           3.0|    30.0|
|           3.1|    31.0|
|             1|    10.0|
|             3|    30.0|
|             2|    20.0|
|             5|    50.0|
|          1000| 10000.0|
|           1.0|    10.0|
|          F21L|    null|
|             4|    40.0|
|          F23L|    null|
|          2200| 22000.0|
|Novelty Note 1|    null|
|          2500| 25000.0|
|Novelty Note 3|    null|
|Novelty Note 2|    null|
|Novelty Note 4|    null|
|            3A|    null|
+--------------+--------+
only showing top 20 rows


scala> devDF.groupBy($"dev_type").count().show
+--------+-----+                                                                
|dev_type|count|
+--------+-----+
|   phone|   50|
+--------+-----+


--nowscala

scala> 

scala> :help

scala> :type sc
org.apache.spark.SparkContext

scala> :history

# spark followed by a dot) and then the TAB key
scala> spark.
!=   asInstanceOf              createDataFrame   eq             hashCode          newSession   readStream     sqlContext     time       →   
##   baseRelationToDataFrame   createDataset     equals         implicits         notify       sessionState   stop           toString       
+    catalog                   emptyDataFrame    experimental   isInstanceOf      notifyAll    sharedState    streams        udf            
->   close                     emptyDataset      formatted      listenerManager   range        sparkContext   synchronized   version        
==   conf                      ensuring          getClass       ne                read         sql            table          wait           

scala> sys.exit
scelisdev02@cca175-m:~$ 

scala> val usersDF = spark.read.json("users.json")
scala> val users = usersDF.take(3)
usersDF: Array[org.apache.spark.sql.Row] =
  Array([null,Alice,94304],
        [30,Brayden,94304],
        [19,Carla,10036])

Chaining Transformations
scala> val nameAgeDF = usersDF.select("name","age")
scala> val nameAgeOver20DF = nameAgeDF.where("age > 20")
scala> nameAgeOver20DF.show
scala> usersDF.select("name","age").where("age > 20").show

scala> val df = Seq(1,2,3,4,5).toDF()
df: org.apache.spark.sql.DataFrame = [value: int]

scala> df.show()
+-----+
|value|
+-----+
|    1|
|    2|
|    3|
|    4|
|    5|
+-----+

scala> spark
res11: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5cccffc3


scala> spark.(+ tabulador) muestra los metodos disponibles

baseRelationToDataFrame   close   createDataFrame   emptyDataFrame   experimental   listenerManager   range   readStream     sharedState    sql          stop      table   udf       
catalog   

RDD's => inmutable collection basic unit of data in spark
----------------------------------------------------------

scala> spark.emptyDataFrame
res0: org.apache.spark.sql.DataFrame = []

scala> val intArray = Array(1,2,3,4,5,6,7,8,9,0)
intArray: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)

scala> val intRDD = sc.(+ tabulador) muestra los metodos disponibles
accumulable             applicationId    clearCallSite           files                     getRDDStorageInfo     killExecutor      newAPIHadoopFile      requestTotalExecutors   setLocalProperty   uiWebUrl         
accumulableCollection   binaryFiles      clearJobGroup           getAllPools               getSchedulingMode     killExecutors     newAPIHadoopRDD       runApproximateJob       setLogLevel        union            
accumulator             binaryRecords    collectionAccumulator   getCheckpointDir          hadoopConfiguration   killTaskAttempt   objectFile            runJob                  sparkUser          version          
addFile                 broadcast        defaultMinPartitions    getConf                   hadoopFile            listFiles         parallelize           sequenceFile            startTime          wholeTextFiles   
addJar                  cancelAllJobs    defaultParallelism      getExecutorMemoryStatus   hadoopRDD             listJars          range                 setCallSite             statusTracker                       
addSparkListener        cancelJob        deployMode              getLocalProperty          isLocal               longAccumulator   register              setCheckpointDir        stop                                
appName                 cancelJobGroup   doubleAccumulator       getPersistentRDDs         isStopped             makeRDD           removeSparkListener   setJobDescription       submitJob                           
applicationAttemptId    cancelStage      emptyRDD                getPoolForName            jars                  master            requestExecutors      setJobGroup             textFile                            

"parallelize" => método para especificar la cantidad de particiones que queremos crear = número de procesos paralelos 
scala> val intRDD = sc.parallelize(intArray) 
intRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:26

scala> intRDD.first()
res1: Int = 1 

scala> intRDD.take(3)
res2: Array[Int] = Array(1, 2, 3)

scala> intRDD.collect()
res3: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)

scala> intRDD.collect().foreach(println)
1
2
3
4
5
6
7
8
9
0

example: if you have 5 partitions and your cluster has 5 processors, entonces cada pparticion sera "ejecutada" en un procesador ..5 cores executing on 5 partitions available
scala> intRDD.partitions.size
res5: Int = 4

scala> val intList = List(1,2,3,4,5,6,7,8,9,0)
intList: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)

scala> val listRDD = sc.parallelize(intList)
listRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at <console>:26

Podemos establecer las particiones que queremos crear, en este caso 6
scala> val intListRDD = sc.parallelize(intList, 6)
intListRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at parallelize at <console>:26

scala> intListRDD.partitions.size
res6: Int = 6

scala> listRDD.partitions.size
res7: Int = 4

crear un dataframe vacio
scala> spark.emptyDataFrame
res0: org.apache.spark.sql.DataFrame = []

read un fichero
-O-J-O- el fichero esta en hdfs
primero hay que subirlo y luego copiarlo a hdfs
scelisdev02@cca175-m:~$ hdfs dfs -put rotten_tomatoes_critic_reviews.csv /user/scelisdev02

scala> val textFile = sc.textFile("/user/scelisdev02/rotten_tomatoes_critic_reviews.csv")
textFile: org.apache.spark.rdd.RDD[String] = /user/scelisdev02/rotten_tomatoes_critic_reviews.csv MapPartitionsRDD[20] at textFile at <console>:24

scala> textFile.count()
res11: Long = 1130018                                                           

scala> :quit

TODO

Creating DataFrames from Data in Memory

▪ You can also create DataFrames from a collection of in-memory data
  ─ Useful for testing and integrations

val mydata = List(("Josiah","Bartlet"),
                  ("Harry","Potter"))
val myDF = spark.createDataFrame(mydata)
myDF.show
+------+-------+
| _1| _2|
+------+-------+
|Josiah|Bartlet|
| Harry| Potter|
+------+-------+

Defining a Schema Programmatically (Scala)
import org.apache.spark.sql.types._
val columnsList = List(
StructField("pcode", StringType),
StructField("lastName", StringType),
StructField("firstName", StringType),
StructField("age", IntegerType))
val peopleSchema = StructType(columnsList)


-O-J-O- < EXERCICES > -O-J-O- 
--- Scenario 1
All the customer records are stored in the HDFS directory 
dataset/retail_db/customers-tabdelimited 
Data is in text format
Data is tab delimited
Schema is:

"_c0" - customer_id int
"_c1" - customer_fname string
"_c2" - customer_lname string
"_c3" - customer_email string
"_c4" - customer_password string
"_c5" - customer_street string
"_c6" - customer_city string
"_c7" - customer_state string
"_c8" - customer_zipcode string

Output all the customers who live in California 
Use text format for the output files 
Place de result data in dataset/results/scenario1/solution 
Result should only contain records that have state value as "CA" 
Output should only contain customer's full name. Example: Robert Hudson

solución a:
----------

(
spark.read.
option("sep", "\t").
csv( "dataset/retail_db/customers-tab-delimited" ). 
select( "_c1", "_c2", "_c7").
ﬁlter( $"_c7" === "CA" ).
select( concat_ws( " ", $"_c1", $"_c2" ) ). 
write.
mode( "overwrite" ).
text( "dataset/result/scenario1/solution" ) 
)

comprobación:
------------

from pyspark.sql.functions import col                                       
from pyspark.sql.functions import split
(
spark.read.
text( "dataset/result/scenario1/solution" ).
select(
split( col("value"), " ")[0].alias( "ﬁrst_name"),
split( col("value"), " ")[1].alias( "last_name" ) ).
show( 5 )
)

+---------+---------+
|ﬁrst_name|last_name|
+---------+---------+
|     Mary|    Jones|
|Katherine|    Smith|
|     Jane|     Luna|
|   Robert|    Smith|
| Margaret|   Wright|
+---------+---------+
only showing top 5 rows

solución b:
----------
(spark.read.
option("inferScheme", True).
option("delimiter", "\t").
csv("dataset/retail_db/customers-tab-delimited").
toDF(
"customer_id",
"customer_fname",
"customer_lname",
"customer_email",
"customer_password",
"customer_street",
"customer_city",
"customer_state",
"customer_zipcode"
).
createOrReplaceTempView("customer_view")
)
(spark.sql
("""
SELECT concat_ws (" ", "customer_fname", "customer_lname")
FROM customer_view
WHERE customer_state = 'CA'
""").
write.
mode("overwrite").
text("dataset/result/scenario1/solution")
)

OJO - 

+--------------+--------------+
|     ﬁrst_name|     last_name|

******************************************************************************
* -O-J-O- <scala> 
*****************************************************************************

----------------------------------------------------------------------------------------------

******************************************************************************
* -O-J-O- <Python> ----Language: Python
*****************************************************************************
----------------------------------------------------------------------------------------------
Acceso a dhfs by URI: hdfs://nnhost:port/file…
-O-J-O- <python> PySpark is the Python API for Spark.
youtube - Apache Spark Tutorial - Python - 
https://www.youtube.com/watch?v=QUiAc3rWtMA

----Language: Python
─ pyspark for Python
$ pyspark 
$ pyspark --master yarn

ctrl+l => clear 

scelisdev02@cca175-m:~$ pyspark
Python 3.7.4 (default, Aug 13 2019, 20:35:49) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.8
      /_/

Using Python version 3.7.4 (default, Aug 13 2019 20:35:49)
SparkSession available as 'spark'.

>>> devDF = spark.read.json("/devsh_loudacre/devices.json")

The printSchema operations is an actions
>>> devDF.printSchema()                                                         
root
 |-- dev_type: string (nullable = true)
 |-- devnum: long (nullable = true)
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)
 |-- release_dt: string (nullable = true)

The show operations is an actions
>>> devDF.show(5)
+--------+------+--------+-----+--------------------+
|dev_type|devnum|    make|model|          release_dt|
+--------+------+--------+-----+--------------------+
|   phone|     1|Sorrento| F00L|2008-10-21T00:00:...|
|   phone|     2| Titanic| 2100|2010-04-19T00:00:...|
|   phone|     3|  MeeToo|  3.0|2011-02-18T00:00:...|
|   phone|     4|  MeeToo|  3.1|2011-09-21T00:00:...|
|   phone|     5|  iFruit|    1|2008-10-21T00:00:...|
+--------+------+--------+-----+--------------------+
only showing top 5 rows

>>> rows = devDF.take(5)
>>> type(rows)
<class 'list'>
>>> for row in rows: print(row)
... 
Row(dev_type='phone', devnum=1, make='Sorrento', model='F00L', release_dt='2008-10-21T00:00:00.000-07:00')
Row(dev_type='phone', devnum=2, make='Titanic', model='2100', release_dt='2010-04-19T00:00:00.000-07:00')
Row(dev_type='phone', devnum=3, make='MeeToo', model='3.0', release_dt='2011-02-18T00:00:00.000-08:00')
Row(dev_type='phone', devnum=4, make='MeeToo', model='3.1', release_dt='2011-09-21T00:00:00.000-07:00')
Row(dev_type='phone', devnum=5, make='iFruit', model='1', release_dt='2008-10-21T00:00:00.000-07:00')

>>> devDF.count()
50
>>> makeModelDF = devDF.select("make","model")
>>> makeModelDF.count()
50
>>> makeModelDF.show()
+--------+--------------+
|    make|         model|
+--------+--------------+
|Sorrento|          F00L|
| Titanic|          2100|
|  MeeToo|           3.0|
|  MeeToo|           3.1|
|  iFruit|             1|
|  iFruit|             3|
|  iFruit|             2|
|  iFruit|             5|
| Titanic|          1000|
|  MeeToo|           1.0|
|Sorrento|          F21L|
|  iFruit|             4|
|Sorrento|          F23L|
| Titanic|          2200|
|   Ronin|Novelty Note 1|
| Titanic|          2500|
|   Ronin|Novelty Note 3|
|   Ronin|Novelty Note 2|
|   Ronin|Novelty Note 4|
|  iFruit|            3A|
+--------+--------------+
only showing top 20 rows

>>> makeModelDF.printSchema()
root
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)


>>> accountsDF = spark.read.table("devsh.accounts")
>>> accountsDF.printSchema()
root
 |-- acct_num: integer (nullable = true)
 |-- acct_create_dt: timestamp (nullable = true)
 |-- acct_close_dt: timestamp (nullable = true)
 |-- first_name: string (nullable = true)
 |-- last_name: string (nullable = true)
 |-- address: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- zipcode: string (nullable = true)
 |-- phone_number: string (nullable = true)
 |-- created: timestamp (nullable = true)
 |-- modified: timestamp (nullable = true)

>>> accountsDF.where("zipcode = 94913").write.option("header", "true").csv("/devsh_loudacre/accounts_zip94913")

scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/
Found 3 items
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-23 21:40 /devsh_loudacre/accounts_zip94913
-rw-r--r--   2 scelisdev02 hadoop       5483 2021-09-23 11:25 /devsh_loudacre/devices.json
drwxr-xr-x   - scelisdev02 hadoop          0 2021-09-21 13:42 /devsh_loudacre/kb

>>> acczip94913DF = spark.read.csv("/devsh_loudacre/accounts_zip94913")
>>> acczip94913DF.printSchema()
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)
 |-- _c9: string (nullable = true)
 |-- _c10: string (nullable = true)
 |-- _c11: string (nullable = true)

>>> acczip94913DF = spark.read.option("inferschema", "true").csv("/devsh_loudacre/accounts_zip94913") 
>>> acczip94913DF.printSchema()
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)
 |-- _c9: string (nullable = true)
 |-- _c10: string (nullable = true)
 |-- _c11: string (nullable = true)


>>> devDF = spark.read.json("/devsh_loudacre/devices.json")

>>> devDF.printSchema()
root
 |-- devnum: long (nullable = true)
 |-- make: string (nullable = true)
 |-- model: string (nullable = true)
 |-- release_dt: timestamp (nullable = true)
 |-- dev_type: string (nullable = true)

>>> from pyspark.sql.types import *

>>> devColumns = [
... StructField("devnum",LongType()),
... StructField("make",StringType()),
... StructField("release_dt",TimestampType()),
... StructField("dev_type",StringType())]

>>> devSchema = StructType(devColumns)

>>> devDF = spark.read.schema(devSchema).json("/devsh_loudacre/devices.json")

>>> devDF.devnum
Column<b'devnum'>

>>> devDF['devnum']
Column<b'devnum'>

>>> devDF.select(devDF.devnum).show()
+------+
|devnum|
+------+
|     1|
|     2|
|     3|
|     4|
|     5|
|     6|
|     7|
|     8|
|     9|
|    10|
|    11|
|    12|
|    13|
|    14|
|    15|
|    16|
|    17|
|    18|
|    19|
|    20|
+------+
only showing top 20 rows

>>> devDF.select("model", devDF.dev_type, devDF.dev_type*10).show()
+--------------+--------+---------------+
|         model|dev_type|(dev_type * 10)|
+--------------+--------+---------------+
|          F00L|   phone|           null|
|          2100|   phone|           null|
|           3.0|   phone|           null|
|           3.1|   phone|           null|
|             1|   phone|           null|
|             3|   phone|           null|
|             2|   phone|           null|
|             5|   phone|           null|
|          1000|   phone|           null|
|           1.0|   phone|           null|
|          F21L|   phone|           null|
|             4|   phone|           null|
|          F23L|   phone|           null|
|          2200|   phone|           null|
|Novelty Note 1|   phone|           null|
|          2500|   phone|           null|
|Novelty Note 3|   phone|           null|
|Novelty Note 2|   phone|           null|
|Novelty Note 4|   phone|           null|
|            3A|   phone|           null|
+--------------+--------+---------------+
only showing top 20 rows

>>> devDF.select("model", devDF.dev_type, devDF.dev_type+"10").show()
+--------------+--------+---------------+
|         model|dev_type|(dev_type + 10)|
+--------------+--------+---------------+
|          F00L|   phone|           null|
|          2100|   phone|           null|
|           3.0|   phone|           null|
|           3.1|   phone|           null|
|             1|   phone|           null|
|             3|   phone|           null|
|             2|   phone|           null|
|             5|   phone|           null|
|          1000|   phone|           null|
|           1.0|   phone|           null|
|          F21L|   phone|           null|
|             4|   phone|           null|
|          F23L|   phone|           null|
|          2200|   phone|           null|
|Novelty Note 1|   phone|           null|
|          2500|   phone|           null|
|Novelty Note 3|   phone|           null|
|Novelty Note 2|   phone|           null|
|Novelty Note 4|   phone|           null|
|            3A|   phone|           null|
+--------------+--------+---------------+
only showing top 20 rows

>>> devDF.where(devDF.model.startswith("A")).show()
+--------+------+----+-----+----------+
|dev_type|devnum|make|model|release_dt|
+--------+------+----+-----+----------+
+--------+------+----+-----+----------+

>>> devDF.where(devDF.model.startswith("")).show()
+--------+------+--------+--------------+--------------------+
|dev_type|devnum|    make|         model|          release_dt|
+--------+------+--------+--------------+--------------------+
|   phone|     1|Sorrento|          F00L|2008-10-21T00:00:...|
|   phone|     2| Titanic|          2100|2010-04-19T00:00:...|
|   phone|     3|  MeeToo|           3.0|2011-02-18T00:00:...|
|   phone|     4|  MeeToo|           3.1|2011-09-21T00:00:...|
|   phone|     5|  iFruit|             1|2008-10-21T00:00:...|
|   phone|     6|  iFruit|             3|2011-11-02T00:00:...|
|   phone|     7|  iFruit|             2|2010-05-20T00:00:...|
|   phone|     8|  iFruit|             5|2013-07-02T00:00:...|
|   phone|     9| Titanic|          1000|2008-10-21T00:00:...|
|   phone|    10|  MeeToo|           1.0|2008-10-21T00:00:...|
|   phone|    11|Sorrento|          F21L|2011-02-28T00:00:...|
|   phone|    12|  iFruit|             4|2012-10-25T00:00:...|
|   phone|    13|Sorrento|          F23L|2011-11-21T00:00:...|
|   phone|    14| Titanic|          2200|2010-05-25T00:00:...|
|   phone|    15|   Ronin|Novelty Note 1|2010-06-20T00:00:...|
|   phone|    16| Titanic|          2500|2012-07-21T00:00:...|
|   phone|    17|   Ronin|Novelty Note 3|2013-04-11T00:00:...|
|   phone|    18|   Ronin|Novelty Note 2|2011-10-02T00:00:...|
|   phone|    19|   Ronin|Novelty Note 4|2013-07-02T00:00:...|
|   phone|    20|  iFruit|            3A|2012-07-21T00:00:...|
+--------+------+--------+--------------+--------------------+
only showing top 20 rows

>>> devDF.select("model", (devDF.model * 10).alias("model_10")).show()
+--------------+--------+
|         model|model_10|
+--------------+--------+
|          F00L|    null|
|          2100| 21000.0|
|           3.0|    30.0|
|           3.1|    31.0|
|             1|    10.0|
|             3|    30.0|
|             2|    20.0|
|             5|    50.0|
|          1000| 10000.0|
|           1.0|    10.0|
|          F21L|    null|
|             4|    40.0|
|          F23L|    null|
|          2200| 22000.0|
|Novelty Note 1|    null|
|          2500| 25000.0|
|Novelty Note 3|    null|
|Novelty Note 2|    null|
|Novelty Note 4|    null|
|            3A|    null|
+--------------+--------+
only showing top 20 rows

>>> devDF.groupBy("dev_type").count().show()
+--------+-----+                                                                
|dev_type|count|
+--------+-----+
|   phone|   50|
+--------+-----+

>>> l = [('Alice', 1)]
>>> spark.createDataFrame(l).collect()
[Row(_1='Alice', _2=1)]

>>> type(spark.createDataFrame(l).collect())
<class 'list'>

>>> type(spark.createDataFrame(l).show())
+-----+---+
|   _1| _2|
+-----+---+
|Alice|  1|
+-----+---+
<class 'NoneType'>


--nowpyspark

>>> file = sc.textFile("/user/scelisdev02/file.txt")

>>> file.count()
2                                                                               

>>> file.first()
'este curso esta siendo una'

>>> def hasPython(linea):
...     return "una"
... 
>>> filtered = file.filter(hasPython)
>>> filtered.take(3)
['este curso esta siendo una', 'perdida de tiempo']

>>> filtered = file.filter(hasPython)
>>> filtered
PythonRDD[4] at RDD at PythonRDD.scala:53
>>> filtered.take(3)
['este curso esta siendo una', 'perdida de tiempo']
>>> file.collect()
['este curso esta siendo una', 'perdida de tiempo']

en algunas versiones devuelve: u' -> que significa unicode

>>> for i in file.collect():
...     print(i)
... 
este curso esta siendo una
perdida de tiempo

>>> len(i)
17

>>> for i in file.collect():
...     long = (file.map(lambda i: len(i)))
... 

>>> long
PythonRDD[15] at RDD at PythonRDD.scala:53

>>> for i in long.collect():
...     print(i)
... 
26
17

>>> file.map(lambda s: len(s)).reduce(lambda a, b: a+b)
43

>>> file.getNumPartitions()
2

def myFunc(s):
    words = s.split(" ")
    return len(words)

file.flatMap(lambda s: len(s)).reduce(lambda a, b: a+b)

file.flatMap(lambda i: len(i))

>>> file = sc.textFile("/user/scelisdev02/file.txt")
    sc.textFile("file.txt").map(myFunc)

# dependencias ó linage
>>> file.toDebugString()
b'(2) /user/scelisdev02/file.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n |  /user/scelisdev02/file.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []'

# 
>>> file.getNumPartitions()
2


myDF = spark.read.format("txt").option("header","true").load("/user/scelisdev02/file.txt")


TODO

02134,Hopper,Grace,52
94020,Turing,Alan,32
94020,Lovelace,Ada,28
87501,Babbage,Charles,49
02134,Wirth,Niklaus,48

spark.read.option("inferSchema","true").csv("people.csv").printSchema()
root
|-- _c0: integer (nullable = true)
|-- _c1: string (nullable = true)
|-- _c2: string (nullable = true)
|-- _c3: integer (nullable = true)

Inferring the Schema of a CSV File (with Header)
pcode,lastName,firstName,age
02134,Hopper,Grace,52
94020,Turing,Alan,32
94020,Lovelace,Ada,28
87501,Babbage,Charles,49
02134,Wirth,Niklaus,48

spark.read.option("inferSchema","true").option("header","true").csv("people.csv").printSchema()
root
|-- pcode: integer (nullable = true)
|-- lastName: string (nullable = true)
|-- firstName: string (nullable = true)
|-- age: integer (nullable = true

─ The pcode column should be a string

spark.read.option("inferSchema","true").option("header","true").csv("people.csv").printSchema()
root
|-- pcode: integer (nullable = true)
|-- lastName: string (nullable = true)
|-- firstName: string (nullable = true)
|-- age: integer (nullable = true)

Defining a Schema Programmatically (Python)
from pyspark.sql.types import *
columnsList = [
StructField("pcode", StringType()),
StructField("lastName", StringType()),
StructField("firstName", StringType()),
StructField("age", IntegerType())]
peopleSchema = StructType(columnsList)

Eager and Lazy Execution
> usersDF = \
spark.read.json("users.json")
> nameAgeDF = \
usersDF.select("name","age")
> nameAgeDF.show()
+-------+----+
| name| age|
+-------+----+
| Alice|null|
|Brayden| 30|
| Carla| 19|

>>> Ctrl+D or type exit. However





-O-J-O- < EXERCICES > -O-J-O- 
--- Scenario 1
All the customer records are stored in the HDFS directory 
dataset/retail_db/customers-tabdelimited 
Data is in text format
Data is tab delimited
Schema is:

"_c0" - customer_id int
"_c1" - customer_fname string
"_c2" - customer_lname string
"_c3" - customer_email string
"_c4" - customer_password string
"_c5" - customer_street string
"_c6" - customer_city string
"_c7" - customer_state string
"_c8" - customer_zipcode string

Output all the customers who live in California 
Use text format for the output files 
Place de result data in dataset/results/scenario1/solution 
Result should only contain records that have state value as "CA" 
Output should only contain customer's full name. Example: Robert Hudson

solución a:
----------

FROM pyspark.sql.functions import col
FROM pyspark.sql.functions import concat_ws
(
spark.read.
option("sep", "\t").
csv( "dataset/retail_db/customers-tab-delimited" ). 
SELECT( "_c1", "_c2", "_c7").
ﬁlter( col("_c7") == "CA" ).
SELECT( concat_ws( " ", col("_c1"), col("_c2") ) ). 
write.
mode( "overwrite" ).
text( "dataset/result/scenario1/solution" ) 
)

comprobación:
------------

FROM pyspark.sql.functions import col                                       
FROM pyspark.sql.functions import split
(
spark.read.
text( "dataset/result/scenario1/solution" ).
SELECT(
split( col("value"), " ")[0].alias( "ﬁrst_name"),
split( col("value"), " ")[1].alias( "last_name" ) ).
show( 5 )
)

+---------+---------+
|ﬁrst_name|last_name|
+---------+---------+
|     Mary|    Jones|
|Katherine|    Smith|
|     Jane|     Luna|
|   Robert|    Smith|
| Margaret|   Wright|
+---------+---------+
only showing top 5 rows

solución b:
----------
(spark.read.
option("inferScheme", True).
option("delimiter", "\t").
csv("dataset/retail_db/customers-tab-delimited").
toDF(
"customer_id",
"customer_fname",
"customer_lname",
"customer_email",
"customer_password",
"customer_street",
"customer_city",
"customer_state",
"customer_zipcode"
).
createOrReplaceTempView("customer_view")
)
(spark.sql
("""
SELECT concat_ws (" ", "customer_fname", "customer_lname")
FROM customer_view
WHERE customer_state = 'CA'
""").
write.
mode("overwrite").
text("dataset/result/scenario1/solution")
)

OJO - revisar ... esta es la comprobación .. 

+--------------+--------------+
|     ﬁrst_name|     last_name|
+--------------+--------------+
|customer_fname|customer_lname|
|customer_fname|customer_lname|
|customer_fname|customer_lname|
|customer_fname|customer_lname|
|customer_fname|customer_lname|
+--------------+--------------+
only showing top 5 rows




-O-J-O- < EXERCICES > -O-J-O- 



******************************************************************************
* -O-J-O- <Python> ----Language: Python
*****************************************************************************

----------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------
-O-J-O- review

stringMap.foreach(_.toIterator)
stringMap.foreach(_.toIterator.take)
df.map(row => row(0)).foreach(println)
df.map(row => row(0))
pwd
sc
spark.emptyDataFrame

-O-J-O- no funciona
val df = sqlContext
                .read()
                .format("com.databricks.spark.csv")
                .option("inferScheme","true")
                .option("header","true")
                .load("path to/data.csv");


(spark.read.
text( "dataset/result/scenario1/solution" ).
select(
split( col("value"), " ")[0].alias( "first_name"),
split( col("value"), " ")[1].alias( "last_name" ) ).
show( 5 ) )


-O-J-O- no funciona
----------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------
-O-J-O- test


scelisdev02@cca175-m:~$ ls
rotten_tomatoes_critic_reviews.csv  scelisdev.txt  training_materials

scelisdev02@cca175-m:~$ ls training_materials/
devsh  jep  jep.zip  jes
scelisdev02@cca175-m:~$ mkdir test
scelisdev02@cca175-m:~$ mkdir test/in
scelisdev02@cca175-m:~$ mkdir test/out
scelisdev02@cca175-m:~$ mkdir test/rdd

scelisdev02@cca175-m:~$ hdfs dfs -mkdir /user/scelisdev02/test/
scelisdev02@cca175-m:~$ hdfs dfs -mkdir /user/scelisdev02/test/rdd
scelisdev02@cca175-m:~$ hdfs dfs -mkdir /user/scelisdev02/test/in
scelisdev02@cca175-m:~$ hdfs dfs -mkdir /user/scelisdev02/test/out

scelisdev02@cca175-m:~$ hdfs dfs -ls /user/scelisdev02/test/rdd/wordCount*
-rw-r--r--   1 scelisdev02 hadoop        802 2021-02-23 22:49 /user/scelisdev02/test/rdd/wordCount.py
scelisdev02@cca175-m:~$ hdfs dfs -rm /user/scelisdev02/test/rdd/wordCount*
Deleted /user/scelisdev02/test/rdd/wordCount.py
scelisdev02@cca175-m:~$ hdfs dfs -put test/rdd/wordCount.py /user/scelisdev02/test/rdd/wordCount.py

sudo -u hdfs hdfs dfs -copyFromLocal input.txt 
sudo -u hdfs hdfs dfs -chmod 765 input.txt

scelisdev02@cca175-m:~$ hdfs dfs -ls /user/scelisdev02/test/rdd/wordCount*
-rw-r--r--   1 scelisdev02 hadoop        823 2021-02-23 22:54 /user/scelisdev02/test/rdd/wordCount.py

-O-J-O- test
----------------------------------------------------------------------------------------------
job

Job failed with message [/opt/conda/default/bin/python: can't open file '/scelisdev02/test/rdd/wordCount.py': [Errno 2] No such file or directory]. Additional details can be found at: https://console.cloud.google.com/dataproc/jobs/job-f70e6dbc?project=cca175-325912&region=europe-west1 gcloud dataproc jobs wait 'job-f70e6dbc' --region 'europe-west1' --project 'cca175-325912' https://console.cloud.google.com/storage/browser/dataproc-staging-europe-west1-198053966411-pf7lws8o/google-cloud-dataproc-metainfo/bc4ef432-ced4-4d37-8851-cf1256b6da40/jobs/job-f70e6dbc/ gs://dataproc-staging-europe-west1-198053966411-pf7lws8o/google-cloud-dataproc-metainfo/bc4ef432-ced4-4d37-8851-cf1256b6da40/jobs/job-f70e6dbc/driveroutput

**********************************************************************************************
***********************
*   -O-J-O- CCA 175   *   FIN
***********************
**********************************************************************************************

**********************************************************************************************
***********************
*   -O-J-O- CCA 159   *   INICIO - CCA Data Analyst
***********************
**********************************************************************************************
-O-J-O- CCA 159
scelisdev02@cca175-m:~$ ls
analyst.zip  scelisdev.txt  test  training_materials

scelisdev02@cca175-m:~$ unzip analyst.zip

scelisdev02@cca175-m:~$ mv analyst training_materials

scelisdev02@cca175-m:~$ ls -l training_materials/
total 84
drwxrwxrwx 6 scelisdev02 scelisdev02  4096 Apr 16  2020 analyst
drwxr-xr-x 6 scelisdev02 scelisdev02  4096 Feb 21 17:33 devsh
drwxr-xr-x 4 scelisdev02 scelisdev02  4096 Jul 26  2020 jep
-rw-rw-r-- 1 scelisdev02 scelisdev02 66361 Sep 17 14:03 jep.zip
drwxr-xr-x 5 scelisdev02 scelisdev02  4096 Nov 18  2015 jes

scelisdev02@cca175-m:~$ cd training_materials/
scelisdev02@cca175-m:~$ hdfs dfs -put analyst /user/scelisdev02/training/analyst
scelisdev02@cca175-m:~$ hdfs dfs -ls /user/scelisdev02/training/analyst
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-25 10:00 /user/scelisdev02/training/analyst/data
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-25 10:00 /user/scelisdev02/training/analyst/examples
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-25 10:00 /user/scelisdev02/training/analyst/exercises
drwxr-xr-x   - scelisdev02 hadoop          0 2021-02-25 10:01 /user/scelisdev02/training/analyst/scripts

----------------------------------------------------------------------------------------------
creación de variables de entorno - repetir después de iniciar
----------------------------------------------------------------------------------------------
------
bashrc

curso=analyst
base=/home/training/training_materials/${curso}
examples=$base/examples
data=${base}/data
export DEVDATA=${data}
alias catchup=${base}/scripts/catchup.sh

bashrc
------

scelisdev02@cca175-m:~$ ADIR=~/training_materials/analyst
scelisdev02@cca175-m:~$ echo $ADIR 



HUE => Hadoop User Experience - managing files in HDFS ─ 
http://hue_server:8888/hue/home

HDFS - -O-J-O- es de lectura, es decir, no hay nada en HDFS que te permita modificar los ficheros, tu puedes modificar el fichero en local y volver a subirlo incluso con el mismo nombre pero aún así estaría en otro bloque.
Almacenamiento en HDFS - Store en HDFS
Los archivos de datos se dividen en bloques (por defecto 128MB) que se distribuyen en el momento de la carga.
Los bloques se almacenan en los worker nodes (que ejecutan el servicio Hadoop HDFS Data Node service) - almacenan los Data
Cada bloque es replicado en multiples DataNodes (default 3x)
El Master ó NameNode (ejecuta el servicio HDFS Name Node service) - almacena los metaData
The Hadoop Distributed File System (HDFS) is the main storage layer for Hadoop
▪ HDFS chunks (fragmenta) data into blocks and distributes them to the cluster when data is stored
▪ HDFS clusters consist of 
─ A single NameNode to manage file metadata
─ Multiple DataNodes to store data
The hdfs dfs command allows you to use and manage files in HDFS

En HDFS cada INSERT genera un fichero, tened en cuenta que en HDFS los ficheros tienen que ser grandes para que sea eficiente, ya que si fuera pequeños y esten distribuidos, cada vez que se acceda a ellos se "carga" el rendimiento.

Acceso a HDFS --> command line, by URI, Sqoop - ingest from RDBMS, Flume - ingest NearRealTime Data, Hive - SQLAnalytics, HBase - NoSQLDatabase, 

Applications --> MapReduce, JAVA API, RESTful interface

0: jdbc:hive2://localhost:10000> INSERT INTO temporal values ('Maria');
0: jdbc:hive2://localhost:10000> INSERT INTO temporal values ('Jose');
[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/temporal/
Found 3 items
-rwxrwxrwx   1 cloudera supergroup          6 2021-03-19 07:58 /user/hive/warehouse/temporal/000000_0
-rwxrwxrwx   1 cloudera supergroup          6 2021-03-19 08:10 /user/hive/warehouse/temporal/000000_0_copy_1
-rwxrwxrwx   1 cloudera supergroup          5 2021-03-19 08:10 /user/hive/warehouse/temporal/000000_0_copy_2
[cloudera@quickstart ~]$ hdfs dfs -cat /user/hive/warehouse/temporal/000000_0_copy_1
Maria
[cloudera@quickstart ~]$ hdfs dfs -cat /user/hive/warehouse/temporal/000000_0_copy_2
Jose

HDFS - Hadoop commands ( hdfs dfs -ls / hadoop fs )
File System Shell: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html
---------------

display the available commands    -->   hdfs dfs
hdfs dfs -ls /
hdfs dfs -ls /user
hdfs dfs -ls /user/hive/
hdfs dfs -ls /user/hive/warehouse
hdfs dfs -ls /user/scelisdev02
hdfs dfs -ls /user/scelisdev02/dataset
hdfs dfs -ls /user/scelisdev02/dataset/retail_db
hdfs dfs -ls /user/scelisdev02/
hdfs dfs -ls /user/scelisdev02/training
hdfs dfs -ls /user/scelisdev02/training/analyst

Display the contents of the HDFS File
hdfs dfs -cat /user/scelisdev02/test/rdd/wordCount.py

copyFromLocal is a synonym for put; 
hdfs dfs -put dataset/retail_db dataset
hdfs dfs -put analyst /user/scelisdev02/training/analyst
hdfs dfs -put $HOME/training_materials /user/scelisdev02/training

Create a directory called input under the user’s home directory
hdfs dfs -mkdir input

hdfs dfs -mkdir /user/scelisdev02

hdfs://user/scelisdev02/test/rdd/wordCount.py

Delete a file
hdfs dfs -rm input_old/myfile

Delete a set of files using a wildcard
hdfs dfs -rm input_old/*
hdfs dfs -rm /user/scelisdev02/test/rdd/wordCount*

Delete the directory input_old and all its contents
hdfs dfs -rm -r input_old

copyToLocal is a synonym for get
hdfs dfs -get /user/scelisdev02/test
hdfs dfs -get /user/scelisdev02/test otrotest
hdfs dfs -get /devsh_loudacre/activations/ /tmp/devsh_activations

-- (.) - /user/cloudera/
hdfs dfs -mv /user/cloudera/dates/fichero1 .

hdfs dfs -cp /user/cloudera/dates/fichero1 .

----------------------
Hive commands - HiveQL + SQL-92 - Tolerante a fallos
----------------------
performing SQL querie(s)-(consulta(s)) on big data in distributed systems generates and runs MapReduce or Spark jobs
Uses a very fast specialized SQL engine, not MapReduce or Spark
allow you to create tables by defining a schema over existing files with 'CREATE EXTERNAL TABLE' statements.
store data and metadata separately:
─ Metadata (information about the table) is kept in the metastore
─ Data is kept in files in a storage system (like HDFS, Kudu, or S3)kk
differ from RDBMSs in
RDBMS is Schema-on-write a la hora de escribir se comprueba que es correcto
 Hive is Schema-on-read
Hive has limited, experimental support for Transactions and updating individual records
─ Index support: limited with Hive
─ Latency: very low for RDBMSs, high for Hive
─ Storage: can handle much more at a low cost

Parse HiveQL + SQL-92
Make optimizations
Plan execution
Submit job(s) to cluster
Monitor progress

Hive turns HiveQL querie(s)-(consulta(s)) into data processing jobs
Then it submits those jobs to the data processing engine (MapReduce or Spark) to execute on the cluster
querie(s)-(consulta(s)) operate on tables, just as querie(s)-(consulta(s)) do in an RDBMS
Metadata ─ Defined when table is created Specifies structure and location of the data Stored in the metastore(definicion de las tablas), which is contained in an RDBMS
Por defecto - Hive store Data in HDFS Default path: /user/hive/warehouse/
each table's data is stored in a subdirectory named after the table

Generates and runs MapReduce or Spark jobs
Has limited, experimental support for Transactions, Index support and updating individual records
Latency high
Can handle much more storage at a low cost

Tables -> un directorio puede contener multiples ficheros, 
no se permiten subdirectorios, excepto, en las tablas particionadas; 
se pueden definir con expresiones regulares
se puede crear donde quiera

Recupera la metadata desde el metastore cada vez que hace una consulta. (por eso es más lento).

Databases -> lineal no en forma de arbol , 
se debe indicar el directorio donde van a estar sus tablas por defecto: /user/hive/warehouse

SHOW DATABASES;
DESCRIBE DATABASE default; nos indica la location ubicación de las tablas de la database por default
-O-J-O- connect to the default database by default - connecta a la db default por defecto, 
hasta que no le indique con USE la database estará trabajando con default

-- querie(s)-(consulta(s)) table in the default database
SELECT * 
FROM customers;

-- querie(s)-(consulta(s)) table in the other database from default
SELECT * 
FROM database.customers; 

USE database;
-- querie(s)-(consulta(s)) table in other  database
SELECT * 
FROM customers;

-- Show tables in the current database
SHOW TABLES;

-- Show tables in the database indicada
SHOW TABLES IN database;

-- Describe displays basic structure for table - estructura de la tabla
DESCRIBE table; 

-- Describe Input and Output formats, file locations, and other info
DESCRIBE FORMATTED table;

-- Describe other info about tables like: Location: hdfs://quickstart.cloudera:8020/user/hive/warehouse/temporal 
DESCRIBE EXTENDED table;


-- comments 

`select` --> si utiliza una palabra reservada entre ` `, no se considera como palabra reservada 
------
SELECT
------
SELECT cust_id, fname, lname 
FROM customers;

SELECT * 
FROM customers;

-- remove duplicates - remove duplicados
SELECT DISTINCT zipcode 
FROM customers;

-- la columna especificada para hacer el ORDER BY debe estar en el select
SELECT brand, name, price 
FROM products
ORDER BY price DESC;

-- este select falla porque price no esta en el select
SELECT brand, name 
FROM products
ORDER BY price DESC;

-- puedo crear una expresion utilizando un alias y ordenar por este
SELECT brand, name, price - cost AS profit 
FROM products
ORDER BY profit

-- maximum - máximo number of rows returned
LIMIT

-- use ORDER BY with LIMIT for top-N querie(s)-(consulta(s))
SELECT brand, name , price
FORM products
ORDER BY price DESC 
LIMIT 10;

-- string comparison are case-sensitive
SELECT *
FROM orders
WHERE order_id = 1287;

SELECT * 
FROM customers
WHERE state IN ('CA', 'OR', 'WA', 'NV', 'AZ');

-- combine AND, OR, LIKE
SELECT *
FROM customers
WHERE fname LIKE 'Ann%'
AND (city='Seattle' OR city='Portland')

-- alias
SELECT c.fname, c.lname, o.order_date
FROM customers c
JOIN orders o ON(c.cust_id = o.cust_id)
WHERE c.zipcode='94306';

-- subquery in FROM clause
SELECT prod_id, brand, name
FROM
(SELECT *
 FROM products
 WHERE (price - cost) / price > 0.65 -- 10 products with profits > 65%
 ORDER BY price DESC
 LIMIT 10) high_profits
WHERE price > 1000
ORDER BY brand, name;

-- subquery in WHERE clause
SELECT cust_id, fname, lname
FROM customers c
WHERE state = 'NY' AND c.cust_id IN
(SELECT cust_id
 FROM orders
 WHERE order_id > 6650000)
 ORDER BY c.cust_id DESC;

-- subquery uncorrelated - no relacionada  - se hace una sóla vez
-- subquery correlated - relacionada - se hace uno cada vez
En una consulta de base de datos SQL, una subconsulta correlacionada (también conocida como subconsulta sincronizada) es una subconsulta (una consulta anidada dentro de otra consulta) que utiliza valores de la consulta externa.
Como la subconsulta puede ser evaluada una vez por cada fila procesada por la consulta externa, puede ser lenta.

El objetivo es encontrar a todos los empleados cuyo salario esté por encima de la media de su departamento.

SELECT employee_number, name
FROM employees emp
WHERE salary > 
(SELECT AVG(salary)
 FROM employees
 WHERE department = emp.department);

-- In the above query the outer (consulta externa) query is

SELECT employee_number, name
FROM employees emp
WHERE salary > ...

and the inner (consulta interna) query (the correlated subquery) is

SELECT AVG(salary)
FROM employees
WHERE department = emp.department

Correlated and uncorrelated subquerie(s)-(consulta(s))

Una subconsulta puede contener una referencia a un objeto definido en una sentencia padre. 
Esto se llama una referencia externa. 
Una subconsulta que contiene una referencia externa se denomina subconsulta correlacionada. 
Las subconsultas correlacionadas no pueden evaluarse independientemente de la consulta externa porque la subconsulta utiliza los valores de la sentencia padre. 
Un subquery correlated referencia uno o más de los valores del outer query.
Ejemplo de subquery correlated.
SELECT order_id, cust_id, order_date
FROM orders o
WHERE EXISTS (SELECT od.order_id
              FROM order_details od
              WHERE od.order_id = o.order_id 
                AND od.prod_id = 126) ;

Es decir, la subconsulta se realiza para cada fila de la sentencia principal. 
Por lo tanto, los resultados de la subconsulta dependen de la fila activa que se esté evaluando en la sentencia padre.

Por ejemplo, la subconsulta de la siguiente sentencia devuelve un valor que depende de la fila activa de la tabla Productos:

SELECT Name, Description
FROM Products
WHERE Quantity < 2 * (
   SELECT AVG( Quantity )
   FROM SalesOrderItems
   WHERE Products.ID=SalesOrderItems.ProductID );

En este ejemplo, la columna Products.ID de esta subconsulta es la referencia externa. 
La consulta extrae los nombres y las descripciones de los productos cuyas cantidades en stock son inferiores al doble de la cantidad media pedida de ese producto; en concreto, el producto que se está comprobando mediante la cláusula WHERE de la consulta principal. 
La subconsulta hace esto escaneando la tabla SalesOrderItems. Pero la columna Products.ID de la cláusula WHERE de la subconsulta se refiere a una columna de la tabla nombrada en la cláusula FROM de la consulta principal, no de la subconsulta. 
A medida que el servidor de la base de datos se desplaza por cada fila de la tabla Productos, utiliza el valor del ID de la fila actual cuando evalúa la cláusula WHERE de la subconsulta.

Una subconsulta que no contiene referencias a objetos en una sentencia padre se denomina subconsulta no relacionada. 
Un subquery uncorrelated no referencia ningún valor del outer query.

En el ejemplo siguiente, la subconsulta calcula exactamente un valor: la cantidad media de la tabla SalesOrderItems. 
Al evaluar la consulta, el servidor de la base de datos calcula este valor una vez, y compara cada valor del campo Cantidad de la tabla Productos con él para determinar si debe seleccionar la fila correspondiente.

SELECT Name, Description
FROM Products
WHERE Quantity <  2 * (
   SELECT AVG( Quantity )
   FROM SalesOrderItems );

-- subquery scalar - escalar

Una subconsulta escalar es una subconsulta que selecciona sólo una columna o expresión y devuelve una fila. Una subconsulta escalar puede utilizarse en cualquier parte de una consulta SQL en la que se pueda utilizar una columna o expresión.
A subquery scalar usa una función agregada para devolver un único valor que se puede usar en contexto escalares.  
  SELECT prod_id, brand, name, price
    FROM products p
      WHERE p.price > (SELECT AVG(price) FROM products ) 
Las subconsultas escalares pueden utilizarse para calcular varios tipos diferentes de agregaciones (max y avg) en la misma sentencia SQL. 
La siguiente consulta utiliza tanto subconsultas escalares como vistas en línea:

SELECT (SELECT MAX(salary) 
        FROM emp) AS highest_salary, 
        emp_name AS employee_name, (SELECT AVG(bonus) 
                                    FROM commission) AS avg_comission, 
        dept_name 
FROM emp, (SELECT dept_name 
           FROM dept WHERE dept = ‘finance’) dept1;

Data types

Integer types
TINYINT
SMALLINT
INT
BIGINT

Decimal types
FLOAT
DOUBLE
DECIMAL(p,s) when exact values are required! (p: precision total number of digits, s: scale number of digits after the decimal point) 

Character types
STRING
CHAR(n)
VARCHAR(n)

Other simple types
BOOLEAN
TIMESTAMP
BINARY

Data Type Conversion 

Auto-converts a STRING column used in numeric context

SELECT zipcode
FROM customers 
LIMIT 1;

-- Esto funciona porque hace la conversión de STRING a números 
SELECT zipcode + 1.5
FROM customers
LIMIT 1;

Valores fuera de rango
Hive cuando el valor esta fuera de rango,
TINYINT (Rango: -128 to 127)

por ejemplo para almacenar 129 y -999 en un TINYINT almacena NULL
(impala almacena los valores máximos permitidos para el tipo de dato, es decir: 127 y -128)

-- se conecta como anonimo y puede dar errores de seguridad.
beeline -u jdbc:hive2://localhost:10000 

-- connect beeline
alias b="beeline -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera"

-- pide la password, key por terminal
alias b="beeline -u jdbc:hive2://localhost:10000 -n cloudera -P

-- use beeline directamente from command line
alias be="beeline -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera -e"

-- para suprimir informational messages - los mensajes de información
alias bq="beeline --silent=true -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera"

-- para cambiar la presentación en la salida de los datos
beeline --silent=true --outputformat=tsv2 -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera

-- para quitar la cabecera en la salida de los datos
beeline --silent=true --outputformat=tsv2 --showHeader=false -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera

-- para ejecutar directamente HiveQL (hql) desde la cli
-- ex: beeline --silent=true  -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera -e "SELECT * FROM customers LIMIT 10";
-- usando el alias: bqe "SELECT * FROM customers LIMIT 10";
alias bqe="beeline --silent=true -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera -e"

-- para ejecutar hql desde un fichero HiveQL 
-- ex: beeline -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera -f file.hql
-- usando el alias bf file.hql;
alias bf="beeline -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera -f"

-- para ejecutar hql desde un fichero HiveQL 
-- ex: beeline --silent=true -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera -f file.hql
-- usando el alias bsf file.hql;
alias bsf="beeline --silent=true -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera -f"

-- ex: beeline --silent=true  -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera -e "use default; SELECT * FROM customers LIMIT 10";
-- usando el alias bqe
alias bqe "use default; SELECT * FROM customers LIMIT 10";

-- mostrar la dbase en el prompt
-- ex: beeline --silent=true  --showDBInPrompt=trule -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera

[cloudera@quickstart ~]$ beeline -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera
scan complete in 2ms
Connecting to jdbc:hive2://localhost:10000
Connected to: Apache Hive (version 1.1.0-cdh5.13.0)
Driver: Hive JDBC (version 1.1.0-cdh5.13.0)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 1.1.0-cdh5.13.0 by Apache Hive
0: jdbc:hive2://localhost:10000> 
0: jdbc:hive2://localhost:10000> SHOW databases;
0: jdbc:hive2://localhost:10000> DESCRIBE database default;
0: jdbc:hive2://localhost:10000> USE default;
0: jdbc:hive2://localhost:10000> SHOW tables;
0: jdbc:hive2://localhost:10000> DESCRIBE customers;
0: jdbc:hive2://localhost:10000> DESCRIBE FORMATTED customers;
0: jdbc:hive2://localhost:10000> !exit

!connect url --> to connects a different Hive server
!help
!verbose --> more information about querie(s)-(consulta(s))
!exit
!run --> ejecutar un fichero

ejecutar commands desde un fichero:
[cloudera@quickstart ~]$ cat file
use default;
SELECT * 
FROM customers
LIMIT 5;

desde la línea de comandos (cli) directamente con -f y el nombre del fichero
[cloudera@quickstart ~]$ beeline --silent=true -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera -f file

desde beeline 
0: jdbc:hive2://localhost:10000> !run file

desde la línea de comandos (cli) ejecutar con -e un commando:
beeline --silent=true -u jdbc:hive2://localhost:10000 -n cloudera -p cloudera -e "USE ANALYST;SHOW TABLES;USE DEFAULT;SELECT * FROM customers"

OPERATORS - OPERADORES
Arithmetic Operators in HiveQL and Impala SQL
Basic four: +, -, *, /
 84 / 4 = 21
 84 - 4 = 80
Modulo: % This is the remainder from integer division
 25 % 7 = 4
Unary negative: - Unary because it takes only one operand
 -(3 - 5) = -(-2) = 2
No operator for exponentiation Use pow() function instead
 pow(5,3) = 5 * 5 * 5 = 125

Comparison Operators in HiveQL and Impala SQL
Equality and Inequality =, != or <>, <, >, <=, >=
 5 IN (2, 3, 5, 7): true
 6 IN (2, 3, 5, 7): false
BETWEEN incluye el origen y el destino
 5 BETWEEN 2 AND 7: true
 9 BETWEEN 2 AND 7: false
 7 BETWEEN 2 and 7: true
LIKE especie de expresion regular 
 'name@example.com' LIKE '%.com': true
 'Mickey' LIKE 'M*ey': false

Comparisons and NULL Values
Typical operators return NULL when at least one operand is NULL - siempre que se compare con NULL el resultado es NULL
 5 = NULL → NULL
 5 != NULL → NULL
 5 < NULL → NULL
 NULL = NULL → NULL
 NULL != NULL → NULL
Alternative operators are IS (NOT) DISTINCT FROM and <=> 
IS DISTINCT FROM 
 Same as != for non-NULL values
 5 IS DISTINCT FROM NULL → true
 NULL IS DISTINCT FROM NULL → false
IS NOT DISTINCT FROM or <=> (NULL-safe equality) 
 Same as = for non-NULL values
 5 <=> NULL → false
 NULL <=> NULL → true

Logical and Null Operators in HiveQL and Impala SQL
Logical operators
 Binary: AND, OR
 Unary: NOT
Null operators
 IS NULL
 IS NOT NULL

SELECT name, price, cost, price - cost
FROM products;

SELECT name, price, cost, price - cost profit
FROM products;

SELECT name, price, cost
FROM products
WHERE cost >= price;

SELECT name, price, cost, pow(price,3)
FROM products

SELECT * 
FROM employee 
WHERE end_date IS NULL 
LIMIT 20;

SELECT * 
FROM employee 
WHERE end_date IS NOT NULL
LIMIT 20;

SELECT * 
FROM employee 
WHERE end_date <=> NULL
LIMIT 20;

FUNCTIONS - FUNCIONES - no son case sensitives
SELECT fname, lname, concat(fname, ' ' lname) fullname
FROM customers
LIMIT 10;

SHOW FUNCTIONS;

DESCRIBE FUNCTION UPPER;

Test a function
SELECT abs(-459.67);

SELECT upper('Sonia celis');

SCALAR FUNCTIONS - se escriben en minusculas
operan independientemente en los valores de cada fila - 

el número de argumentos depende de la function
rand()
abs(-123)
pow(3,5)

argumentos pueden ser referencias a columnas, valores literales ó expresiones
year(order_dt + 5)
round(price * tax, 2)

Round to specified # of decimals 
round(total_price, 2) 
round(23.492, 2)

Return nearest integer above 
ceil(total_price) 
ceil(23.492)

Return nearest integer below 
floor(total_price) 
floor(23.492)

Return absolute value 
abs(temperature) 
abs(-49)

Return square root 
sqrt(area) 
sqrt(64)

Return a random number 
rand()

GROUP_CONCAT([ALL | DISTINCT] expression [, separator])
SELECT GROUP_CONCAT(cast(n AS STRING),"|")
FROM numbers;
+--------------------------------------+
| group_concat(cast(n as string), '|') |
+--------------------------------------+
| 2|4|6|8|10|1|3|5|7|9                 |
+--------------------------------------+

-- Es necesario incluir en la subconsulta el LIMIT para que lo haga
SELECT GROUP_CONCAT(cast(n AS STRING),"-")
FROM
(SELECT n FROM default.numbers ORDER BY n DESC LIMIT 10) nums;
+--------------------------------------+
| group_concat(cast(n as string), '-') |
+--------------------------------------+
| 10-9-8-7-6-5-4-3-2-1                 |
+--------------------------------------+

Traduce - reemplaza cada caracter de from 
por cada caracter de to en input
translate(string input, string from, string to)
SELECT translate ('hello world','world','earth');
+--------------------------------------------+
| translate('hello world', 'world', 'earth') |
+--------------------------------------------+
| hetta earth                                |
+--------------------------------------------+
reemplaza w - e en hello world - hello eorld
reemplaza o - a en hello world - hella earld
reemplaza r - r en hello world - hella earld
reemplaza l - t en hello world - hetta eartd
reemplaza d - h en hello world - hetta earth

These functions work with TIMESTAMP values

Return current date and time 
SELECT current_timestamp();
2021-03-15 15:31:41

Convert to UNIX format 
SELECT unix_timestamp(current_timestamp());
1615822683

Return fecha actual en UNIX format BIGINT
SELECT unix_timestamp();
+------------------+
| unix_timestamp() |
+------------------+
| 1616220221       |
+------------------+

Convert to string format 
SELECT from_unixtime(unix_timestamp(current_timestamp()));
2021-03-15 15:40:24

Return fecha actual en el formato que le pases
SELECT from_unixtime(unix_timestamp(), 'dd/MMM/yyyy');
+------------------------------------------------+
| from_unixtime(unix_timestamp(), 'dd/mmm/yyyy') |
+------------------------------------------------+
| 20/Mar/2021                                    |
+------------------------------------------------+

Extract date portion - this function doesn't exist in mysql;
to_date(order_dt) 
SELECT to_date(current_timestamp());
2021-03-15

Extract year portion 
year(order_dt) 
SELECT year(current_timestamp()); 
2021

Return # of days between dates 
datediff(ship_dt, order_dt) 
SELECT datediff(current_timestamp(), current_timestamp());
0

https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-DateFunctions

SELECT date_format(current_timestamp(), 'yyyy/MM/dd');
2021/03/16

SELECT date_format('2021-03-16', 'yyyy/MM/dd');
2021/03/16

Built-In String Functions
Convert to uppercase 
SELECT upper('Bob');
BOB

Convert to lowercase 
SELECT lower('Bob');
bob

Remove whitespace at start/end 
SELECT trim(' Bob ');
Bob

Remove only whitespace at start 
SELECT ltrim(' Bob '); 
Bob

Remove only whitespace at end 
SELECT rtrim(' Bob ');
 Bob

Extract portion of string 
SELECT substring('Sammy', 2, 4);
ammy

Replace characters in string 
SELECT translate('Samuel', 'uel', 'my');
Sammy

String Concatenation
SELECT concat('soniacelis', '@gmail.com');
soniacelis@gmail.com  

SELECT concat_ws(' ', 'sonia', 'celis');
sonia celis

SELECT concat_ws('*-*', 'sonia', 'celis', 'vasquez');
sonia*-*celis*-*vasquez

Parsing URLs
url=http://www.example.com/click.php?A=42&Z=105#r1

parse_url(url, 'PROTOCOL') http
parse_url(url, 'HOST') www.example.com
parse_url(url, 'PATH') /click.php
parse_url(url, 'QUERY') A=42&Z=105
parse_url(url, 'QUERY', 'A') 42
parse_url(url, 'QUERY', 'Z') 105
parse_url(url, 'REF') r1

Other Built-In Functions
Convert to another type cast(weight AS INT)
3.581 => 3

Selectively return value if(price > 1000, 'A', 'B') 
1500 => A

Selectively return value (multiple cases)
case
 when price > 1000 then 'A'
 when price < 100 then 'C'
 else 'B'
end
5 => C

Aggregate Functions
combine values from multiple rows

They can work over all rows in a table, returning only one row
SELECT AVG(price) 
FROM products;

They can work over groups of rows
SELECT brand, AVG(price) 
FROM products 
GROUP BY brand;

How many products of each brand are in the products table?
SELECT brand, count(prod_id)
FROM products
GROUP BY brand

Count all rows 
How many products are in the products table?
SELECT COUNT(*)
FROM products;

SELECT count(prod_id) 
FROM products;

Count all rows where field is not NULL 
SELECT COUNT(fname)
FROM products;

Count all rows where field is unique and not NULL 
SELECT COUNT(DISTINCT fname)
FROM products;

Return the largest value 
SELECT MAX(price)
FROM products;

Return the smallest value 
SELECT MIN(price)
FROM products;

Add all supplied values and return result 
SELECT SUM(price)
FROM products;

Return the average of all supplied values 
SELECT AVG(price) 
FROM products;

HAVING
------ 
The HAVING Clause
You cannot filter on aggregate functions using WHERE Use HAVING instead
The aggregate function does not have to be in the SELECT list or the GROUP BY clause

What is the average profit of products in the products table, by brand, for brands with at least 50 products?
SELECT brand, AVG(price-cost) AS avg_profit
FROM products
GROUP BY brand
HAVING COUNT(prod_id) >= 50;

GROUP BY with WHERE and HAVING
How many employees do we have in each state with a salary of less than $20,000?
SELECT state, COUNT(*) AS num FROM employees
WHERE salary < 20000
GROUP BY state;

Which states have more than 400 employees whose salary is less than $20,000?
SELECT state, COUNT(*) AS num 
FROM employees
WHERE salary < 20000
GROUP BY state
HAVING COUNT(*) > 400;

SELECT state, COUNT(*) AS num 
FROM employees
WHERE salary < 20000
GROUP BY state
HAVING num > 400;

CREATE DATABASE
Siempre - Always - definir un directorio por defecto de lo contrario la va a crear en: /user/hive/warehouse/name_database.db
El directorio por defecto de la db default es: /user/hive/warehouse/

-- Si la database no existe la crea, si existe no la crea
CREATE DATABASE IF NOT EXISTS dbname;

-- por defecto la crearia en: /user/hive/warehouse/dbname.db
CREATE DATABASE dbname; 

CREATE TABLE
-- Si el directorio existe no hace nada, si no existe se crea vacio
-- por defecto la crearia en: /user/hive/warehouse/tablename, 
CREATE TABLE tablename (colname DATATYPE, ...)
-- si le digo el nombre de la dbase la crea en: /user/hive/warehouse/dbname.db/tablename
CREATE TABLE dbname.tablename (colname DATATYPE, ...);
-- si no le digo el nombre de la dbase la crea en la que este en uso.
-- si le doy la Localización "Location" specify the directory where table DATA resides, crea los fichero de DATOS en el hdfs en la Location.
[quickstart.cloudera:21000] > use temp01;
[quickstart.cloudera:21000] > CREATE TABLE edades (edad int) Location '/edades'
[quickstart.cloudera:21000] > show tables;
Query: show tables
+---------+
| name    |
+---------+
| edades  |
| table01 |
+---------+
[quickstart.cloudera:21000] > INSERT INTO edades values (10), (20);
[quickstart.cloudera:21000] > SELECT * FROM edades;
+------+
| edad |
+------+
| 10   |
| 20   |
+------+
-- crea los archivos en la Location que le indicamos
[cloudera@quickstart ~]$ hdfs dfs -ls /edades/
Found 2 items
-rw-r--r--   1 impala supergroup          6 2021-03-19 10:42 /edades/804ada32a8d3137d-20ff753900000000_2077787507_data.0.
drwxr-xr-x   - impala supergroup          0 2021-03-19 10:42 /edades/_impala_insert_staging
[cloudera@quickstart ~]$ hdfs dfs -cat /edades/804ada32a8d3137d-20ff753900000000_2077787507_data.0.
10
20

CREATE TABLE dbname.tablename (colname DATATYPE, ...)
 ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY char
 STORED AS {TEXTFILE|SEQUENCEFILE|...};

-- Crea la table y define los campos y los tipos de datos
CREATE TABLE dbname.tablename (colname DATATYPE, ...)
-- Indica como van a ester delimitadas las filas en las tablas - Hive default delimiter is Ctrl+A = 001
 ROW FORMAT DELIMITED 
-- Indica cual es el caracter con el que va a delimitar los campos - '\t' por ejemplo con un tabulador
  FIELDS TERMINATED BY char
-- Indica el formato en el que se van a almacenar los datos en el warehouse - default STORED AS TEXFILE
 STORED AS {TEXTFILE|SEQUENCEFILE|...};

--Create Table a partir de otra (CLONE - only metadatos - NO DATA)
--Tabla MANAGED_TABLE (interna) yo la gestiono, los datos me pertenecen si borro la tabla borro los datos.
  Metadata is removed, Data in HDFS is removed
--Tabla EXTERNA yo NO la gestiono, los datos NO me pertenecen no tengo control sobre ellos puedo borrar la tabla pero no los datos.
  Metadata is removed, Data in HDFS is NOT removed

CREATE [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
  LIKE { [db_name.]table_name | PARQUET 'hdfs_path_of_parquet_file' }
  [COMMENT 'table_comment']
  [STORED AS file_format]
  [LOCATION 'hdfs_path']

-- LIKE no permite cambiar:
 ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY char

-- Crea una tabla nueva con la estructura de la que ya existe
CREATE TABLE IF NOT EXISTS temp01.orders_l
  LIKE default.orders
  COMMENT 'tabla like default.orders'

-- Carga los datos con los mismos datos de la tabla orders
LOAD DATA INPATH '/user/hive/warehouse/orders/'
 INTO TABLE temp01.orders_l;

outside HDFS, a fully qualified path will be needed
LOCATION 's3a://path.to.bucket/orders_s'
LOCATION 'adl://path.to.bucket/orders_s'

-- Create tablas externas - external tables - Cuando borro la tabla (DROP) NO se borran los datos
-- sólo se borran los metadatos

CREATE EXTERNAL TABLE adclicks (
campaign_id STRING,
click_time TIMESTAMP,
keyword STRING,
site STRING,
placement STRING,
was_clicked BOOLEAN,
cost SMALLINT
)
LOCATION '/analyst/dualcore/ad_data';

-- Carga los datos desde un fichero1 ubicado en HDFS que después es borrado, se comporta como un (mv)
LOAD DATA INPATH '/user/cloudera/fichero1'
 INTO TABLE adclicks

-- Carga los datos desde un fichero1, ubicado en HDFS pero sobreescribe lo que hay en la tabla adclicks
-- Luego borra los datos del origen (mv)
LOAD DATA INPATH '/user/cloudera/fichero1'
 OVERWRITE INTO TABLE adclicks

-- Crea una tabla externa, 
CREATE EXTERNAL TABLE IF NOT EXISTS temp01.orders_s (orders_id string, custs_id string, orders_date string)
  COMMENT 'tabla externa para orders_s'
  ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'
  LOCATION '/user/cloudera/data/';

Query: describe temp01.orders_s
+-------------+--------+---------+
| name        | type   | comment |
+-------------+--------+---------+
| orders_id   | string |         |
| custs_id    | string |         |
| orders_date | string |         |
+-------------+--------+---------+

los datos estan en HDFS: LOCATION '/user/cloudera/data/';
y para cargar los datos:

LOAD DATA INPATH '/user/cloudera/data/orders_s.txt'
 INTO TABLE temp01.orders_s

people.csv
pcode,lastName,firstName,age
02134,Hopper,Grace,52
94020,Turing,Alan,32
94020,Lovelace,Ada,28
87501,Babbage,Charles,49
02134,Wirth,Niklaus,48

hdfs dfs -put people.csv /user/scelisdev02/

myDF = spark.read.option("header","true").option("inferSchema","true").csv("/devsh_loudacre/people.csv")
myDF.printSchema()

DROP table IF EXISTS people;
CREATE EXTERNAL TABLE people(
  pcode int, 
  lastName string, 
  firstName string, 
  age int)
ROW FORMAT DELIMITED
        FIELDS TERMINATED BY ','
        LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

file.sql
LOAD DATA INPATH '/user/scelisdev02/people.csv' INTO TABLE people;
hive -f file.sql

DROP DATABASE dbname; 
DROP DATABASE IF EXISTS dbname; 
-- Borra TODO incluso datos en HDFS
DROP DATABASE dbname CASCADE; 

DROP TABLE tablename; 
DROP TABLE IF EXISTS dbname; 

-- modify table or its characteristics - caracteristicas
ALTER TABLE tablename;

ALTER TABLE dbname.tablename RENAME TO dbname.tablename;

-- modify el tipo del campo
ALTER TABLE temp01.orders_l CHANGE order_date order_date STRING;

-- modify tanto nombre del campo como el tipo - sólo cambia el metastore - no cambia el HDFS
ALTER TABLE temp01.orders_l CHANGE order_date neworder_date BIGINT;

-- Reordering Columns Hive Only - sólo cambia el metastore - no cambia el HDFS
esto servirá para los nuevos datos no para los antiguos.
0: jdbc:hive2://localhost:10000> ALTER TABLE jobs CHANGE salary salary INT AFTER id;
0: jdbc:hive2://localhost:10000> ALTER TABLE jobs CHANGE salary salary INT FIRST;

-- Adding Columns from a table - no cambia el HDFS
ALTER TABLE jobs ADD COLUMNS (city STRING, bonus INT);

-- Replace todas las definiciones de las columnas - no cambia el HDFS
ALTER TABLE jobs REPLACE COLUMNS (
  id INT,
  title STRING,
  salary INT );
--
Use SHOW CREATE TABLE to display a statement to reproduce the table.
show create table ex_customers;
+----------------------------------------------------+--+
|                   createtab_stmt                   |
+----------------------------------------------------+--+
| CREATE EXTERNAL TABLE `ex_customers`(              |
|   `cust_id` int,                                   |
|   `fname` string,                                  |
|   `lname` string,                                  |
|   `address` string,                                |
|   `city` string,                                   |
|   `state` string,                                  |
|   `zipcode` string)                                |
| COMMENT 'tabla externa para mysql.analyst_dualcore.customers' |
| ROW FORMAT SERDE                                   |
|   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'  |
| WITH SERDEPROPERTIES (                             |
|   'field.delim'='\t',                              |
|   'serialization.format'='\t')                     |
| STORED AS INPUTFORMAT                              |
|   'org.apache.hadoop.mapred.TextInputFormat'       |
| OUTPUTFORMAT                                       |
|   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' |
| LOCATION                                           |
|   'hdfs://quickstart.cloudera:8020/user/hive/warehouse/exam.db/customers' |
| TBLPROPERTIES (                                    |
|   'COLUMN_STATS_ACCURATE'='false',                 |
|   'numFiles'='4',                                  |
|   'numRows'='-1',                                  |
|   'rawDataSize'='-1',                              |
|   'totalSize'='12577346',                          |
|   'transient_lastDdlTime'='1616521069')            |
+----------------------------------------------------+--+
Displays CREATE TABLE statement to create table in its current state
Use instead of recreating sequence of CREATE and ALTER statements
-- Changing Other Properties - Cambiar Propiedades de las tablas
ALTER TABLE tablename SET TBLPROPERTIES ('EXTERNAL' = 'TRUE')
-- Es para que salte la primera línea de los files cvs - saltarse la cabecera - header
'skip.header.line' = '1' 

VIEWS - TO SIMPLIFYING COMPLEX QUERIES - simplificar querie(s)-(consulta(s))
Realizar este querie(s)-consulta(s) cada vez para una orden... lo mejor es crear una vista - tabla temporal
SELECT o.order_id, order_date, p.prod_id, brand, name
FROM orders o
JOIN order_details d ON (o.order_id = d.order_id)
JOIN products p ON (d.prod_id = p.prod_id)
WHERE o.order_id=6584288;

-- crea una vista (es dinámica, se ejecuta cada vez por lo que no es muy eficiente) 
CREATE VIEW order_info AS
SELECT o.order_id, order_date, p.prod_id, brand, name
FROM orders o
JOIN order_details d ON (o.order_id = d.order_id)
JOIN products p ON (d.prod_id = p.prod_id)

SELECT * 
FROM order_info 
WHERE order_id=6584288;

-- muestra la vista en las tablas - show view
SHOW TABLES;
-- descripción completa de la vista - describe view
DESCRIBE FORMATTED order_info;
-- muestra como se creo la vista - create view
SHOW CREATE TABLE order_info;
-- modifica la vista - modify view
ALTER VIEW order_info AS
SELECT order_id, order_date
FROM orders;
-- renombrar la vista - rename view
ALTER VIEW order_info
RENAME TO order_information;
-- drop view to remove view - delete view - borrar vista
DROP VIEW order_info;

-- Almacenar los datos de salida de un querie(s)-consulta(s) en una tabla QUE DEBE EXISTIR - CREATE TABLE dbname.tablename AS
-- El contenido existente será borrado - deleted

-- Inserted 125 row(s)
[quickstart.cloudera:21000] > CREATE TABLE exam.nyc_customers AS 
SELECT * 
FROM exam.ex_customers 
WHERE state = 'NY' 
  AND city = 'New York' ;

-- Add records - Adiciona registros without deleted - El contenido existente NO se borra
-- Modified 201375 row(s)
[quickstart.cloudera:21000] > INSERT INTO TABLE exam.nyc_customers 
SELECT * 
FROM ex_customers;

-- bc -> 125 + 201375 = 201500
[quickstart.cloudera:21000] > SELECT count(*) 
FROM exam.nyc_customers;
+----------+
| count(*) |
+----------+
| 201500   |
+----------+

[quickstart.cloudera:21000] > INSERT OVERWRITE TABLE exam.nyc_customers
SELECT *
FROM ex_customers
WHERE state = 'NY'
  AND city = 'Brooklyn';

[quickstart.cloudera:21000] > SELECT count(*) 
FROM exam.nyc_customers;
+----------+
| count(*) |
+----------+
| 20       |
+----------+

-- CTAS - Creating Tables Based on Existing Data - 
-- CREATE TABLE dbname.tablename AS SELECT (CTAS)
-- Column definitions are derived from the existing table
-- Column names are derived from the existing names
-- Todo lo que quiera cambiar lo cambio en el SELECT incluso puedo utilizar ALIAS y cambios de tipos con cast
CREATE TABLE ny_customers
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
AS
SELECT cust_id, fname, lname
FROM ex_customers
WHERE state = 'NY';

[quickstart.cloudera:21000] > SELECT cust_id, fname, lname
FROM ny_customers;

[quickstart.cloudera:21000] > SHOW CREATE TABLE ny_customers;

-- Hive Only - Writing Output in HDFS
INSERT OVERWRITE DIRECTORY '/analyst/dualcore/ny/'
 ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
SELECT cust_id AS ny_cust_id, fname, lname, address, city, state, cast(zipcode AS INT) 
FROM exam.ex_customers
WHERE state = 'NY';

0: jdbc:hive2://localhost:10000> INSERT OVERWRITE DIRECTORY '/user/hive/warehouse/exam.db/ny/' 
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' 
SELECT cust_id AS ny_cust_id, fname, lname, address, city, state, cast(zipcode AS INT) 
FROM exam.ex_customers 
WHERE state = 'NY';

[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/exam.db/ny/
Found 1 items
-rwxr-xr-x   1 cloudera supergroup     104240 2021-03-25 07:23 /user/hive/warehouse/exam.db/ny/000000_0

's3a://dualcore/ny/'
'adl://dualcore/ny/'

-- Hive Only - Saving to Multiple Directories in HDFS - una forma de escribirlo
0: jdbc:hive2://localhost:10000> INSERT OVERWRITE DIRECTORY 'ny_customers' 
SELECT * 
FROM exam.ex_customers 
WHERE state='NY' 
  AND city='New York';

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/
drwxr-xr-x   - cloudera cloudera          0 2021-03-25 07:55 /user/cloudera/ny_customers

-- Hive Only - Saving to Multiple Directories in HDFS - otra forma de escribirlo, mismo resultado
0: jdbc:hive2://localhost:10000> FROM ex_customers exc 
INSERT OVERWRITE DIRECTORY 'exny_customers' 
SELECT cust_id, fname, lname 
WHERE state='NY' 
  AND city='New York';

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/
drwxr-xr-x   - cloudera cloudera          0 2021-03-25 08:08 /user/cloudera/exny_customers

-- Hive Only - Saving to Multiple Directories in HDFS - una forma de escribirlo
0: jdbc:hive2://localhost:10000> FROM ex_customers exc 
INSERT OVERWRITE DIRECTORY 'exny_names' 
SELECT fname, lname 
INSERT OVERWRITE DIRECTORY 'exny_count' 
SELECT COUNT(DISTINCT cust_id) 
WHERE state='NY' 
  AND city='New York';

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/
drwxr-xr-x   - cloudera cloudera          0 2021-03-25 08:25 /user/cloudera/exny_count
drwxr-xr-x   - cloudera cloudera          0 2021-03-25 08:25 /user/cloudera/exny_names

-- Hive Only - Saving to Multiple Directories in HDFS - otra forma de escribirlo, mismo resultado
0: jdbc:hive2://localhost:10000> FROM (SELECT * FROM ex_customers WHERE state='NY' AND city='New York') exny_cust 
INSERT OVERWRITE DIRECTORY 'exny_names02' 
SELECT fname, lname 
INSERT OVERWRITE DIRECTORY 'exny_count02' 
SELECT COUNT(DISTINCT cust_id);

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/
drwxr-xr-x   - cloudera cloudera          0 2021-03-25 08:33 /user/cloudera/exny_count02
drwxr-xr-x   - cloudera cloudera          0 2021-03-25 08:33 /user/cloudera/exny_names02

Creating DataFrames from Hive Tables
▪ Apache Hive provides database-like access to data in HDFS
  ─ Applies schemas to HDFS files
  ─ Metadata is stored in the Hive metastore
▪ Spark can read from and write to Hive tables
  ─ Infers the DataFrame schema from the Hive metadata
▪ Spark Hive support must be enabled and configured with location of the Hive warehouse in HDFS

usersDF = spark.read.table("users")

Write data to a Hive metastore table called my_table
  ─ Append the data if the table already exists
  ─ Use an alternate location
    myDF.write. \
    mode("append"). \
    option("path","/loudacre/mydata"). \
    saveAsTable("my_table"




HastaAQUI------------------------------ INDICA QUE A PARTIR DE AQUI esta mezclado Hive and Impala HastaAQUI

Particionamiento - Partitioning - Partitioning improves performance of queries that filter on partition columns
Create using PARTITIONED BY (col_name col_type)

Todos los ficheros de una tabla en un directorio
Todos los ficheros del directorio se leen en cada consulta - es decir - cada vez que accedas a los datos leeras todos los ficheros
Algunas consultas me van a permitir consultar sólo en un directorio espécifico
Cuando se crea una tabla particionada no se incluye el campo por el que se particiona

CREATE EXTERNAL TABLE IF NOT EXISTS customers
        (cust_id  int,
         fname    STRING,
         lname    STRING,
         address  STRING,
         city     STRING,
         state    STRING,
         zipcode  STRING)
COMMENT 'tabla para mysql.analyst_dualcore.customers'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/analyst/dualcore/customers';

-O-J-O- Cuando se crea una tabla particionada no se incluye el campo por el que se particiona

CREATE EXTERNAL TABLE IF NOT EXISTS customers
        (cust_id  int,
         fname    STRING,
         lname    STRING,
         address  STRING,
         city     STRING,
         zipcode  STRING)
PARTITIONED BY (state STRING)
COMMENT 'tabla particionada por estado para mysql.analyst_dualcore.customers'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/analyst/dualcore/customers_by_state';

[cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/customers_by_state

con el particionamiento, en "teoría" se crearia un directorio con el nombre state=(state por ejemplo AK) en: '/analyst/dualcore/customers_by_state' pero la tabla no se entera hasta que no adicionemos la partición:

Cargar los datos en una tabla particionada: Load the data into partitioned tables with static or dynamic partitions

1. De manera estática - static partition 
─ For static, create the partition with ADD PARTITION then load data using PARTITION(col_name=value)
─ Se crean manualmente nuevas particiones utilizando ADD PARTITION
─ Cuando se cargan los datos, se especifica en qué partición se van a almacenar

   Example:

   ALTER TABLE test.customers_p ADD PARTITION (state='NY');

   DESCRIBE FORMATTED test.customers_p; 
   numPartitions                                      | 1     

   [cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/customers_by_state
   Found 1 items
   drwxr-xr-x   - impala supergroup          0 2021-03-31 05:53 /analyst/dualcore/customers_by_state/state=NY

   INSERT OVERWRITE TABLE test.customers_p
   PARTITION(state='NY')
   SELECT cust_id, fname, lname, address, city, zipcode
   FROM test.customers WHERE state='NY';

   [cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/customers_by_state/
   Found 2 items
   drwxr-xr-x   - impala supergroup          0 2021-03-31 06:02 /analyst/dualcore/customers_by_state/_impala_insert_staging
   drwxr-xr-x   - impala supergroup          0 2021-03-31 06:02 /analyst/dualcore/customers_by_state/state=NY

   [cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/customers_by_state/state=NY
   Found 1 items
   -rw-r--r--   1 impala supergroup      99272 2021-03-31 06:02 /analyst/dualcore/customers_by_state/state=NY/7b48df4385e33635-d930360300000000_1232922679_data.0.

   Example:

-- Impala - Crea una tabla externa con una partición - partition
CREATE EXTERNAL TABLE IF NOT EXISTS test.call_logs
        (call_time STRING,
         phone STRING,
         event_type STRING,
         details STRING)
PARTITIONED BY (call_date STRING)
COMMENT 'tabla particionamiento estatico'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/analyst/dualcore/call_logs';

   [cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/
   Found 9 items
   drwxr-xr-x   - impala   supergroup          0 2021-03-31 06:31 /analyst/dualcore/call_logs

-- Impala - adiciona las particiones - partitions
   ALTER TABLE test.call_logs ADD PARTITION (call_date='2021-03-30');
   ALTER TABLE test.call_logs ADD PARTITION (call_date='2021-03-31');

   [cloudera@quickstart data]$ ls /home/cloudera/Desktop/CCAl159/call*
   /home/cloudera/Desktop/CCAl159/call-20210330.log  
   /home/cloudera/Desktop/CCAl159/call-20210331.log

   [cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/call_logs/
   [cloudera@quickstart data]$ hdfs dfs -mkdir /analyst/dualcore/call_logs/call_date=2021-03-30
   [cloudera@quickstart data]$ hdfs dfs -mkdir /analyst/dualcore/call_logs/call_date=2021-03-31

   [cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/call_logs/
   Found 2 items
   drwxr-xr-x   - cloudera supergroup          0 2021-03-31 06:37 /analyst/dualcore/call_logs/call_date=2021-03-30
   drwxr-xr-x   - cloudera supergroup          0 2021-03-31 06:37 /analyst/dualcore/call_logs/call_date=2021-03-31

   [cloudera@quickstart data]$ hdfs dfs -put /home/cloudera/Desktop/CCAl159/call-20210330.log /analyst/dualcore/call_logs/call-20210330.log
   [cloudera@quickstart data]$ hdfs dfs -put /home/cloudera/Desktop/CCAl159/call-20210331.log /analyst/dualcore/call_logs/call-20210331.log
   [cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/call_logs/
   Found 4 items
   -rw-r--r--   1 cloudera supergroup        259 2021-03-31 06:51 /analyst/dualcore/call_logs/call-20210330.log
   -rw-r--r--   1 cloudera supergroup        163 2021-03-31 06:51 /analyst/dualcore/call_logs/call-20210331.log
   drwxr-xr-x   - cloudera supergroup          0 2021-03-31 06:37 /analyst/dualcore/call_logs/call_date=2021-03-30
   drwxr-xr-x   - cloudera supergroup          0 2021-03-31 06:37 /analyst/dualcore/call_logs/call_date=2021-03-31

   -- Hive - add la partición automáticamente, crea el directorio en HDFS y crea la partición en el metastore Para cargar los datos - Añado filas a la tabla
   LOAD DATA INPATH 'call-20210330.log'
   INTO TABLE call_logs
   PARTITION(call_date='2021-03-30');

   0: jdbc:hive2://localhost:10000> LOAD DATA INPATH '/analyst/dualcore/call_logs/call-20210330.log' 
                                    INTO TABLE test.call_logs PARTITION(call_date='2021-03-30');

   -- Para sobreescribir los datos - Sobreescribo el fichero, es decir, sustituye lo que haya en la tabla
   LOAD DATA INPATH 'call-20210330.log'
   OVERWRITE INTO TABLE call_logs
   PARTITION(call_date='2021-03-30');

   0: jdbc:hive2://localhost:10000> LOAD DATA INPATH '/analyst/dualcore/call_logs/call-20210331.log' 
                                    INTO TABLE test.call_logs PARTITION(call_date='2021-03-31');

   [cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/call_logs/
   Found 2 items
   drwxr-xr-x   - cloudera supergroup          0 2021-03-31 06:57 /analyst/dualcore/call_logs/call_date=2021-03-30
   drwxr-xr-x   - cloudera supergroup          0 2021-03-31 07:05 /analyst/dualcore/call_logs/call_date=2021-03-31

   [cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/call_logs/call_date=2021-03-30
   Found 1 items
   -rwxr-xr-x   1 cloudera supergroup        259 2021-03-31 06:51 /analyst/dualcore/call_logs/call_date=2021-03-30/call-20210330.log

   [cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/call_logs/call_date=2021-03-31
   Found 1 items
   -rwxr-xr-x   1 cloudera supergroup        163 2021-03-31 06:51 /analyst/dualcore/call_logs/call_date=2021-03-31/call-20210331.log

[quickstart.cloudera:21000] > SELECT * FROM  test.call_logs;
Fetched 10 row(s) in 10.13s

[quickstart.cloudera:21000] > SELECT * FROM  test.call_logs WHERE call_date = '2021-03-30';
Fetched 6 row(s) in 1.67s

[quickstart.cloudera:21000] > SELECT * FROM  test.call_logs WHERE call_date = '2021-03-31';
Fetched 4 row(s) in 0.63s

2. De manera dinámica - dynamic partition
─ For dynamic, load the data using PARTITION(col_name) and the engine creates the partitions automatically
─ Hive/Impala crea automáticamente las particiones
─ Los datos insertados se almacenan en las particiones correctas en función de los valores de las columnas
Cargar datos con INSERT y PARTITION
─ La(s) columna(s) de partición debe(n) estar incluida(s) en la cláusula PARTITION
─ La(s) columna(s) de la partición debe(n) especificarse en último lugar en la lista SELECT
Hive o Impala crea particiones e inserta datos basados en los valores de la columna de partición
Los valores de la(s) columna(s) de partición no se incluyen en los archivos

INSERT OVERWRITE TABLE test.customers_p
PARTITION(state)
SELECT cust_id, fname, lname, address, city, zipcode, state 
FROM test.customers;

   [cloudera@quickstart data]$ hdfs dfs -ls /analyst/dualcore/customers_by_state
   Found 52 items
   drwxr-xr-x   - impala supergroup          0 2021-03-31 09:51 /analyst/dualcore/customers_by_state/_impala_insert_staging
   drwxr-xr-x   - impala supergroup          0 2021-03-31 09:51 /analyst/dualcore/customers_by_state/state=AK
   drwxr-xr-x   - impala supergroup          0 2021-03-31 09:51 /analyst/dualcore/customers_by_state/state=AL
   ...
   ...
Creará 51 partitions - particiones

[quickstart.cloudera:21000] > SELECT count(dstate.state) 
                                FROM (SELECT DISTINCT state 
                                        FROM test.customers) dstate;
+---------------------+
| count(dstate.state) |
+---------------------+
| 51                  |
+---------------------+

0: jdbc:hive2://localhost:10000> describe formatted test.customers_p;

                              | numPartitions                                      | 51  

─ El valor por defecto de Hive no permite utilizar el particionamiento dinámico, con la intención de evitar que los usuarios creen accidentalmente un gran número de particiones 
  SET hive.exec.dynamic.partition.mode=nonstrict;

0: jdbc:hive2://localhost:10000> SET hive.exec.dynamic.partition.mode=nonstrict;

Nota: Las propiedades de Hive establecidas en Beeline son sólo para la sesión actual. 
El administrador del sistema puede configurar las propiedades de forma permanente

Precaución: Si la columna de la partición tiene muchos valores únicos, se crearán muchas particiones

▪ Existen tres propiedades de configuración de Hive para limitar esto

─ Número máximo de particiones dinámicas que puede crear cualquier nodo implicado en una consulta Por defecto 100
  hive.exec.max.dynamic.partitions.pernode

─ Número total de particiones dinámicas que puede crear una sentencia HiveQL Por defecto, 1000
  hive.exec.max.dynamic.partitions

─ Máximo total de archivos (en todos los nodos) creados por una consulta Por defecto 100000
  hive.exec.max.created.files

-- To view the current partitions in a table
   SHOW PARTITIONS call_logs;

0: jdbc:hive2://localhost:10000> show partitions users_partitioned;
+----------------------+--+
|      partition       |
+----------------------+--+
| country=AU/state=AC  |
| country=AU/state=NS  |
| country=CA/state=AB  |
| country=CA/state=BC  |
| country=US/state=AK  |
| country=US/state=AR  |
| country=US/state=AZ  |
| country=US/state=CA  |
+----------------------+--+

-- Use ALTER TABLE to add partitions
   ALTER TABLE call_logs
   ADD PARTITION (call_date='2016-06-05');

-- Use ALTER TABLE to drop partitions
   ALTER TABLE call_logs
   DROP PARTITION (call_date='2016-06-05');

Format options
─ TEXTFILE: Text files: basic, convenient, human-readable
─ SEQUENCEFILE
─ AVRO: best choice for general-purpose storage
─ PARQUET: best choice for columnar storage 
─ RCFILE
─ ORCFILE (Hive only) 

Consideraciones para la elección de un formato:
─ Ingest pattern - patron de ingestión de los datos: como realizamos la ingesta de los datos, (ex: en un TEXTFILE - los registros se separan con un salto de linea y los campos se separan con un caracter, pero en otros formatos puede ser diferente), es decir, dependiendo de la estructura de los datos.
─ Tool compatibility - hay muchas herramientas asociadas a hdfs que son compatibles con los ficheros  
─ Expected lifetime - cuanto tiempo se quieren tener estos datos, fácilidad de actualización
─ Storage and performance requirements - 

─ TEXTFILE - Text File Format - Good interoperability, but poor performance
Text files are the most basic file type in Hadoop
─ Can be read or written from virtually any programming language
─ Comma- and tab-delimited files are compatible with many applications
Text files are human-readable
─ All values are represented as strings
─ Useful when debugging
At scale, this format is inefficient
─ Representing numeric values as strings wastes storage space
─ Difficult to represent binary data such as images
Often resort to techniques such as Base64 encoding Conversion to/from native types adds performance penalty

─ SEQUENCEFILE - SequenceFile Format - Good performance, but poor interoperability
Sólo sirve en Java para el map-reduce antiguo
SequenceFiles store key-value pairs in a binary container format
─ Less verbose and more efficient than text files
─ Capable of storing binary data such as images
─ Format is Java-specific and tightly coupled to Hadoop

-----------------------
para ficheros avro y parquet revisar este doc --> /home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/CCA175/Crear clúster Spark en Google Cloud v2.pdf
-----------------------
─ AVRO - Apache Avro File Format - Excellent interoperability and performance - specify the avro.schema.url property in TBLPROPERTIES
Best choice for general-purpose storage in Hadoop
efficient data serialization framework, defines a data file format for storing Avro records
─ Similar to SequenceFile format - Efficient storage due to optimized binary encoding
Widely supported throughout the Hadoop ecosystem
Can also be used outside of Hadoop
Ideal for long-term storage of important data
─ Many languages can read and write Avro files
─ Embeds schema in the file, so will always be readable - Los ficheros avro tienen el schema embebido dentro del fichero
─ Schema evolution can accommodate changes
─ Los ficheros de schema *.avsc que se crean con sqoop se generan en el lugar donde se ejecuta el comando

Avro Tools - Herramienta que nos permite extraer el formato de un fichero avro.
(avro-tools getschema hdfs://quickstart/retail/employee/part-m-00000.avro > employee.avsc)

Columnar Formats ─ Very efficient when selecting only a subset of a table’s columns
These organize data storage on disk by column, rather than by row

Organization row-based formats ----------> (almacenamiento en disk: 1 Alice Palo Alto Accountant 2 Bob..)

id name city occupation 
1 Alice Palo Alto Accountant 
2 Bob Sunnyvale Accountant
3 Bob Palo Alto Dentist
4 Bob Palo Alto Manager
5 Carol Palo Alto Manager

Organization columnar formats 	|
	                  	|
		 		|
		 		|
		 		V

(almacenamiento en disk: Alice Bob (x3) Carol - Palo Alto.... )

Columnar File Formats: RCFile and ORCFile
RCFile: Poor performance and limited interoperability
─ A column-oriented format originally created for Hive tables
─ All data stored as strings (inefficient)
ORCFile: Improved performance but limited interoperability
─ An improved version of RCFile
─ Currently supported only in Hive, not Impala
─ More efficient than RCFile, but not well supported outside of Hive

Read an Avro file
myDF = spark.read. \
format("avro"). \
load("/loudacre/myData.avro")

-----------------------
para ficheros avro y parquet revisar este doc --> /home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/CCA175/Crear clúster Spark en Google Cloud v2.pdf
-----------------------
─ PARQUET - Columnar File Formats: Apache Parquet - Excellent interoperability and performance - in Impala, use LIKE PARQUET to specify a Parquet schema
Apache Parquet is an open source columnar format
─ Originally developed by engineers at Cloudera and Twitter
─ Now an Apache Software Foundation project
─ Supported in MapReduce, Hive, Pig, Impala, Spark, and others
─ Schema is embedded in the file (like Avro) - Los ficheros parquet tienen el schema embebido dentro del fichero
Uses advanced optimizations described in Google’s Dremel paper
─ Reduces storage space
─ Increases performance
Most efficient when adding many records at once
Some optimizations rely on identifying repeated patterns
Best choice for column-based access patterns

DataFrames and Apache Parquet files

▪ Parquet is a very common file format for DataFrame data
▪ Features of Parquet
  ─ Optimized binary storage of structured data
  ─ Schema metadata is embedded in the file
  ─ Efficient performance and size for large amounts of data
  ─ Supported by many Hadoop ecosystem tools
    ─ Spark, Hadoop MapReduce, Hive, and others
▪ Use parquet-tools to view Parquet file schema and data
  ─ Use head to display the first few records

─ Use head to display the first few records
$ parquet-tools head mydatafile.parquet

─ Use schema to view the schema
$ parquet-tools schema mydatafile.parquet

─ Write data as Parquet files in the mydata directory
myDF.write.save("mydata")

─ Read Parquet files
parquetDF = spark.read.parquet("/devsh_loudacre/devices_parquet")

Using Avro File Format 
Avro incorpora una definición de esquema en el propio archivo
Una tabla Avro debe tener una definición de esquema

-- Para almacenar el esquema de la tabla en el metastrore:
CREATE TABLE order_details_avro (order_id INT, prod_id INT)
STORED AS AVRO;

-- Para utilizar un archivo de esquema independiente, especifique la propiedad avro.schema.url
CREATE TABLE order_details_avro
STORED AS AVRO
TBLPROPERTIES ('avro.schema.url'='hdfs://analyst/dualcore/order_details.avsc');

-- Avro schemas are represented in JSON - Para crear una tabla usando un esquema JSON especificar la propiedad avro.schema.literal 
CREATE TABLE order_details_avro
STORED AS AVRO
TBLPROPERTIES ('avro.schema.literal'=
'{"name": "order_details",
  "type": "record",
  "fields": [
   {"name": "order_id", "type": "int"},
   {"name": "prod_id", "type": "int"}
  ]}');

Using Parquet File Format 
-- Create a new table stored in Parquet format
CREATE TABLE order_details_parquet (
order_id INT,
prod_id INT)
STORED AS PARQUET;

-- Load data from another table into a Parquet table
INSERT OVERWRITE TABLE order_details_parquet
SELECT * FROM order_details;

-- Parquet and Avro embeds a schema definition in the file itself
-- In Impala, use LIKE PARQUET to create a table using the column definitions of an existing Parquet data file

Example: Create a new table to access an existing Parquet file in HDFS

-- Impala Only - Crea una tabla extrayendo el esquema que esta indicado en LIKE PARQUET (es un fichero parquet), y lo almacena como Parquet
CREATE EXTERNAL TABLE ad_data
LIKE PARQUET '/dualcore/ad_data/datafile1.parquet'
STORED AS PARQUET
LOCATION '/dualcore/ad_data/';

-- Impala - Crea una tabla extrayendo el esquema que esta indicado en LIKE PARQUET (es un fichero parquet), y lo almacena como text parquet
CREATE EXTERNAL TABLE ad_data
LIKE PARQUET '/dualcore/ad_data/datafile1.parquet'
LOCATION '/dualcore/ad_data/';

El esquema de una tabla parquet siempre esta en el metastore

UNION ALL unifies output from multiple SELECTs into a single result set
─ The order and types of columns in each query must match
─ In Hive, the names of columns also must match

SELECT cast(cust_id AS string) AS id, fname, lname
FROM customers
WHERE state = 'NY'
LIMIT 10; 

SELECT emp_id AS id, fname, lname
FROM employees
WHERE state = 'NY'
LIMIT 10; 

UNION ALL
---------

SELECT cast(cust_id AS string) AS id, fname, lname
FROM customers
WHERE state = 'NY'
UNION ALL
SELECT emp_id AS id, fname, lname
FROM employees
WHERE state = 'NY'

Without the ALL keyword, UNION also removes duplicate values

─ Impala supports this
─ Hive supports this as of version 1.2.0

Joins - Joining disparate datasets is a common operation
─ Uses a shared column between the datasets
─ Combines rows by matching values in the shared columns
Hive and Impala support several types of joins
─ Inner joins
─ Outer joins (left, right, and full)
─ Cross joins
─ Left semi-joins
Join conditions must use equality comparisons when using Hive
─ Valid: customers.cust_id = orders.cust_id
─ Invalid: customers.cust_id <> orders.cust_id
─ Impala supports some non-equijoin queries
For best performance in Hive, list the largest table last in your query

0: jdbc:hive2://localhost:10000> describe analyst.customers;
+-----------+------------+----------+--+
| col_name  | data_type  | comment  |
+-----------+------------+----------+--+
| cust_id   | int        |          |
| fname     | string     |          |
| lname     | string     |          |
| address   | string     |          |
| city      | string     |          |
| state     | string     |          |
| zipcode   | string     |          |
+-----------+------------+----------+--+
0: jdbc:hive2://localhost:10000> describe analyst.orders;
+-------------+------------+----------+--+
|  col_name   | data_type  | comment  |
+-------------+------------+----------+--+
| order_id    | int        |          |
| cust_id     | int        |          |
| order_date  | string     |          |
+-------------+------------+----------+--+


0: jdbc:hive2://localhost:10000> SELECT count(cust_id) 
FROM analyst.orders;
+----------+--+
|   _c0    |
+----------+--+
| 1662951  |
+----------+--+
0: jdbc:hive2://localhost:10000> SELECT count(cust_id) 
FROM analyst.customers;
+---------+--+
|   _c0   |
+---------+--+
| 201375  |
+---------+--+

SELECT count(gorders.total)
FROM ( SELECT cust_id, count (order_id) total   
         FROM analyst.orders
       GROUP BY cust_id) gorders;

[quickstart.cloudera:21000] > SELECT sum(gorders.total)
FROM ( SELECT cust_id, count (order_id) total   
         FROM analyst.orders
       GROUP BY cust_id) gorders;
+--------------------+
| sum(gorders.total) |
+--------------------+
| 1662951            |
+--------------------+
Fetched 1 row(s) in 2.58s
-- 
[quickstart.cloudera:21000] > SELECT count(gorders.total)
                            > FROM ( SELECT cust_id, count (order_id) total
                            >          FROM analyst.orders
                            >        GROUP BY cust_id) gorders
                            > ;
+----------------------+
| count(gorders.total) |
+----------------------+
| 191961               |
+----------------------+
mysql> CREATE TABLE IF NOT EXISTS custo ( cust_id VARCHAR(1) PRIMARY KEY, name VARCHAR(20), country VARCHAR(2) );
Query OK, 0 rows affected (0.16 sec)

mysql> INSERT INTO custo VALUES ('a', 'Alice', 'us'), ('b', 'Bob', 'ca'), ('c', 'Carlos', 'mx'), ('d', 'Dieter', 'de');
Query OK, 4 rows affected (0.03 sec)
Records: 4  Duplicates: 0  Warnings: 0

mysql> select * from custo;
+---------+--------+---------+
| cust_id | name   | country |
+---------+--------+---------+
| a       | Alice  | us      |
| b       | Bob    | ca      |
| c       | Carlos | mx      |
| d       | Dieter | de      |
+---------+--------+---------+
4 rows in set (0.00 sec)

mysql> CREATE TABLE IF NOT EXISTS orden ( order_id INT(1) PRIMARY KEY, cust_id VARCHAR(1), total INT(4) );
Query OK, 0 rows affected (0.00 sec)

mysql> INSERT INTO orden VALUES (1, 'a', 1539), (2,'c',1871), (3,'a',6352), (4,'b',1456), (5,'z',2137);
Query OK, 5 rows affected (0.00 sec)
Records: 5  Duplicates: 0  Warnings: 0

mysql> select * from orden;
+----------+---------+-------+
| order_id | cust_id | total |
+----------+---------+-------+
|        1 | a       |  1539 |
|        2 | c       |  1871 |
|        3 | a       |  6352 |
|        4 | b       |  1456 |
|        5 | z       |  2137 |
+----------+---------+-------+
5 rows in set (0.00 sec)

sqoop-import --connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root --password cloudera \
--table custo \
--target-dir '/user/hive/warehouse/custo' \
--delete-target-dir \
-m 1 \
--as-avrodatafile

sqoop-import --connect jdbc:mysql://quickstart:3306/analyst_dualcore \
--username root --password cloudera \
--table orden \
--target-dir '/user/hive/warehouse/orden' \
--delete-target-dir \
-m 1 \
--as-avrodatafile

[cloudera@quickstart CCAl159]$ hdfs dfs -put /home/cloudera/custo.avsc /user/cloudera/schemas/custo.avsc
[cloudera@quickstart CCAl159]$ hdfs dfs -put /home/cloudera/orden.avsc /user/cloudera/schemas/orden.avsc



[quickstart.cloudera:21000] > DROP TABLE IF EXISTS test.custo;
[quickstart.cloudera:21000] > CREATE TABLE IF NOT EXISTS test.custo
                            > STORED AS AVRO
                            > LOCATION '/user/hive/warehouse/custo'
                            > TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera:8020/user/cloudera/schemas/custo.avsc'); 
                            > ;

[quickstart.cloudera:21000] > SELECT * FROM test.custo ;
+---------+--------+---------+
| cust_id | name   | country |
+---------+--------+---------+
| a       | Alice  | us      |
| b       | Bob    | ca      |
| c       | Carlos | mx      |
| d       | Dieter | de      |
+---------+--------+---------+

[quickstart.cloudera:21000] > DROP TABLE IF EXISTS test.orden;
[quickstart.cloudera:21000] > CREATE TABLE IF NOT EXISTS test.orden
STORED AS AVRO
LOCATION '/user/hive/warehouse/orden'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera:8020/user/cloudera/schemas/orden.avsc');

[quickstart.cloudera:21000] > SELECT * FROM test.orden;
+----------+---------+-------+
| order_id | cust_id | total |
+----------+---------+-------+
| 1        | a       | 1539  |
| 2        | c       | 1871  |
| 3        | a       | 6352  |
| 4        | b       | 1456  |
| 5        | z       | 2137  |
+----------+---------+-------+
+-----------------------+
|-O-J-O- JOIN's - join's|
+-----------------------+

-- Inner joins exclude records with non-matching key values
[quickstart.cloudera:21000] > SELECT c.cust_id, name, total
FROM custo c
JOIN orden o ON (c.cust_id = o.cust_id);
+---------+--------+-------+
| cust_id | name   | total |
+---------+--------+-------+
| a       | Alice  | 6352  |
| a       | Alice  | 1539  |
| b       | Bob    | 1456  |
| c       | Carlos | 1871  |
+---------+--------+-------+

-- Inner joins exclude records with non-matching key values
[quickstart.cloudera:21000] > SELECT c.cust_id, name, total
FROM custo c
INNER JOIN orden o ON (c.cust_id = o.cust_id);
+---------+--------+-------+
| cust_id | name   | total |
+---------+--------+-------+
| a       | Alice  | 6352  |
| a       | Alice  | 1539  |
| b       | Bob    | 1456  |
| c       | Carlos | 1871  |
+---------+--------+-------+

-- Inner joins exclude records with non-matching key values
[quickstart.cloudera:21000] > SELECT c.cust_id, name, total
FROM custo c, orden o WHERE (c.cust_id = o.cust_id);
+---------+--------+-------+
| cust_id | name   | total |
+---------+--------+-------+
| a       | Alice  | 6352  |
| a       | Alice  | 1539  |
| b       | Bob    | 1456  |
| c       | Carlos | 1871  |
+---------+--------+-------+

-- Contain all records from the right (second) table, and Contain records from the left only if they match
-- which rows are excluded: records non-matching from the left (first) table
[quickstart.cloudera:21000] > SELECT c.cust_id, name, total
FROM custo c
RIGHT OUTER JOIN orden o ON (c.cust_id = o.cust_id);
+---------+--------+-------+
| cust_id | name   | total |
+---------+--------+-------+
| a       | Alice  | 6352  |
| a       | Alice  | 1539  |
| b       | Bob    | 1456  |
| c       | Carlos | 1871  |
| NULL    | NULL   | 2137  |
+---------+--------+-------+

-- Contain all records from the left (first) table, and Contain records from the right only if they match
-- which rows are excluded: records non-matching from the right (second) table
[quickstart.cloudera:21000] > SELECT c.cust_id, name, total
FROM custo c
LEFT OUTER JOIN orden o ON (c.cust_id = o.cust_id);
+---------+--------+-------+
| cust_id | name   | total |
+---------+--------+-------+
| a       | Alice  | 6352  |
| a       | Alice  | 1539  |
| b       | Bob    | 1456  |
| c       | Carlos | 1871  |
| d       | Dieter | NULL  |
+---------+--------+-------+

-- Outer joins include all records with non-matching key values
-- which rows are excluded: any records are excluded
[quickstart.cloudera:21000] > SELECT c.cust_id, name, total
FROM custo c
FULL OUTER JOIN orden o ON (c.cust_id = o.cust_id);
+---------+--------+-------+
| cust_id | name   | total |
+---------+--------+-------+
| a       | Alice  | 6352  |
| a       | Alice  | 1539  |
| b       | Bob    | 1456  |
| c       | Carlos | 1871  |
| d       | Dieter | NULL  |
| NULL    | NULL   | 2137  |
+---------+--------+-------+

UNMATCHED Entries

-- Outer joins include records with non-matching key values
-- WHERE force list all records with out matching
[quickstart.cloudera:21000] > SELECT c.cust_id, name, total
FROM custo c
FULL OUTER JOIN orden o ON (c.cust_id = o.cust_id)
WHERE c.cust_id IS NULL
OR o.total IS NULL;
+---------+--------+-------+
| cust_id | name   | total |
+---------+--------+-------+
| d       | Dieter | NULL  |
| NULL    | NULL   | 2137  |
+---------+--------+-------+

NULL Values in Join Key Columns - NULL values typically are not matched
NULL-Safe Join <=> Use NULL-safe equality to match NULLs
SELECT c.cust_id, name, total
FROM custo c
JOIN orden o
ON (c.cust_id <=> o.cust_id);
+---------+--------+-------+
| cust_id | name   | total |
+---------+--------+-------+
| a       | Alice  | 6352  |
| a       | Alice  | 1539  |
| b       | Bob    | 1456  |
| c       | Carlos | 1871  |
+---------+--------+-------+

-- Probar con tablas que contengan NULL - si hay match deberia mostrar los siguientes registros
[quickstart.cloudera:21000] > SELECT c.cust_id, name, total                                       
FROM customers_with_null c
JOIN orders_with_null o
ON (c.cust_id <=> o.cust_id);
+---------+--------+-------+
| cust_id | name   | total |
+---------+--------+-------+
| a       | Alice  | 6352  |
| a       | Alice  | 1539  |
| b       | Bob    | 1456  |
| c       | Carlos | 1871  |
| NULL    | Unknown| 1789  | --> normalmente no se permiten NULL values for primary keys
+---------+--------+-------+

-- Non-Equijoin - NON-EQUIJOIN(Impala Only)
Join conditions expressed as one or more equality conditions
Impala allows non-equijoins
Join conditions expressed by inequalities
─ Unequal: != or <>
─ Less than: <, <=
─ Greater than: >, >=
─ BETWEEN
Join conditions expressed by other comparisons
─ IN
─ LIKE
─ REGEXP

[quickstart.cloudera:21000] > CREATE TABLE IF NOT EXISTS test.employees (fname STRING, lname STRING, salary INT);
[quickstart.cloudera:21000] > INSERT INTO test.employees VALUES('sean', 'Baca', 19554),('Ana', 'Bobo', 26596), ('Mary','Beal',53485), ('Gary', 'Burt', 21191);
[quickstart.cloudera:21000] > CREATE TABLE IF NOT EXISTS test.salary_grades (grade INT, min_amt INT, max_amt INT);
[quickstart.cloudera:21000] > INSERT INTO test.salary_grades VALUES(1,10000,19999),(2,20000,29999),(3,30000,39999),(4,40000,49999),(5,50000,59999);

SELECT * FROM employees;
+------------------+------------------+-------------------+--+
| employees.fname  | employees.lname  | employees.salary  |
+------------------+------------------+-------------------+--+
| sean             | Baca             | 19554             |
| Ana              | Bobo             | 26596             |
| Mary             | Beal             | 53485             |
| Gary             | Burt             | 21191             |
+------------------+------------------+-------------------+-

SELECT * FROM salary_grades;
+----------------------+------------------------+------------------------+--+
| salary_grades.grade  | salary_grades.min_amt  | salary_grades.max_amt  |
+----------------------+------------------------+------------------------+--+
| 1                    | 10000                  | 19999                  |
| 2                    | 20000                  | 29999                  |
| 3                    | 30000                  | 39999                  |
| 4                    | 40000                  | 49999                  |
| 5                    | 50000                  | 59999                  |
+----------------------+------------------------+------------------------+--

-- Non-Equijoin - Inequalities (Impala Only)
[quickstart.cloudera:21000] > SELECT fname, lname, grade FROM
                            > employees e
                            > JOIN salary_grades g
                            > ON (salary >= min_amt
                            > AND salary <= max_amt);
+-------+-------+-------+
| fname | lname | grade |
+-------+-------+-------+
| sean  | Baca  | 1     |
| Ana   | Bobo  | 2     |
| Mary  | Beal  | 5     |
| Gary  | Burt  | 2     |
+-------+-------+-------+
-- Non-Equijoin - Inequalities (Impala Only)
[quickstart.cloudera:21000] > SELECT fname, lname, grade FROM
                            > employees e
                            > JOIN salary_grades g
                            > ON (salary BETWEEN
                            > min_amt AND max_amt);
+-------+-------+-------+
| fname | lname | grade |
+-------+-------+-------+
| sean  | Baca  | 1     |
| Ana   | Bobo  | 2     |
| Mary  | Beal  | 5     |
| Gary  | Burt  | 2     |
+-------+-------+-------+

-- Non-Equijoin - Inequalities (Impala Only)
[quickstart.cloudera:21000] > SELECT fname, lname, grade FROM
                            > employees e
                            > JOIN salary_grades g
                            > ON (salary >= min_amt
                            > AND salary <= max_amt);
+-------+-------+-------+
| fname | lname | grade |
+-------+-------+-------+
| sean  | Baca  | 1     |
| Ana   | Bobo  | 2     |
| Mary  | Beal  | 5     |
| Gary  | Burt  | 2     |
+-------+-------+-------+


[quickstart.cloudera:21000] > CREATE TABLE IF NOT EXISTS employees_citizenship (name STRING, citizenship STRING, residence STRING);
[quickstart.cloudera:21000] > CREATE TABLE IF NOT EXISTS eu_countries (country STRING, code STRING);
[quickstart.cloudera:21000] > INSERT INTO employees_citizenship VALUES('Alice','us','fr'), ('Bob','gb','us'), ('Carlos','mx','us'), ('Dieter','de','de');
[quickstart.cloudera:21000] > INSERT INTO eu_countries VALUES ('France','fr'), ('Germany','de'), ('United Kingdom','gb');

[quickstart.cloudera:21000] > SELECT * FROM employees_citizenship;
+--------+-------------+-----------+
| name   | citizenship | residence |
+--------+-------------+-----------+
| Alice  | us          | fr        |
| Bob    | gb          | us        |
| Carlos | mx          | us        |
| Dieter | de          | de        |
+--------+-------------+-----------+
[quickstart.cloudera:21000] > SELECT * FROM eu_countries;
+----------------+------+
| country        | code |
+----------------+------+
| France         | fr   |
| Germany        | de   |
| United Kingdom | gb   |
+----------------+------+

-- Non-Equijoin - Inequalities (Impala Only)
[quickstart.cloudera:21000] > SELECT * FROM employees_citizenship e JOIN eu_countries c
                            > ON (e.citizenship IN (c.code));
+--------+-------------+-----------+----------------+------+
| name   | citizenship | residence | country        | code |
+--------+-------------+-----------+----------------+------+
| Bob    | gb          | us        | United Kingdom | gb   |
| Dieter | de          | de        | Germany        | de   |
+--------+-------------+-----------+----------------+------+

Cross Joins - Cross joins form Cartesian products
─ Involves no matching at all
─ Creates every possible combination
─ Potentially generates a huge amount of data

[quickstart.cloudera:21000] > DROP TABLE IF EXISTS test.disks;
[quickstart.cloudera:21000] > CREATE TABLE IF NOT EXISTS test.disks (name STRING);
[quickstart.cloudera:21000] > INSERT INTO TABLE test.disks VALUES( 'Internal hard disk'), ('External hard disk');
[quickstart.cloudera:21000] > DROP TABLE IF EXISTS test.sizes;
[quickstart.cloudera:21000] > CREATE TABLE IF NOT EXISTS test.sizes (size STRING);
[quickstart.cloudera:21000] > INSERT INTO TABLE test.sizes VALUES ('1.0 terabytes'), ('2.0 terabytes'), ('3.0 terabytes'), ('4.0 terabytes');

[quickstart.cloudera:21000] > SELECT * FROM test.disks;
+--------------------+
| name               |
+--------------------+
| Internal hard disk |
| External hard disk |
+--------------------+
[quickstart.cloudera:21000] > SELECT * FROM test.sizes;
+---------------+
| size          |
+---------------+
| 1.0 terabytes |
| 2.0 terabytes |
| 3.0 terabytes |
| 4.0 terabytes |
+---------------+

-- Cross Join - Cartesian products
─ Involves no matching at all ─ Creates every possible combination ─ Potentially generates a huge amount of data
[quickstart.cloudera:21000] > SELECT * FROM test.disks 
CROSS JOIN test.sizes;
+--------------------+---------------+
| name               | size          |
+--------------------+---------------+
| Internal hard disk | 1.0 terabytes |
| Internal hard disk | 2.0 terabytes |
| Internal hard disk | 3.0 terabytes |
| Internal hard disk | 4.0 terabytes |
| External hard disk | 1.0 terabytes |
| External hard disk | 2.0 terabytes |
| External hard disk | 3.0 terabytes |
| External hard disk | 4.0 terabytes |
+--------------------+---------------+
-- Using Cross Join for Non-Equijoin (Impala/Hive)
0: jdbc:hive2://localhost:10000> SELECT fname, lname, grade
0: jdbc:hive2://localhost:10000> FROM employees
0: jdbc:hive2://localhost:10000> CROSS JOIN salary_grades
0: jdbc:hive2://localhost:10000> WHERE salary
0: jdbc:hive2://localhost:10000> BETWEEN min_amt AND max_amt;
+--------+--------+--------+--+
| fname  | lname  | grade  |
+--------+--------+--------+--+
| sean   | Baca   | 1      |
| Ana    | Bobo   | 2      |
| Gary   | Burt   | 2      |
| Mary   | Beal   | 5      |
+--------+--------+--------+--+

-O-J-O- ALL OF THEM- PRODUCE THE SAME RESULT = result IGUAL RESULTADO Cross Joins without CROSS Keyword - Cross joins also occur when you leave off join conditions 
─ Using CROSS JOIN
SELECT * FROM disks CROSS JOIN sizes;
─ Using explicit join syntax and no join condition (ON)
SELECT * FROM disks JOIN sizes;
─ Using implicit join syntax and no join condition (WHERE)
SELECT * FROM disks, sizes;
+--------------------+---------------+
| name               | size          |
+--------------------+---------------+
| Internal hard disk | 1.0 terabytes |
| Internal hard disk | 2.0 terabytes |
| Internal hard disk | 3.0 terabytes |
| Internal hard disk | 4.0 terabytes |
| External hard disk | 1.0 terabytes |
| External hard disk | 2.0 terabytes |
| External hard disk | 3.0 terabytes |
| External hard disk | 4.0 terabytes |
+--------------------+---------------+

--Left Semi-Joins 
A less common type of join is the LEFT SEMI JOIN
─ It is a special (and efficient) type of inner join ─ It behaves more like a filter than a join
Left semi-joins only return records from the table on the left
─ There must be a match in the table on the right
─ Join conditions and other criteria are specified 

[quickstart.cloudera:21000] > SELECT c.cust_id
FROM custo c
LEFT SEMI JOIN orden o
ON (c.cust_id = o.cust_id AND o.total > 1500);
+---------+
| cust_id |
+---------+
| a       |
| c       |
+---------+

[quickstart.cloudera:21000] > SELECT cust_id FROM custo c
WHERE c.cust_id IN
(SELECT cust_id FROM orden WHERE total > 1500);
+---------+
| cust_id |
+---------+
| a       |
| c       |
+---------+
Fetched 2 row(s) in 1.37s

HastaAQUI------------------------------

--OJO - revisar la teoría del capítulo 9

Analytic Functions only for windowing

SELECT subject, course, price, ROW_NUMBER () OVER(PARTITION by subject ORDER BY price) rownumber, RANK() OVER(PARTITION by subject ORDER BY price) rank, DENSE_RANK () OVER(PARTITION by subject ORDER BY price) denserank, PERCENT_RANK() OVER(PARTITION by subject ORDER BY price) percentrank, NTILE(4) OVER(PARTITION by subject ORDER BY price) ntile4, NTILE(3) OVER(PARTITION by subject ORDER BY price) ntile3, NTILE(2) OVER(PARTITION by subject ORDER BY price) ntile2, NTILE(1) OVER(PARTITION by subject ORDER BY price) ntile1, CUME_DIST() OVER(PARTITION by subject ORDER BY price) cume_dist
FROM test.courses
;


SELECT order_id, cust_id, order_date, to_date(order_date) ddate
  FROM analyst.orders
;

-- Es la fila que le toque, cuando tienen el mismo precio, selecciona uno de los dos y le pone el número que le corresponda según el orden 
ROW_NUMBER()
SELECT subject, course, price, ROW_NUMBER () OVER(PARTITION by subject ORDER BY price) rownumber
FROM test.courses
;
+---------+------------------+----------+-----------+
| subject | course           | price    | rownumber |
+---------+------------------+----------+-----------+
| Hadoop  | OOzie            | 1500.000 | 1         |
| Hadoop  | Zookeeper        | 1550.000 | 2         |
| Hadoop  | Impala           | 2400.000 | 3         |
| Hadoop  | Pig              | 2400.000 | 4         |
| Hadoop  | Mahout           | 2600.000 | 5         |
| Hadoop  | Hive             | 4000.000 | 6         |
| Spark   | SQL              | 1450.000 | 1         |
| Spark   | Spark            | 2400.000 | 2         |
| Spark   | Machine Learning | 5000.000 | 3         |
+---------+------------------+----------+-----------+
-- Cuando tienen el mismo precio, le pone el mismo número del rango que le corresponda, pero si tiene un rango de 6 él último será el 6, no salé el 4
RANK() 
SELECT subject, course, price, RANK() OVER(PARTITION by subject ORDER BY price) rank
FROM test.courses
;
+---------+------------------+----------+------+
| subject | course           | price    | rank |
+---------+------------------+----------+------+
| Hadoop  | OOzie            | 1500.000 | 1    |
| Hadoop  | Zookeeper        | 1550.000 | 2    |
| Hadoop  | Impala           | 2400.000 | 3    |
| Hadoop  | Pig              | 2400.000 | 3    |
| Hadoop  | Mahout           | 2600.000 | 5    |
| Hadoop  | Hive             | 4000.000 | 6    | --> rango de 6
| Spark   | SQL              | 1450.000 | 1    |
| Spark   | Spark            | 2400.000 | 2    |
| Spark   | Machine Learning | 5000.000 | 3    |
+---------+------------------+----------+------+

-- Cuando tienen el mismo precio, le pone el mismo número del rango que le corresponda, pero si tiene un rango de 6 él último será el que le corresponda en consecutivo,   no sabemos si hay 6
DENSE_RANK() 
SELECT subject, course, price, DENSE_RANK () OVER(PARTITION by subject ORDER BY price) denserank
FROM test.courses
;
+---------+------------------+----------+-----------+
| subject | course           | price    | denserank |
+---------+------------------+----------+-----------+
| Hadoop  | OOzie            | 1500.000 | 1         |
| Hadoop  | Zookeeper        | 1550.000 | 2         |
| Hadoop  | Impala           | 2400.000 | 3         |
| Hadoop  | Pig              | 2400.000 | 3         |
| Hadoop  | Mahout           | 2600.000 | 4         |
| Hadoop  | Hive             | 4000.000 | 5         | --> rango de 6
| Spark   | SQL              | 1450.000 | 1         |
| Spark   | Spark            | 2400.000 | 2         |
| Spark   | Machine Learning | 5000.000 | 3         |
+---------+------------------+----------+-----------+

-- En que quartile, divide el conjunto en n partes, si son iguales no hay problema, si sobran 2 los completa por arriba (conjunto proporcionales dentro de los elementos   que haya
NTILE(n)
SELECT subject, course, price, NTILE(4) OVER(PARTITION by subject ORDER BY price) ntile4, NTILE(3) OVER(PARTITION by subject ORDER BY price) ntile3, NTILE(2) OVER(PARTITION by subject ORDER BY price) ntile2, NTILE(1) OVER(PARTITION by subject ORDER BY price) ntile1
FROM test.courses
;
+---------+------------------+----------+--------+--------+--------+--------+
| subject | course           | price    | ntile4 | ntile3 | ntile2 | ntile1 |
+---------+------------------+----------+--------+--------+--------+--------+
| Hadoop  | OOzie            | 1500.000 | 1      | 1      | 1      | 1      |
| Hadoop  | Zookeeper        | 1550.000 | 1      | 1      | 1      | 1      |
| Hadoop  | Impala           | 2400.000 | 2      | 2      | 1      | 1      |
| Hadoop  | Pig              | 2400.000 | 3      | 2      | 2      | 1      |
| Hadoop  | Mahout           | 2600.000 | 3      | 3      | 2      | 1      |
| Hadoop  | Hive             | 4000.000 | 4      | 3      | 2      | 1      |
| Spark   | SQL              | 1450.000 | 1      | 1      | 1      | 1      |
| Spark   | Spark            | 2400.000 | 2      | 2      | 1      | 1      |
| Spark   | Machine Learning | 5000.000 | 3      | 3      | 2      | 1      |
+---------+------------------+----------+--------+--------+--------+--------+

Primero decide cuantos elementos enteros deben haber por grupo) 6/4 = 1
SELECT subject, course, price, NTILE(4) OVER(PARTITION by subject ORDER BY price) ntile4
FROM test.courses
;
+---------+------------------+----------+--------+
| subject | course           | price    | ntile4 |
+---------+------------------+----------+--------+
| Hadoop  | OOzie            | 1500.000 | 1      |
| Hadoop  | Zookeeper        | 1550.000 | 1      | --> los precios estan entre el 0 y el 25%
| Hadoop  | Impala           | 2400.000 | 2      | --> los precios estan entre el 25% y el 50%
| Hadoop  | Pig              | 2400.000 | 3      |
| Hadoop  | Mahout           | 2600.000 | 3      | --> los precios estan entre el 50% y el 75%
| Hadoop  | Hive             | 4000.000 | 4      | --> los precios estan entre el 75% y el 100%, 4 conjuntos en las 6 líneas cada conjunto es el 25%
| Spark   | SQL              | 1450.000 | 1      |
| Spark   | Spark            | 2400.000 | 2      |
| Spark   | Machine Learning | 5000.000 | 3      |
+---------+------------------+----------+--------+


-- Para que sea un porcentaje real, multiplique por 100 
PERCENT_RANK() = (RANK - 1)/(number of rows - 1)
SELECT subject, course, price, PERCENT_RANK() OVER(PARTITION by subject ORDER BY price) percentrank
FROM test.courses
;
+---------+------------------+----------+-------------+
| subject | course           | price    | percentrank |
+---------+------------------+----------+-------------+
| Hadoop  | OOzie            | 1500.000 | 0           |
| Hadoop  | Zookeeper        | 1550.000 | 0.2         |
| Hadoop  | Impala           | 2400.000 | 0.4         |
| Hadoop  | Pig              | 2400.000 | 0.4         |
| Hadoop  | Mahout           | 2600.000 | 0.8         |
| Hadoop  | Hive             | 4000.000 | 1           |
| Spark   | SQL              | 1450.000 | 0           |
| Spark   | Spark            | 2400.000 | 0.5         |
| Spark   | Machine Learning | 5000.000 | 1           |
+---------+------------------+----------+-------------+

-- Para el orden ascendente: proporción de filas con valores <= valor actual
CUME_DIST()
SELECT subject, course, price, CUME_DIST() OVER(PARTITION by subject ORDER BY price) cume_dist
FROM test.courses
;
+---------+------------------+----------+--------------------+
| subject | course           | price    | cume_dist          |
+---------+------------------+----------+--------------------+
| Hadoop  | Hive             | 4000.000 | 1                  |
| Hadoop  | Mahout           | 2600.000 | 0.8333333333333334 |
| Hadoop  | Impala           | 2400.000 | 0.6666666666666666 |
| Hadoop  | Pig              | 2400.000 | 0.6666666666666666 |
| Hadoop  | Zookeeper        | 1550.000 | 0.3333333333333333 |
| Hadoop  | OOzie            | 1500.000 | 0.1666666666666667 |
| Spark   | Machine Learning | 5000.000 | 1                  |
| Spark   | Spark            | 2400.000 | 0.6666666666666666 |
| Spark   | SQL              | 1450.000 | 0.3333333333333333 |
+---------+------------------+----------+--------------------+

CUME_DIST
SELECT subject, course, price, CUME_DIST() OVER(PARTITION by subject ORDER BY price DESC) cume_dist
FROM test.courses
;
+---------+------------------+----------+--------------------+
| subject | course           | price    | cume_dist          |
+---------+------------------+----------+--------------------+
| Hadoop  | OOzie            | 1500.000 | 1                  |
| Hadoop  | Zookeeper        | 1550.000 | 0.8333333333333334 |
| Hadoop  | Impala           | 2400.000 | 0.6666666666666666 |
| Hadoop  | Pig              | 2400.000 | 0.6666666666666666 |
| Hadoop  | Mahout           | 2600.000 | 0.3333333333333333 |
| Hadoop  | Hive             | 4000.000 | 0.1666666666666667 |
| Spark   | SQL              | 1450.000 | 1                  |
| Spark   | Spark            | 2400.000 | 0.6666666666666666 |
| Spark   | Machine Learning | 5000.000 | 0.3333333333333333 |
+---------+------------------+----------+--------------------+

FIRST_VALUE
SELECT subject, course, price, FIRST_VALUE(price) OVER(PARTITION by subject ORDER BY price DESC) firstvalue
FROM test.courses
;
+---------+------------------+----------+------------+
| subject | course           | price    | firstvalue |
+---------+------------------+----------+------------+
| Hadoop  | Hive             | 4000.000 | 4000.000   |
| Hadoop  | Mahout           | 2600.000 | 4000.000   |
| Hadoop  | Impala           | 2400.000 | 4000.000   |
| Hadoop  | Pig              | 2400.000 | 4000.000   |
| Hadoop  | Zookeeper        | 1550.000 | 4000.000   |
| Hadoop  | OOzie            | 1500.000 | 4000.000   |
| Spark   | Machine Learning | 5000.000 | 5000.000   |
| Spark   | Spark            | 2400.000 | 5000.000   |
| Spark   | SQL              | 1450.000 | 5000.000   |
+---------+------------------+----------+------------+

LAST_VALUE
SELECT subject, course, price, LAST_VALUE(price) OVER(PARTITION by subject ORDER BY price DESC) lastvalue
FROM test.courses
;
+---------+------------------+----------+-----------+
| subject | course           | price    | lastvalue |
+---------+------------------+----------+-----------+
| Hadoop  | Hive             | 4000.000 | 4000.000  |
| Hadoop  | Mahout           | 2600.000 | 2600.000  |
| Hadoop  | Impala           | 2400.000 | 2400.000  |
| Hadoop  | Pig              | 2400.000 | 2400.000  |
| Hadoop  | Zookeeper        | 1550.000 | 1550.000  |
| Hadoop  | OOzie            | 1500.000 | 1500.000  |
| Spark   | Machine Learning | 5000.000 | 5000.000  |
| Spark   | Spark            | 2400.000 | 2400.000  |
| Spark   | SQL              | 1450.000 | 1450.000  |
+---------+------------------+----------+-----------+

-- El valor en la columna especificada en la nth siguiente fila
LEAD()
SELECT subject, course, price, LEAD(price) OVER(PARTITION by subject ORDER BY price) lead, price - LEAD(price) OVER(PARTITION by subject ORDER BY price) diff 
FROM test.courses
;
+---------+------------------+----------+----------+-----------+
| subject | course           | price    | lead     | diff      |
+---------+------------------+----------+----------+-----------+
| Hadoop  | OOzie            | 1500.000 | 1550.000 | -50.000   |
| Hadoop  | Zookeeper        | 1550.000 | 2400.000 | -850.000  |
| Hadoop  | Impala           | 2400.000 | 2400.000 | 0.000     |
| Hadoop  | Pig              | 2400.000 | 2600.000 | -200.000  |
| Hadoop  | Mahout           | 2600.000 | 4000.000 | -1400.000 |
| Hadoop  | Hive             | 4000.000 | NULL     | NULL      |
| Spark   | SQL              | 1450.000 | 2400.000 | -950.000  |
| Spark   | Spark            | 2400.000 | 5000.000 | -2600.000 |
| Spark   | Machine Learning | 5000.000 | NULL     | NULL      |
+---------+------------------+----------+----------+-----------+

-- El valor en la columna especificada en la nth precedente fila
LAG()
SELECT subject, course, price, LAG(price) OVER(PARTITION by subject ORDER BY price) lag, price - LAG(price) OVER(PARTITION by subject ORDER BY price) diff 
FROM test.courses
;
+---------+------------------+----------+----------+----------+
| subject | course           | price    | lag      | diff     |
+---------+------------------+----------+----------+----------+
| Hadoop  | OOzie            | 1500.000 | NULL     | NULL     |
| Hadoop  | Zookeeper        | 1550.000 | 1500.000 | 50.000   |
| Hadoop  | Impala           | 2400.000 | 1550.000 | 850.000  |
| Hadoop  | Pig              | 2400.000 | 2400.000 | 0.000    |
| Hadoop  | Mahout           | 2600.000 | 2400.000 | 200.000  |
| Hadoop  | Hive             | 4000.000 | 2600.000 | 1400.000 |
| Spark   | SQL              | 1450.000 | NULL     | NULL     |
| Spark   | Spark            | 2400.000 | 1450.000 | 950.000  |
| Spark   | Machine Learning | 5000.000 | 2400.000 | 2600.000 |
+---------+------------------+----------+----------+----------+

SELECT order_id, cust_id, order_date, to_date(order_date) ddate
  FROM analyst.orders
;

--Usando windowing a partir de la tabla orders
--Hacer un select que muestre order_id, cust_id, año/mes/dia, el número de órdenes que se produjeron en ese año/mes/dia
SELECT order_id, cust_id, to_date(order_date), COUNT(order_id) OVER (PARTITION BY to_date(order_date))
  FROM analyst.orders
;

1. What is the price of the least expensive product
SELECT min(price)
FROM analyst.products;
+------------+
| min(price) |
+------------+
| 19         |
+------------+

2. Which product is least expensive
SELECT brand, name, price
FROM analyst.products
WHERE price = (SELECT min(price) 
               FROM analyst.products);
+-------+------------------------------------------------------+-------+
| brand | name                                                 | price |
+-------+------------------------------------------------------+-------+
| TPS   | 1GB DDR2-667 (PC2-5300) Desktop Memory Kit (2x512MB) | 19    |
| Duff  | 1GB DDR2-667 (PC2-5300) Desktop Memory Kit (2x512MB) | 19    |
+-------+------------------------------------------------------+-------+

3. What is the price of the least expensive product in each brand?
SELECT brand, min(price) as min
from analyst.products
group by brand
order by brand;

+------------------+-------+
| brand            | min   |
+------------------+-------+
| ACME             | 279   |
| ARCAM            | 30999 |
| Argo             | 17719 |
| Artie            | 13269 |
| BDT              | 389   |
| Bargain Barn     | 279   |
| Bigdeal          | 59    |
| Bitbucket        | 2029  |
| Bitmonkey        | 43459 |
| BuckLogix        | 499   |
| Bytefortress     | 23259 |
| Byteweasel       | 1279  |
| Chatter Audio    | 4099  |
| Chestnut         | 49    |
| DevNull          | 39    |
| Dorx             | 1929  |
| Dualcore         | 119   |
| Duff             | 19    |
| Electrosaurus    | 409   |
| Foocorp          | 2989  |
| Gigabux          | 399   |
| Homertech        | 1949  |
| Krustybitz       | 1329  |
| Lemmon           | 1939  |
| McDowell         | 169   |
| Megachango       | 29    |
| Olde-Gray        | 14909 |
| Orion            | 169   |
| Overtop          | 309   |
| Sparky           | 409   |
| Spindown         | 1199  |
| Sprite           | 12549 |
| SuperGamer       | 8029  |
| TPS              | 19    |
| Terrapin Sands   | 2719  |
| Texi             | 1949  |
| Tortoise         | 1259  |
| Tyrell           | 49    |
| Ultramegaco      | 29    |
| United Digistuff | 829   |
| Weebits          | 4629  |
| Weisenheimer     | 2069  |
| Wernham          | 4199  |
| Whiteacre        | 5899  |
| Wolfpack         | 2819  |
| XYZ              | 59    |
| Yoyodyne         | 44119 |
+------------------+-------+

4.  What is difference between a product’s price and the minimum price for that brand?
SELECT name, brand, price, price - min(price) OVER (PARTITION by brand) as diff
FROM analyst.products;

Esto me permitiria ordenar por precio para ver fácilmente cual es el más barato del brand
--SELECT name, brand, price, price - min(price) OVER (PARTITION by brand ORDER BY price) as diff

+--------------------------------------------------------------------+------------------+--------+--------+
| name                                                               | brand            | price  | diff   |
+--------------------------------------------------------------------+------------------+--------+--------+
| Professional DDR3 Motherboard                                      | ACME             | 17759  | 17480  |
| 4x2GB DDR3 1333MHz Triple Channel Desktop RAM                      | ACME             | 37929  | 37650  |
| 2x2GB DDR3 1600MHz Desktop RAM                                     | ACME             | 20969  | 20690  |
| 64MB PCI Graphics Card                                             | ACME             | 2989   | 2710   |
| Fireware 800 Card (PCI)                                            | ACME             | 6929   | 6650   |
| 3.8GHz CPU                                                         | ACME             | 27829  | 27550  |
| 3.2GHz CPU                                                         | ACME             | 15739  | 15460  |
| 16 GB Micro SD (High Performance)                                  | ACME             | 5849   | 5570   |
| 8 GB Micro SD                                                      | ACME             | 2829   | 2550   |
| 64 GB SSD Disk                                                     | ACME             | 21929  | 21650  |
| 500 GB Portable External USB 2.0 Disk                              | ACME             | 7609   | 7330   |
| 16 GB USB Flash Drive                                              | ACME             | 4049   | 3770   |
| 2.0 TB SATA3 Disk (10K RPM)                                        | ACME             | 19219  | 18940  |
| 2.0 TB SATA3 Disk                                                  | ACME             | 14759  | 14480  |
| 1.0 TB SATA3 Disk (10K RPM)                                        | ACME             | 15029  | 14750  |
| All-in-one Print/Scan/Copy                                         | ACME             | 23369  | 23090  |
| Scanner                                                            | ACME             | 27769  | 27490  |
| Laser Printer (Color, Duplex)                                      | ACME             | 48039  | 47760  |
| Laser Printer (Color)                                              | ACME             | 38459  | 38180  |
| Laser Printer (Color)                                              | ACME             | 38849  | 38570  |
| Laser Printer (Black-and-White)                                    | ACME             | 37889  | 37610  |
| Laser Printer (Black-and-White)                                    | ACME             | 37839  | 37560  |
| Laser Printer (Black-and-White)                                    | ACME             | 38319  | 38040  |
| Laser Printer (Black-and-White)                                    | ACME             | 40059  | 39780  |
| Laser Printer (Black-and-White)                                    | ACME             | 39279  | 39000  |
| Inkjet Printer                                                     | ACME             | 18169  | 17890  |
| Dot Matrix Printer                                                 | ACME             | 29719  | 29440  |
| 3.5mm Mini Male to Standard Phone Female                           | ACME             | 279    | 0      |
| Stylus for PDA                                                     | ACME             | 1929   | 1650   |
| Laptop Cooling Pad                                                 | ACME             | 4279   | 4000   |
| Extension cord (20 ft., outdoor)                                   | ACME             | 5439   | 5160   |
| UPS 1000 Watt                                                      | ACME             | 15069  | 14790  |
| Wheel Control                                                      | ACME             | 4349   | 4070   |
| Trackball                                                          | ACME             | 5159   | 4880   |
| Rechargeable Batteries (C, 2 pack)                                 | ACME             | 1919   | 1640   |
| Batteries (AA, 4 pack)                                             | ACME             | 729    | 450    |
| Compact Charger                                                    | ACME             | 3619   | 3340   |
| Screen Cleaning Cloth (black)                                      | ACME             | 1889   | 1610   |
| Sleeve for Tablet - Leather                                        | ACME             | 3239   | 2960   |
| Sleeve for Mini Tablet - Green                                     | ACME             | 1479   | 1200   |
| Headphone Adapter (1/4 in. to 1/8 in.)                             | ACME             | 769    | 490    |
| Mini-Stereo to RCA Audio Cable                                     | ACME             | 1449   | 1170   |
| ATX Computer Case                                                  | ACME             | 6049   | 5770   |
| 2.5" External USB 3.0 HDD Case                                     | ACME             | 1309   | 1030   |
| USB 3.0 Front Panel - 4 Port                                       | ACME             | 1479   | 1200   |
| USB 3.0 Front Panel - 4 Port                                       | ACME             | 1519   | 1240   |
| Molex to SATA Power Adapter                                        | ACME             | 579    | 300    |
| F Jack Male-to-Male Cable (18 in.)                                 | ACME             | 1199   | 920    |
| Phono Plug to UHF Jack                                             | ACME             | 619    | 340    |
| BNC Jack to UHF Plug                                               | ACME             | 619    | 340    |
| 1/4 in. Standard Phone Male to RCA Phono Female Adapter            | ACME             | 409    | 130    |
| 24" Ultra-Premium LCD Monitor                                      | ACME             | 46939  | 46660  |
| 22" Wide Screen LCD Monitor                                        | ACME             | 17989  | 17710  |
| Soundcard (Premium Multimedia)                                     | ACME             | 8889   | 8610   |
| Camcorder (VHS)                                                    | ARCAM            | 30999  | 0      |
| Gaming Console (WiFi)                                              | ARCAM            | 40599  | 9600   |
| Digital Camera (Professional)                                      | ARCAM            | 38769  | 7770   |
| 42" Wide Screen TV                                                 | ARCAM            | 55559  | 24560  |
| 22" Wide Screen LCD Monitor                                        | Argo             | 17719  | 0      |
| 22" Wide Screen LCD Monitor                                        | Argo             | 17769  | 50     |
| 32" Wide Screen TV                                                 | Argo             | 37739  | 20020  |
| Camcorder (VHS)                                                    | Argo             | 29439  | 11720  |
| Digital Video Recorder (750 GB)                                    | Argo             | 32439  | 14720  |
| Blu-Ray Player                                                     | Artie            | 13269  | 0      |
| Camcorder (Digital)                                                | Artie            | 41939  | 28670  |
| Camcorder (Betamax)                                                | Artie            | 31219  | 17950  |
| 32" Wide Screen TV                                                 | Artie            | 38019  | 24750  |
| Digital Camera (Pro-Am)                                            | Artie            | 20759  | 7490   |
| 27" Wide Screen TV                                                 | Artie            | 27829  | 14560  |
| Network Switch (Gigabit, 16-port)                                  | BDT              | 7869   | 7480   |
| Fireware 400 Card (PCI)                                            | BDT              | 5019   | 4630   |
| Ethernet NIC (100 MBps)                                            | BDT              | 4319   | 3930   |
| Soundcard (Basic)                                                  | BDT              | 1299   | 910    |
| Fireware 800 Card (PCI)                                            | BDT              | 6999   | 6610   |
| ATX 24-Pin Extension 12 inch                                       | BDT              | 389    | 0      |
| Network Hub (10 MBps, 16-port)                                     | BDT              | 4109   | 3720   |
| Firewall                                                           | BDT              | 27909  | 27520  |
| 1GB DDR2 800 Desktop RAM                                           | BDT              | 13169  | 12780  |
| Wireless G USB Adapter                                             | BDT              | 5859   | 5470   |
| Ethernet NIC (Gigabit)                                             | BDT              | 6179   | 5790   |
| Mid Tower Computer Case                                            | BDT              | 8009   | 7620   |
| Network Switch (100 MBps, 16-port)                                 | BDT              | 4949   | 4560   |
| Cable Modem                                                        | BDT              | 6639   | 6250   |
| 4x2GB DDR3 1333MHz Triple Channel Desktop RAM                      | BDT              | 38609  | 38220  |
| USB 3.0 Front Panel - 2 Port                                       | BDT              | 1279   | 890    |
| Power Supply (500W)                                                | BDT              | 2509   | 2120   |
| Network Switch (Gigabit, 16-port)                                  | BDT              | 7669   | 7280   |
| 128MB AGP Graphics Card                                            | Bargain Barn     | 5819   | 5540   |
| 512MB PCI-X Graphics Card (OEM)                                    | Bargain Barn     | 7979   | 7700   |
| 27" Wide Screen LCD Monitor                                        | Bargain Barn     | 37739  | 37460  |
| Stereo USB Headset                                                 | Bargain Barn     | 2709   | 2430   |
| USB 3.0 Front Panel - 4 Port                                       | Bargain Barn     | 1489   | 1210   |
| USB 3.0 Card Reader                                                | Bargain Barn     | 829    | 550    |
| 24" Wide Screen LCD Monitor                                        | Bargain Barn     | 23539  | 23260  |
| Fan Adapter Cable - 16 inch                                        | Bargain Barn     | 409    | 130    |
| Headphone Adapter (1/4 in. to 1/8 in.)                             | Bargain Barn     | 779    | 500    |
| 30" Wide Screen LCD Monitor                                        | Bargain Barn     | 49949  | 49670  |
| Extension cord (10 ft., outdoor)                                   | Bargain Barn     | 3839   | 3560   |
| Mobile Bluetooth Keyboard and Stand for Tablets                    | Bargain Barn     | 7009   | 6730   |
| BNC Plug to UHF Jack                                               | Bargain Barn     | 559    | 280    |
| 2x1GB Value DDR2 - 667 RAM                                         | Bargain Barn     | 13169  | 12890  |
| MP3 Player (32 GB internal memory)                                 | Bargain Barn     | 17669  | 17390  |
| MP3 Player (16 GB internal memory)                                 | Bargain Barn     | 13099  | 12820  |
| 16 GB Micro SD                                                     | Bargain Barn     | 4289   | 4010   |
| All-weather Speakers                                               | Bargain Barn     | 28169  | 27890  |
| MP3 Player (16 GB internal memory)                                 | Bargain Barn     | 12369  | 12090  |
| Amplified Multimedia Speakers                                      | Bargain Barn     | 9039   | 8760   |
| High-range Premium Speakers                                        | Bargain Barn     | 45639  | 45360  |
| Keyboard (basic PC101)                                             | Bargain Barn     | 3079   | 2800   |
| Wireless laser mouse (three button, with wheel)                    | Bargain Barn     | 3949   | 3670   |
| MP3 Player (4 GB internal memory)                                  | Bargain Barn     | 8299   | 8020   |
| 3.6GHz CPU                                                         | Bargain Barn     | 22199  | 21920  |
| 32 GB SSD Disk                                                     | Bargain Barn     | 15129  | 14850  |
| 3.8GHz CPU                                                         | Bargain Barn     | 28319  | 28040  |
| Batteries (AAA, 4 pack)                                            | Bargain Barn     | 749    | 470    |
| Digital Camera (Basic)                                             | Bargain Barn     | 12649  | 12370  |
| 500 GB Portable External USB 3.0 Disk                              | Bargain Barn     | 10489  | 10210  |
| Stereo Component Turntable                                         | Bargain Barn     | 29759  | 29480  |
| Digital Camera (Basic)                                             | Bargain Barn     | 13049  | 12770  |
| 1/4 in. Standard Phone Male to 3-Pin XLR Female Adapter            | Bargain Barn     | 639    | 360    |
| Auto Charger                                                       | Bargain Barn     | 3989   | 3710   |
| SCSI Expansion Card (PCI)                                          | Bargain Barn     | 7909   | 7630   |
| TV Tuner Card w/HDMI Output (PCI)                                  | Bargain Barn     | 10159  | 9880   |
| 3.5mm Mini Female to Female, Barrel Adapter                        | Bargain Barn     | 279    | 0      |
| 16 GB USB Flash Drive                                              | Bargain Barn     | 4059   | 3780   |
| 8 GB USB Flash Drive                                               | Bargain Barn     | 2679   | 2400   |
| Surge Protector (8-Outlet)                                         | Bargain Barn     | 2499   | 2220   |
| 1/4 in. Standard Phone Male to RCA Phono Female Adapter            | Bargain Barn     | 369    | 90     |
| 2.5" External USB 3.0 HDD Case                                     | Bargain Barn     | 1309   | 1030   |
| Laptop Cooling Pad                                                 | Bargain Barn     | 4309   | 4030   |
| High Speed HDMI Cable (24 in.)                                     | Bargain Barn     | 2009   | 1730   |
| Stereo PC Headset                                                  | Bargain Barn     | 3839   | 3560   |
| Dual USB Power Adapter (auto)                                      | Bargain Barn     | 2969   | 2690   |
| AT Computer Case                                                   | Bargain Barn     | 7919   | 7640   |
| 3.5mm Mini Male to Standard Phone Female                           | Bargain Barn     | 309    | 30     |
| Gamer Headphone                                                    | Bargain Barn     | 7669   | 7390   |
| 20" Wide Screen LCD Monitor                                        | Bargain Barn     | 15559  | 15280  |
| ATX Computer Case                                                  | Bargain Barn     | 6039   | 5760   |
| Composite AV Cable (24 in.)                                        | Bargain Barn     | 2489   | 2210   |
| HDD Bracket                                                        | Bargain Barn     | 809    | 530    |
| Dual USB Power Adapter                                             | Bargain Barn     | 2109   | 1830   |
| F Jack Male-to-Male Cable (60 in.)                                 | Bargain Barn     | 2779   | 2500   |
| Stylus for PDA                                                     | Bargain Barn     | 2049   | 1770   |
| Composite AV Cable (24 in.)                                        | Bargain Barn     | 2189   | 1910   |
| ATX Computer Case                                                  | Bargain Barn     | 5929   | 5650   |
| 3.5mm Mini Male to RCA Phono Female Adapter                        | Bargain Barn     | 379    | 100    |
| Composite AV Cable (36 in.)                                        | Bargain Barn     | 2489   | 2210   |
| RCA Phono Male to Mini Female Adapter                              | Bargain Barn     | 379    | 100    |
| 24" Wide Screen LCD Monitor                                        | Bargain Barn     | 23639  | 23360  |
| High Speed HDMI Cable (60 in.)                                     | Bargain Barn     | 4749   | 4470   |
| Batteries (AAA, 4 pack)                                            | Bigdeal          | 649    | 590    |
| 8GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x4GB)               | Bigdeal          | 59     | 0      |
| UPS 1500 Watt                                                      | Bigdeal          | 17909  | 17850  |
| Network Switch (100 MBps, 4-port)                                  | Bigdeal          | 4199   | 4140   |
| Surge Protector (8-Outlet)                                         | Bigdeal          | 2369   | 2310   |
| Extension cord (60 in.)                                            | Bigdeal          | 2359   | 2300   |
| Wireless N Router                                                  | Bigdeal          | 6959   | 6900   |
| Rechargeable Batteries (9V, 1 pack)                                | Bigdeal          | 1259   | 1200   |
| 1/4 in. Standard Phone Female to Female Adapter                    | Bigdeal          | 429    | 370    |
| Batteries (AA, 4 pack)                                             | Bigdeal          | 689    | 630    |
| UPS 2500 Watt (rackmount)                                          | Bigdeal          | 47679  | 47620  |
| Battery Charger                                                    | Bigdeal          | 3469   | 3410   |
| USB 7-Port Powered Desktop Hub                                     | Bigdeal          | 3679   | 3620   |
| Network Switch (100 MBps, 8-port)                                  | Bigdeal          | 4519   | 4460   |
| RCA Phono Male to 3-Pin XLR Female Adapter                         | Bigdeal          | 709    | 650    |
| Screen Cleaning Cloth (white)                                      | Bigdeal          | 1669   | 1610   |
| Keyboard (gamer PC104)                                             | Bigdeal          | 9469   | 9410   |
| RCA Phono Male to 3-Pin XLR Female Adapter                         | Bigdeal          | 659    | 600    |
| Quad USB Power Adapter                                             | Bigdeal          | 3959   | 3900   |
| Dual Band Wireless Range Extender                                  | Bigdeal          | 12559  | 12500  |
| High Speed HDMI Cable (36 in.)                                     | Bigdeal          | 2919   | 2860   |
| Sleeve for Mini Tablet - Red                                       | Bigdeal          | 1689   | 1630   |
| Batteries (AAA, 2 pack)                                            | Bigdeal          | 389    | 330    |
| Network Switch (100 MBps, 8-port)                                  | Bigdeal          | 4509   | 4450   |
| Sleeve for Tablet - Green                                          | Bigdeal          | 1849   | 1790   |
| Phono Plug to BNC Jack                                             | Bigdeal          | 609    | 550    |
| Dual Band Wireless Range Extender                                  | Bigdeal          | 13219  | 13160  |
| 3-Pin XLR Female to Female, Barrel Adapter                         | Bigdeal          | 779    | 720    |
| 16 GB Micro SD                                                     | Bitbucket        | 4579   | 2550   |
| 8 GB USB Flash Drive                                               | Bitbucket        | 2679   | 650    |
| 16 GB USB Flash Drive                                              | Bitbucket        | 3929   | 1900   |
| 128 GB SSD Disk                                                    | Bitbucket        | 40539  | 38510  |
| Office Suite (Basic Edition)                                       | Bitbucket        | 32149  | 30120  |
| 8 GB USB Flash Drive                                               | Bitbucket        | 2779   | 750    |
| 1.5 TB SATA3 Disk                                                  | Bitbucket        | 13319  | 11290  |
| 4 GB USB Flash Drive                                               | Bitbucket        | 2029   | 0      |
| 750 GB Portable External USB 3.0 Disk                              | Bitbucket        | 12889  | 10860  |
| 3.0 TB SATA3 Disk                                                  | Bitbucket        | 18799  | 16770  |
| 32 GB SSD Disk                                                     | Bitbucket        | 15599  | 13570  |
| Basic Desktop                                                      | Bitmonkey        | 43459  | 0      |
| All-in-one Desktop (24 in. display)                                | Bitmonkey        | 188579 | 145120 |
| All-in-one Desktop (24 in. display)                                | Bitmonkey        | 194389 | 150930 |
| Beige 16X DVD-RW                                                   | BuckLogix        | 3529   | 3030   |
| Mid Tower Computer Case                                            | BuckLogix        | 7949   | 7450   |
| Power Supply (700W)                                                | BuckLogix        | 2989   | 2490   |
| Ethernet NIC (Gigabit)                                             | BuckLogix        | 6139   | 5640   |
| Mid Tower Computer Case                                            | BuckLogix        | 7899   | 7400   |
| ITX Computer Case                                                  | BuckLogix        | 4059   | 3560   |
| AT Computer Case                                                   | BuckLogix        | 8119   | 7620   |
| XTREME Motherboard                                                 | BuckLogix        | 17789  | 17290  |
| Ethernet NIC (Gigabit)                                             | BuckLogix        | 5929   | 5430   |
| 2GB DDR2 667 RAM                                                   | BuckLogix        | 15029  | 14530  |
| 1GB DDR 400 Desktop RAM                                            | BuckLogix        | 8109   | 7610   |
| Fan Adapter Cable - 22 inch                                        | BuckLogix        | 499    | 0      |
| 3.5" Drive Bay Adapter                                             | BuckLogix        | 1509   | 1010   |
| 512MB PCI-X Graphics Card (OEM)                                    | BuckLogix        | 7919   | 7420   |
| VPN Appliance (250 Clienti license)                                | BuckLogix        | 227269 | 226770 |
| 512MB PCI-X Graphics Card (OEM)                                    | BuckLogix        | 7819   | 7320   |
| Network Switch (Gigabit, 24-port)                                  | BuckLogix        | 12729  | 12230  |
| Tablet PC (7 in. display, 8 GB)                                    | BuckLogix        | 31439  | 30940  |
| Server (1U rackmount, hex-core, 32GB, 8TB)                         | BuckLogix        | 478489 | 477990 |
| Server (1U rackmount, hex-core, 32GB, 8TB)                         | BuckLogix        | 483989 | 483490 |
| Office Suite (Home Edition)                                        | Bytefortress     | 23259  | 0      |
| Tablet PC (10 in. display, 16 GB)                                  | Byteweasel       | 38949  | 37670  |
| Virus Scanner (incl. updates for three years)                      | Byteweasel       | 4869   | 3590   |
| Tablet PC (10 in. display, 64 GB)                                  | Byteweasel       | 37999  | 36720  |
| Office Suite (Student Edition)                                     | Byteweasel       | 13519  | 12240  |
| Ultra-portable Notebook                                            | Byteweasel       | 79099  | 77820  |
| Office Suite (Basic Edition)                                       | Byteweasel       | 33989  | 32710  |
| SCSI Terminator                                                    | Byteweasel       | 4109   | 2830   |
| 4 GB Micro SD                                                      | Byteweasel       | 1279   | 0      |
| 2.0 TB SATA3 Disk                                                  | Byteweasel       | 14939  | 13660  |
| Virus Scanner (incl. updates for one year)                         | Byteweasel       | 2859   | 1580   |
| 16 GB USB Flash Drive                                              | Byteweasel       | 4039   | 2760   |
| 16 GB USB Flash Drive                                              | Byteweasel       | 3989   | 2710   |
| 64 GB USB Flash Drive                                              | Byteweasel       | 9169   | 7890   |
| Hadoop Cluster, Economy (4-node)                                   | Byteweasel       | 975149 | 973870 |
| Multimedia Headset                                                 | Chatter Audio    | 5739   | 1640   |
| MP3 Player (16 GB internal memory)                                 | Chatter Audio    | 12619  | 8520   |
| MP3 Player (32 GB internal memory)                                 | Chatter Audio    | 17999  | 13900  |
| Multimedia Headset                                                 | Chatter Audio    | 5529   | 1430   |
| Stereo Component CD Player                                         | Chatter Audio    | 19869  | 15770  |
| Portable CD Player                                                 | Chatter Audio    | 4099   | 0      |
| MP3 Player (32 GB internal memory)                                 | Chatter Audio    | 18669  | 14570  |
| Microphone (model U47)                                             | Chatter Audio    | 18439  | 14340  |
| MP3 Player (16 GB internal memory)                                 | Chatter Audio    | 12609  | 8510   |
| Extension cord (60 in.)                                            | Chestnut         | 2389   | 2340   |
| Extension cord (36 in., heavy duty)                                | Chestnut         | 2849   | 2800   |
| 1/4 in. Standard Phone Male to 3.5mm Mini Female Adapter           | Chestnut         | 299    | 250    |
| Digital Video Recorder (2.0 TB)                                    | Chestnut         | 60519  | 60470  |
| Keyboard (basic PC104)                                             | Chestnut         | 3649   | 3600   |
| RCA Phono Male to Mini Female Adapter                              | Chestnut         | 419    | 370    |
| 8GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x4GB)               | Chestnut         | 49     | 0      |
| 1/4 in. Standard Phone Male to 3.5mm Mini Female Adapter           | Chestnut         | 289    | 240    |
| Blu-Ray Player                                                     | Chestnut         | 12529  | 12480  |
| S-Video Female-to-Female Cable (24 in.)                            | Chestnut         | 1549   | 1500   |
| S-Video Female-to-Female Cable (24 in.)                            | Chestnut         | 1439   | 1390   |
| USB Card Reader                                                    | Chestnut         | 1839   | 1790   |
| Rechargeable Batteries (D, 2 pack)                                 | Chestnut         | 2039   | 1990   |
| F Jack Male-to-Male Cable (36 in.)                                 | Chestnut         | 2019   | 1970   |
| 3.5mm Mini Female to Female, Barrel Adapter                        | Chestnut         | 319    | 270    |
| 8-cell Battery                                                     | Chestnut         | 6549   | 6500   |
| 42" Wide Screen TV                                                 | Chestnut         | 59789  | 59740  |
| Phono Jack to BNC Plug                                             | Chestnut         | 539    | 490    |
| Batteries (AA, 4 pack)                                             | Chestnut         | 729    | 680    |
| F Jack Male-to-Male Cable (18 in.)                                 | Chestnut         | 1369   | 1320   |
| Batteries (AAA, 4 pack)                                            | Chestnut         | 759    | 710    |
| 27" Wide Screen TV                                                 | Chestnut         | 27589  | 27540  |
| 6-cell Battery                                                     | Chestnut         | 4019   | 3970   |
| Executive Stylus and Pen                                           | Chestnut         | 4849   | 4800   |
| RCA Phono Female to Female, Barrel Adapter                         | Chestnut         | 469    | 420    |
| Compact Charger                                                    | Chestnut         | 3529   | 3480   |
| 3-Pin XLR Female to Female, Barrel Adapter                         | Chestnut         | 859    | 810    |
| USB 3.0 4-Port Ultra Mini Hub                                      | Chestnut         | 4679   | 4630   |
| 48" Wide Screen TV                                                 | Chestnut         | 70939  | 70890  |
| USB 3.0 4-Port Ultra Mini Hub                                      | Chestnut         | 4079   | 4030   |
| Case for Notebook Computer                                         | Chestnut         | 4839   | 4790   |
| Wired mouse (three button)                                         | Chestnut         | 4199   | 4150   |
| Keyboard (wireless, with nub)                                      | Chestnut         | 13709  | 13660  |
| Extension cord (36 in., heavy duty)                                | Chestnut         | 2919   | 2870   |
| Compact Charger                                                    | DevNull          | 4139   | 4100   |
| USB Card Reader                                                    | DevNull          | 1929   | 1890   |
| Sleeve for Tablet - Black                                          | DevNull          | 2079   | 2040   |
| 6-cell Battery                                                     | DevNull          | 4599   | 4560   |
| Batteries (AA, 2 pack)                                             | DevNull          | 399    | 360    |
| Batteries (D, 2 pack)                                              | DevNull          | 989    | 950    |
| Batteries (AA, 2 pack)                                             | DevNull          | 389    | 350    |
| Trackball                                                          | DevNull          | 4949   | 4910   |
| 2.0 TB SATA3 Disk                                                  | DevNull          | 14289  | 14250  |
| F Jack Male-to-Male Cable (72 in.)                                 | DevNull          | 2849   | 2810   |
| 64 GB SSD Disk                                                     | DevNull          | 21509  | 21470  |
| 1/4 in. Standard Phone Female to Female Adapter                    | DevNull          | 399    | 360    |
| Phono Plug to F Jack                                               | DevNull          | 629    | 590    |
| 4GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x2GB)               | DevNull          | 39     | 0      |
| Surge Protector (6-Outlet)                                         | DevNull          | 2109   | 2070   |
| F Jack Male-to-Male Cable (36 in.)                                 | DevNull          | 1909   | 1870   |
| 500 GB Portable External USB 2.0 Disk                              | DevNull          | 8299   | 8260   |
| Stylus for PDA                                                     | DevNull          | 2109   | 2070   |
| 3.0 TB SATA3 Disk                                                  | DevNull          | 18379  | 18340  |
| 3.5mm Mini Female to Female, Barrel Adapter                        | DevNull          | 299    | 260    |
| RCA Phono Male to Mini Female Adapter                              | DevNull          | 419    | 380    |
| 8 GB USB Flash Drive                                               | DevNull          | 2849   | 2810   |
| Battery Charger                                                    | DevNull          | 3199   | 3160   |
| UPS 500 Watt                                                       | DevNull          | 8699   | 8660   |
| 3-Pin XLR Male to RCA Phono Female Adapter                         | DevNull          | 919    | 880    |
| USB 4-Port Powered Desktop Hub                                     | DevNull          | 2569   | 2530   |
| UPS 1500 Watt                                                      | DevNull          | 17289  | 17250  |
| RCA Phono Male to 1/4 in. Standard Phone Female Adapter            | DevNull          | 609    | 570    |
| RCA Phono Male to 1/4 in. Standard Phone Female Adapter            | DevNull          | 579    | 540    |
| 8GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x4GB)               | DevNull          | 59     | 20     |
| 16 GB Micro SD (High Performance)                                  | DevNull          | 5619   | 5580   |
| Office Suite (Enterprise Edition)                                  | DevNull          | 50539  | 50500  |
| 8 GB Micro SD                                                      | DevNull          | 2849   | 2810   |
| 32 GB Micro SD                                                     | DevNull          | 6059   | 6020   |
| Rechargeable Batteries (D, 2 pack)                                 | DevNull          | 2029   | 1990   |
| Extension cord (36 in.)                                            | DevNull          | 1979   | 1940   |
| Extension cord (36 in.)                                            | DevNull          | 2029   | 1990   |
| Virus Scanner (incl. updates for two years)                        | DevNull          | 3619   | 3580   |
| 9.1 GB SCSI Disk                                                   | DevNull          | 8229   | 8190   |
| S-Video Female-to-Female Cable (12 in.)                            | DevNull          | 849    | 810    |
| Phono Jack to UHF Plug                                             | DevNull          | 599    | 560    |
| Virus Scanner (incl. updates for two years)                        | DevNull          | 3759   | 3720   |
| Virus Scanner (incl. updates for one year)                         | DevNull          | 2849   | 2810   |
| 3.5mm Mini Male to Standard Phone Female                           | DevNull          | 299    | 260    |
| Virus Scanner (incl. updates for one year)                         | DevNull          | 2919   | 2880   |
| Phono Plug to UHF Jack                                             | DevNull          | 549    | 510    |
| Phono Plug to F Jack                                               | DevNull          | 599    | 560    |
| SCSI Terminator                                                    | Dorx             | 4059   | 2130   |
| 16 GB USB Flash Drive                                              | Dorx             | 4019   | 2090   |
| 8 GB USB Flash Drive                                               | Dorx             | 2879   | 950    |
| 6 GB IDE Disk                                                      | Dorx             | 5209   | 3280   |
| Stereo USB Headset                                                 | Dorx             | 2829   | 900    |
| 4 GB USB Flash Drive                                               | Dorx             | 1929   | 0      |
| 3.0 TB SATA3 Disk                                                  | Dorx             | 18879  | 16950  |
| 2.0 TB SATA3 Disk                                                  | Dorx             | 14469  | 12540  |
| 64 GB Micro SD                                                     | Dorx             | 9859   | 7930   |
| 16 GB Micro SD                                                     | Dorx             | 4329   | 2400   |
| 8 GB Micro SD                                                      | Dorx             | 2839   | 910    |
| 8 GB Micro SD                                                      | Dorx             | 2959   | 1030   |
| MP3 Player (16 GB internal memory)                                 | Dorx             | 12819  | 10890  |
| High-range Premium Speakers                                        | Dorx             | 46699  | 44770  |
| Digital Recorder (pocket size)                                     | Dorx             | 8899   | 6970   |
| LTO Tape Backup Drive (SCSI)                                       | Dorx             | 79969  | 78040  |
| Basic Headphone                                                    | Dorx             | 2029   | 100    |
| 1 TB NAS Server                                                    | Dorx             | 16769  | 14840  |
| Portable CD Player                                                 | Dorx             | 4079   | 2150   |
| 64 GB USB Flash Drive                                              | Dorx             | 8669   | 6740   |
| 32 GB USB Flash Drive                                              | Dorx             | 5639   | 3710   |
| 8GB DDR3-1600 (PC3-12800) Dual Channel Desktop Memory Kit (2x4GB)  | Dualcore         | 119    | 0      |
| Premium Gamer Desktop                                              | Dualcore         | 131539 | 131420 |
| USB 4-Port Powered Desktop Hub                                     | Dualcore         | 2619   | 2500   |
| Wireless G Router                                                  | Dualcore         | 8249   | 8130   |
| Ethernet NIC (Gigabit)                                             | Dualcore         | 6139   | 6020   |
| Black 8X BD-ROM 16X DVD-ROM                                        | Dualcore         | 7909   | 7790   |
| High Speed HDMI Cable (72 in.)                                     | Dualcore         | 5839   | 5720   |
| USB Card Reader                                                    | Dualcore         | 2029   | 1910   |
| Dual USB Power Adapter                                             | Dualcore         | 2019   | 1900   |
| Sports Headphone                                                   | Dualcore         | 3039   | 2920   |
| Ethernet NIC (10 Gigabit)                                          | Dualcore         | 18939  | 18820  |
| VPN Appliance (16 Client license)                                  | Dualcore         | 22699  | 22580  |
| USB Power Adapter (single)                                         | Dualcore         | 1259   | 1140   |
| Extension cord (20 ft., outdoor)                                   | Dualcore         | 5049   | 4930   |
| Multimedia Headset                                                 | Dualcore         | 5609   | 5490   |
| 3.5mm Mini Male to Standard Phone Female                           | Dualcore         | 299    | 180    |
| Professional Series Desktop                                        | Dualcore         | 90989  | 90870  |
| Basic Desktop                                                      | Dualcore         | 63409  | 63290  |
| Dual USB Power Adapter (auto)                                      | Dualcore         | 3009   | 2890   |
| F Jack Male-to-Male Cable (6 in.)                                  | Dualcore         | 649    | 530    |
| USB 4-Port Powered Desktop Hub                                     | Dualcore         | 2719   | 2600   |
| Sports Headphone                                                   | Dualcore         | 3139   | 3020   |
| Home Media Server                                                  | Dualcore         | 62139  | 62020  |
| Basic Headphone                                                    | Dualcore         | 2049   | 1930   |
| Hard Disk Anti-Vibration Screws                                    | Dualcore         | 409    | 290    |
| Dual USB Power Adapter                                             | Dualcore         | 2159   | 2040   |
| Server (1U rackmount, hex-core, 16GB, 8TB)                         | Dualcore         | 448239 | 448120 |
| Black 16X DVD-RW                                                   | Dualcore         | 3529   | 3410   |
| Black 16X DVD-RW                                                   | Dualcore         | 3519   | 3400   |
| TV Tuner Card (PCI)                                                | Dualcore         | 4999   | 4880   |
| RCA Phono Male to 1/4 in. Standard Phone Female Adapter            | Dualcore         | 559    | 440    |
| Compact Charger                                                    | Dualcore         | 3949   | 3830   |
| VR Glove                                                           | Dualcore         | 41529  | 41410  |
| Wireless G Router                                                  | Dualcore         | 8249   | 8130   |
| DLT-III Tape Backup Drive (SCSI)                                   | Dualcore         | 62179  | 62060  |
| Wireless N Modem Router                                            | Dualcore         | 9869   | 9750   |
| Stereo Component Turntable                                         | Dualcore         | 29419  | 29300  |
| Stereo Component 8-Track Player                                    | Dualcore         | 6919   | 6800   |
| 750 GB Portable External USB 3.0 Disk                              | Dualcore         | 12409  | 12290  |
| Portable Cassette Player                                           | Dualcore         | 3069   | 2950   |
| Portable Cassette Player                                           | Dualcore         | 2999   | 2880   |
| ADSL Modem + Wireless N Router                                     | Dualcore         | 9139   | 9020   |
| 3.4GHz CPU                                                         | Dualcore         | 18059  | 17940  |
| 3.2GHz CPU                                                         | Dualcore         | 16229  | 16110  |
| Wireless N USB Adapter                                             | Dualcore         | 6559   | 6440   |
| Batteries (AAA, 2 pack)                                            | Dualcore         | 439    | 320    |
| MP3 Player (4 GB internal memory)                                  | Dualcore         | 7699   | 7580   |
| Batteries (C, 2 pack)                                              | Dualcore         | 739    | 620    |
| MP3 Player (8 GB internal memory)                                  | Dualcore         | 9669   | 9550   |
| 2 GB Micro SD                                                      | Dualcore         | 979    | 860    |
| Keyboard (basic PC101)                                             | Dualcore         | 3009   | 2890   |
| 8 GB Micro SD                                                      | Dualcore         | 2809   | 2690   |
| 16 GB Micro SD                                                     | Dualcore         | 4309   | 4190   |
| MP3 Player (16 GB internal memory)                                 | Dualcore         | 13119  | 13000  |
| 1/4 in. Standard Phone Male to 3.5mm Mini Female Adapter           | Dualcore         | 279    | 160    |
| Extension cord (36 in.)                                            | Dualcore         | 1789   | 1670   |
| Extension cord (36 in.)                                            | Dualcore         | 1969   | 1850   |
| Extension cord (18 in.)                                            | Dualcore         | 1369   | 1250   |
| VPN Appliance (50 Clienti license)                                 | Dualcore         | 48049  | 47930  |
| 1.5 TB SATA3 Disk                                                  | Dualcore         | 13419  | 13300  |
| 1.5 TB SATA3 Disk                                                  | Dualcore         | 12619  | 12500  |
| 1.5 TB SATA3 Disk                                                  | Dualcore         | 13059  | 12940  |
| Multi-Performance PC Headset                                       | Dualcore         | 5129   | 5010   |
| USB 3.0 Front Panel - 2 Port                                       | Dualcore         | 1289   | 1170   |
| Extension cord (18 in.)                                            | Dualcore         | 1249   | 1130   |
| Extension cord (18 in.)                                            | Dualcore         | 1329   | 1210   |
| Portable transistor radio (AM/FM)                                  | Dualcore         | 3019   | 2900   |
| 24" Wide Screen LCD Monitor                                        | Dualcore         | 22719  | 22600  |
| Surge Protector (6-Outlet)                                         | Dualcore         | 2209   | 2090   |
| Surge Protector (6-Outlet)                                         | Dualcore         | 2269   | 2150   |
| Surge Protector (8-Outlet)                                         | Dualcore         | 2429   | 2310   |
| Portable CD Player                                                 | Dualcore         | 3819   | 3700   |
| Compact Charger                                                    | Dualcore         | 3859   | 3740   |
| Network Switch (Gigabit, 8-port)                                   | Dualcore         | 6899   | 6780   |
| Wireless N Router                                                  | Dualcore         | 6889   | 6770   |
| BNC Plug to UHF Jack                                               | Duff             | 559    | 540    |
| BNC Jack to UHF Plug                                               | Duff             | 579    | 560    |
| S-Video Female-to-Female Cable (24 in.)                            | Duff             | 1559   | 1540   |
| Phono Jack to F Plug                                               | Duff             | 459    | 440    |
| ADSL Modem + Wireless N Router                                     | Duff             | 8969   | 8950   |
| Phono Plug to UHF Jack                                             | Duff             | 649    | 630    |
| F Jack Male-to-Male Cable (36 in.)                                 | Duff             | 1819   | 1800   |
| 1GB DDR2-667 (PC2-5300) Desktop Memory Kit (2x512MB)               | Duff             | 19     | 0      |
| VPN Appliance (50 Clienti license)                                 | Duff             | 50029  | 50010  |
| RCA Phono Female to Female, Barrel Adapter                         | Duff             | 499    | 480    |
| High Speed HDMI Cable (48 in.)                                     | Duff             | 4209   | 4190   |
| High Speed HDMI Cable (48 in.)                                     | Duff             | 4029   | 4010   |
| Screen Cleaning Cloth (white)                                      | Duff             | 1969   | 1950   |
| Wireless N USB Adapter                                             | Duff             | 6489   | 6470   |
| Batteries (AA, 2 pack)                                             | Duff             | 379    | 360    |
| Batteries (AAA, 4 pack)                                            | Duff             | 659    | 640    |
| Batteries (AAA, 2 pack)                                            | Duff             | 399    | 380    |
| Batteries (AAA, 2 pack)                                            | Duff             | 379    | 360    |
| Rechargeable Batteries (AA, 4 pack)                                | Duff             | 2759   | 2740   |
| Rechargeable Batteries (C, 2 pack)                                 | Duff             | 1889   | 1870   |
| Keyboard (basic PC101)                                             | Duff             | 2799   | 2780   |
| Network Switch (Gigabit, 8-port)                                   | Duff             | 6639   | 6620   |
| UPS 1200 Watt                                                      | Duff             | 16249  | 16230  |
| Surge Protector (4-Outlet)                                         | Duff             | 1749   | 1730   |
| Laptop Cooling Pad                                                 | Duff             | 4209   | 4190   |
| Network Switch (Gigabit, 16-port)                                  | Duff             | 7989   | 7970   |
| Network Switch (Gigabit, 24-port)                                  | Duff             | 12789  | 12770  |
| Network Switch (Gigabit, 24-port)                                  | Duff             | 13159  | 13140  |
| 3.5mm Mini Male to RCA Phono Female Adapter                        | Duff             | 409    | 390    |
| Composite AV Cable (72 in.)                                        | Duff             | 2589   | 2570   |
| Composite AV Cable (36 in.)                                        | Duff             | 2539   | 2520   |
| ATX Computer Case                                                  | Electrosaurus    | 6099   | 5690   |
| Fan Adapter Cable - 22 inch                                        | Electrosaurus    | 509    | 100    |
| 3D Digital TV Glasses                                              | Electrosaurus    | 7599   | 7190   |
| 1024MB PCI-X Graphics Card with HDMI output (OEM)                  | Electrosaurus    | 9939   | 9530   |
| 1024MB PCI-X Graphics Card with HDMI output (OEM)                  | Electrosaurus    | 9919   | 9510   |
| Full Tower Computer Case                                           | Electrosaurus    | 13089  | 12680  |
| Full Tower Computer Case                                           | Electrosaurus    | 13039  | 12630  |
| 24" Wide Screen TV                                                 | Electrosaurus    | 24139  | 23730  |
| 512MB PCI-X Graphics Card (OEM)                                    | Electrosaurus    | 7919   | 7510   |
| 128MB AGP Graphics Card                                            | Electrosaurus    | 5829   | 5420   |
| 16MB VESA Graphics Card                                            | Electrosaurus    | 1979   | 1570   |
| 3.0GHz CPU                                                         | Electrosaurus    | 13129  | 12720  |
| TV Tuner Card (PCI)                                                | Electrosaurus    | 4949   | 4540   |
| 3.6GHz CPU                                                         | Electrosaurus    | 22049  | 21640  |
| Beige 16X DVD-RW                                                   | Electrosaurus    | 3449   | 3040   |
| Fireware 400 Card (PCI)                                            | Electrosaurus    | 4969   | 4560   |
| 48" Wide Screen TV                                                 | Electrosaurus    | 68489  | 68080  |
| Power Supply (600W)                                                | Electrosaurus    | 2799   | 2390   |
| 42" Wide Screen TV                                                 | Electrosaurus    | 58279  | 57870  |
| All-in-one Desktop (21 in. display)                                | Electrosaurus    | 173069 | 172660 |
| Tablet PC (7 in. display, 16 GB)                                   | Electrosaurus    | 35019  | 34610  |
| Server (1U rackmount, quad-core, 4GB, 2TB)                         | Electrosaurus    | 295269 | 294860 |
| Soundcard (Premium Multimedia)                                     | Electrosaurus    | 8979   | 8570   |
| Hard Disk Anti-Vibration Screws                                    | Electrosaurus    | 409    | 0      |
| 2.5" External USB 3.0 HDD Case                                     | Electrosaurus    | 1289   | 880    |
| Ethernet NIC (Gigabit)                                             | Foocorp          | 6039   | 3050   |
| Ethernet NIC (10 Gigabit)                                          | Foocorp          | 18319  | 15330  |
| VPN Appliance (250 Clienti license)                                | Foocorp          | 229329 | 226340 |
| Wireless G Access Point                                            | Foocorp          | 8669   | 5680   |
| DSL Modem                                                          | Foocorp          | 6879   | 3890   |
| Firewall                                                           | Foocorp          | 27029  | 24040  |
| Network Hub (10 MBps, 8-port)                                      | Foocorp          | 2989   | 0      |
| RCA Phono Male to Mini Female Adapter                              | Gigabux          | 409    | 10     |
| Wheel Control                                                      | Gigabux          | 4209   | 3810   |
| Composite AV Cable (12 in.)                                        | Gigabux          | 2549   | 2150   |
| 1/4 in. Standard Phone Female to Female Adapter                    | Gigabux          | 399    | 0      |
| Surge Protector (4-Outlet)                                         | Gigabux          | 1889   | 1490   |
| 61" Wide Screen TV                                                 | Gigabux          | 122919 | 122520 |
| 1/4 in. Standard Phone Female to Female Adapter                    | Gigabux          | 399    | 0      |
| Joystick                                                           | Gigabux          | 2249   | 1850   |
| Battery Charger                                                    | Gigabux          | 3319   | 2920   |
| 3-Pin XLR Male to Male, Barrel Adapter                             | Gigabux          | 749    | 350    |
| Batteries (D, 2 pack)                                              | Gigabux          | 959    | 560    |
| Wired mouse (three button, with wheel)                             | Gigabux          | 4419   | 4020   |
| UPS 2500 Watt (rackmount)                                          | Gigabux          | 47879  | 47480  |
| UPS 1000 Watt                                                      | Gigabux          | 13209  | 12810  |
| Composite AV Cable (24 in.)                                        | Gigabux          | 2419   | 2020   |
| Keyboard (basic PC101)                                             | Gigabux          | 3079   | 2680   |
| Keyboard (basic PC101)                                             | Gigabux          | 2979   | 2580   |
| RCA Phono Male to 1/4 in. Standard Phone Female Adapter            | Gigabux          | 629    | 230    |
| Digital Video Recorder (1.0 TB)                                    | Gigabux          | 38389  | 37990  |
| Batteries (AA, 4 pack)                                             | Gigabux          | 669    | 270    |
| Server (2U rackmount, eight-core, 64GB, 12TB)                      | Gigabux          | 614559 | 614160 |
| 24" Wide Screen TV                                                 | Gigabux          | 23899  | 23500  |
| Phono Jack to F Plug                                               | Gigabux          | 509    | 110    |
| Phono Jack to UHF Plug                                             | Gigabux          | 619    | 220    |
| RCA Phono Male to Mini Female Adapter                              | Gigabux          | 419    | 20     |
| 500 GB SATA3 Disk                                                  | Homertech        | 9159   | 7210   |
| 32 GB USB Flash Drive                                              | Homertech        | 6199   | 4250   |
| 16 GB USB Flash Drive                                              | Homertech        | 4079   | 2130   |
| Ethernet NIC (Gigabit)                                             | Homertech        | 6039   | 4090   |
| Ethernet NIC (Gigabit)                                             | Homertech        | 5909   | 3960   |
| Professional Series Desktop                                        | Homertech        | 88319  | 86370  |
| Wireless N USB Adapter                                             | Homertech        | 6629   | 4680   |
| 64 GB Micro SD                                                     | Homertech        | 9809   | 7860   |
| 1.5 TB SATA3 Disk                                                  | Homertech        | 13219  | 11270  |
| 4 GB USB Flash Drive                                               | Homertech        | 1949   | 0      |
| Economy Netbook                                                    | Homertech        | 29249  | 27300  |
| SCSI Terminator                                                    | Homertech        | 3909   | 1960   |
| Tablet PC (7 in. display, 32 GB)                                   | Homertech        | 40989  | 39040  |
| VPN Appliance (16 Client license)                                  | Homertech        | 23019  | 21070  |
| Wireless G USB Adapter                                             | Homertech        | 5839   | 3890   |
| Tablet PC (7 in. display, 16 GB)                                   | Homertech        | 35219  | 33270  |
| Basic Wireless Modem Router                                        | Homertech        | 7109   | 5160   |
| 32 GB SSD Disk                                                     | Homertech        | 14789  | 12840  |
| 4 GB Micro SD                                                      | Krustybitz       | 1329   | 0      |
| 32 GB USB Flash Drive                                              | Krustybitz       | 5779   | 4450   |
| 64 GB USB Flash Drive                                              | Krustybitz       | 9339   | 8010   |
| 2 GB Compact Flash                                                 | Krustybitz       | 1949   | 620    |
| 64 GB SSD Disk                                                     | Krustybitz       | 21619  | 20290  |
| 32 GB SSD Disk                                                     | Krustybitz       | 14699  | 13370  |
| 750 GB Portable External USB 3.0 Disk                              | Krustybitz       | 12729  | 11400  |
| 32 GB Micro SD (High Performance)                                  | Krustybitz       | 7219   | 5890   |
| Server (2U rackmount, eight-core, 64GB, 12TB)                      | Krustybitz       | 599319 | 597990 |
| 1.0 TB SATA3 Disk                                                  | Krustybitz       | 10749  | 9420   |
| Professional Series Notebook                                       | Krustybitz       | 71159  | 69830  |
| 750 GB SATA3 Disk                                                  | Krustybitz       | 10209  | 8880   |
| 750 GB SATA3 Disk                                                  | Krustybitz       | 10399  | 9070   |
| Server (1U rackmount, hex-core, 8GB, 4TB)                          | Krustybitz       | 409949 | 408620 |
| Basic Desktop                                                      | Krustybitz       | 61989  | 60660  |
| 173 GB SAS Disk                                                    | Krustybitz       | 48519  | 47190  |
| 20 GB PATA Disk                                                    | Krustybitz       | 6219   | 4890   |
| Server (1U rackmount, hex-core, 16GB, 8TB)                         | Krustybitz       | 459179 | 457850 |
| Premium Gamer Notebook                                             | Lemmon           | 91819  | 89880  |
| Basic Headphone                                                    | Lemmon           | 1939   | 0      |
| Gamer Headphone                                                    | Lemmon           | 7789   | 5850   |
| Gamer Headphone                                                    | Lemmon           | 8109   | 6170   |
| 27" Wide Screen LCD Monitor                                        | Lemmon           | 36159  | 34220  |
| DLT-III Tape Backup Drive (SCSI)                                   | Lemmon           | 58159  | 56220  |
| 8 TB NAS Server                                                    | Lemmon           | 46179  | 44240  |
| Stereo Component CD Player                                         | Lemmon           | 20229  | 18290  |
| 32 GB USB Flash Drive                                              | Lemmon           | 5869   | 3930   |
| DSP PC Headset                                                     | Lemmon           | 5119   | 3180   |
| 4 GB USB Flash Drive                                               | Lemmon           | 2039   | 100    |
| 1.0 TB SATA3 Disk                                                  | Lemmon           | 10679  | 8740   |
| Portable transistor radio (AM/FM)                                  | Lemmon           | 2909   | 970    |
| 20" Wide Screen LCD Monitor                                        | Lemmon           | 15119  | 13180  |
| MP3 Player (64 GB internal memory)                                 | Lemmon           | 26469  | 24530  |
| Subwoofer (12 in.)                                                 | Lemmon           | 16509  | 14570  |
| Video Cassette Recorder                                            | Lemmon           | 13669  | 11730  |
| MP3 Player (16 GB internal memory)                                 | Lemmon           | 13619  | 11680  |
| MP3 Player (16 GB internal memory)                                 | Lemmon           | 12899  | 10960  |
| 32" Wide Screen TV                                                 | Lemmon           | 36069  | 34130  |
| 32 GB Micro SD                                                     | Lemmon           | 6009   | 4070   |
| Digital Video Recorder (500 GB)                                    | Lemmon           | 29089  | 27150  |
| 61" Wide Screen TV                                                 | Lemmon           | 131149 | 129210 |
| 32" Wide Screen TV                                                 | Lemmon           | 36909  | 34970  |
| Digital Video Recorder (750 GB)                                    | Lemmon           | 34449  | 32510  |
| 36" Wide Screen TV                                                 | Lemmon           | 43129  | 41190  |
| Server (1U rackmount, hex-core, 8GB, 4TB)                          | Lemmon           | 397159 | 395220 |
| Digital Recorder (pocket size)                                     | Lemmon           | 8749   | 6810   |
| 42" Wide Screen TV                                                 | Lemmon           | 55169  | 53230  |
| Inkjet Printer                                                     | McDowell         | 17699  | 17530  |
| Laser Printer (Black-and-White)                                    | McDowell         | 39579  | 39410  |
| Laser Printer (Black-and-White)                                    | McDowell         | 40399  | 40230  |
| Laser Printer (Black-and-White)                                    | McDowell         | 37629  | 37460  |
| Inkjet Printer                                                     | McDowell         | 17729  | 17560  |
| 42" Wide Screen TV                                                 | McDowell         | 58029  | 57860  |
| Extension cord (10 ft., outdoor)                                   | McDowell         | 4219   | 4050   |
| Laser Printer (Black-and-White)                                    | McDowell         | 39559  | 39390  |
| USB Card Reader                                                    | McDowell         | 2149   | 1980   |
| Laser Printer (Black-and-White, Duplex)                            | McDowell         | 66679  | 66510  |
| USB 4-Port Powered Desktop Hub                                     | McDowell         | 2349   | 2180   |
| Laser Printer (Color, Duplex)                                      | McDowell         | 47229  | 47060  |
| 42" Wide Screen TV                                                 | McDowell         | 56919  | 56750  |
| Extension cord (50 ft., outdoor)                                   | McDowell         | 7019   | 6850   |
| All-in-one Print/Scan/Copy                                         | McDowell         | 23029  | 22860  |
| Wide Format Printer                                                | McDowell         | 87719  | 87550  |
| 42" Wide Screen TV                                                 | McDowell         | 55649  | 55480  |
| 48" Wide Screen TV                                                 | McDowell         | 64909  | 64740  |
| All-in-one Print/Fax/Scan/Copy                                     | McDowell         | 27729  | 27560  |
| Digital Camera (Pro-Am)                                            | McDowell         | 20119  | 19950  |
| Scanner                                                            | McDowell         | 28349  | 28180  |
| USB 2.0 4-Port Mini Hub                                            | McDowell         | 2129   | 1960   |
| Scanner                                                            | McDowell         | 28309  | 28140  |
| 36" Wide Screen TV                                                 | McDowell         | 42859  | 42690  |
| S-Video Female-to-Female Cable (12 in.)                            | McDowell         | 729    | 560    |
| Inkjet Photo Printer                                               | McDowell         | 23419  | 23250  |
| Laser Printer (Black-and-White)                                    | McDowell         | 39019  | 38850  |
| Inkjet Printer                                                     | McDowell         | 17649  | 17480  |
| Keyboard (gamer PC104)                                             | McDowell         | 8659   | 8490   |
| Inkjet Printer                                                     | McDowell         | 17839  | 17670  |
| S-Video Female-to-Female Cable (24 in.)                            | McDowell         | 1449   | 1280   |
| 16GB DDR3-1600 (PC3-12800) Dual Channel Desktop Memory Kit (2x8GB) | McDowell         | 169    | 0      |
| Digital Optical Audio Cable                                        | McDowell         | 2689   | 2520   |
| Phono Jack to F Plug                                               | McDowell         | 469    | 300    |
| Headphone Adapter (1/4 in. to 1/8 in.)                             | McDowell         | 839    | 670    |
| RCA Phono Female to Female, Barrel Adapter                         | McDowell         | 529    | 360    |
| UPS 1500 Watt                                                      | McDowell         | 19629  | 19460  |
| Digital Video Recorder (1.5 TB)                                    | McDowell         | 47309  | 47140  |
| Digital Video Recorder (1.5 TB)                                    | McDowell         | 47449  | 47280  |
| Digital Video Recorder (1.0 TB)                                    | McDowell         | 38489  | 38320  |
| Digital Video Recorder (1.0 TB)                                    | McDowell         | 39719  | 39550  |
| UPS 1200 Watt                                                      | McDowell         | 15249  | 15080  |
| Headphone Adapter (1/4 in. to 1/8 in.)                             | McDowell         | 769    | 600    |
| S-Video Female-to-Female Cable (36 in.)                            | McDowell         | 1739   | 1570   |
| 3-Pin XLR Male to 1/4 in. Standard Phone Female Adapter            | McDowell         | 859    | 690    |
| RCA Phono Male to Mini Female Adapter                              | McDowell         | 389    | 220    |
| 3.5mm Mini Female to Female, Barrel Adapter                        | McDowell         | 329    | 160    |
| Rechargeable Batteries (9V, 1 pack)                                | McDowell         | 1389   | 1220   |
| Rechargeable Batteries (9V, 1 pack)                                | McDowell         | 1379   | 1210   |
| Gaming Console                                                     | McDowell         | 36599  | 36430  |
| Phono Jack to UHF Plug                                             | McDowell         | 649    | 480    |
| Extension cord (36 in., heavy duty)                                | McDowell         | 2939   | 2770   |
| Camcorder (Betamax)                                                | McDowell         | 31749  | 31580  |
| 8GB DDR3-1600 (PC3-12800) Dual Channel Desktop Memory Kit (2x4GB)  | Megachango       | 119    | 90     |
| 4GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x2GB)               | Megachango       | 39     | 10     |
| ATX Computer Case                                                  | Megachango       | 6009   | 5980   |
| Beige 16X DVD-ROM                                                  | Megachango       | 2499   | 2470   |
| Beige 8X BD-ROM 16X DVD-ROM                                        | Megachango       | 8109   | 8080   |
| Beige 8X BD-ROM 16X DVD-ROM                                        | Megachango       | 8059   | 8030   |
| Air Filter (Washable, 120mm)                                       | Megachango       | 609    | 580    |
| High Speed HDMI Cable (36 in.)                                     | Megachango       | 3019   | 2990   |
| Netbook Case                                                       | Megachango       | 2759   | 2730   |
| Case for Notebook Computer                                         | Megachango       | 4189   | 4160   |
| Sleeve for Mini Tablet - Black                                     | Megachango       | 1509   | 1480   |
| Sleeve for Mini Tablet - Black                                     | Megachango       | 1629   | 1600   |
| Sleeve for Tablet - Red                                            | Megachango       | 1979   | 1950   |
| Sleeve for Tablet - Blue                                           | Megachango       | 1999   | 1970   |
| Sleeve for Tablet - Black                                          | Megachango       | 1979   | 1950   |
| 8-cell Battery                                                     | Megachango       | 6409   | 6380   |
| Auto Charger                                                       | Megachango       | 4339   | 4310   |
| Rechargeable Batteries (AA, 4 pack)                                | Megachango       | 3059   | 3030   |
| Rechargeable Batteries (AAA, 4 pack)                               | Megachango       | 2959   | 2930   |
| Keyboard (basic PC104)                                             | Megachango       | 3209   | 3180   |
| Joystick                                                           | Megachango       | 2319   | 2290   |
| Gamepad                                                            | Megachango       | 2239   | 2210   |
| UPS 1000 Watt                                                      | Megachango       | 13169  | 13140  |
| UPS 800 Watt                                                       | Megachango       | 12429  | 12400  |
| Surge Protector (4-Outlet)                                         | Megachango       | 1909   | 1880   |
| Extension cord (72 in.)                                            | Megachango       | 3029   | 3000   |
| 1GB DDR2-667 (PC2-5300) Desktop Memory Kit (2x512MB)               | Megachango       | 29     | 0      |
| 64 GB USB Flash Drive                                              | Megachango       | 8959   | 8930   |
| F Jack Male-to-Male Cable (36 in.)                                 | Megachango       | 1749   | 1720   |
| 32 GB Micro SD (High Performance)                                  | Megachango       | 7089   | 7060   |
| 8 GB Micro SD                                                      | Megachango       | 2989   | 2960   |
| F Jack Male-to-Male Cable (12 in.)                                 | Megachango       | 939    | 910    |
| 4 TB NAS Server                                                    | Megachango       | 31159  | 31130  |
| 2 GB Micro SD                                                      | Megachango       | 1019   | 990    |
| 1/4 in. Standard Phone Male to 3.5mm Mini Female Adapter           | Megachango       | 309    | 280    |
| 1.0 TB SATA3 Disk                                                  | Megachango       | 11159  | 11130  |
| 500 GB Portable External USB 3.0 Disk                              | Megachango       | 9929   | 9900   |
| 1 GB Micro SD                                                      | Megachango       | 799    | 770    |
| 500 GB Portable External USB 3.0 Disk                              | Megachango       | 10489  | 10460  |
| 1/4 in. Standard Phone Male to RCA Phono Female Adapter            | Megachango       | 379    | 350    |
| 64 GB SSD Disk                                                     | Megachango       | 22829  | 22800  |
| 64 GB SSD Disk                                                     | Megachango       | 22479  | 22450  |
| 3-Pin XLR Male to 1/4 in. Standard Phone Female Adapter            | Megachango       | 879    | 850    |
| RCA Phono Female to Female, Barrel Adapter                         | Megachango       | 489    | 460    |
| 4 GB Compact Flash                                                 | Megachango       | 2689   | 2660   |
| 173 GB SAS Disk                                                    | Megachango       | 48709  | 48680  |
| 512MB PCI-X Graphics Card (OEM)                                    | Megachango       | 7929   | 7900   |
| S-Video Female-to-Female Cable (24 in.)                            | Megachango       | 1619   | 1590   |
| Power Supply (500W)                                                | Megachango       | 2529   | 2500   |
| 32 GB USB Flash Drive                                              | Megachango       | 5629   | 5600   |
| 32 GB USB Flash Drive (High Performance)                           | Megachango       | 6919   | 6890   |
| F Jack Male-to-Male Cable (60 in.)                                 | Megachango       | 2519   | 2490   |
| 2GB DDR2 667 RAM                                                   | Megachango       | 15239  | 15210  |
| 64 GB USB Flash Drive                                              | Megachango       | 8799   | 8770   |
| Full Tower Computer Case                                           | Megachango       | 12829  | 12800  |
| 3.0 TB SATA3 Disk                                                  | Megachango       | 18789  | 18760  |
| Professional DDR3 Motherboard                                      | Megachango       | 17939  | 17910  |
| Scanner                                                            | Olde-Gray        | 27349  | 12440  |
| Daisy Wheel Printer                                                | Olde-Gray        | 27839  | 12930  |
| Scanner                                                            | Olde-Gray        | 27979  | 13070  |
| Tablet PC (7 in. display, 16 GB, WiFi-only)                        | Olde-Gray        | 32589  | 17680  |
| Wide Format Printer                                                | Olde-Gray        | 87619  | 72710  |
| All-in-one Print/Fax/Scan/Copy                                     | Olde-Gray        | 28059  | 13150  |
| 20" Wide Screen LCD Monitor                                        | Olde-Gray        | 14909  | 0      |
| 24" Wide Screen LCD Monitor                                        | Olde-Gray        | 22579  | 7670   |
| Economy Netbook                                                    | Olde-Gray        | 30639  | 15730  |
| 24" LED Backlit Monitor                                            | Olde-Gray        | 29579  | 14670  |
| All-in-one Print/Fax/Scan/Copy                                     | Olde-Gray        | 27379  | 12470  |
| Server (1U rackmount, hex-core, 8GB, 4TB)                          | Olde-Gray        | 393989 | 379080 |
| 40" Large-Format LCD Display                                       | Olde-Gray        | 85979  | 71070  |
| Inkjet Printer                                                     | Olde-Gray        | 17669  | 2760   |
| 40" Large-Format LCD Display                                       | Olde-Gray        | 85219  | 70310  |
| 40" Large-Format LCD Display                                       | Olde-Gray        | 86159  | 71250  |
| 30" Wide Screen LCD Monitor                                        | Olde-Gray        | 50349  | 35440  |
| 24" Ultra-Premium LCD Monitor                                      | Olde-Gray        | 46479  | 31570  |
| Premium Gamer Desktop                                              | Olde-Gray        | 129219 | 114310 |
| Laser Printer (Black-and-White)                                    | Olde-Gray        | 37899  | 22990  |
| Laser Printer (Black-and-White)                                    | Olde-Gray        | 38189  | 23280  |
| Basic Desktop                                                      | Olde-Gray        | 63639  | 48730  |
| All-in-one Print/Fax/Scan/Copy                                     | Olde-Gray        | 27569  | 12660  |
| Home Media Server                                                  | Olde-Gray        | 62469  | 47560  |
| 22X DVD RW SATA Drive                                              | Orion            | 3909   | 3740   |
| Wired mouse (three button)                                         | Orion            | 3939   | 3770   |
| Air Filter (Washable, 120mm)                                       | Orion            | 609    | 440    |
| Air Filter (Washable, 120mm)                                       | Orion            | 599    | 430    |
| Air Filter (Washable, 120mm)                                       | Orion            | 609    | 440    |
| 16 GB USB Flash Drive (Red)                                        | Orion            | 42999  | 42830  |
| 16 GB USB Flash Drive (Green)                                      | Orion            | 4299   | 4130   |
| 16 GB USB Flash Drive (Blue)                                       | Orion            | 4299   | 4130   |
| Joystick                                                           | Orion            | 2619   | 2450   |
| Battery Charger                                                    | Orion            | 3179   | 3010   |
| 1 TB NAS Server                                                    | Orion            | 16239  | 16070  |
| Fan Adapter Cable - 10 inch                                        | Orion            | 299    | 130    |
| 6-cell Battery                                                     | Orion            | 4359   | 4190   |
| 64MB PCI Graphics Card                                             | Orion            | 2999   | 2830   |
| Sleeve for Tablet - Blue                                           | Orion            | 2129   | 1960   |
| Batteries (AAA, 4 pack)                                            | Orion            | 749    | 580    |
| Batteries (AAA, 4 pack)                                            | Orion            | 749    | 580    |
| Case for Notebook Computer                                         | Orion            | 4419   | 4250   |
| 3.8GHz CPU                                                         | Orion            | 28079  | 27910  |
| Netbook Case                                                       | Orion            | 2709   | 2540   |
| Professional DDR3 Motherboard                                      | Orion            | 18189  | 18020  |
| Batteries (D, 2 pack)                                              | Orion            | 1089   | 920    |
| Server Motherboard                                                 | Orion            | 27319  | 27150  |
| Mid Tower Computer Case                                            | Orion            | 7959   | 7790   |
| Rechargeable Batteries (AAA, 4 pack)                               | Orion            | 2789   | 2620   |
| Composite AV Cable (72 in.)                                        | Orion            | 2339   | 2170   |
| Composite AV Cable (72 in.)                                        | Orion            | 2289   | 2120   |
| 4 GB Micro SD                                                      | Orion            | 1239   | 1070   |
| Composite AV Cable (24 in.)                                        | Orion            | 2289   | 2120   |
| Keyboard (basic PC101)                                             | Orion            | 2659   | 2490   |
| USB Card Reader                                                    | Orion            | 1909   | 1740   |
| Basic Desktop                                                      | Orion            | 43929  | 43760  |
| Professional Series Notebook                                       | Orion            | 68829  | 68660  |
| Ultra-portable Notebook                                            | Orion            | 81959  | 81790  |
| ATX Computer Case                                                  | Orion            | 5909   | 5740   |
| Keyboard (basic PC104)                                             | Orion            | 3789   | 3620   |
| Tablet PC (10 in. display, 64 GB)                                  | Orion            | 49069  | 48900  |
| Tablet PC (10 in. display, 32 GB)                                  | Orion            | 43869  | 43700  |
| Tablet PC (10 in. display, 32 GB)                                  | Orion            | 41969  | 41800  |
| Tablet PC (10 in. display, 32 GB)                                  | Orion            | 42869  | 42700  |
| Tablet PC (10 in. display, 16 GB)                                  | Orion            | 37449  | 37280  |
| Tablet PC (10 in. display, 16 GB, WiFi-only)                       | Orion            | 35889  | 35720  |
| Tablet PC (7 in. display, 32 GB)                                   | Orion            | 42939  | 42770  |
| 2 GB Compact Flash                                                 | Orion            | 1939   | 1770   |
| USB 7-Port Powered Desktop Hub                                     | Orion            | 4239   | 4070   |
| F Jack Male-to-Male Cable (12 in.)                                 | Orion            | 959    | 790    |
| USB 3.0 4-Port Ultra Mini Hub                                      | Orion            | 4459   | 4290   |
| Extension cord (20 ft., outdoor)                                   | Orion            | 4639   | 4470   |
| 3.5mm Mini Male to Standard Phone Female                           | Orion            | 319    | 150    |
| Extension cord (72 in.)                                            | Orion            | 2809   | 2640   |
| Extension cord (72 in.)                                            | Orion            | 3179   | 3010   |
| 500 GB SATA3 Disk                                                  | Orion            | 8989   | 8820   |
| 16GB DDR3-1600 (PC3-12800) Dual Channel Desktop Memory Kit (2x8GB) | Orion            | 169    | 0      |
| 750 GB SATA3 Disk                                                  | Orion            | 9919   | 9750   |
| Surge Protector (6-Outlet, angled plug)                            | Orion            | 2429   | 2260   |
| 4 GB USB Flash Drive                                               | Orion            | 1979   | 1810   |
| 4 GB USB Flash Drive                                               | Orion            | 2009   | 1840   |
| BNC Jack to Angled Plug                                            | Orion            | 759    | 590    |
| USB 3.0 Front Panel - 4 Port                                       | Overtop          | 1509   | 1200   |
| A3000 Gaming Cabinet                                               | Overtop          | 9869   | 9560   |
| USB 3.0 Front Panel - 4 Port                                       | Overtop          | 1529   | 1220   |
| 5.25" Drive Bay Adapter                                            | Overtop          | 1309   | 1000   |
| 3.6GHz CPU                                                         | Overtop          | 22289  | 21980  |
| Desktop Motherboard                                                | Overtop          | 7929   | 7620   |
| Molex to SATA Power Adapter                                        | Overtop          | 579    | 270    |
| 3.5" Drive Bay Adapter                                             | Overtop          | 1509   | 1200   |
| Hard Disk Anti-Vibration Screws                                    | Overtop          | 399    | 90     |
| 6-Pin Video Card Extension 12 inch                                 | Overtop          | 789    | 480    |
| 1GB DDR2 667 Laptop RAM                                            | Overtop          | 10009  | 9700   |
| HDD Bracket                                                        | Overtop          | 809    | 500    |
| Fan Adapter Cable - 10 inch                                        | Overtop          | 309    | 0      |
| 512MB PCI-X Graphics Card (OEM)                                    | Sparky           | 7869   | 7460   |
| 1GB DDR 400 Desktop RAM                                            | Sparky           | 7939   | 7530   |
| 1024MB PCI-X Graphics Card with HDMI output (OEM)                  | Sparky           | 9959   | 9550   |
| 128MB AGP Graphics Card                                            | Sparky           | 5939   | 5530   |
| 3.2GHz CPU                                                         | Sparky           | 16239  | 15830  |
| Tablet PC (7 in. display, 8 GB, WiFI-only)                         | Sparky           | 30359  | 29950  |
| DISGRUNT Motherboard                                               | Sparky           | 12939  | 12530  |
| 2GB DDR2 800MHz Laptop RAM                                         | Sparky           | 18989  | 18580  |
| Beige 16X DVD-ROM                                                  | Sparky           | 2509   | 2100   |
| PERTURB Motherboard                                                | Sparky           | 16239  | 15830  |
| Server (1U rackmount, hex-core, 16GB, 8TB)                         | Sparky           | 470019 | 469610 |
| Server (1U rackmount, hex-core, 32GB, 8TB)                         | Sparky           | 478459 | 478050 |
| All-in-one Desktop (21 in. display)                                | Sparky           | 173989 | 173580 |
| Fan Adapter Cable - 16 inch                                        | Sparky           | 409    | 0      |
| Fireware 800 Card (PCI)                                            | Sparky           | 7089   | 6680   |
| Power Supply (600W)                                                | Sparky           | 2819   | 2410   |
| Fan Adapter Cable - 22 inch                                        | Sparky           | 509    | 100    |
| 1024MB PCI-X Graphics Card with HDMI output (OEM)                  | Sparky           | 9769   | 9360   |
| 5.25" Drive Bay Adapter                                            | Sparky           | 1309   | 900    |
| 4 GB Compact Flash                                                 | Spindown         | 2699   | 1500   |
| 128 GB SSD Disk                                                    | Spindown         | 38689  | 37490  |
| LTO Tape Backup Drive (SCSI)                                       | Spindown         | 79959  | 78760  |
| 1.0 TB SATA3 Disk                                                  | Spindown         | 11089  | 9890   |
| 1 GB Compact Flash                                                 | Spindown         | 1199   | 0      |
| 8 GB USB Flash Drive                                               | Spindown         | 2759   | 1560   |
| 1.0 TB SATA3 Disk                                                  | Spindown         | 10509  | 9310   |
| 2 GB Compact Flash                                                 | Spindown         | 1889   | 690    |
| 16 GB USB Flash Drive                                              | Spindown         | 3949   | 2750   |
| 16 GB USB Flash Drive                                              | Spindown         | 3859   | 2660   |
| 19" Wide Screen LCD Monitor                                        | Sprite           | 13389  | 840    |
| 24" LED Backlit Monitor                                            | Sprite           | 29619  | 17070  |
| Video Cassette Recorder (Betamax format)                           | Sprite           | 12549  | 0      |
| 36" Wide Screen TV                                                 | Sprite           | 45309  | 32760  |
| 27" Wide Screen TV                                                 | Sprite           | 25799  | 13250  |
| Gaming Console (WiFi)                                              | Sprite           | 39699  | 27150  |
| Gaming Console                                                     | Sprite           | 35649  | 23100  |
| 22" Wide Screen LCD Monitor                                        | Sprite           | 18039  | 5490   |
| 2GB Super Gamer Graphics Card                                      | SuperGamer       | 18949  | 10920  |
| Professional Series Notebook                                       | SuperGamer       | 70069  | 62040  |
| 3.0GHz CPU                                                         | SuperGamer       | 12899  | 4870   |
| Mid Tower Computer Case                                            | SuperGamer       | 8029   | 0      |
| 1GB DDR2 800 Desktop RAM                                           | SuperGamer       | 13199  | 5170   |
| 1GB DDR2 667 Laptop RAM                                            | SuperGamer       | 10019  | 1990   |
| Premium Gamer Notebook                                             | SuperGamer       | 87729  | 79700  |
| F Jack Male-to-Male Cable (18 in.)                                 | TPS              | 1229   | 1210   |
| Composite AV Cable (12 in.)                                        | TPS              | 2619   | 2600   |
| 1GB DDR2-667 (PC2-5300) Desktop Memory Kit (2x512MB)               | TPS              | 19     | 0      |
| 1024MB PCI-X Graphics Card with HDMI output (OEM)                  | TPS              | 9829   | 9810   |
| Batteries (9V, 1 pack)                                             | TPS              | 599    | 580    |
| Dial-up Modem (33.6K)                                              | TPS              | 2389   | 2370   |
| Server Motherboard                                                 | TPS              | 26919  | 26900  |
| Ethernet NIC (Gigabit)                                             | TPS              | 5979   | 5960   |
| 4GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x2GB)               | TPS              | 29     | 10     |
| 4GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x2GB)               | TPS              | 29     | 10     |
| Compact Charger                                                    | TPS              | 3859   | 3840   |
| 2x2GB DDR3 1600MHz Desktop RAM                                     | TPS              | 20669  | 20650  |
| 4GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x2GB)               | TPS              | 29     | 10     |
| Power Supply (600W)                                                | TPS              | 2779   | 2760   |
| Power Supply (500W)                                                | TPS              | 2499   | 2480   |
| 1GB DDR2 667 Laptop RAM                                            | TPS              | 9909   | 9890   |
| 1GB DDR 400 Desktop RAM                                            | TPS              | 8099   | 8080   |
| Batteries (AA, 4 pack)                                             | TPS              | 719    | 700    |
| 3.4GHz CPU                                                         | TPS              | 17769  | 17750  |
| Batteries (AAA, 2 pack)                                            | TPS              | 389    | 370    |
| 16GB DDR3-1600 (PC3-12800) Dual Channel Desktop Memory Kit (2x8GB) | TPS              | 159    | 140    |
| 8GB DDR3-1600 (PC3-12800) Dual Channel Desktop Memory Kit (2x4GB)  | TPS              | 129    | 110    |
| BNC Jack to Angled Plug                                            | TPS              | 659    | 640    |
| TV Tuner Card w/HDMI Output (PCI)                                  | TPS              | 10149  | 10130  |
| Wired mouse (three button)                                         | TPS              | 4029   | 4010   |
| VPN Appliance (50 Clienti license)                                 | TPS              | 50159  | 50140  |
| Wireless laser mouse (three button, with wheel)                    | TPS              | 3719   | 3700   |
| Firewall                                                           | TPS              | 27339  | 27320  |
| Trackball (wireless)                                               | TPS              | 7859   | 7840   |
| Trackball (wireless)                                               | TPS              | 7549   | 7530   |
| Basic Wireless Modem Router                                        | TPS              | 6869   | 6850   |
| 3.5mm Mini Female to Female, Barrel Adapter                        | TPS              | 289    | 270    |
| Joystick                                                           | TPS              | 2309   | 2290   |
| USB 4-Port Powered Desktop Hub                                     | TPS              | 2369   | 2350   |
| USB 2.0 4-Port Mini Hub                                            | TPS              | 2109   | 2090   |
| Fan Adapter Cable - 16 inch                                        | TPS              | 399    | 380    |
| 22X DVD RW SATA Drive                                              | TPS              | 3929   | 3910   |
| Keyboard (basic PC104)                                             | TPS              | 3459   | 3440   |
| ATX 24-Pin Extension 12 inch                                       | TPS              | 379    | 360    |
| ADSL2 Modem + Wireless G Router                                    | TPS              | 10179  | 10160  |
| Surge Protector (8-Outlet)                                         | TPS              | 2579   | 2560   |
| Black 16X DVD-ROM                                                  | TPS              | 2529   | 2510   |
| ADSL Modem + Wireless N Router                                     | TPS              | 8989   | 8970   |
| UPS 800 Watt                                                       | TPS              | 13799  | 13780  |
| 22X DVD RW SATA Drive                                              | TPS              | 3969   | 3950   |
| 1/4 in. Standard Phone Male to 3.5mm Mini Female Adapter           | TPS              | 289    | 270    |
| MP3 Player (8 GB internal memory)                                  | Terrapin Sands   | 10169  | 7450   |
| MP3 Player (8 GB internal memory)                                  | Terrapin Sands   | 10349  | 7630   |
| MP3 Player (8 GB internal memory)                                  | Terrapin Sands   | 9739   | 7020   |
| MP3 Player (16 GB internal memory)                                 | Terrapin Sands   | 12539  | 9820   |
| Portable CD Player                                                 | Terrapin Sands   | 4159   | 1440   |
| Portable transistor radio (AM/FM)                                  | Terrapin Sands   | 2929   | 210    |
| Subwoofer (12 in.)                                                 | Terrapin Sands   | 17269  | 14550  |
| Multimedia Headset                                                 | Terrapin Sands   | 5519   | 2800   |
| Amplified Multimedia Speakers                                      | Terrapin Sands   | 8559   | 5840   |
| Amplified Multimedia Speakers                                      | Terrapin Sands   | 8839   | 6120   |
| Stereo USB Headset                                                 | Terrapin Sands   | 2719   | 0      |
| Wireless Microphone                                                | Terrapin Sands   | 13179  | 10460  |
| Network Hub (10 MBps, 4-port)                                      | Texi             | 1949   | 0      |
| Wireless G Access Point                                            | Texi             | 8599   | 6650   |
| Cable Modem                                                        | Texi             | 6829   | 4880   |
| Wireless G USB Adapter                                             | Texi             | 5609   | 3660   |
| ADSL2 Modem + Wireless G Router                                    | Texi             | 10059  | 8110   |
| DSL Modem                                                          | Texi             | 6779   | 4830   |
| Dial-up Modem (56K)                                                | Texi             | 4019   | 2070   |
| 32 GB USB Flash Drive                                              | Tortoise         | 5839   | 4580   |
| 32 GB USB Flash Drive (High Performance)                           | Tortoise         | 6929   | 5670   |
| 4 GB Micro SD                                                      | Tortoise         | 1259   | 0      |
| 2.0 TB SATA3 Disk (10K RPM)                                        | Tortoise         | 19319  | 18060  |
| 2.0 TB SATA3 Disk (10K RPM)                                        | Tortoise         | 19429  | 18170  |
| 2.0 TB SATA3 Disk                                                  | Tortoise         | 14289  | 13030  |
| 1.0 TB SATA3 Disk (10K RPM)                                        | Tortoise         | 15619  | 14360  |
| 64 GB SSD Disk                                                     | Tortoise         | 22109  | 20850  |
| Laser Printer (Black-and-White, Duplex)                            | Tyrell           | 69899  | 69850  |
| Keyboard (gamer PC104)                                             | Tyrell           | 8389   | 8340   |
| UPS 800 Watt                                                       | Tyrell           | 13119  | 13070  |
| UPS 500 Watt                                                       | Tyrell           | 8489   | 8440   |
| Inkjet Printer                                                     | Tyrell           | 18539  | 18490  |
| Inkjet Printer                                                     | Tyrell           | 17799  | 17750  |
| Wired mouse (two button)                                           | Tyrell           | 3269   | 3220   |
| Laser Printer (Color)                                              | Tyrell           | 39309  | 39260  |
| Auto Charger                                                       | Tyrell           | 4079   | 4030   |
| Laser Printer (Black-and-White, Duplex)                            | Tyrell           | 71149  | 71100  |
| 3.5mm Mini Male to RCA Phono Female Adapter                        | Tyrell           | 419    | 370    |
| Auto Charger                                                       | Tyrell           | 4329   | 4280   |
| Inkjet Printer                                                     | Tyrell           | 17689  | 17640  |
| Inkjet Printer                                                     | Tyrell           | 18579  | 18530  |
| Batteries (9V, 1 pack)                                             | Tyrell           | 609    | 560    |
| Batteries (9V, 1 pack)                                             | Tyrell           | 639    | 590    |
| Laser Printer (Black-and-White)                                    | Tyrell           | 39689  | 39640  |
| USB 3.0 4-Port Ultra Mini Hub                                      | Tyrell           | 4379   | 4330   |
| Laser Printer (Color)                                              | Tyrell           | 38059  | 38010  |
| Inkjet Printer                                                     | Tyrell           | 17999  | 17950  |
| Extension cord (60 in.)                                            | Tyrell           | 2609   | 2560   |
| Scanner                                                            | Tyrell           | 27549  | 27500  |
| Laser Printer (Black-and-White)                                    | Tyrell           | 37969  | 37920  |
| Inkjet Photo Printer                                               | Tyrell           | 24129  | 24080  |
| Inkjet Printer                                                     | Tyrell           | 18439  | 18390  |
| 3.5mm Mini Male to Standard Phone Female                           | Tyrell           | 289    | 240    |
| F Jack Male-to-Male Cable (12 in.)                                 | Tyrell           | 819    | 770    |
| F Jack Male-to-Male Cable (12 in.)                                 | Tyrell           | 969    | 920    |
| Sleeve for Mini Tablet - Gray                                      | Tyrell           | 1639   | 1590   |
| Phono Plug to F Jack                                               | Tyrell           | 609    | 560    |
| Sleeve for Mini Tablet - Blue                                      | Tyrell           | 1599   | 1550   |
| Sleeve for Mini Tablet - Blue                                      | Tyrell           | 1519   | 1470   |
| Phono Plug to UHF Jack                                             | Tyrell           | 659    | 610    |
| 3-Pin XLR Male to Male, Barrel Adapter                             | Tyrell           | 749    | 700    |
| Composite AV Cable (12 in.)                                        | Tyrell           | 2349   | 2300   |
| 36" Wide Screen TV                                                 | Tyrell           | 47669  | 47620  |
| Composite AV Cable (36 in.)                                        | Tyrell           | 2409   | 2360   |
| 48" Wide Screen TV                                                 | Tyrell           | 66759  | 66710  |
| Composite AV Cable (36 in.)                                        | Tyrell           | 2239   | 2190   |
| DVD Player                                                         | Tyrell           | 5909   | 5860   |
| USB Power Adapter (single)                                         | Tyrell           | 1249   | 1200   |
| Quad USB Power Adapter                                             | Tyrell           | 4069   | 4020   |
| Digital Video Recorder (500 GB)                                    | Tyrell           | 30079  | 30030  |
| 8GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x4GB)               | Tyrell           | 59     | 10     |
| 8GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x4GB)               | Tyrell           | 49     | 0      |
| 16GB DDR3-1600 (PC3-12800) Dual Channel Desktop Memory Kit (2x8GB) | Tyrell           | 149    | 100    |
| Phono Jack to BNC Plug                                             | Tyrell           | 529    | 480    |
| F Jack Male-to-Male Cable (60 in.)                                 | Tyrell           | 2489   | 2440   |
| Sleeve for Tablet - Gray                                           | Tyrell           | 2039   | 1990   |
| Sleeve for Mini Tablet - Gray                                      | Tyrell           | 1729   | 1680   |
| F Jack Male-to-Male Cable (24 in.)                                 | Tyrell           | 1519   | 1470   |
| Wired mouse (three button, with wheel)                             | Ultramegaco      | 4019   | 3990   |
| 19" Wide Screen LCD Monitor                                        | Ultramegaco      | 13359  | 13330  |
| Batteries (9V, 1 pack)                                             | Ultramegaco      | 629    | 600    |
| Rechargeable Batteries (AA, 4 pack)                                | Ultramegaco      | 2799   | 2770   |
| Full Tower Computer Case                                           | Ultramegaco      | 12879  | 12850  |
| Extension cord (36 in.)                                            | Ultramegaco      | 1759   | 1730   |
| Surge Protector (6-Outlet, angled plug)                            | Ultramegaco      | 2599   | 2570   |
| 3.5" Drive Bay Adapter                                             | Ultramegaco      | 1529   | 1500   |
| Wireless laser mouse (three button, with wheel)                    | Ultramegaco      | 4069   | 4040   |
| Keyboard (wireless, with trackball)                                | Ultramegaco      | 13689  | 13660  |
| Battery Charger                                                    | Ultramegaco      | 3249   | 3220   |
| Composite AV Cable (24 in.)                                        | Ultramegaco      | 2329   | 2300   |
| Black 8X BD-ROM 16X DVD-ROM                                        | Ultramegaco      | 8059   | 8030   |
| 21" LED Backlit Monitor                                            | Ultramegaco      | 24859  | 24830  |
| 5.25" Drive Bay Adapter                                            | Ultramegaco      | 1299   | 1270   |
| USB Card Reader                                                    | Ultramegaco      | 1949   | 1920   |
| 24" Wide Screen LCD Monitor                                        | Ultramegaco      | 22909  | 22880  |
| ATX Computer Case                                                  | Ultramegaco      | 6039   | 6010   |
| Screen Cleaning Cloth (gray)                                       | Ultramegaco      | 1839   | 1810   |
| 512MB PCI-X Graphics Card (OEM)                                    | Ultramegaco      | 7959   | 7930   |
| 16GB DDR3-1600 (PC3-12800) Dual Channel Desktop Memory Kit (2x8GB) | Ultramegaco      | 159    | 130    |
| 8GB DDR3-1600 (PC3-12800) Dual Channel Desktop Memory Kit (2x4GB)  | Ultramegaco      | 119    | 90     |
| F Jack Male-to-Male Cable (60 in.)                                 | Ultramegaco      | 2629   | 2600   |
| 1GB DDR2-667 (PC2-5300) Desktop Memory Kit (2x512MB)               | Ultramegaco      | 29     | 0      |
| F Jack Male-to-Male Cable (36 in.)                                 | Ultramegaco      | 1909   | 1880   |
| 1024MB PCI-X Graphics Card with HDMI output (OEM)                  | Ultramegaco      | 9939   | 9910   |
| Fireware 800 Card (PCI)                                            | Ultramegaco      | 7089   | 7060   |
| F Jack Male-to-Male Cable (36 in.)                                 | Ultramegaco      | 1949   | 1920   |
| Phono Plug to BNC Jack                                             | Ultramegaco      | 599    | 570    |
| 1/4 in. Standard Phone Male to 3.5mm Mini Female Adapter           | Ultramegaco      | 309    | 280    |
| 2x2GB DDR3 1600MHz Desktop RAM                                     | Ultramegaco      | 21179  | 21150  |
| Case for Notebook Computer                                         | Ultramegaco      | 4499   | 4470   |
| Case for Notebook Computer                                         | Ultramegaco      | 4869   | 4840   |
| RCA Phono Female to Female, Barrel Adapter                         | Ultramegaco      | 519    | 490    |
| 2x1GB Value DDR2 - 667 RAM                                         | Ultramegaco      | 13079  | 13050  |
| USB 2.0 4-Port Mini Hub                                            | Ultramegaco      | 2369   | 2340   |
| Extension cord (50 ft., outdoor)                                   | Ultramegaco      | 7329   | 7300   |
| Batteries (C, 2 pack)                                              | Ultramegaco      | 809    | 780    |
| High Speed HDMI Cable (60 in.)                                     | Ultramegaco      | 5049   | 5020   |
| 500 GB SATA3 Disk                                                  | United Digistuff | 8579   | 7750   |
| 64 GB SSD Disk                                                     | United Digistuff | 22129  | 21300  |
| 750 GB SATA3 Disk                                                  | United Digistuff | 10469  | 9640   |
| 173 GB SAS Disk                                                    | United Digistuff | 47419  | 46590  |
| 173 GB SAS Disk                                                    | United Digistuff | 49459  | 48630  |
| 1.0 TB SATA3 Disk (10K RPM)                                        | United Digistuff | 15219  | 14390  |
| USB 3.0 Card Reader                                                | United Digistuff | 829    | 0      |
| 1.5 TB SATA3 Disk                                                  | United Digistuff | 12619  | 11790  |
| 1.5 TB SATA3 Disk                                                  | United Digistuff | 12369  | 11540  |
| 60 GB PATA Disk                                                    | United Digistuff | 8719   | 7890   |
| 8 GB USB Flash Drive                                               | United Digistuff | 2799   | 1970   |
| 8 GB USB Flash Drive                                               | United Digistuff | 2939   | 2110   |
| Power Supply (700W)                                                | United Digistuff | 2969   | 2140   |
| ITX Computer Case                                                  | United Digistuff | 3949   | 3120   |
| Tablet PC (10 in. display, 64 GB)                                  | United Digistuff | 49579  | 48750  |
| SCSI Expansion Card (PCI)                                          | United Digistuff | 7939   | 7110   |
| SCSI Expansion Card (PCI)                                          | United Digistuff | 7889   | 7060   |
| ATX Computer Case                                                  | United Digistuff | 6049   | 5220   |
| Server (2U rackmount, eight-core, 64GB, 12TB)                      | United Digistuff | 597049 | 596220 |
| Soundcard (Basic)                                                  | United Digistuff | 1289   | 460    |
| Tablet PC (7 in. display, 16 GB, WiFi-only)                        | United Digistuff | 32589  | 31760  |
| Soundcard (Basic)                                                  | United Digistuff | 1319   | 490    |
| 32 GB Micro SD                                                     | United Digistuff | 5989   | 5160   |
| Black 16X DVD-ROM                                                  | United Digistuff | 2509   | 1680   |
| 16MB VESA Graphics Card                                            | United Digistuff | 1969   | 1140   |
| 16 GB Micro SD                                                     | United Digistuff | 4479   | 3650   |
| 3.4GHz CPU                                                         | United Digistuff | 17919  | 17090  |
| Virus Scanner (incl. updates for three years)                      | Weebits          | 4629   | 0      |
| Subwoofer (15 in.)                                                 | Weisenheimer     | 19059  | 16990  |
| Stereo Component CD Player                                         | Weisenheimer     | 19179  | 17110  |
| Subwoofer (12 in.)                                                 | Weisenheimer     | 16469  | 14400  |
| 36" Wide Screen TV                                                 | Weisenheimer     | 42289  | 40220  |
| Amplified Multimedia Speakers                                      | Weisenheimer     | 8799   | 6730   |
| Wireless Microphone                                                | Weisenheimer     | 12839  | 10770  |
| Stereo Component Streaming Media Player                            | Weisenheimer     | 17969  | 15900  |
| Portable transistor radio (AM-only)                                | Weisenheimer     | 2069   | 0      |
| MP3 Player (4 GB internal memory)                                  | Weisenheimer     | 8039   | 5970   |
| MP3 Player (16 GB internal memory)                                 | Weisenheimer     | 13429  | 11360  |
| Gaming Console                                                     | Weisenheimer     | 35559  | 33490  |
| MP3 Player (16 GB internal memory)                                 | Weisenheimer     | 12859  | 10790  |
| 61" Wide Screen TV                                                 | Wernham          | 128579 | 124380 |
| Blu-Ray Player                                                     | Wernham          | 13299  | 9100   |
| MP3 Player (64 GB internal memory)                                 | Wernham          | 26819  | 22620  |
| Video Cassette Recorder                                            | Wernham          | 12959  | 8760   |
| Stereo PC Headset                                                  | Wernham          | 4199   | 0      |
| 3D Digital TV Glasses                                              | Wernham          | 8369   | 4170   |
+--------------------------------------------------------------------+------------------+--------+--------+

5. Which product is the least expensive for each brand? 
SELECT name, brand, price, diff
  FROM (SELECT name, brand, price, price - min(price) OVER (PARTITION by brand ORDER BY price) as diff
        FROM analyst.products) sq
WHERE diff = 0;
+--------------------------------------------------------------------+------------------+-------+------+
| name                                                               | brand            | price | diff |
+--------------------------------------------------------------------+------------------+-------+------+
| 3.5mm Mini Male to Standard Phone Female                           | ACME             | 279   | 0    |
| Camcorder (VHS)                                                    | ARCAM            | 30999 | 0    |
| 22" Wide Screen LCD Monitor                                        | Argo             | 17719 | 0    |
| Blu-Ray Player                                                     | Artie            | 13269 | 0    |
| ATX 24-Pin Extension 12 inch                                       | BDT              | 389   | 0    |
| 3.5mm Mini Female to Female, Barrel Adapter                        | Bargain Barn     | 279   | 0    |
| 8GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x4GB)               | Bigdeal          | 59    | 0    |
| 4 GB USB Flash Drive                                               | Bitbucket        | 2029  | 0    |
| Basic Desktop                                                      | Bitmonkey        | 43459 | 0    |
| Fan Adapter Cable - 22 inch                                        | BuckLogix        | 499   | 0    |
| Office Suite (Home Edition)                                        | Bytefortress     | 23259 | 0    |
| 4 GB Micro SD                                                      | Byteweasel       | 1279  | 0    |
| Portable CD Player                                                 | Chatter Audio    | 4099  | 0    |
| 8GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x4GB)               | Chestnut         | 49    | 0    |
| 4GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x2GB)               | DevNull          | 39    | 0    |
| 4 GB USB Flash Drive                                               | Dorx             | 1929  | 0    |
| 8GB DDR3-1600 (PC3-12800) Dual Channel Desktop Memory Kit (2x4GB)  | Dualcore         | 119   | 0    |
| 1GB DDR2-667 (PC2-5300) Desktop Memory Kit (2x512MB)               | Duff             | 19    | 0    |
| Hard Disk Anti-Vibration Screws                                    | Electrosaurus    | 409   | 0    |
| Network Hub (10 MBps, 8-port)                                      | Foocorp          | 2989  | 0    |
| 1/4 in. Standard Phone Female to Female Adapter                    | Gigabux          | 399   | 0    |
| 1/4 in. Standard Phone Female to Female Adapter                    | Gigabux          | 399   | 0    |
| 4 GB USB Flash Drive                                               | Homertech        | 1949  | 0    |
| 4 GB Micro SD                                                      | Krustybitz       | 1329  | 0    |
| Basic Headphone                                                    | Lemmon           | 1939  | 0    |
| 16GB DDR3-1600 (PC3-12800) Dual Channel Desktop Memory Kit (2x8GB) | McDowell         | 169   | 0    |
| 1GB DDR2-667 (PC2-5300) Desktop Memory Kit (2x512MB)               | Megachango       | 29    | 0    |
| 20" Wide Screen LCD Monitor                                        | Olde-Gray        | 14909 | 0    |
| 16GB DDR3-1600 (PC3-12800) Dual Channel Desktop Memory Kit (2x8GB) | Orion            | 169   | 0    |
| Fan Adapter Cable - 10 inch                                        | Overtop          | 309   | 0    |
| Fan Adapter Cable - 16 inch                                        | Sparky           | 409   | 0    |
| 1 GB Compact Flash                                                 | Spindown         | 1199  | 0    |
| Video Cassette Recorder (Betamax format)                           | Sprite           | 12549 | 0    |
| Mid Tower Computer Case                                            | SuperGamer       | 8029  | 0    |
| 1GB DDR2-667 (PC2-5300) Desktop Memory Kit (2x512MB)               | TPS              | 19    | 0    |
| Stereo USB Headset                                                 | Terrapin Sands   | 2719  | 0    |
| Network Hub (10 MBps, 4-port)                                      | Texi             | 1949  | 0    |
| 4 GB Micro SD                                                      | Tortoise         | 1259  | 0    |
| 8GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x4GB)               | Tyrell           | 49    | 0    |
| 1GB DDR2-667 (PC2-5300) Desktop Memory Kit (2x512MB)               | Ultramegaco      | 29    | 0    |
| USB 3.0 Card Reader                                                | United Digistuff | 829   | 0    |
| Virus Scanner (incl. updates for three years)                      | Weebits          | 4629  | 0    |
| Portable transistor radio (AM-only)                                | Weisenheimer     | 2069  | 0    |
| Stereo PC Headset                                                  | Wernham          | 4199  | 0    |
| DVD Player                                                         | Whiteacre        | 5899  | 0    |
| 8 GB Micro SD                                                      | Wolfpack         | 2819  | 0    |
| 8GB DDR3-1333 (PC3-10600) Desktop Memory Kit (2x4GB)               | XYZ              | 59    | 0    |
| Basic Desktop                                                      | Yoyodyne         | 44119 | 0    |
+--------------------------------------------------------------------+------------------+-------+------+

Sliding Windows


--OJO - revisar la teoría del capítulo 10

Complex Values

Complex Values - Hive
---------------------
DROP TABLE IF EXISTS complex.customers;
CREATE TABLE IF NOT EXISTS complex.customers(
       cust_id STRING,
       name STRING
)
;

INSERT INTO complex.customers VALUES
('a', 'Alice'),
('b', 'Bob'),
('c', 'Carlos')
;

HIVE and IMPALA ven el mismo METASTORE. 
EXISTEN HASTA 8 NIVELES, para separar los tipos de datos

Nivel 1 - ^A por defecto cuando no ponemos FIELDS TERMINATED BY
Nivel 2 - ^B por defecto cuando no ponemos la COLECTION ITEMS TERMINATED BY
Nivel 3 - ^C por defecto cuando no ponemos la MAP KEYS TERMINATED BY
Nivel 4 - ^D No se puede modificar
Nivel 5 - ^E No se puede modificar
Nivel 6 - ^F No se puede modificar
Nivel 7 - ^G No se puede modificar
Nivel 8 - ^H No se puede modificar

COMPLEX DATA TYPE:

- ARRAY   -       ARRAY<STRING>   -       COLLECTION ITEMS TERMINATED BY '|'

- MAP     -       

MAP<KEY-TYPE,VALUE-TYPE>        - Tipo de datos de la CLAVE-Tipo de datos del VALOR       
MAP<STRING,STRING>      -       MAP KEYS TERMINATED BY ':';
MAP<STRING, ARRAY<STRING>>      -       

- STRUCT  -


ARRAY   -       ARRAY<STRING>   -       COLLECTION ITEMS TERMINATED BY '|'
-- Creación de la tabla con datos complejos - necesito un separador de campo para los datos que estan dentro del ARRAY

DROP TABLE IF EXISTS complex.customers_phones;
CREATE TABLE IF NOT EXISTS complex.customers_phones (
       cust_id STRING,
       name STRING,
       phones ARRAY<STRING> )
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '|';

describe complex.customers_phones;
+-----------+----------------+----------+--+
| col_name  |   data_type    | comment  |
+-----------+----------------+----------+--+
| cust_id   | string         |          |
| name      | string         |          |
| phones    | array<string>  |          |
+-----------+----------------+----------+--+

datafile
a,Alice,555-1111|555-2222|555-3333
b,Bob,555-4444
c,Carlos,555-5555|555-6666

hdfs dfs -put datafile /user/hive/warehouse/complex.db/customers_phones
hdfs dfs -ls /user/hive/warehouse/complex.db/customers_phones
-rw-r--r--   1 cloudera supergroup         77 2021-04-09 06:23 /user/hive/warehouse/

SELECT *
  FROM complex.customers_phones;

+---------------------------+------------------------+-------------------------------------+--+
| customers_phones.cust_id  | customers_phones.name  |       customers_phones.phones       |
+---------------------------+------------------------+-------------------------------------+--+
| a                         | Alice                  | ["555-1111","555-2222","555-3333"]  |
| b                         | Bob                    | ["555-4444"]                        |
| c                         | Carlos                 | ["555-5555","555-6666"]             |
+---------------------------+------------------------+-------------------------------------+-

HIVE - Para trabajar Datos Complejos es mejor usar HIVE para casí todo.

SELECT phones 
  FROM complex.customers_phones;
+-------------------------------------+--+
|               phones                |
+-------------------------------------+--+
| ["555-1111","555-2222","555-3333"]  |
| ["555-4444"]                        |
| ["555-5555","555-6666"]             |
+-------------------------------------+--+

-- size - numero de elementos que hay en un campo complejo.
SELECT size(phones) 
  FROM complex.customers_phones;
+------+--+
| _c0  |
+------+--+
| 3    |
| 1    |
| 2    |
+------+--+

SELECT cust_id, name, size (phones) 
  FROM complex.customers_phones;
+----------+---------+------+--+
| cust_id  |  name   | _c2  |
+----------+---------+------+--+
| a        | Alice   | 3    |
| b        | Bob     | 1    |
| c        | Carlos  | 2    |
+----------+---------+------+--+

-- explode - en el SELECT solo se puede mostrar esta columna para ver todas las columnas se debe usar LATERAL VIEW
SELECT explode(phones) 
  FROM complex.customers_phones;
+-----------+--+
|    col    |
+-----------+--+
| 555-1111  |
| 555-2222  |
| 555-3333  |
| 555-4444  |
| 555-5555  |
| 555-6666  |
+-----------+--+

-- Este SELECT da Error, se debe usar LATERAL VIEW
SELECT cust_id, name, explode (phones) 
  FROM complex.customers_phones;
Error: Error while compiling statement: 
FAILED: SemanticException [Error 10081]: UDTF's are not supported outside the SELECT clause, nor nested in expressions (state=42000,code=10081)

-- LATERAL VIEW - coje el campo complejo array y desnormaliza los datos, es decir que "desdobla-desnormalización" los datos
SELECT cust_id, name, telefonos 
  FROM complex.customers_phones 
       LATERAL VIEW explode (phones) p AS telefonos
;
+----------+---------+------------+--+
| cust_id  |  name   | telefonos  |
+----------+---------+------------+--+
| a        | Alice   | 555-1111   |
| a        | Alice   | 555-2222   |
| a        | Alice   | 555-3333   |
| b        | Bob     | 555-4444   |
| c        | Carlos  | 555-5555   |
| c        | Carlos  | 555-6666   |
+----------+---------+------------+--+

SELECT * 
  FROM complex.customers_phones 
       LATERAL VIEW explode(phones) myTable AS myCol
+---------------------------+------------------------+-------------------------------------+----------------+--+
| customers_phones.cust_id  | customers_phones.name  |       customers_phones.phones       | mytable.mycol  |
+---------------------------+------------------------+-------------------------------------+----------------+--+
| a                         | Alice                  | ["555-1111","555-2222","555-3333"]  | 555-1111       |
| a                         | Alice                  | ["555-1111","555-2222","555-3333"]  | 555-2222       |
| a                         | Alice                  | ["555-1111","555-2222","555-3333"]  | 555-3333       |
| b                         | Bob                    | ["555-4444"]                        | 555-4444       |
| c                         | Carlos                 | ["555-5555","555-6666"]             | 555-5555       |
| c                         | Carlos                 | ["555-5555","555-6666"]             | 555-6666       |
+---------------------------+------------------------+-------------------------------------+----------------+--+


MAP<KEY-TYPE,VALUE-TYPE>        - Tipo de datos de la CLAVE-Tipo de datos del VALOR       

MAP<STRING,STRING>              - MAP KEYS TERMINATED BY ':';
-- Creación de la tabla con datos complejos - necesito un separador de campo para los datos que estan dentro del MAP

DROP TABLE IF EXISTS complex.customers_mapphones;
CREATE EXTERNAL TABLE IF NOT EXISTS complex.customers_mapphones (
       cust_id STRING,
       name STRING,
       phones MAP<STRING, STRING> )
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '|'
MAP KEYS TERMINATED BY ':';

describe complex.customers_mapphones;
+-----------+---------------------+----------+--+
| col_name  |      data_type      | comment  |
+-----------+---------------------+----------+--+
| cust_id   | string              |          |
| name      | string              |          |
| phones    | map<string,string>  |          |
+-----------+---------------------+----------+--+

datafile

a,Alice,home:555-1111|work:555-2222|mobile:555-3333
b,Bob,mobile:555-4444
c,Carlos,work:555-5555|home:555-6666

hdfs dfs -put datafile2 /user/hive/warehouse/complex.db/customers_mapphones
hdfs dfs -ls /user/hive/warehouse/complex.db/customers_mapphones
-rw-r--r--   1 cloudera supergroup        111 2021-04-09 07:23 /user/hive/warehouse/complex.db/customers_mapphones/datafile2

SELECT * 
  FROM complex.customers_mapphones;
+------------------------------+---------------------------+----------------------------------------------------+--+
| customers_mapphones.cust_id  | customers_mapphones.name  |             customers_mapphones.phones             |
+------------------------------+---------------------------+----------------------------------------------------+--+
| a                            | Alice                     | {"home":"555-1111","work":"555-2222","mobile":"555-3333"} |
| b                            | Bob                       | {"mobile":"555-4444"}                              |
| c                            | Carlos                    | {"work":"555-5555","home":"555-6666"}              |
+------------------------------+---------------------------+----------------------------------------------------+--+

SELECT phones 
  FROM complex.customers_mapphones;
+----------------------------------------------------+--+
|                       phones                       |
+----------------------------------------------------+--+
| {"home":"555-1111","work":"555-2222","mobile":"555-3333"} |
| {"mobile":"555-4444"}                              |
| {"work":"555-5555","home":"555-6666"}              |
+----------------------------------------------------+--+

SELECT size(phones) 
  FROM complex.customers_mapphones;
+------+--+
| _c0  |
+------+--+
| 3    |
| 1    |
| 2    |
+------+--+


SELECT cust_id, name, size(phones)
  FROM complex.customers_mapphones;

+----------+---------+------+--+
| cust_id  |  name   | _c2  |
+----------+---------+------+--+
| a        | Alice   | 3    |
| b        | Bob     | 1    |
| c        | Carlos  | 2    |
+----------+---------+------+--+

SELECT explode(phones) 
  FROM complex.customers_mapphones;
+---------+-----------+--+
|   key   |   value   |
+---------+-----------+--+
| home    | 555-1111  |
| work    | 555-2222  |
| mobile  | 555-3333  |
| mobile  | 555-4444  |
| work    | 555-5555  |
| home    | 555-6666  |
+---------+-----------+--+

SELECT cust_id, name, explode(phones) 
  FROM complex.customers_mapphones;
Error: Error while compiling statement: FAILED: SemanticException [Error 10081]: UDTF's are not supported outside the SELECT clause, nor nested in expressions (state=42000,code=10081)

SELECT cust_id, name, telefonos
  FROM complex.customers_mapphones
       LATERAL VIEW explode (phones) p AS telefonos;


SELECT * 
FROM complex.customers_mapphones 
     LATERAL VIEW explode(phones) myTable AS myCol, myCol2;
+------------------------------+---------------------------+----------------------------------------------------+----------------+-----------------+--+
| customers_mapphones.cust_id  | customers_mapphones.name  |             customers_mapphones.phones             | mytable.mycol  | mytable.mycol2  |
+------------------------------+---------------------------+----------------------------------------------------+----------------+-----------------+--+
| a                            | Alice                     | {"home":"555-1111","work":"555-2222","mobile":"555-3333"} | home           | 555-1111        |
| a                            | Alice                     | {"home":"555-1111","work":"555-2222","mobile":"555-3333"} | work           | 555-2222        |
| a                            | Alice                     | {"home":"555-1111","work":"555-2222","mobile":"555-3333"} | mobile         | 555-3333        |
| b                            | Bob                       | {"mobile":"555-4444"}                              | mobile         | 555-4444        |
| c                            | Carlos                    | {"work":"555-5555","home":"555-6666"}              | work           | 555-5555        |
| c                            | Carlos                    | {"work":"555-5555","home":"555-6666"}              | home           | 555-6666        |
+------------------------------+---------------------------+----------------------------------------------------+----------------+-----------------+--+


- OJO - pendiente de hacer esta combinación
MAP<STRING, ARRAY<STRING>>      - MAP KEYS TERMINATED BY ':';
-- Creación de la tabla con datos complejos - necesito un separador de campo para los datos que estan dentro del MAP

DROP TABLE IF EXISTS complex.customers_mapphones;
CREATE EXTERNAL TABLE IF NOT EXISTS complex.customers_mapphones (
       cust_id STRING,
       name STRING,
       phones MAP<STRING, <ARRAY>> )
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '|'
MAP KEYS TERMINATED BY ':';

datafile
???
a,Alice,home:555-1111|555-2222|555-3333
b,Bob,1600 Pennsylvania Ave NW|Washington|DC|20500,home:555-4444
c,Carlos,342 Gravelpit Terrace|Bedrock,home:555-5555|555-6666

STRUCT
-- Creación de la tabla con datos complejos - necesito un separador de campo para los datos que estan dentro del STRUCT

DROP TABLE IF EXISTS complex.customers_structaddres;
CREATE EXTERNAL TABLE IF NOT EXISTS complex.customers_structaddres(
       cust_id STRING,
       name STRING,
       addres STRUCT<street:STRING,
                     city:STRING,
                     state:STRING,
                     zipcode:STRING> )
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '|';

describe customers_structaddres;
+-----------+----------------------------------------------------+----------+--+
| col_name  |                     data_type                      | comment  |
+-----------+----------------------------------------------------+----------+--+
| cust_id   | string                                             |          |
| name      | string                                             |          |
| addres    | struct<street:string,city:string,state:string,zipcode:string> |          |
+-----------+----------------------------------------------------+----------+--+

datafile

a,Alice,742 Evergreen Terrace|Springfield|OR|97477
b,Bob,1600 Pennsylvania Ave NW|Washington|DC|20500
c,Carlos,342 Gravelpit Terrace|Bedrock

hdfs dfs -put datafile /user/hive/warehouse/complex.db/customers_structaddres
hdfs dfs -ls /user/hive/warehouse/complex.db/customers_structaddres

SELECT * 
  FROM customers_structaddres;
+---------------------------------+------------------------------+----------------------------------------------------+--+
| customers_structaddres.cust_id  | customers_structaddres.name  |           customers_structaddres.addres            |
+---------------------------------+------------------------------+----------------------------------------------------+--+
| a                               | Alice                        | {"street":"742 Evergreen Terrace","city":"Springfield","state":"OR","zipcode":"97477"} |
| b                               | Bob                          | {"street":"1600 Pennsylvania Ave NW","city":"Washington","state":"DC","zipcode":"20500"} |
| c                               | Carlos                       | {"street":"342 Gravelpit Terrace","city":"Bedrock","state":null,"zipcode":null} |
+---------------------------------+------------------------------+----------------------------------------------------+--+

SELECT addres
  FROM customers_structaddres;
+----------------------------------------------------+--+
|                       addres                       |
+----------------------------------------------------+--+
| {"street":"742 Evergreen Terrace","city":"Springfield","state":"OR","zipcode":"97477"} |
| {"street":"1600 Pennsylvania Ave NW","city":"Washington","state":"DC","zipcode":"20500"} |
| {"street":"342 Gravelpit Terrace","city":"Bedrock","state":null,"zipcode":null} |
+----------------------------------------------------+--+

SELECT cust_id, name, addres 
  FROM complex.customers_structaddres;
+----------+---------+----------------------------------------------------+--+
| cust_id  |  name   |                       addres                       |
+----------+---------+----------------------------------------------------+--+
| a        | Alice   | {"street":"742 Evergreen Terrace","city":"Springfield","state":"OR","zipcode":"97477"} |
| b        | Bob     | {"street":"1600 Pennsylvania Ave NW","city":"Washington","state":"DC","zipcode":"20500"} |
| c        | Carlos  | {"street":"342 Gravelpit Terrace","city":"Bedrock","state":null,"zipcode":null} |
+----------+---------+----------------------------------------------------+--+

SELECT cust_id, name, addres.street
  FROM complex.customers_structaddres;
+----------+---------+---------------------------+--+
| cust_id  |  name   |          street           |
+----------+---------+---------------------------+--+
| a        | Alice   | 742 Evergreen Terrace     |
| b        | Bob     | 1600 Pennsylvania Ave NW  |
| c        | Carlos  | 342 Gravelpit Terrace     |
+----------+---------+---------------------------+--+
SELECT cust_id, name, addres.street, addres.city, addres.state, addres.zipcode
  FROM complex.customers_structaddres;
+----------+---------+---------------------------+--------------+--------+----------+--+
| cust_id  |  name   |          street           |     city     | state  | zipcode  |
+----------+---------+---------------------------+--------------+--------+----------+--+
| a        | Alice   | 742 Evergreen Terrace     | Springfield  | OR     | 97477    |
| b        | Bob     | 1600 Pennsylvania Ave NW  | Washington   | DC     | 20500    |
| c        | Carlos  | 342 Gravelpit Terrace     | Bedrock      | NULL   | NULL     |
+----------+---------+---------------------------+--------------+--------+----------+--+

Complex Values - Impala
-----------------------
Impala lets you reference an ARRAY column as if it were a separate table
─ With one row for each ARRAY element
─ Containing the pseudocolumns item and pos

-- Create in Hive
DROP TABLE IF EXIST complex.cust_phones_parquet;
CREATE TABLE IF NOT EXISTS complex.cust_phones_parquet
  STORED AS PARQUET
AS
  SELECT *
    FROM complex.customer_phones;


--OJO - revisar - Create in Hive
DROP TABLE IF EXIST complex.cust_phones_parquet;
CREATE TABLE IF NOT EXISTS complex.cust_phones_parquet
  STORED AS PARQUET
;

INSERT INTO TABLE complex.cust_phones_parquet
SELECT *
  FROM complex.customers_phones
;

describe complex.cust_phones_parquet;

SELECT item, pos
  FROM cust_phones_parquet.phones
;


Use implicit join notation to join ARRAY elements with rows of the table
─ Returns ARRAY elements with scalar column values from same rows
SELECT name, phones.item AS phone
  FROM complex.cust_phones_parquet, 
       complex.cust_phones_parquet.phones;

You can reference a MAP column in Impala as if it were a separate table
─ With one row for each MAP element
─ Containing the pseudocolumns key and value

SELECT key, value
  FROM cust_phones_parquet.phones;

Use implicit join notation to join MAP elements with rows of the table
─ Can use key and value in the SELECT list and WHERE clause

SELECT name, phones.value AS home
  FROM cust_phones_parquet, 
       cust_phones_parquet.phones
WHERE phones.key = 'home';

Querying STRUCT Columns with Impala
The Impala query syntax for STRUCT columns is the same as with Hive

SELECT name, address.state, address.zipcode
  FROM cust_addr_parquet;


You can issue SELECT * queries on tables with complex columns
─ But Impala omits the complex columns from the results
SELECT * 
  FROM cust_phones_parquet;

Returning the Number of Items in a Collection with Impala
Impala does not support the size function or other collection functions
─ Use COUNT and GROUP BY to count the items in an ARRAY or MAP

SELECT name, COUNT(*) AS num
  FROM cust_phones_parquet, 
       cust_phones_parquet.phones
GROUP BY cust_id, name;

Loading Data Containing Complex Types
▪ Impala supports querying complex types only in Parquet tables
▪ Impala cannot INSERT data containing complex types
▪ Workaround: Use Hive to INSERT data into Parquet tables

CREATE TABLE in Hive or Impala, then INSERT in Hive

INSERT INTO TABLE cust_phones_parquet
SELECT * 
  FROM customers_phones;

Or use a CREATE TABLE AS SELECT (CTAS) statement in Hive

CREATE TABLE cust_phones_parquet STORED AS PARQUET
AS
SELECT * 
  FROM customers_phones;

Denormalizing Tables with a One-to-Many Relationship
▪ Denormalizing tables allows you to query data without the need to use joins
▪ To denormalize tables with a one-to-many relationship, use ARRAY<STRUCT<>>
▪ One row of the new table can include all associated rows from another table

CREATE EXTERNAL TABLE rated_products (
prod_id INT,
brand STRING,
name STRING,
price INT,
ratings ARRAY<STRUCT <rating:TINYINT,
message:STRING> >)
STORED AS PARQUET;

Populating a Denormalized Table
▪ This step must be performed in Hive
▪ Use named_struct() to cast a row of the detail table into the proper structure
▪ Use collect_list() to collect multiple rows into an array

INSERT OVERWRITE TABLE rated_products
SELECT p.prod_id, p.brand, p.name, p.price, collect_list(named_struct('rating', r.rating, 'message', r.message))
  FROM products p LEFT OUTER JOIN ratings r ON (p.prod_id = r.prod_id)
GROUP BY p.prod_id, p.brand, p.name, p.price;

Querying the Denormalized Table
▪ You can query the table efficiently in Impala
▪ Example: Show the 30 highest-rated products with number of ratings and average rating. 
The nvl function in the ORDER BY clause returns the first argument (average_rating) if it is not NULL and the second argument (0) if the first is NULL

SELECT brand, name, COUNT(ratings.rating) num_ratings, AVG(ratings.rating) average_rating
  FROM rated_products, rated_products.ratings
GROUP BY brand, name
ORDER BY nvl(average_rating, 0) DESC
LIMIT 30;

--OJO - revisar la teoría del capítulo 11

Regular Expression - The REGEXP operator performs regular expression (regex) matching
─ Returns true if the text matches the pattern
SELECT *
  FROM analyst.products
 WHERE brand LIKE '%Dual%'
;

Dualcore Contains the literal string Dualcore

-- Starts with - ^ busca todo lo que empieza con '^Dual'
SELECT * 
  FROM analyst.products 
 WHERE brand REGEXP '^Dual'
;

-- Ends with core - $ - busca todo lo que termina con 'core$';
SELECT *
  FROM analyst.products
 WHERE brand REGEXP 'core$';
;

-- Is the literal string Dualcore with no other caracters - busca todo lo que empieza... por y termina por... '^Dualcore$'
SELECT *
  FROM analyst.products
 WHERE brand REGEXP '^Dualcore$'
;

-- Is Dual followed by zero or more other characters - '^Dual.*$'
busca todo lo que empieza por Dual
. cualquier caracter
.* cualquier caracter 0 ó más veces
.*$ termine por cualquier cantidad de caracteres

SELECT *
  FROM analyst.products
 WHERE brand REGEXP '^Dual.*$'
;

-- Is one or more uppercase or lowercase letters - '^[A-Za-z]+$'
^[A-Za-z] - cualquier caracter dentro de los corchetes,  
+ funciona como el asterisco pero una o más veces que los anteriores, a...sal
+$ - una o más "a"...sal

SELECT *
  FROM analyst.products
 WHERE brand REGEXP '^[A-Za-z]+$'
;

-- Busca dentro del campo todo lo que tenga ua ó au -> Dualcore y Electrosaurus
SELECT *
  FROM analyst.products
 WHERE brand REGEXP '[ua][au]'
;

-- Is exactly eight word characters ([0-9A-Za-z_]) - '^\\w{8}$'
busca que tiene que haber 8 caracteres
w significa cualquier letra ([0-9A-Za-z_])
SELECT *
  FROM analyst.products
 WHERE brand REGEXP '^\\w{8}$'
;

-- Is between five and nine word characters (inclusive)
SELECT *
  FROM analyst.products
 WHERE brand REGEXP '^\\w{5,9}$'
;

-- matching is case-sensitive in Hive and Impala -  case sensitive
SELECT * 
  FROM analyst.products
 WHERE lower(brand) REGEXP '^dual'
;

-- Impala Only - operator IREGEXP in CDH 5.7 and higher
SELECT * 
  FROM analyst.products
 WHERE brand IREGEXP '^dual'
;

? -> simbolo del fin
* -> cualquier caracter que se repita 0 ó más veces 
. es cualquier caracter
.* es de 0 a n caracteres
.*$ es termine por 0 a n 
X -> o ó más veces
+ -> tiene que haber almenos un caracter (uno o más de los anteriores)
^[A-Za-z]+$ - cualquier caracter dentro de los corchetes,  
^A+$ - una o más a...sal
[ua][au] -> Dualcore y Electrosaurus
'^[^D]' -> Cualquier caracter que no sea D, los que no empiezan por D
^[^DUXTWOELGSKMAC]' 
^\\w{8}$ -> '^\\w{1}'
w -> cualquier letra ([0-9A-Za-z_])
{8} -> 8 caracteres iguales al anterior
^\\w{8}$ -> '^\\w{1}'
W -> cualquier que no sea esto ([0-9A-Za-z_])
s -> cualquier espacio, blanco o tabulación
S -> lo contrario a la s minúscula
05/23/2016 19:48:37 312-555-7834 COMPLAINT "Item damaged" 
"([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) \"([^\"]*)\""; 
([^ ]*) -> Lo que sea diferente de espacios y un espcio(*)
\"([^\"]*)\" -> Lo que este entre comillas
[.] --> entre corchetes un punto significa un punto
Each pair of parentheses denotes a field 
Si esto fuera un TIMESTAMP por ejemplo: 05/23/2016 19:48:37 ([^ ]* [^ ]*)

-- Extracting and Replacing Text - Hive and Impala include basic functions for extracting or replacing text
Including substring and translate

DROP TABLE IF EXISTS test.message;
CREATE TABLE IF NOT EXISTS test.message(
       txt STRING);
INSERT INTO test.message VALUES
("It\'s on Oak St\. or Maple St in 90210");
SELECT txt
  FROM test.message;

+------------------------------------+
|         1         2         3      |
|123456789012345678901234567890123456|
|                                    |
|It's on Oak St. or Maple St in 90210|
+------------------------------------+

-- substring
SELECT substring(txt, 32, 5) 
  FROM test.message
;
+-----------------------+
| substring(txt, 32, 5) |
+-----------------------+
| 90210                 |
+-----------------------+

-- translate
SELECT translate(txt, '.', '') 
  FROM test.message
;
+-------------------------------------+
| translate(txt, '.', '')             |
+-------------------------------------+
| It's on Oak St or Maple St in 90210 |
+-------------------------------------+

These are not flexible enough for working with free-form text fields
Instead use regular expressions

Regular Expression Extract and Replace Functions
Hive and Impala have functions for regular expression extract and replace
─ regexp_extract returns matched text
─ regexp_replace substitutes another value for matched text

SELECT regexp_extract(txt, '(\\d{5})', 1)
  FROM test.message
;
+------------------------------------+
| regexp_extract(txt, '(\\d{5})', 1) |
+------------------------------------+
| 90210                              |
+------------------------------------+

SELECT regexp_replace(txt, 'St\\.?\\s+', 'Street ')
  FROM test.message
;
+----------------------------------------------+
| regexp_replace(txt, 'st\\.?\\s+', 'street ') |
+----------------------------------------------+
| It's on Oak Street or Maple Street in 90210  |
+----------------------------------------------+

Single quotes in strings, like the one in It's, can cause problems. 
Use double quotes around the string; in Impala (before 2.12.0), also escape the single quote with a backslash: "It\'s"

Matched Portions
Regular expressions can also be used to extract or replace matched text
─ REGEXP operator returns true if the pattern is somewhere in the full text
─ When extracting or replacing, the exact portion being matched is important
Regular Expression String (matched portion in blue)
Dualcore I wish Dualcore had 2 stores in 90210.

DROP TABLE IF EXISTS test.message;
CREATE TABLE IF NOT EXISTS test.message(
       txt STRING);
INSERT INTO test.message VALUES
("Dualcore I wish Dualcore had 2 stores in 90210.");
SELECT txt
  FROM test.message;

+-------------------------------------------------+
| txt                                             |
+-------------------------------------------------+
| Dualcore I wish Dualcore had 2 stores in 90210. |
+-------------------------------------------------+

+-----------------------------------------------+
|         1         2         3         4       |
|1234567890123456789012345678901234567890123456 |
|                                               |
|Dualcore I wish Dualcore had 2 stores in 90210.|
+-----------------------------------------------+


'\\d'
\\d{5} 
\\d\\s\\w+ 
\\w{5,9} 
.?\\. 
.*\\. 
2[^ ] 



([^]*) ([^]*) (-|\\[^\\]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\".*\") ([^ \"]*|\".*\"))?"

--OJO - revisar la teoría del capítulo 




--OJO - revisar Me falta toda la teoría a partir del capítulo 12



HastaAQUI------------------------------ INDICA QUE A PARTIR DE AQUI esta mezclado Hive and Impala HastaAQUI


[cloudera@quickstart ~]$ hive
hive> show functions;

hive> describe function to_date;

hive> describe function extended to_date;
OK
to_date(expr) - Extracts the date part of the date or datetime expression expr
Example:
   > SELECT to_date('2009-07-30 04:17:52') FROM src LIMIT 1;
  '2009-07-30'
Time taken: 0.022 seconds, Fetched: 4 row(s)

hive> dfs -ls;

hive> CREATE TABLE page_view(viewTime INT, userid BIGINT,
    > page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User')
    > COMMENT 'This is the page view table'
    > PARTITIONED BY(dt STRING, country STRING)
    > STORED AS SEQUENCEFILE;

hive> CREATE TABLE page_view(viewTime INT, userid BIGINT,
    > page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User')
    > COMMENT 'This is the page view table'
    > PARTITIONED BY(dt STRING, country STRING)
    > ROW FORMAT DELIMITED
    > FIELDS TERMINATED BY '1'
    > STORED AS SEQUENCEFILE;


----------------------
---------------
Impala commands - ImpalaSQL + HiveQL + SQL-92 - NO es Tolerante a fallos
---------------
performing SQL querie(s)-(consulta(s)) on big data in distributed systems directly
Uses a very fast specialized SQL engine, not MapReduce or Spark
To save time during querie(s)-(consulta(s)), Impala does not poll constantly for metadata changes. 
So the first thing we must do is tell Impala that its metadata is out of date. 
Then we should see our tables show up, ready to be querie(s)-consulta(s)d:
allow you to create tables by defining a schema over existing files with 'CREATE EXTERNAL TABLE' statements.
store data and metadata separately:
─ Metadata (information about the table) is kept in the metastore
─ Data is kept in files in a storage system (like HDFS, Kudu, or S3)
differ from RDBMSs in
 RDBMS is Schema-on-write a la hora de escribir se comprueba que es correcto
Impala is Schema-on-read
Impala does not have support about: Transactions and updating individual records
─ Index support: none 
─ Latency: very low for RDBMSs, low for Impala
─ Storage: can handle much more at a low cost

Parse Impala SQL + HiveQL + SQL-92 
Make optimizations
Plan execution
Executes Impala SQL querie(s)-(consulta(s)) directly on the cluster
Metadata ─ Defined when table is created Specifies structure and location of the data Stored in the metastore(definición de las tablas), which is contained in an RDBMS
Por defecto - Impala store Data in HDFS Default path: /user/hive/warehouse/
each table's data is stored in a subdirectory named after the table

Almost always at least five times faster than either Hive or Pig
Impala executes querie(s)-(consulta(s)) directly on the distributed system 
Uses a very highly and fast specialized SQL engine optimized for querie(s)-(consulta(s)), not MapReduce or Spark
Does not have support for Transactions, Index support and updating individual records
Latency low
Can handle much more storage at a low cost

-O-J-O - Todo lo que no se hace a través de impala, impala no se entera; por eso hay que hacer invalidate metadata OJO NUNCA en producción
-O-J-O- connect to the default database by default - connecta a la db default por defecto

Cachea la metadata por eso mucho más rápida. (Invalidate Metadata) anula este cacheo y cuando vuelva a necesitar los datos "hace el refresco".

Para actualizar - update la Metadata Cache hay varias formas:

1. Refresh recarga la metadata para una tabla concreta inmediatamente, recarga los bloques de almacenamiento para los nuevos ficheros unicamente.
REFRESH default.customers;

2. Invalidate metadata - Marca la metadata como invalida y cuando los necesita los actualiza;
invalidate metadata default.customers;
Es importante que después del balanceo de HDFS (el responsable de este),incluya en un shell algo como:  impala-shell -q invalidate metadata

-O-J-O- INVALIDATE METADATA (SIN TABLA ESPECIFICA), afecta a todos los users-usuarios. ELIMINA LA CACHE.

-O-J-O- Prueba de como funciona refresh and invalidate

-- CREATE Crear una tabla temporal en hive, insertar datos, hacer un SELECT para comprobar
0: jdbc:hive2://localhost:10000> CREATE TABLE temporal (nombre STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
0: jdbc:hive2://localhost:10000> INSERT INTO temporal values ('Peter');
0: jdbc:hive2://localhost:10000> SELECT * from TEMPORAL;
+------------------+--+
| temporal.nombre  |
+------------------+--+
| Peter            |
+------------------+--+
-- Listar en HDFS
[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/temporal/
-rwxrwxrwx   1 cloudera supergroup          6 2021-03-19 07:58 /user/hive/warehouse/temporal/000000_0
[cloudera@quickstart ~]$ hdfs dfs -cat /user/hive/warehouse/temporal/000000_0
Peter

En impala:
-- Ejecutar el refresh default.temporal; da error porque la tabla no la tiene en la cache
quickstart.cloudera:21000] > refresh default.temporal;
Query: refresh default.temporal
Query submitted at: 2021-03-19 08:07:13 (Coordinator: http://quickstart.cloudera:25000)
ERROR: AnalysisException: Table does not exist: default.temporal

-- con show tables, vemos que no esta la tabla
[quickstart.cloudera:21000] > show tables;
Query: show tables
+---------------+
| name          |
+---------------+
| account       |
| branch        |
| business      |
| customer      |
| customers     |
| department    |
| employee      |
| employees     |
| individual    |
| officer       |
| order_details |
| orders        |
| products      |
| suppliers     |
+---------------+
Fetched 14 row(s) in 0.43s

-- Ejecuto el invalidate metadata default.temporal funciona, el primer query que se haga a la tabla ira lento por que en ese momento se esta actualizado la cache, 
[quickstart.cloudera:21000] > invalidate metadata temporal;
Query: invalidate metadata temporal
Query submitted at: 2021-03-19 08:08:07 (Coordinator: http://quickstart.cloudera:25000)
Query progress can be monitored at: http://quickstart.cloudera:25000/query_plan?query_id=5f472f09883d725a:21275f6600000000
Fetched 0 row(s) in 1.60s
[quickstart.cloudera:21000] > select * from temporal;
Query: select * from temporal
Query submitted at: 2021-03-19 08:08:19 (Coordinator: http://quickstart.cloudera:25000)
Query progress can be monitored at: http://quickstart.cloudera:25000/query_plan?query_id=c249a3383c6e003f:90bd54c900000000
+--------+
| nombre |
+--------+
| Peter  |
+--------+
Fetched 1 row(s) in 5.74s

-- la siguiente petición es inmediata.
[quickstart.cloudera:21000] > select * from temporal;
Query: select * from temporal
Query submitted at: 2021-03-19 08:09:01 (Coordinator: http://quickstart.cloudera:25000)
Query progress can be monitored at: http://quickstart.cloudera:25000/query_plan?query_id=b64f3fc99cfba4ad:ff4b7bf400000000
+--------+
| nombre |
+--------+
| Peter  |
+--------+
Fetched 1 row(s) in 0.86s

0: jdbc:hive2://localhost:10000> INSERT INTO temporal values ('Maria');
0: jdbc:hive2://localhost:10000> INSERT INTO temporal values ('Jose');

[quickstart.cloudera:21000] > select * from temporal;
Query: select * from temporal
Query submitted at: 2021-03-19 08:13:08 (Coordinator: http://quickstart.cloudera:25000)
Query progress can be monitored at: http://quickstart.cloudera:25000/query_plan?query_id=144307e8797cd2d3:2dd7f09800000000
+--------+
| nombre |
+--------+
| Peter  |
+--------+
Fetched 1 row(s) in 0.52s
[quickstart.cloudera:21000] > refresh default.temporal;
Query: refresh default.temporal
Query submitted at: 2021-03-19 08:13:22 (Coordinator: http://quickstart.cloudera:25000)
Query progress can be monitored at: http://quickstart.cloudera:25000/query_plan?query_id=34e4f4517c03159:2a3d96d000000000
Fetched 0 row(s) in 7.12s
[quickstart.cloudera:21000] > select * from temporal;
Query: select * from temporal
Query submitted at: 2021-03-19 08:13:31 (Coordinator: http://quickstart.cloudera:25000)
Query progress can be monitored at: http://quickstart.cloudera:25000/query_plan?query_id=3a46a0639693edc5:d0b4e18c00000000
+--------+
| nombre |
+--------+
| Maria  |
| Jose   |
| Peter  |
+--------+
Fetched 3 row(s) in 0.46s
[quickstart.cloudera:21000] > 

Tables -> un directorio puede contener multiples ficheros, 
no se permiten subdirectorios, excepto, en las tablas particionadas; 
se pueden definir con expresiones regulares
se puede crear donde quiera

Databases -> lineal no en forma de arbol , 
se debe indicar el directorio donde van a estar sus tablas por defecto: /user/hive/warehouse

SHOW DATABASES;
DESCRIBE DATABASE default; nos indica la location ubicación de las tablas de la database por default
-O-J-O- connect to the default database by default - connecta a la db default por defecto, 
hasta que no le indique con USE la database estará trabajando con default

-- querie(s)-(consulta(s)) table in the default database
SELECT * 
FROM customers;

-- querie(s)-(consulta(s)) table in the other database from default
SELECT * 
FROM database.customers; 

USE database;
-- querie(s)-(consulta(s)) table in other  database
SELECT * 
FROM customers;

-- Show tables in the current database
SHOW TABLES;

-- Show tables in the database indicada
SHOW TABLES IN database;

-- Describe displays basic structure for table - estructura de la tabla
DESCRIBE table; 

-- Describe Input and Output formats, file locations, and other info
DESCRIBE FORMATTED table;

-- comments 
/* comments
   comments */

`select` --> si utiliza una palabra reservada entre ` `, no se considera como palabra reservada 
------
SELECT
------
SELECT cust_id, fname, lname 
FROM customers;

SELECT * 
FROM customers;

-- remove duplicates - remove duplicados
SELECT DISTINCT zipcode 
FROM customers;

-- no es necesario que la columna del order by este en el select 
SELECT brand, name 
FROM products
ORDER BY price DESC;

-- puedo crear una expresion utilizando un alias(AS) y ordenar por este
SELECT brand, name, price - cost AS profit 
FROM products
ORDER BY profit

-- maximum - máximo number of rows returned
LIMIT

-- use ORDER BY with LIMIT for top-N querie(s)-(consulta(s))
SELECT brand, name , price
FORM products
ORDER BY price DESC 
LIMIT 10;

-- string comparison are case-sensitive
SELECT *
FROM orders
WHERE order_id = 1287;

SELECT * 
FROM customers
WHERE state IN ('CA', 'OR', 'WA', 'NV', 'AZ');

-- combine AND, OR, LIKE
SELECT *
FROM customers
WHERE fname LIKE 'Ann%'
AND (city='Seattle' OR city='Portland')

-- alias (AS)
SELECT c.fname, c.lname, o.order_date
FROM customers c
JOIN orders o ON(c.cust_id = o.cust_id)
WHERE c.zipcode='94306';

-- subquery in FROM clause
SELECT prod_id, brand, name
FROM
(SELECT *
 FROM products
 WHERE (price - cost) / price > 0.65 -- 10 products with profits > 65%
 ORDER BY price DESC
 LIMIT 10) high_profits
WHERE price > 1000
ORDER BY brand, name;

-- subquery in WHERE clause
SELECT cust_id, fname, lname
FROM customers c
WHERE state = 'NY' AND c.cust_id IN
(SELECT cust_id
 FROM orders
 WHERE order_id > 6650000)
 ORDER BY c.cust_id DESC;

Data types

Integer types
TINYINT
SMALLINT
INT
BIGINT

Decimal types
FLOAT
DOUBLE
DECIMAL(p,s) when exact values are required! (p: precision total number of digits, s: scale number of digits after the decimal point)

Character types
STRING
CHAR(n)
VARCHAR(n)

Other simple types
BOOLEAN
TIMESTAMP

Data Type Conversion

-- Esto NO funciona 
SELECT zipcode + 1.5
FROM customers
LIMIT 1;

-- Se debe de hacer el cast explicitamente
SELECT cast(zipcode AS FLOAT) + 1.5
FROM customers
LIMIT 1;

Valores fuera de rango
Impala cuando el valor esta fuera de rango, 
TINYINT (Rango: -128 to 127)

por ejemplo para almacenar 129 y -999 en un TINYINT impala almacena los valores máximos permitidos para el tipo de dato, es decir: 127 y -128
(Hive almacena NULL)

-- lanzar el shell de impala
[cloudera@quickstart ~]$ impala-shell

-- to connect to another server
[cloudera@quickstart ~]$ impala-shell -i localhost:21000


-- Tab twice - doble tabulador 
[cloudera@quickstart ~]$ impala-shell
[quickstart.cloudera:21000] > 
alter     connect   delete    describe  exit      help      insert    profile   rerun     set       show      src       tip       update    use       version
compute   create    desc      drop      explain   history   load      quit      select    shell     source    summary   unset     upsert    values    with

[quickstart.cloudera:21000] > impala shell --help

[quickstart.cloudera:21000] > use default;
[quickstart.cloudera:21000] > show tables;

--ejecutar un comando del sistema operativo desde el shell de impala
[quickstart.cloudera:21000] > shell date;
Sun Mar 14 08:45:14 PDT 2021
--------
Executed in 0.03s

[quickstart.cloudera:21000] > shell ls;

[quickstart.cloudera:21000] > shell hdfs dfs -mkdir /user/cloudera/test/;
--------
Executed in 18.87s

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/
Found 1 items
drwxr-xr-x   - cloudera cloudera          0 2021-03-14 08:47 /user/cloudera/test

ejecutar commands desde un fichero:
[cloudera@quickstart ~]$ cat Desktop/CCAl159/file.hql 
SELECT * FROM customers LIMIT 10;

--ejecutar comandos en un fichero file desde el shell de impala invocando el shell
[quickstart.cloudera:21000] > shell impala-shell -f file 

[quickstart.cloudera:21000] > shell impala-shell -f Desktop/CCAl159/file.hql;

[quickstart.cloudera:21000] > shell impala-shell -q "use analyst; show tables; use default; select * from customers limit 10";

--ejecutar un comando directamente desde la línea de comandos
[cloudera@quickstart ~]$ impala-shell -q "use analyst; show tables; use default; select * from customers limit 10";
--desde el alias alias ie="impala-shell -q"
[cloudera@quickstart ~]$ ie "use analyst; show tables; use default; select * from customers limit 10";

--ejecutar comandos en un fichero file desde la línea de comandos directamente
alias if="impala-shell -f"
[cloudera@quickstart ~]$ if Desktop/CCAl159/file.hql

--suprimir messages de información y ejecutar querie(s)-(consulta(s))
alias iqe="impala-shell --quiet -q"
[cloudera@quickstart ~]$ iqe "select * from customers limit 10";

--ejecutar comandos en un fichero file desde la línea de comandos directamente suprimiendo messages de información
alias iqf="impala-shell --quiet -f"
[cloudera@quickstart ~]$ iqf Desktop/CCAl159/file.hql

--ejecutar comandos en un fichero file desde la línea de comandos (suprimiendo messages de información, y delimita la salida de la tabla por un \'t'-tabulador por defecto y suprime la cabecera)
alias iqdf="impala-shell --quiet --delimited -f"
[cloudera@quickstart ~]$ iqdf Desktop/CCAl159/file.hql

--ejecutar comandos en un fichero file desde la línea de comandos (delimita la salida de la tabla por una ',') y envia la salida a un fichero 
[cloudera@quickstart ~]$ impala-shell -f Desktop/CCAl159/file.hql --quiet --delimited --output_delimiter=',' -o Desktop/CCAl159/results.txt
[cloudera@quickstart ~]$ cat Desktop/CCAl159/results.txt

--ejecutar comandos en un fichero file desde la línea de comandos (suprimiendo messages de información, y creando un fichero de salida llamado out con la ejecución)
alias iqof="impala-shell --quiet -o out -f"
[cloudera@quickstart ~]$ iqof Desktop/CCAl159/file.hql

--ejecutar comandos en un fichero file desde la línea de comandos (creando un fichero de salida llamado out con la ejecución)
alias iof="impala-shell -o out -f"
[cloudera@quickstart ~]$ iof Desktop/CCAl159/file.hql

OPERATORS - OPERADORES
Arithmetic Operators in HiveQL and Impala SQL
Basic four: +, -, *, /
 84 / 4 = 21
 84 - 4 = 80
Modulo: % This is the remainder from integer division
 25 % 7 = 4
Unary negative: - Unary because it takes only one operand
 -(3 - 5) = -(-2) = 2
No operator for exponentiation Use pow() function instead
 pow(5,3) = 5 * 5 * 5 = 125

Comparison Operators in HiveQL and Impala SQL
Equality and Inequality =, != or <>, <, >, <=, >=
 5 IN (2, 3, 5, 7): true
 6 IN (2, 3, 5, 7): false
BETWEEN incluye el origen y el destino
 5 BETWEEN 2 AND 7: true
 9 BETWEEN 2 AND 7: false
 7 BETWEEN 2 and 7: true
LIKE especie de expresion regular 
 'name@example.com' LIKE '%.com': true
 'Mickey' LIKE 'M*ey': false

Comparisons and NULL Values
Typical operators return NULL when at least one operand is NULL - siempre que se compare con NULL el resultado es NULL
 5 = NULL → NULL
 5 != NULL → NULL
 5 < NULL → NULL
 NULL = NULL → NULL
 NULL != NULL → NULL
Alternative operators are IS (NOT) DISTINCT FROM and <=> 
IS DISTINCT FROM 
 Same as != for non-NULL values
 5 IS DISTINCT FROM NULL → true
 NULL IS DISTINCT FROM NULL → false
IS NOT DISTINCT FROM or <=> (NULL-safe equality) 
 Same as = for non-NULL values
 5 <=> NULL → false
 NULL <=> NULL → true

Logical and Null Operators in HiveQL and Impala SQL
Logical operators
 Binary: AND, OR
 Unary: NOT
Null operators
 IS NULL
 IS NOT NULL

SELECT name, price, cost, price - cost
FROM products;

SELECT name, price, cost, price - cost profit
FROM products;

SELECT name, price, cost
FROM products
WHERE cost >= price;

SELECT name, price, cost, pow(price,3)
FROM products

SELECT * 
FROM employee 
WHERE end_date IS NULL 
LIMIT 20;

SELECT * 
FROM employee 
WHERE end_date IS NOT NULL
LIMIT 20;

SELECT * 
FROM employee 
WHERE end_date <=> NULL
LIMIT 20;

FUNCTIONS - FUNCIONES - no son case sensitives
SELECT fname, lname, concat(fname, ' ' lname) fullname
FROM customers
LIMIT 10;

SHOW FUNCTIONS IN _IMPALA_BUILTINS;

Test a function
SELECT abs(-459.67);

SELECT upper('Sonia celis');

SCALAR FUNCTIONS - se escriben en minusculas
operan independientemente en los valores de cada fila - 

el número de argumentos depende de la function
rand()
abs(-123)
pow(3,5)

argumentos pueden ser referencias a columnas, valores literales ó expresiones
year(order_dt + 5)
round(price * tax, 2)

Round to specified # of decimals 
round(total_price, 2) 
round(23.492, 2)

Return nearest integer above 
ceil(total_price) 
ceil(23.492)

Return nearest integer below 
floor(total_price) 
floor(23.492)

Return absolute value 
abs(temperature) 
abs(-49)

Return square root 
sqrt(area) 
sqrt(64)

Return a random number 
rand()

GROUP_CONCAT([ALL | DISTINCT] expression [, separator])
SELECT GROUP_CONCAT(cast(n AS STRING),"|")
FROM numbers;
+--------------------------------------+
| group_concat(cast(n as string), '|') |
+--------------------------------------+
| 2|4|6|8|10|1|3|5|7|9                 |
+--------------------------------------+

-- Es necesario incluir en la subconsulta el LIMIT para que lo haga
SELECT GROUP_CONCAT(cast(n AS STRING),"-")
FROM
(SELECT n FROM default.numbers ORDER BY n DESC LIMIT 10) nums;
+--------------------------------------+
| group_concat(cast(n as string), '-') |
+--------------------------------------+
| 10-9-8-7-6-5-4-3-2-1                 |
+--------------------------------------+

Traduce - reemplaza cada caracter de from 
por cada caracter de to en input
translate(string input, string from, string to)
SELECT translate ('hello world','world','earth');
+--------------------------------------------+
| translate('hello world', 'world', 'earth') |
+--------------------------------------------+
| hetta earth                                |
+--------------------------------------------+
reemplaza w - e en hello world - hello eorld
reemplaza o - a en hello world - hella earld
reemplaza r - r en hello world - hella earld
reemplaza l - t en hello world - hetta eartd
reemplaza d - h en hello world - hetta earth
These functions work with TIMESTAMP values

Return current date and time 
SELECT current_timestamp();
2021-03-15 15:31:41

Convert to UNIX format 
SELECT unix_timestamp(current_timestamp());
1615822683

Return fecha actual en UNIX format BIGINT
SELECT unix_timestamp();
+------------------+
| unix_timestamp() |
+------------------+
| 1616220221       |
+------------------+

Convert to string format 
SELECT from_unixtime(unix_timestamp(current_timestamp()));
2021-03-15 15:40:24

Return fecha actual en el formato que le pases
SELECT from_unixtime(unix_timestamp(), 'dd/MMM/yyyy');
+------------------------------------------------+
| from_unixtime(unix_timestamp(), 'dd/mmm/yyyy') |
+------------------------------------------------+
| 20/Mar/2021                                    |
+------------------------------------------------+

Extract date portion - this function doesn't exist in mysql;
to_date(order_dt) 
SELECT to_date(current_timestamp());
2021-03-15

Extract year portion 
year(order_dt) 
SELECT year(current_timestamp()); 
2021

Return # of days between dates 
datediff(ship_dt, order_dt) 
SELECT datediff(current_timestamp(), current_timestamp());
0

https://docs.cloudera.com/documentation/enterprise/5-10-x/topics/impala_datetime_functions.html

SELECT from_timestamp(current_timestamp(), 'yyyy/MM/dd');
2021/03/16

SELECT from_timestamp('2021-03-16','yyyy/MM/dd');
2021/03/16 

Pasar un string date de un formato determinado a un timestamp
to_timestamp('08-1987-21', 'mm-yyyy-dd')

Built-In String Functions
Convert to uppercase 
SELECT upper('Bob');
BOB

Convert to lowercase 
SELECT lower('Bob');
bob

Remove whitespace at start/end 
SELECT trim(' Bob ');
Bob

Remove only whitespace at start 
SELECT ltrim(' Bob '); 
Bob

Remove only whitespace at end 
SELECT rtrim(' Bob ');
 Bob

Extract portion of string 
SELECT substring('Sammy', 2, 4);
ammy

Replace characters in string 
SELECT translate('Samuel', 'uel', 'my');
Sammy

String Concatenation
SELECT concat('soniacelis', '@gmail.com');
soniacelis@gmail.com  

SELECT concat_ws(' ', 'sonia', 'celis');
sonia celis

SELECT concat_ws('*-*', 'sonia', 'celis', 'vasquez');
sonia*-*celis*-*vasquez

Parsing URLs
url=http://www.example.com/click.php?A=42&Z=105#r1

parse_url(url, 'PROTOCOL') http
parse_url(url, 'HOST') www.example.com
parse_url(url, 'PATH') /click.php
parse_url(url, 'QUERY') A=42&Z=105
parse_url(url, 'QUERY', 'A') 42
parse_url(url, 'QUERY', 'Z') 105
parse_url(url, 'REF') r1

Other Built-In Functions
Convert to another type cast(weight AS INT)
3.581 => 3

Selectively return value if(price > 1000, 'A', 'B') 
1500 => A

Selectively return value (multiple cases)
case
 when price > 1000 then 'A'
 when price < 100 then 'C'
 else 'B'
end

Aggregate Functions
combine values from multiple rows

They can work over all rows in a table, returning only one row
SELECT AVG(price) 
FROM products;

They can work over groups of rows
SELECT brand, AVG(price) 
FROM products 
GROUP BY brand;

How many products of each brand are in the products table?
SELECT brand, count(prod_id)
FROM products
GROUP BY brand

Count all rows 
How many products are in the products table?
SELECT COUNT(*)
FROM products;

SELECT count(prod_id) 
FROM products;

Count all rows where field is not NULL 
SELECT COUNT(fname)
FROM products;

Count all rows where field is unique and not NULL 
SELECT COUNT(DISTINCT fname)
FROM products;

Return the largest value 
SELECT MAX(price)
FROM products;

Return the smallest value 
SELECT MIN(price)
FROM products;

Add all supplied values and return result 
SELECT SUM(price)
FROM products;

Return the average of all supplied values 
SELECT AVG(price) 
FROM products;

HAVING
------
The HAVING Clause
You cannot filter on aggregate functions using WHERE Use HAVING instead
The aggregate function does not have to be in the SELECT list or the GROUP BY clause

What is the average profit of products in the products table, by brand, for brands with at least 50 products?
SELECT brand, AVG(price-cost) AS avg_profit
FROM products
GROUP BY brand
HAVING COUNT(prod_id) >= 50;

GROUP BY with WHERE and HAVING
How many employees do we have in each state with a salary of less than $20,000?
SELECT state, COUNT(*) AS num FROM employees
WHERE salary < 20000
GROUP BY state;

--Which states have more than 400 employees whose salary is less than $20,000?
SELECT state, COUNT(*) AS num FROM employees
WHERE salary < 20000
GROUP BY state
HAVING COUNT(*) > 400;

SELECT state, COUNT(*) AS num 
FROM employees
WHERE salary < 20000
GROUP BY state
HAVING num > 400;

CREATE DATABASE
Siempre - Always - definir un directorio por defecto de lo contrario la va a crear en: /user/hive/warehouse/name_database.db
El directorio por defecto de la db default es: /user/hive/warehouse/

-- Si la database no existe la crea, si existe no la crea
CREATE DATABASE IF NOT EXISTS dbname;

-- por defecto la crearia en: /user/hive/warehouse/dbname.db
CREATE DATABASE dbname; 

CREATE TABLE
-- Si el directorio existe no hace nada, si no existe se crea vacio
-- por defecto la crearia en: /user/hive/warehouse/tablename, 

CREATE TABLE tablename (colname DATATYPE, ...)
-- si le digo el nombre de la dbase la crea en: /user/hive/warehouse/dbname.db/tablename

CREATE TABLE dbname.tablename (colname DATATYPE, ...);
-- si no le digo el nombre de la dbase la crea en la que este en uso.
-- si le doy la Localización "Location" specify the directory where table DATA resides, crea los fichero de DATOS en el hdfs en la Location.

[quickstart.cloudera:21000] > use temp01;
[quickstart.cloudera:21000] > CREATE TABLE edades (edad int) Location '/edades'
[quickstart.cloudera:21000] > show tables;
Query: show tables
+---------+
| name    |
+---------+
| edades  |
| table01 |
+---------+
[quickstart.cloudera:21000] > INSERT INTO edades values (10), (20);
[quickstart.cloudera:21000] > SELECT * FROM edades;
+------+
| edad |
+------+
| 10   |
| 20   |
+------+

-- crea los archivos en la Location que le indicamos
[cloudera@quickstart ~]$ hdfs dfs -ls /edades/
Found 2 items
-rw-r--r--   1 impala supergroup          6 2021-03-19 10:42 /edades/804ada32a8d3137d-20ff753900000000_2077787507_data.0.
drwxr-xr-x   - impala supergroup          0 2021-03-19 10:42 /edades/_impala_insert_staging
[cloudera@quickstart ~]$ hdfs dfs -cat /edades/804ada32a8d3137d-20ff753900000000_2077787507_data.0.
10
20

CREATE TABLE dbname.tablename (colname DATATYPE, ...)
 ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY 'char'
 STORED AS {TEXTFILE|SEQUENCEFILE|...};

-- Crea la table y define los campos y los tipos de datos
CREATE TABLE dbname.tablename (colname DATATYPE, ...)

-- Indica como van a ester delimitadas las filas en las tablas - Hive default delimiter is Ctrl+A = 001
 ROW FORMAT DELIMITED 

-- Indica cual es el caracter con el que va a delimitar los campos - '\t' por ejemplo con un tabulador
  FIELDS TERMINATED BY char

-- Indica el formato en el que se van a almacenar los datos en el warehouse - default STORED AS TEXFILE
 STORED AS {TEXTFILE|SEQUENCEFILE|...};

--Create Table a partir de otra (CLONE - only metadatos - NO DATA)
--Tabla MANAGED_TABLE (interna) yo la gestiono, los datos me pertenecen si borro la tabla borro los datos.
  Metadata is removed, Data in HDFS is removed
--Tabla EXTERNA yo NO la gestiono, los datos NO me pertenecen no tengo control sobre ellos puedo borrar la tabla pero no los datos.
  Metadata is removed, Data in HDFS is NOT removed

CREATE [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
  LIKE { [db_name.]table_name | PARQUET 'hdfs_path_of_parquet_file' }
  [COMMENT 'table_comment']
  [STORED AS file_format]
  [LOCATION 'hdfs_path']

-- LIKE no permite cambiar:
 ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY char

-- Crea una tabla nueva con la estructura de la que ya existe
CREATE TABLE IF NOT EXISTS temp01.orders_l
  LIKE default.orders
  COMMENT 'tabla like default.orders'

-- Carga los datos con los mismos datos de la tabla orders
LOAD DATA INPATH '/user/hive/warehouse/orders/'
 INTO TABLE temp01.orders_l;

outside HDFS, a fully qualified path will be needed
LOCATION 's3a://path.to.bucket/orders_s'
LOCATION 'adl://path.to.bucket/orders_s'

-- Create tablas externas - external tables - Cuando borro la tabla (DROP) NO se borran los datos
-- sólo se borran los metadatos

CREATE EXTERNAL TABLE adclicks (
campaign_id STRING,
click_time TIMESTAMP,
keyword STRING,
site STRING,
placement STRING,
was_clicked BOOLEAN,
cost SMALLINT
)
LOCATION '/analyst/dualcore/ad_data';

-- Carga los datos desde un fichero1 ubicado en HDFS que después es borrado, se comporta como un (mv)
LOAD DATA INPATH '/user/cloudera/fichero1'
 INTO TABLE adclicks

-- Carga los datos desde un fichero1, ubicado en HDFS pero sobreescribe lo que hay en la tabla adclicks
-- Luego borra los datos del origen (mv)
LOAD DATA INPATH '/user/cloudera/fichero1'
 OVERWRITE INTO TABLE adclicks

-- Crea una tabla externa, 
CREATE EXTERNAL TABLE IF NOT EXISTS temp01.orders_s (orders_id string, custs_id string, orders_date string)
  COMMENT 'tabla externa para orders_s'
  ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'
  LOCATION '/user/cloudera/data/';

Query: describe temp01.orders_s
+-------------+--------+---------+
| name        | type   | comment |
+-------------+--------+---------+
| orders_id   | string |         |
| custs_id    | string |         |
| orders_date | string |         |
+-------------+--------+---------+

los datos estan en HDFS: LOCATION '/user/cloudera/data/';
y para cargar los datos:

LOAD DATA INPATH '/user/cloudera/data/orders_s.txt'
 INTO TABLE temp01.orders_s

DROP DATABASE dbname; 
DROP DATABASE IF EXISTS dbname; 
-- Borra TODO incluso datos en HDFS
DROP DATABASE dbname CASCADE; 

DROP TABLE tablename; 
DROP TABLE IF EXISTS dbname; 

-- modify table or its characteristics - caracteristicas
ALTER TABLE tablename;

ALTER TABLE dbname.tablename RENAME TO dbname.tablename;

-- modify el tipo del campo
ALTER TABLE temp01.orders_l CHANGE order_date order_date STRING;

-- modify tanto nombre del campo como el tipo - sólo cambia el metastore - no cambia el HDFS
ALTER TABLE temp01.orders_l CHANGE order_date neworder_date BIGINT;

-- Adding Columns from a table - no cambia el HDFS
ALTER TABLE jobs ADD COLUMNS (city STRING, bonus INT);

-- Removing  columns from a table - Impala Only - no cambia el HDFS
ALTER TABLE jobs DROP COLUMN salary;

-- Replace todas las definiciones de las columnas - no cambia el HDFS
ALTER TABLE jobs REPLACE COLUMNS (
  id INT,
  title STRING,
  salary INT );
--
Use SHOW CREATE TABLE to display a statement to reproduce the table.
show create table ex_customers;
+----------------------------------------------------+--+
|                   createtab_stmt                   |
+----------------------------------------------------+--+
| CREATE EXTERNAL TABLE `ex_customers`(              |
|   `cust_id` int,                                   |
|   `fname` string,                                  |
|   `lname` string,                                  |
|   `address` string,                                |
|   `city` string,                                   |
|   `state` string,                                  |
|   `zipcode` string)                                |
| COMMENT 'tabla externa para mysql.analyst_dualcore.customers' |
| ROW FORMAT SERDE                                   |
|   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'  |
| WITH SERDEPROPERTIES (                             |
|   'field.delim'='\t',                              |
|   'serialization.format'='\t')                     |
| STORED AS INPUTFORMAT                              |
|   'org.apache.hadoop.mapred.TextInputFormat'       |
| OUTPUTFORMAT                                       |
|   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' |
| LOCATION                                           |
|   'hdfs://quickstart.cloudera:8020/user/hive/warehouse/exam.db/customers' |
| TBLPROPERTIES (                                    |
|   'COLUMN_STATS_ACCURATE'='false',                 |
|   'numFiles'='4',                                  |
|   'numRows'='-1',                                  |
|   'rawDataSize'='-1',                              |
|   'totalSize'='12577346',                          |
|   'transient_lastDdlTime'='1616521069')            |
+----------------------------------------------------+--+
Displays CREATE TABLE statement to create table in its current state
Use instead of recreating sequence of CREATE and ALTER statements
-- Changing Other Properties - Cambiar Propiedades de las tablas
ALTER TABLE tablename SET TBLPROPERTIES ('EXTERNAL' = 'TRUE')
-- Es para que salte la primera línea de los files cvs - saltarse la cabecera - header
'skip.header.line' = '1' 

VIEWS - TO SIMPLIFYING COMPLEX QUERIES - simplificar querie(s)-(consulta(s))
Realizar este querie(s)-consulta(s) cada vez para una orden... lo mejor es crear una vista - tabla temporal
SELECT o.order_id, order_date, p.prod_id, brand, name
FROM orders o
JOIN order_details d ON (o.order_id = d.order_id)
JOIN products p ON (d.prod_id = p.prod_id)
WHERE o.order_id=6584288;

-- crea una vista (es dinámica, se ejecuta cada vez por lo que no es muy eficiente) 
CREATE VIEW order_info AS
SELECT o.order_id, order_date, p.prod_id, brand, name
FROM orders o
JOIN order_details d ON (o.order_id = d.order_id)
JOIN products p ON (d.prod_id = p.prod_id)

SELECT * 
FROM order_info 
WHERE order_id=6584288;

-- muestra la vista en las tablas - show view
SHOW TABLES;
-- descripción completa de la vista - describe view
DESCRIBE FORMATTED order_info;
-- muestra como se creo la vista - create view
SHOW CREATE TABLE order_info;
-- modifica la vista - modify view
ALTER VIEW order_info AS
SELECT order_id, order_date
FROM orders;
-- renombrar la vista - rename view
ALTER VIEW order_info
RENAME TO order_information;
-- drop view to remove view - delete view - borrar vista
DROP VIEW order_info;

-- Almacenar los datos de salida de un querie(s)-consulta(s) en una tabla QUE DEBE EXISTIR - CREATE TABLE dbname.tablename AS
-- El contenido existente será borrado - deleted

-- Inserted 125 row(s)
[quickstart.cloudera:21000] > CREATE TABLE exam.nyc_customers AS 
SELECT * 
FROM exam.ex_customers 
WHERE state = 'NY' 
  AND city = 'New York' ;

-- Add records - Adiciona registros without deleted - El contenido existente NO se borra
-- Modified 201375 row(s)
[quickstart.cloudera:21000] > INSERT INTO TABLE exam.nyc_customers 
SELECT * 
FROM ex_customers;

-- bc -> 125 + 201375 = 201500
[quickstart.cloudera:21000] > SELECT count(*) 
FROM exam.nyc_customers;
+----------+
| count(*) |
+----------+
| 201500   |
+----------+

[quickstart.cloudera:21000] > INSERT OVERWRITE TABLE exam.nyc_customers
SELECT *
FROM ex_customers
WHERE state = 'NY'
  AND city = 'Brooklyn';

[quickstart.cloudera:21000] > SELECT count(*) 
FROM exam.nyc_customers;
+----------+
| count(*) |
+----------+
| 20       |
+----------+

-- CTAS - Creating Tables Based on Existing Data - 
-- CREATE TABLE dbname.tablename AS SELECT (CTAS)
-- Column definitions are derived from the existing table
-- Column names are derived from the existing names
-- Todo lo que quiera cambiar lo cambio en el SELECT incluso puedo utilizar ALIAS y cambios de tipos con cast
CREATE TABLE ny_customers
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
AS
SELECT cust_id, fname, lname
FROM ex_customers
WHERE state = 'NY';

[quickstart.cloudera:21000] > SELECT cust_id, fname, lname
FROM ny_customers;

[quickstart.cloudera:21000] > SHOW CREATE TABLE ny_customers;

HastaAQUI------------------------------

HastaAQUI------------------------------






---

SELECT DISTINCT(category_id)
FROM categories
ORDER BY category_id

SELECT COUNT(*)
FROM categories

---

revenue ó ingreso
revenue = sales revenue = the amount of gross incomme produced through sales of products or services.
revenue = Sales * Average Price of Service
revenue = Sales * Sales Price

profit ó beneficio
profit = price - cost a financial gain, especially the difference between the amount earned and the amount spent in buying.

SELECT oi.order_item_product_id AS product, oi.order_item_product_price * oi.quantity AS revenue
FROM
    (
        SELECT order_item_product_id , order_item_product_price, sum(order_item_quantity) AS quantity
        FROM order_items
        GROUP BY order_item_product_id, order_item_product_price
    ) AS oi
ORDER BY revenue DESC
LIMIT 10
---

SELECT p.product_id, p.product_name, r.revenue
FROM products AS p 
INNER JOIN
(SELECT oi.order_item_product_id AS id_product, sum(cast(oi.order_item_subtotal AS float)) AS revenue
 FROM order_items oi 
 INNER JOIN orders o ON oi.order_item_order_id = o.order_id
 WHERE o.order_status <> 'CANCELED' AND o.order_status <> 'SUSPECTED_FRAUD'
 GROUP BY order_item_product_id) AS r
ON p.product_id = r.id_product
ORDER BY r.revenue DESC
LIMIT 10

---

SELECT sum(t.products)
FROM 
(SELECT c.category_id, count(p.product_id) as products
 FROM categories AS c
 INNER JOIN products AS p ON c.category_id = p.product_category_id
 GROUP BY c.category_id
 ORDER BY c.category_id) AS t

---

SELECT o.order_id, sum(oi.order_item_subtotal)
FROM orders AS o
INNER JOIN order_items AS oi ON order_id = order_item_order_id
GROUP BY o.order_id

---

SELECT sum(oi.order_item_subtotal)
FROM orders AS o
INNER JOIN order_items AS oi ON order_id = order_item_order_id
WHERE o.order_status <> 'CLOSED' AND o.order_status <> 'SUSPECTED_FRAUD' 

---

SELECT  order_item_product_id , count(order_item_quantity) AS quantity
FROM order_items 
GROUP BY  order_item_product_id 
ORDER BY quantity DESC
LIMIT 10;

---

SELECT c.category_name, count(oi.order_item_quantity) AS total
FROM categories AS c
INNER JOIN products AS p ON c.category_id = p.product_category_id
INNER JOIN order_items AS oi ON oi.order_item_product_id = p.product_id
INNER JOIN orders AS o ON o.order_id = oi.order_item_order_id
WHERE o.order_status <> 'CANCELED' AND o.order_status <> 'SUSPECTED_FRAUD'
GROUP BY c.category_name


SELECT sum(products)
FROM 
(SELECT c.category_id, count(p.product_id) as products
 FROM categories AS c
 INNER JOIN products AS p ON c.category_id = p.product_category_id
 GROUP BY c.category_i) AS t
ORDER BY c.category_id

---

SELECT sum(products)
FROM 
(SELECT c.category_id, count(p.product_id) as products
 FROM categories AS c
 INNER JOIN products AS p
 ON c.category_id = p.product_category_id
 GROUP BY c.category_i) AS t
ORDER BY c.category_id

INNER JOIN products AS p ON c.category_id = p.product_category_id
INNER JOIN order_items AS oi ON oi.order_item_product_id = p.product_id
INNER JOIN orders AS o ON o.order_id = oi.order_item_order_id
WHERE o.order_status <> 'CANCELED' AND o.order_status <> 'SUSPECTED_FRAUD'
GROUP BY c.category_name
LIMIT 10

---

SELECT c.category_name, count(oi.order_item_quantity) AS total
FROM order_items AS oi
INNER JOIN orders AS o ON o.order_id = oi.order_item_order_id
INNER JOIN products AS p ON p.product_id = oi.order_item_product_id
INNER JOIN categories AS c ON c.category_id = p.product_category_id
WHERE o.order_status <> 'CANCELED' AND o.order_status <> 'SUSPECTED_FRAUD'
GROUP BY c.category_name
ORDER BY total DESC
LIMIT 10

--- 

SELECT c.customer_zipcode, sum(oi.order_item_subtotal) as total
FROM customers AS c
INNER JOIN orders AS o ON o.order_customer_id = c.customer_id
INNER JOIN order_items AS oi ON oi.order_item_order_id = o.order_id
WHERE o.order_status <> 'CANCELED' AND o.order_status <> 'SUSPECTED_FRAUD'
GROUP BY c.customer_zipcode
ORDER BY total DESC
LIMIT 10

---

SELECT count(oi.order_item_quantity) AS total
FROM order_items AS oi
ORDER BY total DESC
LIMIT 10

---

SELECT o.order_id, c.customer_fname, c.customer_lname, o.order_date
FROM customers c
INNER JOIN orders o ON(o.order_customer_id = c.customer_id)
INNER JOIN order_items oi ON(oi.order_item_order_id = o.order_id)

---

SELECT brand, round(avg(price),2) as avg_price, round(avg(cost),2), round(avg(price - cost),2) AS avg_profit   
FROM products 
GROUP BY brand 
HAVING avg_price > 10000 
ORDER BY avg_price DESC
LIMIT 10;

---

---


---------------
--------------
Sqoop commands - 

Sqoop User Guide: https://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html
                  http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html
                  http://sqoop.apache.org/docs/1.99.7/user.html
--------------
Importing Tables FROM MySQL database and write the data to HDFS
Sqoop utiliza 4 hilos de ejecución
Suponiendo que la tabla tiene clave primaria, sqoop calcula el minimo y el máximo y lo divide en 4 hilos en donde cada hilo utiliza uno de esos 4 rangos para realizar la ejecución

Si el directorio existe, sqoop da un error a menos que le pongas --append que te permitirá anadir tablas en el mismo directorio(da igual la estructura- sqoop no comprueba las estructuras)

--direct  se utiliza si tenemos un connector directo a la database, NO usar con all-databases

-Dorg.apache.sqoop.splitter.allow_text_splitter=true se utiliza cuando la primary key es string or si quiero splitting una columna de texto


--split-by <column-name> 	Indicar la columna de la tabla que va a ser usada para splitting los trabajos. 
                                No puede usar split-by con --autoreset-to-one-mapper option. 
                                Suponiendo que no tiene clave primaria ó no quieres utilizar la clave primaria sino otro campo ó esa clave primaria es de tipo string, 
                                sqoop utiliza el split-by para repartir el trabajo

-m,--num-mappers <n>            m 1, si elijo un sólo mapper, es decir, sólo un hilo de ejecución(no es muy eficiente) que coje toda la tabla en un sólo trabajo, 
--autoreset-to-one-mapper 	Si no tengo clave primaria debo utilizar auto-reset-to-mapper ó use one mapper
                                No se puede usar con split-by

--target-dir <dir> 	para definir el directorio de destino en HDFS destination dir
--warehouse-dir <dir> 	para definir el directorio principal como directorio de destino en HDFS parent for table destination 
                        creará un directorio con el nombre de la tabla con las columnas seleccionadas

sqoop help
sqoop version
driver --> jdbc:mysql://host:puerto/db

* no se le define un destino por lo tanto se va al /user/cloudera utiliza el nombre de la tabla que se importa 
          hdfs dfs -ls /user/cloudera/customers 

sqoop import \
	--connect jdbc:mysql://quickstart:3306/retail_db \
	--username=retail_dba \
	--password=cloudera \
	--table customers

* importa los ficheros en el directorio definido en target-dir
          hdfs dfs -ls /mydata/customers

sqoop import \
	--connect jdbc:mysql://quickstart:3306/retail_db \
	--username=retail_dba \
	--password=cloudera \
	--table customers \
	--target-dir /mydata/customers

          
* también importa los ficheros en el directorio definido en warehouse-dir el nombre del directorio conside con el nombre de la tabla
  crea un directorio por cada tabla
          hdfs dfs -ls /mydata/customers

sqoop import \
	--connect jdbc:mysql://quickstart:3306/retail_db \
	--username=retail_dba \
	--password=cloudera \
	--table customers \
	--warehouse-dir /mydata


* importa los ficheros en el directorio definido en warehouse-dir
          hdfs dfs -ls /user/hive/warehouse/ categories... etc one folder for table
          In Apache Parquet format: optimize data storage and retriveval in these scenarios.
          It is also creating tables to represent the HDFS files in Impala / Apache Hive with matching schema.
          The number of .parquet files shown will be equal to the number of mappers used by Sqoop (m 1).
          On a single-node you will just see one, but larger clusters will have a greater number of files. 
          --hive-import Import tables into Hive (Uses Hive’s default delimiters if none are set.) 
  	    Load Data from a Relational Database - Creates the table in the metastore
            Imports data from the RDBMS to the table's directory in HDFS
            Importa los datos y crea la tabla
          --hive-database default
	  --hive-table employees

sqoop import-all-tables \
    -m 1 \
    --connect jdbc:mysql://quickstart:3306/retail_db \
    --username=retail_dba \
    --password=cloudera \
    --compression-codec=snappy \
    --as-parquetfile \
    --warehouse-dir=/user/hive/warehouse \
    --hive-import

con esto vi las tablas en HUE
sqoop import-all-tables \
 -m 1  \
--connect jdbc:mysql://quickstart:3306/analyst_dualcore  \
--username=root  \
--password=cloudera  \
--compression-codec=snappy  \
--as-parquetfile  \
--warehouse-dir=/user/hive/warehouse  \
--hive-import

ó

sqoop import-all-tables \
-m 1 \
--connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba \
--password cloudera \
--compression-codec=snappy \
--as-parquetfile \
--warehouse-dir=/user/cloudera/retail

ó

sqoop import-all-tables \
    -m 1 \
    --connect jdbc:mysql://quickstart:3306/analyst_dualcore \
    --username=root \
    --password=cloudera \
    --warehouse-dir=/user/hive/warehouse \

ó

sqoop import-all-tables \
    --connect jdbc:mysql://quickstart:3306/analyst_dualcore \
    --username=root \
    --password=cloudera \
    --warehouse-dir /user/hive/warehouse \
    --fields-terminated-by '\t'

---

* importa los ficheros en 
          hdfs dfs -ls /mydata/customers
          indica que los campos estan separados por tabulador

sqoop import \
        --connect jdbc:mysql://quickstart:3306/retail_db \
        --username=retail_dba \
        --password=cloudera \
        --table customers \
        --fields-terminated-by '\t' \
        --target-dir /mydata/customers


---

* Importar tablas parciales - Importing partial tables - 
          sólo importa las columnas definidas

sqoop import \
        --connect jdbc:mysql://quickstart:3306/retail_db \
        --username=retail_dba \
        --password=cloudera \
        --table customers \
        --columns "customer_id,customer_fname,customer_lname" \
        --target-dir /mydata/customers

* Importar tablas parciales - Importing partial tables - 
  importa los ficheros en hdfs dfs -ls /mydata/customers
          sólo importa los registros que cumplan la condicion especificada

sqoop import \
	--connect jdbc:mysql://quickstart:3306/retail_db \
	--username=retail_dba \
	--password=cloudera \
	--table customers \
        --warehouse-dir /mydata \
        --where "customer_zipcode=00725"

ó

        --where "price >= 1000"

---

* Importación incremental de tablas - Importing incremental
  importa los ficheros en hdfs dfs -ls /mydata/customers
          incremental append mode importa sólo - only new records - add

sqoop import \
	--connect jdbc:mysql://quickstart:3306/retail_db \
	--username=retail_dba \
	--password=cloudera \
	--table customers \
        --warehouse-dir /mydata \
        --incremental append \
        --check-column customer_id \
        --last-value 4584

---


* Importación incremental de tablas - Importing incremental
  y manejo de las modificaciones de los registros - adds and updates records
  importa los ficheros en hdfs dfs -ls /mydata/orders/orders \
          incremental lastmodified append and updates records (you must maintain a timestamp column in your table)

sqoop import \
	--connect jdbc:mysql://quickstart:3306/retail_db \
	--username=retail_dba \
	--password=cloudera \
	--table orders \
        --warehouse-dir /mydata/orders\
        --incremental lastmodified \
        --check-column order_date 

* importa los ficheros en hdfs dfs -ls /mydata/orders/orders
          incremental lastmodified append and updates records (you must maintain a timestamp column in your table)
          en este caso usamos incremental append porque el directorio ya existe

sqoop import \
	--connect jdbc:mysql://quickstart:3306/retail_db \
	--username=retail_dba \
	--password=cloudera \
	--table orders \
        --warehouse-dir /mydata/orders \
        --incremental append \
        --check-column order_date \ 
        --last-value "2021-03-05 09:09:36"

---

* Importa la tabla campos separados por tabulador y crea un directorio para la tabla
sqoop import \
        -Dorg.apache.sqoop.splitter.allow_text_splitter=true \
        --connect jdbc:mysql://quickstart:3306/analyst_dualcore\
        --username=root \
        --password=cloudera \
        --table employees \
        --warehouse-dir /user/hive/warehouse \
        --fields-terminated-by '\t' 

---

* Importa los ficheros en hdfs dfs -ls /user/hive/warehouse 
          -Dorg.apache.sqoop.splitter.allow_text_splitter=true (no es necesario The primary key is not a character string)
          esta tabla no tiene clave primaria propia
          --split-by divide el trabajo de importación entre las tareas en función de los valores del campo order_id
          -m 1 para forzar a sqoop a importar todos los datos con una sola tarea pero reduce el rendimiento

sqoop import \
        -m 1 \
	--connect jdbc:mysql://quickstart:3306/analyst_dualcore\
	--username=root \
	--password=cloudera \
	--table order_details \
        --warehouse-dir /user/hive/warehouse \
        --fields-terminated-by '\t' \
        --hive-import \
        --split-by order_id

---

* Importa los ficheros en hdfs dfs -ls /user/hive/warehouse 
sqoop import \
         -m 1 \
        --connect jdbc:mysql://quickstart:3306/analyst_dualcore \
        --username=root \
        --password=cloudera \
        --table suppliers \
        --fields-terminated-by '\t' \
        --hive-import 

-O-J-O- no funciona

* importa los ficheros utilizando el --option-file indicado

sqoop \
       --options-file /user/cloudera/sqoop/work/import.txt \
       --table orders

el fichero /user/cloudera/sqoop/work/import.txt
import
--connect 
jdbc:mysql://quickstart:3306/retail_db \
--username
retail_dba
--password
cloudera
--target-dir
/mydata/customers

* The target table must already exist in the RDBMS - debe existir en la RDBMS
sqoop export \
        --connect jdbc:mysql://quickstart:3306/retail_db \
	--username=retail_dba \
        --password=cloudera \
        --table categories \
        --export-dir /mydata/categories/

-O-J-O- no funciona

--------------

https://medium.com/google-cloud/moving-data-with-apache-sqoop-in-google-cloud-dataproc-4056b8fa2600
https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#bigquery-import-gcs-file-cli
https://cloud.google.com/bigquery/docs/bq-command-line-tool
https://cloud.google.com/storage/docs/creating-buckets
https://cloud.google.com/dataproc/docs/guides/submit-job
https://cloud.google.com/dataproc/docs/guides/manage-cluster#deleting_a_cluster
https://cloud.google.com/dataproc/docs/concepts/workflows/overview
https://cloud.google.com/sql/docs/mysql/quickstart
https://cloud.google.com/sql/docs/mysql/quickstart#clean-up
https://cloud.google.com/shell/docs/quickstart

https://xrhpimwo2rgtpccrz4jfnnw2ia-dot-europe-west1.dataproc.googleusercontent.com/gateway/default/sparkhistory/?showIncomplete=false

**********************************************************************************************
***********************
*   -O-J-O- CCA 159   *   FIN
***********************
**********************************************************************************************


**********************************************************************************************
***********************
*   -O-J-O- SQL       *  INICIO
***********************
**********************************************************************************************

Orden de ejecución:

               6. SELECT
1. FROM
   2. JOIN ON
      3. WHERE
         4. GROUP BY
            5. HAVING
                  7. ORDER BY
                     8. LIMIT

WHERE es a FROM como 
HAVING es a GROUP BY

?? - ??

Query Clauses - ORDER DE ANALISIS
1. FROM —> Identifies the tables from which to draw data and how the tables should be joined, defines the tables used by a query, along with the means of linking the tables together.
2. SELECT -> Determines which of all possible columns should be included in the query’s result set.
3. GROUP BY —> Used to group rows together by common column values 
4. HAVING —> Filters out unwanted groups 
5. WHERE —> Filters out unwanted data
6. ORDER BY —> Sorts the rows of the final result set by one or more columns

**********************************************************************************************
* -O-J-O- MySQL - CCA175
**********************************************************************************************
cca175-325912:europe-west2:mycloudsql
dirección Estática: 
ipstaticamycloudsql	
35.205.96.244	
europe-west1	
IPv4	
Instancia de VM cca175-325912-m (Zona europe-west1-d)	
Premium	

mycloudsql
My Cloud SQL 01 

Nueva red: MyRedCloud01

Project ID: cca175-325912

Crear instancia, conectar VM, Crear bucket, network.
Tutorial
Connect from a VM instance
https://cloud.google.com/sql/docs/mysql


MySQL 5.7	
Name: cca175network 	
External Address: 34.77.42.105 	      --> https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#CIDR_notation
Region: europe-west1 	
Type: Static 	
Version: IPv4 	
In use by: VM instance cca175-m (Zone europe-west1-b) 	
Network Tier: Premium 
Zone: europe-west1-b 

Network interfaces
Name 	Network 	Subnetwork 	Primary internal IP 	Alias IP ranges 	External IP 	Network Tier IP forwarding 	Network details
nic0 	default 	default 	10.132.0.3 — 	cca175network (34.77.42.105) Premium 	Off
	


Cloud Shell
scelisdev02@cloudshell:
Public IP address: 34.65.193.110
Connection name: cca175-325912:europe-west6:mysql
sudo apt-get update
sudo apt-get install default-mysql-server
scelisdev02@cloudshell:~ (cca175-325912)$ gcloud sql connect mysql --user=root --quiet
password: My SQL 01 *

show databases;
create database bank;
use bank;
MySQL [bank]> source /home/scelisdev02/LearningSQLExample.sql
MySQL [bank]> show tables;
+----------------+
| Tables_in_bank |
+----------------+
| account        |
| branch         |
| business       |
| customer       |
| department     |
| employee       |
| individual     |
| officer        |
| product        |
| product_type   |
| transaction    |
+----------------+

describe account;
describe branch;
describe business;
describe customer;
describe department;
describe employee;
describe individual;
describe officer;
describe product;
describe product_type;
describe transaction;

MySQL [bank]> describe account;
+--------------------+----------------------------------+------+-----+---------+----------------+
| Field              | Type                             | Null | Key | Default | Extra          |
+--------------------+----------------------------------+------+-----+---------+----------------+
| account_id         | int(10) unsigned                 | NO   | PRI | NULL    | auto_increment |
| product_cd         | varchar(10)                      | NO   | MUL | NULL    |                |
| cust_id            | int(10) unsigned                 | NO   | MUL | NULL    |                |
| open_date          | date                             | NO   |     | NULL    |                |
| close_date         | date                             | YES  |     | NULL    |                |
| last_activity_date | date                             | YES  |     | NULL    |                |
| status             | enum('ACTIVE','CLOSED','FROZEN') | YES  |     | NULL    |                |
| open_branch_id     | smallint(5) unsigned             | YES  | MUL | NULL    |                |
| open_emp_id        | smallint(5) unsigned             | YES  | MUL | NULL    |                |
| avail_balance      | float(10,2)                      | YES  |     | NULL    |                |
| pending_balance    | float(10,2)                      | YES  |     | NULL    |                |
+--------------------+----------------------------------+------+-----+---------+----------------+
11 rows in set (0.016 sec)

MySQL [bank]> describe branch;
+-----------+----------------------+------+-----+---------+----------------+
| Field     | Type                 | Null | Key | Default | Extra          |
+-----------+----------------------+------+-----+---------+----------------+
| branch_id | smallint(5) unsigned | NO   | PRI | NULL    | auto_increment |
| name      | varchar(20)          | NO   |     | NULL    |                |
| address   | varchar(30)          | YES  |     | NULL    |                |
| city      | varchar(20)          | YES  |     | NULL    |                |
| state     | varchar(2)           | YES  |     | NULL    |                |
| zip       | varchar(12)          | YES  |     | NULL    |                |
+-----------+----------------------+------+-----+---------+----------------+
6 rows in set (0.015 sec)

MySQL [bank]> describe business;
+-------------+------------------+------+-----+---------+-------+
| Field       | Type             | Null | Key | Default | Extra |
+-------------+------------------+------+-----+---------+-------+
| cust_id     | int(10) unsigned | NO   | PRI | NULL    |       |
| name        | varchar(40)      | NO   |     | NULL    |       |
| state_id    | varchar(10)      | NO   |     | NULL    |       |
| incorp_date | date             | YES  |     | NULL    |       |
+-------------+------------------+------+-----+---------+-------+
4 rows in set (0.014 sec)

MySQL [bank]> describe customer;
+--------------+------------------+------+-----+---------+----------------+
| Field        | Type             | Null | Key | Default | Extra          |
+--------------+------------------+------+-----+---------+----------------+
| cust_id      | int(10) unsigned | NO   | PRI | NULL    | auto_increment |
| fed_id       | varchar(12)      | NO   |     | NULL    |                |
| cust_type_cd | enum('I','B')    | NO   |     | NULL    |                |
| address      | varchar(30)      | YES  |     | NULL    |                |
| city         | varchar(20)      | YES  |     | NULL    |                |
| state        | varchar(20)      | YES  |     | NULL    |                |
| postal_code  | varchar(10)      | YES  |     | NULL    |                |
+--------------+------------------+------+-----+---------+----------------+
7 rows in set (0.014 sec)

MySQL [bank]> describe department;
+---------+----------------------+------+-----+---------+----------------+
| Field   | Type                 | Null | Key | Default | Extra          |
+---------+----------------------+------+-----+---------+----------------+
| dept_id | smallint(5) unsigned | NO   | PRI | NULL    | auto_increment |
| name    | varchar(20)          | NO   |     | NULL    |                |
+---------+----------------------+------+-----+---------+----------------+
2 rows in set (0.014 sec)

MySQL [bank]> describe employee;
+--------------------+----------------------+------+-----+---------+----------------+
| Field              | Type                 | Null | Key | Default | Extra          |
+--------------------+----------------------+------+-----+---------+----------------+
| emp_id             | smallint(5) unsigned | NO   | PRI | NULL    | auto_increment |
| fname              | varchar(20)          | NO   |     | NULL    |                |
| lname              | varchar(20)          | NO   |     | NULL    |                |
| start_date         | date                 | NO   |     | NULL    |                |
| end_date           | date                 | YES  |     | NULL    |                |
| superior_emp_id    | smallint(5) unsigned | YES  | MUL | NULL    |                |
| dept_id            | smallint(5) unsigned | YES  | MUL | NULL    |                |
| title              | varchar(20)          | YES  |     | NULL    |                |
| assigned_branch_id | smallint(5) unsigned | YES  | MUL | NULL    |                |
+--------------------+----------------------+------+-----+---------+----------------+
9 rows in set (0.014 sec)

MySQL [bank]> describe individual;
+------------+------------------+------+-----+---------+-------+
| Field      | Type             | Null | Key | Default | Extra |
+------------+------------------+------+-----+---------+-------+
| cust_id    | int(10) unsigned | NO   | PRI | NULL    |       |
| fname      | varchar(30)      | NO   |     | NULL    |       |
| lname      | varchar(30)      | NO   |     | NULL    |       |
| birth_date | date             | YES  |     | NULL    |       |
+------------+------------------+------+-----+---------+-------+
4 rows in set (0.014 sec)

MySQL [bank]> describe officer;
+------------+----------------------+------+-----+---------+----------------+
| Field      | Type                 | Null | Key | Default | Extra          |
+------------+----------------------+------+-----+---------+----------------+
| officer_id | smallint(5) unsigned | NO   | PRI | NULL    | auto_increment |
| cust_id    | int(10) unsigned     | NO   | MUL | NULL    |                |
| fname      | varchar(30)          | NO   |     | NULL    |                |
| lname      | varchar(30)          | NO   |     | NULL    |                |
| title      | varchar(20)          | YES  |     | NULL    |                |
| start_date | date                 | NO   |     | NULL    |                |
| end_date   | date                 | YES  |     | NULL    |                |
+------------+----------------------+------+-----+---------+----------------+
7 rows in set (0.014 sec)

MySQL [bank]> describe product;
+-----------------+-------------+------+-----+---------+-------+
| Field           | Type        | Null | Key | Default | Extra |
+-----------------+-------------+------+-----+---------+-------+
| product_cd      | varchar(10) | NO   | PRI | NULL    |       |
| name            | varchar(50) | NO   |     | NULL    |       |
| product_type_cd | varchar(10) | NO   | MUL | NULL    |       |
| date_offered    | date        | YES  |     | NULL    |       |
| date_retired    | date        | YES  |     | NULL    |       |
+-----------------+-------------+------+-----+---------+-------+
5 rows in set (0.014 sec)

MySQL [bank]> describe product_type;
+-----------------+-------------+------+-----+---------+-------+
| Field           | Type        | Null | Key | Default | Extra |
+-----------------+-------------+------+-----+---------+-------+
| product_type_cd | varchar(10) | NO   | PRI | NULL    |       |
| name            | varchar(50) | NO   |     | NULL    |       |
+-----------------+-------------+------+-----+---------+-------+
2 rows in set (0.014 sec)

MySQL [bank]> describe transaction;
+---------------------+----------------------+------+-----+---------+----------------+
| Field               | Type                 | Null | Key | Default | Extra          |
+---------------------+----------------------+------+-----+---------+----------------+
| txn_id              | int(10) unsigned     | NO   | PRI | NULL    | auto_increment |
| txn_date            | datetime             | NO   |     | NULL    |                |
| account_id          | int(10) unsigned     | NO   | MUL | NULL    |                |
| txn_type_cd         | enum('DBT','CDT')    | YES  |     | NULL    |                |
| amount              | double(10,2)         | NO   |     | NULL    |                |
| teller_emp_id       | smallint(5) unsigned | YES  | MUL | NULL    |                |
| execution_branch_id | smallint(5) unsigned | YES  | MUL | NULL    |                |
| funds_avail_date    | datetime             | YES  |     | NULL    |                |
+---------------------+----------------------+------+-----+---------+----------------+
8 rows in set (0.014 sec)

-- campos from account;
account_id, product_cd, cust_id, open_date , close_date, last_activity_date, status, open_branch_id, open_emp_id, avail_balance, pending_balance

-- campos from branch;
branch_id, name, address, city, state, zip

-- campos from business;
cust_id, name, state_id, incorp_date

-- campos from customer;
cust_id, fed_id, cust_type_cd, address, city, state, postal_code

-- campos from department;
dept_id, name

-- campos from employee;
emp_id, fname, lname, start_date, end_date, superior_emp_id, dept_id, title, assigned_branch_id

-- campos from individual;
cust_id, fname, lname, birth_date

-- campos from officer;
officer_id, cust_id, fname, lname, title, start_date, end_date

-- campos from product;
product_cd, name, product_type_cd, date_offered, date_retired

-- campos from product_type;
product_type_cd, name

-- campos from transaction;   
txn_id, txn_date, account_id, txn_type_cd, amount, teller_emp_id, execution_branch_id, funds_avail_date

Cloud Storage
bucket:  scdev2bucket

https://console.cloud.google.com/storage/browser/scdev2bucket
gs://scdev2bucket

Allowlisting your IP for incoming connection for 5 minutes...done.
Connecting to database with SQL user [root].Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 1188
Server version: 5.7.32-google-log (Google)
Copyright (c) 2000, 2021, Oracle and/or its affiliates.
Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> help

For information about MySQL products and services, visit:
   http://www.mysql.com/
For developer information, including the MySQL Reference Manual, visit:
   http://dev.mysql.com/
To buy MySQL Enterprise support, training, or other products, visit:
   https://shop.mysql.com/

List of all MySQL commands:
Note that all text commands must be first on line and end with ';'
?         (\?) Synonym for `help'.
clear     (\c) Clear the current input statement.
connect   (\r) Reconnect to the server. Optional arguments are db and host.
delimiter (\d) Set statement delimiter.
edit      (\e) Edit command with $EDITOR.
ego       (\G) Send command to mysql server, display result vertically.
exit      (\q) Exit mysql. Same as quit.
go        (\g) Send command to mysql server.
help      (\h) Display this help.
nopager   (\n) Disable pager, print to stdout.
notee     (\t) Don't write into outfile.
pager     (\P) Set PAGER [to_pager]. Print the query results via PAGER.
print     (\p) Print current command.
prompt    (\R) Change your mysql prompt.
quit      (\q) Quit mysql.
rehash    (\#) Rebuild completion hash.
source    (\.) Execute an SQL script file. Takes a file name as an argument.
status    (\s) Get status information from the server.
system    (\!) Execute a system shell command.
tee       (\T) Set outfile [to_outfile]. Append everything into given outfile.
use       (\u) Use another database. Takes database name as argument.
charset   (\C) Switch to another charset. Might be needed for processing binlog with multi-byte charsets.
warnings  (\W) Show warnings after every statement.
nowarning (\w) Don't show warnings after every statement.
resetconnection(\x) Clean session context.
query_attributes Sets string parameters (name1 value1 name2 value2 ...) for the next query to pick up.

Para crear la db y las tablas: 
1. importar el dbsetup.sql sólo con la creación de las tablas.
source /home/cloudera/training_materials/analyst/data/mysql/dbsetup.sql

2. importar cada fichero csv (,) de datos a cada tabla, uno a uno.

mysql> show databases;
mysql> use analyst_dualcore;
mysql> show tables;
mysql> describe employees;
mysql> DROP TABLE IF EXISTS analyst_dualcore.orders;

-O-J-O- MySQL
**********************************************************************************************
**********************************************************************************************
Detalles de instancia de VM
Respuesta de REST equivalente
Esta es la respuesta original de REST desde el servicio de Google Compute Engine.
{
  "cpuPlatform": "Intel Haswell",
  "creationTimestamp": "2021-02-21T02:32:54.505-08:00",
  "deletionProtection": false,
  "disks": [
    {
      "autoDelete": true,
      "boot": true,
      "deviceName": "persistent-disk-0",
      "diskSizeGb": "100",
      "guestOsFeatures": [
        {
          "type": "VIRTIO_SCSI_MULTIQUEUE"
        },
        {
          "type": "UEFI_COMPATIBLE"
        }
      ],
      "index": 0,
      "interface": "SCSI",
      "kind": "compute#attachedDisk",
      "licenses": [
        "projects/cloud-dataproc/global/licenses/dataproc"
      ],
      "mode": "READ_WRITE",
      "source": "projects/cca175-325912/zones/europe-west1-d/disks/cca175-325912-m",
      "type": "PERSISTENT"
    }
  ],
  "fingerprint": "hHR1GXVNFw4=",
  "id": "6640626824204224571",
  "kind": "compute#instance",
  "labelFingerprint": "e-BD4pRNKyw=",
  "labels": {
    "goog-dataproc-cluster-name": "cca175-325912",
    "goog-dataproc-cluster-uuid": "bc4ef432-ced4-4d37-8851-cf1256b6da40",
    "goog-dataproc-location": "europe-west1"
  },
  "lastStartTimestamp": "2021-02-27T02:40:02.154-08:00",
  "lastStopTimestamp": "2021-02-27T02:30:05.986-08:00",
  "machineType": "projects/cca175-325912/zones/europe-west1-d/machineTypes/n1-standard-8",
  "metadata": {
    "fingerprint": "5zJYcaCfo5Q=",
    "items": [
      {
        "key": "dataproc-bucket",
        "value": "dataproc-staging-europe-west1-198053966411-pf7lws8o"
      },
      {
        "key": "dataproc-cloud-logging-enabled",
        "value": "false"
      },
      {
        "key": "dataproc-cluster-configuration-directory",
        "value": "gs://dataproc-staging-europe-west1-198053966411-pf7lws8o/google-cloud-dataproc-metainfo/bc4ef432-ced4-4d37-8851-cf1256b6da40/"
      },
      {
        "key": "dataproc-cluster-name",
        "value": "cca175-325912"
      },
      {
        "key": "dataproc-cluster-properties",
        "value": "ZIPPY:eJyNV11v2zgQfPevMNDXEysp/qoBPbiNe8ldYgdWergeDBgURcusRVIlKcfOr7+lJCd2TDVFAdlJl9Ts7szs5sOXvNSGqm6hZEGVYVR3PsSl6H6lSTcMun44vgrH/bD7ED92Qz8MOik2GILJcnz8huCxPyCcUWEQFWkhmTDRxphCL8cfP9LSXu09UW0C9HKG5LJMUSZlllNEJP/ouliTDU3LnCrE8d4jUpBSKXiL90MmOgoHnQNWwqNitxx/nyxmq9n8eno/mU3+nC5WN9PJQ3z73zTq+b7/HmpNtWZwvZRbRqPVXqu168iGYmUSig16kmoLsNaK/iypIAe4gURe0OEQSNPluP4sCUXNBylKtCNSUR2d1rB5fYHVFqWK7eBOygtzQD+wiowqqQsGFMIomSOD9RZVCLRBUHOqdjhHnOU501G/SptpQ4pTOPAN7t5hJAujI+9fvh8ORtz9Fl5IAfA0wsSwHTY02gq571aF8yrknQp53YH4YbL4e3U9md7PZ6t7eC6+V7XnddByXGdJ95SURipUF6P3UjLbS4SLAr2CxRzS07JUFngSXfnDsLNJ10AreCCBORUypVB7tWMQssEitWQhsgQCBn5Fj+bi4z0cC0CukD3YfPcyhQldl7mXUkias4oLnmGcytJ40FkdjQaQivvd0I0UqwOyhPdwmsKbdOSj6t9y/Gk0GP1uisAR74Ujvz5jcUK23ksXw15/yNvpxymXgPFYxMs4UNQGyGKDFKQERAROw4+AKseQkFQZoMAgSChzKuUpnl2INhrd1MdjaAZVd3RH8zSJDbAmttfEdY/cXJiKHZo/TGef7ybxavbtfvV4s5hOruPIJSjL4DfpONiL81w+oWeqZKNVXcvptYU2tGqhq2cX1rKYxvNviy8t9nJCtBPPYoLxknsWCsHGcgoAB37YO69CyVAhlYn88183dgDOt6C6zE3Mnml0Ner5TrVyKRhUkokM2obJ9mgmAic5jdY41y21B+OAA4IA6Y68qNO+mVzP5w+rv+afVze38SPIOZ4u/rlI3aGJcx2GZ+U50d1l4Uc+eLqjQ9Uscfap77C4C8pXJf+lnXNsh+AbO7/yWypW3xwFQTAMuAsvK4gT7bB9DiXQM5idiKXRXm0Kxp9kqDJTEKKeez/WQjyFDLfr+9zS28zg0vzryJb+uIxp9L4JaqcLfmprrLNSPSevWhx26HcsuOUYgjONEpDbFmkrl+CqFwbDYThqAhrr0jpvlJHa8htJZK6jx7t4F/xRPdHx0zlvcramORNQ/IK84ElYZtMia8OIxy2X+++uHTWGBAtBm2nv7lnL8nC6EGiiDoWxt+9Y+ir8N573dmS+k0DwLjNqKVhfs0watKvRsXOciasec/yorUGLzcGYghCjGNFHy3vdfKxoB86in21MTEAwSytHdq9NG/BOIAs80ZoasqnPwSVgqXY3iAQsRifvsaiO09NILwNwDb1ad7iaBkWZ5AyMSGpjW/O6NLd7gJdK47l36nqbLqG3Nl9LMFgTTl5e1TiXGexumaJZXYAzoM69oGmhjobnLTuZdECN6OvkdvH+Et8QBkCmIOFPv6eQJ5po0DQ1F0P8zBucxjM8VYxmvAD1MtC9JkACmJanQnkz8x9v76d3t7Opc+65SwU/wRxtE+krB7VRFNielqruQk3dU+7C/xaKcTDVlRT5wTXC9c8ckUT+fguRhqUIpr2qmFHk1NC0Qgyd6J95KGgMWyyIVCtf+xpx3FJq2YKufd6yDeG9YxuqXYPYzZKZg/cSfnH8GIKUlAaldI1hJUJSgddBF71Cgo5skZhq+aPnzTpwWabaGyTSsIvZsLA/6PwPoZJ4XA=="
      },
      {
          "networkTier": "PREMIUM",
          "type": "ONE_TO_ONE_NAT"
        }
      ],
      "fingerprint": "OG9ZDxet2CY=",
      "kind": "compute#networkInterface",
      "name": "nic0",
      "network": "projects/cca175-325912/global/networks/default",
      "networkIP": "10.132.0.2",
      "subnetwork": "projects/cca175-325912/regions/europe-west1/subnetworks/default"
    }
  ],
  "scheduling": {
    "automaticRestart": true,
    "onHostMaintenance": "MIGRATE",
    "preemptible": false
  },
  "selfLink": "projects/cca175-325912/zones/europe-west1-d/instances/cca175-325912-m",
  "serviceAccounts": [
    {
      "email": "198053966411-compute@developer.gserviceaccount.com",
      "scopes": [
        "https://www.googleapis.com/auth/bigquery",
        "https://www.googleapis.com/auth/bigtable.admin.table",
        "https://www.googleapis.com/auth/bigtable.data",
        "https://www.googleapis.com/auth/cloud.useraccounts.readonly",
        "https://www.googleapis.com/auth/devstorage.full_control",
        "https://www.googleapis.com/auth/devstorage.read_write",
        "https://www.googleapis.com/auth/logging.write"
      ]
    }
  ],
  "shieldedInstanceConfig": {
    "enableIntegrityMonitoring": true,
    "enableSecureBoot": false,
    "enableVtpm": true
  },
  "shieldedInstanceIntegrityPolicy": {
    "updateAutoLearnPolicy": true
  },
  "startRestricted": false,
  "status": "RUNNING",
  "tags": {
    "fingerprint": "42WmSpB8rSM="
  },
  "zone": "projects/cca175-325912/zones/europe-west1-d"
}

Detalles de instancia de VM
**********************************************************************************************
**********************************************************************************************
Vbox - cloudera-quickstart-vm-5.13.0-0-virtualbox

en la MVbox hice una mezcla de dos db (analyst_dualcore + retail_db) pero no funciono.. jeje los campos son diferentes

+---------------------+
| Tables_in_retail_db |
+---------------------+
| categories          |
| customers           |
| departments         |
| order_items         |
| orders              |
| products            |
+---------------------+

+----------------------------+
| Tables_in_analyst_dualcore |
+----------------------------+
| customers                  |
| employees                  |
| order_details              |
| orders                     |
| products                   |
| suppliers                  |
+----------------------------+

/user/hive/warehouse/categories
*/user/hive/warehouse/customers 
/user/hive/warehouse/departments
/user/hive/warehouse/order_items
/user/hive/warehouse/orders
*/user/hive/warehouse/products

/user/hive/warehouse/employees
/user/hive/warehouse/order_details
*/user/hive/warehouse/customers
*/user/hive/warehouse/orders
*/user/hive/warehouse/products
/user/hive/warehouse/suppliers

* para que no diera error de ya existia el fichero: 
        --target-dir /user/hive/warehouse/products2 \

Vbox - cloudera-quickstart-vm-5.13.0-0-virtualbox 
**********************************************************************************************
-----------------------------------------------------------------------------
Esto lo hice por probar el tema de hive creando una instancia, pero no es necesario
y aún así algo no funciono del todo.. ver en otro momento...

Hive on console.cloud.google.com

Instance ID: hive-metastore 
Type: MySQL 5.7	
Public IP address: 34.140.187.86 
Instance connection name: cca175-325912:europe-west1:hive-metastore
High availability: ENABLED
Location: europe-west1-b
	
https://cloud.google.com/architecture/using-apache-hive-on-cloud-dataproc?hl=en&_ga=2.129686412.-187909411.1631526784&_gac=1.47501781.1631801377.Cj0KCQjw1ouKBhC5ARIsAHXNMI92RZAl9diZlvijPBdMvCXxbLGON3X6tB9ijc77a2DMU5-437m5AZoaAsaHEALw_wcB#initialize_the_environment

scelisdev02@cloudshell:~ (cca175-325912)$ export REGION=europe-west1
scelisdev02@cloudshell:~ (cca175-325912)$ export ZONE=europe-west1-b
scelisdev02@cloudshell:~ (cca175-325912)$ gcloud config set compute/zone ${ZONE}
scelisdev02@cloudshell:~ (cca175-325912)$ gcloud config set account scelisdev02@gmail.com
scelisdev02@cloudshell:~ (cca175-325912)$ gcloud config set project cca175-325912
scelisdev02@cloudshell:~ (cca175-325912)$ gcloud services enable dataproc.googleapis.com sqladmin.googleapis.com
Operation "operations/acf.p2-615171933800-233dfb35-28dd-4a97-80d9-24bc86043b85" finished successfully.
scelisdev02@cloudshell:~ (cca175-325912)$ export PROJECT=$(gcloud info --format='value(config.project)')
scelisdev02@cloudshell:~ (cca175-325912)$ gsutil mb -l ${REGION} gs://${PROJECT}-warehouse
Creating gs://cca175-325912-warehouse/...
scelisdev02@cloudshell:~ (cca175-325912)$
scelisdev02@cloudshell:~ (cca175-325912)$ gcloud sql instances create hive-metastore \
--database-version="MYSQL_5_7" \
--activation-policy=ALWAYS \
--zone ${ZONE}
Created [https://sqladmin.googleapis.com/sql/v1beta4/projects/cca175-325912/instances/hive-metastore].
NAME: hive-metastore
DATABASE_VERSION: MYSQL_5_7
LOCATION: europe-west1-b
TIER: db-n1-standard-1
PRIMARY_ADDRESS: 34.140.187.86
PRIVATE_ADDRESS: -
STATUS: RUNNABLE
scelisdev02@cloudshell:~ (cca175-325912)$Creating Cloud SQL instance...working...

Querying Hive with Beeline
scelisdev02@cloudshell:~ (cca175-325912)$ gcloud compute ssh hive-cca175-325912-m
WARNING: The private SSH key file for gcloud does not exist.
WARNING: The public SSH key file for gcloud does not exist.
WARNING: You do not have an SSH key for gcloud.
WARNING: SSH keygen will be executed to generate a key.
This tool needs to create the directory [/home/scelisdev02/.ssh]
before being able to generate SSH keys.

Do you want to continue (Y/n)?  Y

Generating public/private rsa key pair.
Enter passphrase (empty for no passphrase):     - le dí enter
Enter same passphrase again:
Your identification has been saved in /home/scelisdev02/.ssh/google_compute_engine.
Your public key has been saved in /home/scelisdev02/.ssh/google_compute_engine.pub.
The key fingerprint is:
SHA256:4p70d193SqAL5w6c98yBu06hiM1aQIT7WLTSY8nI944 scelisdev02@cs-259003285496-default-boost-g29vg
The key's randomart image is:
+---[RSA 2048]----+
|   ..            |
|  ...            |
| . *.o           |
|  =.X            |
|   B.o. S . .    |
|  . .*.+ o + .   |
|    .oB * * . . +|
|    E=.o B.*.o oo|
|    . o .+Bo+.o  |
+----[SHA256]-----+
Did you mean zone [europe-west1-c] for instance:
[hive-cca175-325912-m] (Y/n)?  n

Unable to find an instance with name [hive-cca175-325912-m].
For the following instance:
 - [hive-cca175-325912-m]
choose a zone:
 [1] asia-east1-a
 [2] asia-east1-b
 [3] asia-east1-c
 [4] asia-east2-a
 [5] asia-east2-b
 [6] asia-east2-c
 [7] asia-northeast1-a
 [8] asia-northeast1-b
 [9] asia-northeast1-c
 [10] asia-northeast2-a
 [11] asia-northeast2-b
 [12] asia-northeast2-c
 [13] asia-northeast3-a
 [14] asia-northeast3-b
 [15] asia-northeast3-c
 [16] asia-south1-a
 [17] asia-south1-b
 [18] asia-south1-c
 [19] asia-south2-a
 [20] asia-south2-b
 [21] asia-south2-c
 [22] asia-southeast1-a
 [23] asia-southeast1-b
 [24] asia-southeast1-c
 [25] asia-southeast2-a
 [26] asia-southeast2-b
 [27] asia-southeast2-c
 [28] australia-southeast1-a
 [29] australia-southeast1-b
 [30] australia-southeast1-c
 [31] australia-southeast2-a
 [32] australia-southeast2-b
 [33] australia-southeast2-c
 [34] europe-central2-a
 [35] europe-central2-b
 [36] europe-central2-c
 [37] europe-north1-a
 [38] europe-north1-b
 [39] europe-north1-c
 [40] europe-west1-b
 [41] europe-west1-c
 [42] europe-west1-d
 [43] europe-west2-a
 [44] europe-west2-b
 [45] europe-west2-c
 [46] europe-west3-a
 [47] europe-west3-b
 [48] europe-west3-c
 [49] europe-west4-a
 [50] europe-west4-b
Did not print [35] options.
Too many options [85]. Enter "list" at prompt to print choices fully.
Please enter your numeric choice:  40

ERROR: (gcloud.compute.ssh) Could not fetch resource:
 - The resource 'projects/cca175-325912/zones/europe-west1-b/instances/hive-cca175-325912-m' was not found

scelisdev02@cloudshell:~ (cca175-325912)$ gcloud compute ssh hive-metastore
scelisdev02@cloudshell:~ (cca175-325912)$ gcloud sql connect hive-metastore --user=root --quiet


scelisdev02@cca175-m:~$ beeline -u "jdbc:hive2://localhost:10000"
Connecting to jdbc:hive2://localhost:10000
Connected to: Apache Hive (version 2.3.7)
Driver: Hive JDBC (version 2.3.7)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 2.3.7 by Apache Hive
0: jdbc:hive2://localhost:10000> 

-----------------------------------------------------------------------------
SCALA - sc
-----------------------------------------------------------------------------

Scala es un lenguaje escalable que se ejecuta en la máquina virtual Java (JVM)
- Interopera con Java, pero tiene una sintaxis mucho más concisa
- Soporta programación imperativa, orientada a objetos y funcional
- Diseñado para ayudar a los desarrolladores a escribir mejor código en menos tiempo
Puede utilizarse tanto para el análisis exploratorio como para el operativo
- El REPL, or (Read-Evaluate-Print Loop), o shell, de Scala es útil para el trabajo interactivo
- También se puede compilar y empaquetar el código Scala, como con Java
Apache Spark es un marco de procesamiento distribuido de código abierto
- Spark está escrito en Scala
- Proporciona APIs de Scala para desarrollar tus propios programas Spark
- Permite escalar tu programa sin necesidad de cambiar tu código

What is Scala?
- Martin Odersky started developing Scala in 2001
  – Sca (scalable) La (language)
- Design goals
  – Eliminate common distributed processing bugs
  – Assign scale decisions to the programming framework
  – Enable the programmer to take control when necessary
- Strongly influenced by functional programming style
  – Pure functions
  – Immutable data
  – Implicit looping and iteration

Scala and Java
- Scala is a superset of Java
  – Scala compiles to Java bytecode
  – Runs on the Java Virtual Machine (JVM)
- It is interoperable with Java programs and libraries
  – All Java programs can be ported to Scala
  – Some Scala programs, however, cannot directly be ported to Java
- Scala is more expressive than Java
  – Concise code
  – Implicit processing

Features and Benefits of Scala
- Strong typing in Scala allows many errors to be caught at compile time
  – Such errors aren’t evident until runtime in some languages
- Integration with popular programming tools
  – Integrated Development Environments (IDEs)
  – Unit test frameworks
- Support for multiple programming paradigms, including
  – Imperative
  – Object-oriented
  – Functional

The Promise of Functional Programming
- Goals of functional programming languages like Scala, LISP, and Haskell
  – Fewer programming errors
  – Better modularity
  – Higher-level abstractions
  – Shorter, more concise code
  – Increased developer productivity

Functional Programming Adoption
- Functional programming has long been popular in academia
  – It is now also becoming popular in industry
- Software complexity was the catalyst needed for adoption
  – Driven by need for multi-core and distributed programming

Software Challenges
- Triple challenge
1. Parallel: - How to make use of multicores, GPUs, clusters?
2. Async: - How to deal with asynchronous events?
3. Distributed - How to deal with delays and failures?
- Mutable state is a liability in each of the above challenges
  – Scala encourages the use of immutable variables

Programming at Scale
- Some enterprises still process large amounts of data serially
  – Divide the data into subsets
  – Process a subset locally
  – Repeat the process until all the data has been processed
  – This painfully slow approach is driven by tools that do not scale
- What would a system look like that could scale?
  – Write the program one time and never have to re-engineer it
  – The same program would run
    – On one CPU or on thousands of CPUs
    – Serially on one server or in parallel across a cluster of servers

The Spark Distributed Processing Framework
- Apache Spark is an open source distributed processing framework
  – Includes an engine for parallel processing of data across many servers
  – Provides an elegant model for writing concise programs
- The Spark framework parallelizes the code you write using its APIs
  – Distributes and executes that code across a group of servers
  – This group of servers is known as a cluster
  – Spark scales linearly as more servers are added to the cluster
- Apache Spark originated at UC Berkeley AMPLab
  – It was contributed to the Apache Software Foundation

Spark in a Nutshell
- Programs written in Spark can process huge amounts of data efficiently
  – The underlying storage system is commonly HDFS
  – The Spark framework is fault tolerant and resilient
- Spark’s in-memory approach to processing provides a performance boost over other distributed frameworks
- Spark runs on top of the Java Virtual Machine (JVM)
  – This makes it possible to use debugging tools built for the Java stack

- Scala programs call the Spark API
- Spark API talks to the cluster to
  – Run programs
  – Perform file I/O

Overview of Big Data Analysis
- Successful analysis of Big Data requires many steps
  – Pre-processing the data to clean and format it (ETL)
  – Profiling data to understand the characteristics of the dataset
  – Iterating over data to achieve the desired results
  – Rebuilding analytical models as needs change
  – Processing all of the data, rather than being limited to a subset
- Apache Spark helps to support these needs
  – Scala is widely used for writing Spark applications

ETL and Pre-Processing Data
- We can use Scala code in Spark for ETL and other pre-processing tasks
  – Clean data
  – Format data
  – Fuse columns
  – Convert data

Profiling Data
- We profile data with Scala and Spark because
  – Big Data is too large for a human to inspect manually
  – Data can be messy, since it may lack rigid structure
  – It helps us understand the nature of the data in each column
  – It is useful to determine the ranges or length of values in each column

Iterating over Data
- Spark is efficient for algorithms that iterate over data
  – Modeling and analysis may require multiple passes for the same data
  – Some algorithms require iteration in order for results to converge
  – Features in Scala support writing iterative programs for Spark

Rebuilding Models
- Analytical models are used in services that inform real-world decisions
- Coding models in Scala for Spark provides flexibility
  – Models can easily be revised to adapt to changing conditions
  – They may be rebuilt periodically, even in near real-time

Putting Solutions into Production
- Historically, experiments were performed using a language like R
  – The resulting solutions were re-written in C++ or Java for production
- Goal: deploy code used for running experiments directly to production
  – Spark makes this possible

Why Write Your Spark Application in Scala?
- Spark is written in Scala, making Scala a top-tier language for Spark
  – Performance benefits in using Spark’s native language
  – New Spark features are typically available for Scala well before Python
- Using Scala helps you to understand the Spark philosophy
  – Leads to using the Spark platform more effectively
- You will learn to write programs in Scala during this course
  – Another course will teach you to use Scala code to call the Spark APIs

One Framework for Exploration and Operation
- Code for Spark is most often written in Scala or Python
  – Spark provides APIs for both of these languages
- Spark supports both exploratory and operational analytics
- Use the Spark REPL, or (Read-Evaluate-Print Loop), or shell, for exploratory analytics
  – Allows you to run code interactively
- Compile Spark code for operational analytics
  – Write production-grade analytics in Spark

What about Java and Spark?
- Spark also provides a Java API, but there is no Java-based REPL, or (Read-Evaluate-Print Loop)
- Using the same language for exploration and operational deployment is advantageous
  – This advantage is available with Scala and Python
  – Java only supports operational deployment

Key Scala Characteristics
- Scala code is written as code blocks
  – Scala code blocks are expressions that return a value
  – In Scala, all code-blocks return a value, even if the value is “nothing”
- Functions are fundamental to Scala
  – They can take the place of a typical variable
- Functions can be passed as parameters to other functions
  – Example: function1(function2)
  – The call to function1 passes function2 as a parameter
  – function1 is called a higher-order function - Funcion de orden superior - funciones que pueden aceptar funciones como parámetros

    Programmer                                                      Platform

+------------------------+          +----------------+
|                        |          | open file      |
| Imperative Programming |          | while (!EOF) { |          +------------------+
| - Issue commands       |   ===>>> |   read line    |  ===>>>  | Execute commands |
|                        |          |   print line   |          +------------------+
+------------------------+          | }              |
                                    | close file     |
                                    +----------------+

+------------------------+          +----------------+          +-------------------------+
|                        |          |                |          | Use intelligence about  |
| Functional Programming |          | read file      |          | the platform to...      |
| - Define requirements- |   ===>>> | foreach {write}|  ===>>>  |  - read                 |
|                        |          |                |          |  - iterate              |
+------------------------+          +----------------+          +-------------------------+

- Functional programming languages like Scala
  – Are inspired by math
  – Encourage - (fomentan) pure functions
  – Discourage - (desaconsejar) functions with side effects (otros efectos)
    - Modificar el estado, de una variable global o un argumento
    - Provoca una excepción
    - Escribe datos en una pantalla o archivo
    - Lee datos
    - Llama a otras funciones que causan efectos secundarios
    Por lo tanto el comportamiento y los resultados del programa pueden depender del orden de evaluación
- Usa variables inmutables y programación funcional tanto como sea posible para aprovechar los beneficios de Scala

- A pure function
  – Consistently computes the same result given the same inputs - calcula el mismo resultado con las mismas entradas
  – Returns a result determined only by its input values - devuelve un resultado determinado SOLO por los valores de entrada
  – Has no observable side effects, such as mutation or output of data - no tiene mutaciones
  – Provides deterministic results, making it easy to write test cases

Differences between Immutable and Mutable Variables
- Scala variables are declared as either mutable or immutable
  – val (value) – immutable
  – var (variable) – mutable

- Pure functions are preferred
  – Use immutable variables whenever possible
  – Scala controls scope to limit changes to mutable data

- How does Scala make iteration over the elements in a collection scalable?
- Scala uses a method uncommon in other languages
  – It creates implicit variables to pass data between parts of an expression
       - Crea variables implícitas para pasar datos entre partes de una expresión
  – Eliminates the need to write your own counters and state variables
       - Elimina la necesidad de escribir sus propios contadores y variables de estado
- Removes a common source of bugs in distributed systems
  – These implicit variables cannot be addressed or changed
      - Estas variables implícitas no pueden ser abordadas o modificadas
- Allows the framework to optimize execution
  – The framework can determine how the iteration is implemented

Imperative Programming

file = open("loudacre.log")
while not EOF {
  line = file.readline()
  print line
}
file.close()

Functional Programming - https://www.coursera.org/learn/scala-functional-programming

Source.fromFile("loudacre.log").foreach(print)


* Scala Characteristics
- In Scala, flow is implied from the order of chained expressions - el flujo está implícito en el orden de las expresiones encadenadas
- Cada una de ellas está conectada por un punto
  – Each of these is connected by a period
- Iteration over a collection is often implicit, as seen with foreach - iteracion está implicita

- You can work with Scala in two ways
1. Interactively, using the Scala shell
2. By executing compiled Scala programs, similar to working with compiled Java programs

- The Scala shell is referred to as a REPL, or (Read-Evaluate-Print Loop)
  – When you enter an expression, Scala immediately evaluates it, assigns the value to an implicit variable, and prints it to the console

- Start the Scala shell from the OS command line by typing: scala

The Scala shell offers tab completion
– Type a partial keyword, class, method, or variable name
– Hit the [TAB] key to see possible completions

Command     - Description
:help       - Get a list of commands or help on a specific command
:history    - Show previous commands
:h? string  - Search the command history for string
:quit       - Exit the shell
:sh         - Run a shell command or script in the operating system shell
:load path  - Load and execute Scala source code from a file at path

- Scala creates result variables automatically
  – res0, res1, res2,… resN

scala> val myInt = 123
myInt: Int = 123

scala> myInt
res0: Int = 123

scala> println(res0)
123

Basic Keyboar Input
- The scala.io.StdIn.readLine function reads input from the user

Use the built-in print() function
print(variable)

Formatted Printing
- Print formatted string:
  – formatStrName.format(var1, var2, var3…)
  – Formatting: %d for decimal integer, %f for float

Basic File I/O
- Import the scala.io.Source library
- Use Source.fromFile
  – Returns a variable of type scala.io.BufferedSource

Compiling and Executing Scala Programs
- Edit code in a file using a text or graphical editor, or an IDE such as Eclipse
  – The program file must declare an object with a main method
  – Scala files have a.scala extension
- Compile to a .class file with scalac command
- Execute the class using the scala command

Accessing Data in Files
- Scala tiene muchas bibliotecas, incluyendo scala.io.Source que proporciona métodos para el acceso a archivos
- The data for this course is in a directory that you can easily access using the Linux environment variable, $SCADATA
- Use $SCADATA in your Scala program with the envOrElse method in the Scala Properties library
- In the example below, we set a Scala variable named datadir to the path in $SCADATA
  – If $SCADATA is undefined we set datadir to the current directory
    val datadir = scala.util.Properties.envOrElse("SCADATA", ".")

Declaring a Variable
- We did not specify a type for the variable
  – Based on the context, Scala considers datadir to be a String
  – This feature of Scala is known as type inference - inferencia de tipos

val datadir = scala.util.Properties.envOrElse("SCADATA", ".")

datadir: String = /home/training/training_materials/jes/data

Basic I/O with Scala
- Scala comes with many libraries
- Use the import command to access the Source library

import scala.io.Source

- Use the fromFile method in the Source library to get a reference to the Loudacre log file
- Iterate through each line using the foreach method
- Output each line of the file using the print method

Source.fromFile(datadir + "/loudacre.log").foreach(print)

https://www.scala-lang.org/
https://learning.oreilly.com/library/view/programming-scala-2nd/9781491950135/
https://www.manning.com/books/scala-in-action
https://horstmann.com/scala/
https://www.lightbend.com/ebooks
https://www.coursera.org/course/progfun

Scala Variables: Mutability
- Variables must be initialized when declared
- Variables are either mutable or immutable
  – Mutable: can reassign a value of the same type
    – Syntax: var name: type = value
  – Immutable: value cannot be reassigned after initialization
    – Syntax: val name: type = value

scala> val phoneModel: Int = 3
phoneModel: Int = 3

scala> phoneModel
   val phoneModel: Int

scala> phoneModel=4
<console>:13: error: reassignment to val
       phoneModel=4
                 ^

Types may either be explicitly declared or inferred
  – Scala makes a best guess based on assignment
  – You can also explicitly declare the type

- Type inference - inferencia de tipos

scala> var phoneModel = 3
phoneModel: Int = 3

scala> phoneModel=4
phoneModel: Int = 4

- Explicit typing

scala> var phoneModel: Short = 3
phoneModel: Short = 3

- Variables are statically typed
  – Scala does not support dynamic typing
  – The type is established on first use and never reassigned
  – Using the same variable name with a different type will cause an error

scala> var phoneModel = 3
phoneModel: Int = 3

scala> phoneModel = "iFruit 9000"
<console>:13: error: type mismatch;
 found   : String("iFruit 9000")
 required: Int
       phoneModel = "iFruit 9000"
                    ^
Redefining Variables
- Although you cannot reassign an immutable variable, or assign a new type to any defined variable, you can redefine a variable

scala> var phoneModel = 3
phoneModel: Int = 3

scala> phoneModel
   var phoneModel: Int

scala> phoneModel
res1: Int = 3

scala> var phoneModel = "iFruit 9000"
phoneModel: String = iFruit 9000

Some Important Types
- The table below shows examples of common data types in Scala
+------------------------------------------------------------+
|Type    | Description            | Example                  |
+------------------------------------------------------------+
|Byte    | 8-bit signed integer   | 3                        |
|Short   | 16-bit signed integer  | 32                       |
|Int     | 32-bit signed integer  | 327                      |
|Long    | 64-bit signed integer  | 32754L                   |
|Double  | 8-byte floating point  | 3.1415                   |
|Float   | 4-byte floating point  | 3.1415F                  |
|Char    | Single character       | 'c' (single quotes)      |
|String  | Sequence of characters | "iFruit" (double quotes) |
|Boolean | Either true or false   | true (case sensitive)    |
+------------------------------------------------------------+

Special Unit Type
- Unit
  – When a function passes back “nothing” in Scala, it passes back Unit - Cuando una función devuelve "nada" en Scala, devuelve Unit
  – This is equivalent to the void return type in a Java method
  – There is only one Unit in Scala; it is non-instantiable

scala> val myreturn = println("Hello, world")
Hello, world
myreturn: Unit = ()

Any Type and Explicit Casting of Type
- Any
  – Used when Scala cannot determine which specific type to use - Se utiliza cuando Scala no puede determinar qué tipo específico utilizar
  – Can be cast to a specific type using the method asInstanceOf[type] - Casting

scala> val myreturn = if (true) "hi"
myreturn: Any = hi

scala> val mystring = myreturn.asInstanceOf[String]
mystring: String = hi

scala> myreturn
res2: Any = hi

scala> mystring
res3: String = hi

Interrogating Variables with getClass
- Use getClass to determine the class of an object
- The code below also shows that getClass is an example of an arity-0 function
  – The arity of a function is the number of parameters it accepts

Arity-0 methods that are purely functional (cause no side effects), can be called without empty parentheses

scala> val PhoneStyle: Char = 'd'
PhoneStyle: Char = d

scala> PhoneStyle.getClass
res4: Class[Char] = char

Calling with empty parentheses will also work

scala> PhoneStyle.getClass()
res5: Class[Char] = char

Numeric Variables and Arithmetic
- Scala does not have operators, per se
  – It uses operator notation to invoke methods
- In most cases, Scala determines operator precedence based on the first character of the methods used in operator notation
  – For example, the * character has higher precedence than +

scala> val ab = 1 + 2 * 3
ab: Int = 7

- Scala supports assignment operators
  – ab += bc is equivalent to ab = ab + bc

- The table shows precedence in decreasing order
  – Precedence is based on the first character of a method
- Characters on the same line have equal precedence

+----------------------------------+
|       Operator Precedence        |
+----------------------------------+
|   (all other special characters) |
|               *                  |
|               /                  |
|               +                  |
|               %                  |
|               -                  |
|               :                  |
|               =                  |
|               !                  |
|               < >                |
|               &                  |
|               ^                  |
|               |                  |
|         (all letters)            |
|   (all assignment operators)     |
+----------------------------------+

- Exception to the precedence rule
  – Concerns assignment operators, which end in an equals character
  – If an operator ends with = and the operator is not one of the comparison operators, 
    then the precedence of the operator is the same as that of the simple assignment operator, =

     x *= y + 1   <<====>>   x *= (y + 1)

math Library
- Scala provides no standard operator for exponentiation
  – Use the math library

scala> math.pow(3, 2)
res9: Double = 9.0

Division

- Scala integer division
scala> val quotientInt = 7 / 5
quotientInt: Int = 1

- Getting the remainder of integer division
scala> val remainder = 7 % 5
remainder: Int = 2

- Scala float division
scala> val quotientFloat = 7.0 / 5.0
quotientFloat: Double = 1.4

Numeric Conversion
- Scala automatically performs type conversions for numeric operations when operands are of different types

scala> val tempCelsius = 35
tempCelsius: Int = 35

scala> val tempFahrenheit = 9.0 / 5.0 * tempCelsius + 32
tempFahrenheit: Double = 95.0

Numeric Conversion
- Some Scala types provide methods for converting between types

Casting a Double to an Int
scala> val fahrInt: Int = tempFahrenheit.toInt
fahrInt: Int = 95

Casting an Int to a Double
scala> val iLat = 331913
iLat: Int = 331913

scala> val dLat = iLat.toDouble * 1000
dLat: Double = 3.31913E8

Booleans to Control Program Flow
- Boolean variables are used to control program flow
  – Such as branching, conditional execution, and looping
- Boolean variables can be set to true or false
  – Lower case true and false only

• Creates a Boolean variable
scala> val gpsStatus: Boolean = false
gpsStatus: Boolean = false

scala> val gpsStatus = true
gpsStatus: Boolean = true

• Creates a string of characters
scala> val gpsStatus = "true"
gpsStatus: String = true

• True and TRUE are not Boolean literals
scala> val gpsStatus = True
<console>:12: error: not found: value True
       val gpsStatus = True
                       ^

Relational, Logical, and Bitwise Operations
- You can mix numeric types
  – the result of 1 == 1.0 is true

Relational Operators
<   > <=  >=  == !=

Logical Operators
&&  ||

Bitwise Operators
&(and)  |(or)  ^ (xor)

with Special Characters
- In Scala, string literals are enclosed in double quotes
- Escape special character literals using backslash (\)

+---------------------------------------------------+
| Escape Sequence | Corresponding Character Literal |
+---------------------------------------------------+
|       \t        | Tab                             |
|       \n        | Newline                         |
|       \b        | Backspace                       |
|       \r        | Crriage return                  |
|       \\        | Backslash                       |
+---------------------------------------------------+

scala> "Scala\text"
res10: String = Scala	ext

scala> "Scala\\text"
res11: String = Scala\text

String Interpretation
- Use raw to prevent Scala from interpreting a backslash as an escape
- Alternately, you can use three double quotes to do this

scala> raw"Scala\text"
res12: String = Scala\text

scala> """Scala\text"""
res13: String = Scala\text

Getting a Character from a String with Indexing
- String(offset)
  – Returns the character at the specified offset
  – The offset is zero-based

                                                        +-----------------------------------+
scala> val style = "MeToo 4.1"                          | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |
style: String = MeToo 4.1                        style: | M | e | T | o | o |   | 4 | . | 1 |
                                                        +-----------------------------------+

scala> val oneChar = style(2)                 style(2):           T
oneChar: Char = T

Slice of a String Using substring
- Use String.substring(from, until) to extract a substring
                                                        
scala> val version = style.substring(6, 9)              
version: String = 4.1                          version:                           4   .   1  

Methods for Processing Strings
- There are many methods currently defined for the String type

scala> val s = "bananas, apples, and oranges"
s: String = bananas, apples, and oranges

scala> s.getClass
res14: Class[_ <: String] = class java.lang.String

scala> s.sorted
res15: String = "   ,,aaaaaabdeeglnnnnopprsss"

scala> s.toUpperCase
res18: String = BANANAS, APPLES, AND ORANGES

scala> s.toArray
res19: Array[Char] = Array(b, a, n, a, n, a, s, ,,  , a, p, p, l, e, s, ,,  , a, n, d,  , o, r, a, n, g, e, s)

Splitting Strings
- Scala provides multiple ways to split strings

scala> s.splitAt(4)
res20: (String, String) = (bana,nas, apples, and oranges)

scala> s.split(',')
res21: Array[String] = Array(bananas, " apples", " and oranges")

Comparing Strings
- In Scala, you test equality with the == method

scala> val s1 = "bananas"
s1: String = bananas

scala> val s2 = "oranges"
s2: String = oranges

scala> val s3 = "bananas"
s3: String = bananas

scala> s1 == s2
res0: Boolean = false

scala> s1 == s3
res1: Boolean = true

Chaining Methods
- In Scala, methods can be chained
the output from the sorted method are passed as input to the toUpperCase method

scala> val device: String = "titanic 2300"
device: String = titanic 2300

scala> device.toUpperCase
res2: String = TITANIC 2300

scala> device.sorted
res3: String = " 0023aciintt"

scala> device.toUpperCase.sorted
res4: String = " 0023ACIINTT"

Listing Methods in the Shell
- Enter a literal, value, or variable followed by a period, then press TAB to see available methods

scala> val phoneName = "Ronin"
phoneName: String = Ronin

scala> phoneName
   val phoneName: String

scala> phoneName.
*                 companion             foldLeft             lastIndexOf           reduce                sorted         toIterable      
+                 compare               foldRight            lastIndexOfSlice      reduceLeft            span           toIterator      
++                compareTo             forall               lastIndexWhere        reduceLeftOption      split          toList          
++:               compareToIgnoreCase   foreach              lastOption            reduceOption          splitAt        toLong          
+:                compose               format               length                reduceRight           startsWith     toLowerCase     
/:                concat                formatLocal          lengthCompare         reduceRightOption     stringPrefix   toMap           
:+                contains              genericBuilder       lift                  regionMatches         stripLineEnd   toSeq           
:\                containsSlice         getBytes             lines                 replace               stripMargin    toSet           
<                 contentEquals         getChars             linesIterator         replaceAll            stripPrefix    toShort         
<=                copyToArray           groupBy              linesWithSeparators   replaceAllLiterally   stripSuffix    toStream        
>                 copyToBuffer          grouped              map                   replaceFirst          subSequence    toString        
>=                corresponds           hasDefiniteSize      matches               repr                  substring      toTraversable   
addString         count                 hashCode             max                   reverse               sum            toUpperCase     
aggregate         diff                  head                 maxBy                 reverseIterator       tail           toVector        
andThen           distinct              headOption           min                   reverseMap            tails          transpose       
apply             drop                  indexOf              minBy                 runWith               take           trim            
applyOrElse       dropRight             indexOfSlice         mkString              sameElements          takeRight      union           
canEqual          dropWhile             indexWhere           nonEmpty              scan                  takeWhile      unzip           
capitalize        endsWith              indices              offsetByCodePoints    scanLeft              to             unzip3          
charAt            equals                init                 orElse                scanRight             toArray        updated         
chars             equalsIgnoreCase      inits                padTo                 segmentLength         toBoolean      view            
codePointAt       exists                intern               par                   self                  toBuffer       withFilter      
codePointBefore   filter                intersect            partition             seq                   toByte         zip             
codePointCount    filterNot             isDefinedAt          patch                 size                  toCharArray    zipAll          
codePoints        find                  isEmpty              permutations          slice                 toDouble       zipWithIndex    
collect           flatMap               isTraversableAgain   prefixLength          sliding               toFloat                        
collectFirst      flatten               iterator             product               sortBy                toIndexedSeq                   
combinations      fold                  last                 r                     sortWith              toInt                          

Substituting Variables in Output
- Let’s define some variables to use:

scala> val phoneName = "Titanic"
phoneName: String = Titanic

scala> val phoneTemp = 35
phoneTemp: Int = 35

- Precede a string with s to substitute the value of the named variable
scala> println(s"Name: $phoneName")
Name: Titanic

scala> println("Name: $phoneName")
Name: $phoneName

scala> println(s"Name: $phoneName", s" Temp: $phoneTemp")
(Name: Titanic, Temp: 35)

scala> println("Name: $phoneName", " Temp: $phoneTemp")
(Name: $phoneName, Temp: $phoneTemp)

Formatting Output Using Format Strings
- Use f to format the string using C language-style format strings

Format Strings
%c - character
%s - string
%d - decimal
%e - exponential
%f - floating point
%i - integer
%o - octal
%h - hexadecimal

scala> val phoneTemp = 46
phoneTemp: Int = 46

scala> println(f"Temp: $phoneTemp%f")
Temp: 46.000000

scala> println(f"Temp: $phoneTemp%.2f")
Temp: 46.00

scala> println(f"Temp: $phoneTemp%h as hex")
Temp: 2e as hex

https://www.artima.com/weblogs/viewpost.jsp?thread=328540
https://docs.scala-lang.org/style/method-invocation.html
https://docs.scala-lang.org/overviews/core/string-interpolation.html

Store a Record of Data
- A Tuple in Scala consists of a fixed number of individual values that can be treated as a single entity
- Common uses of Tuples
  – For returning more than one value from a function
  – Key-value pairs
  – Frequently used with and returned by iteration methods such as map
- Tuples are more restrictive and less flexible than Collections
  – The number of values in a tuple cannot be changed after it is initialized
  – Tuples consist of between two (minimum) and 22 (maximum) values

Tuple2: For Storing Records Having Two Elements
- Tuple2, also called a pair, can be declared with several syntaxes

Explicit Declaration

scala> val myTup2 = Tuple2(4, "iFruit")
myTup2: (Int, String) = (4,iFruit)

Alternate Syntax

scala> val myTup2 = 4 -> "iFruit"
myTup2: (Int, String) = (4,iFruit)

scala> val myTup2 = (4, "iFruit")
myTup2: (Int, String) = (4,iFruit)

scala> myTup2.getClass
res7: Class[_ <: (Int, String)] = class scala.Tuple2
scala> myTup2.getClass
res8: Class[_ <: (Int, String)] = class scala.Tuple2
scala> myTup2.getClass
res9: Class[_ <: (Int, String)] = class scala.Tuple2

- Elements of a Tuple
– Can be of different types
– Are accessed using the ._1, ._2 syntax

scala> myTup2._1
res10: Int = 4

scala> myTup2._2
res11: String = iFruit

scala> myTup2.swap
res12: (String, Int) = (iFruit,4)

About Storing Larger Records?
- Tuples with more than two elements are stored in TupleN
– The maximum value for N is 22

scala> val myTup5 = (4, "MeToo", "1.0", 37.5, 41.3)
myTup5: (Int, String, String, Double, Double) = (4,MeToo,1.0,37.5,41.3)

scala> myTup5.getClass
res13: Class[_ <: (Int, String, String, Double, Double)] = class scala.Tuple5

scala> println(myTup5._3 + " / " + myTup5._5)
1.0 / 41.3

Class and Size of a Tuple
- Use method productPrefix to obtain the tuple's class name as a string
- Use method productArity to obtain the tuple size as an integer

scala> val myTup6 = ("Plato", "Kant", "Voltaire", "Descartes",
     | "deBeauvoir", "Camus")
myTup6: (String, String, String, String, String, String) = (Plato,Kant,Voltaire,Descartes,deBeauvoir,Camus)

scala> myTup6.productPrefix
res15: String = Tuple6

scala> myTup6.productArity
res16: Int = 6

partition Method Converts String to Tuple
- Use the toString method to convert the tuple to a string
- Use the partition method to convert a string to a tuple
– The first element in the tuple contains data that satisfies the condition
– The second element contains data that failed to satisfy the condition

scala> val myTup6 = ("Plato", "Kant", "Voltaire", "Descartes",
     | "deBeauvoir", "Camus")
myTup6: (String, String, String, String, String, String) = (Plato,Kant,Voltaire,Descartes,deBeauvoir,Camus)

scala> myTup6.getClass
res17: Class[_ <: (String, String, String, String, String, String)] = class scala.Tuple6

scala> val myStr = myTup6.toString
myStr: String = (Plato,Kant,Voltaire,Descartes,deBeauvoir,Camus)

scala> myStr.getClass
res18: Class[_ <: String] = class java.lang.String

scala> val newTup = myStr.partition(_.isUpper)
newTup: (String, String) = (PKVDBC,(lato,ant,oltaire,escartes,deeauvoir,amus))

scala> newTup.getClass
res19: Class[_ <: (String, String)] = class scala.Tuple2

scala> newTup
res20: (String, String) = (PKVDBC,(lato,ant,oltaire,escartes,deeauvoir,amus))

scala> val sortedLastNameInitials = newTup._1.sorted
sortedLastNameInitials: String = BCDKPV

scala> sortedLastNameInitials
res21: String = BCDKPV

scala> sortedLastNameInitials.getClass
res22: Class[_ <: String] = class java.lang.String

Storing Fixed- and Variable-Sized Data
- In Scala there are a large number of collection classes available
  – Classes are optimized for use in particular circumstances
  – They may be optimized for head/tail access or for fast update
- Collection classes vary in the methods they support
  – Immutable Collection classes are defined in package scala.collection.immutable
  – Mutable Collection classes are defined in package scala.collection.mutable

Traversable: Using foreach to Scale Out
- Declare an object of type Traversable to use the very important foreach method which facilitates parallel and distributed processing
  – Performs a specified action on all members of the collection
- Scala will apply the function you supply to foreach to each element
  – Allows the platform to parallelize processing and improve performance

The Traversable foreach method receives a function as a parameter; for example println, 
which will be called once for each element in the collection.

scala> val modelTrav = Traversable("MeToo", "Ronin", "iFruit")
modelTrav: Traversable[String] = List(MeToo, Ronin, iFruit)

scala> modelTrav.getClass
res25: Class[_ <: Traversable[String]] = class scala.collection.immutable.$colon$colon

scala> modelTrav.foreach(println)
MeToo
Ronin
iFruit

Iterable: Managing Memory Wisely
- Iterable adds the ability to iterate through each element, one at a time
  – Data is resident in memory only as it is used

scala> val models = Iterable("MeToo", "Ronin", "iFruit")
models: Iterable[String] = List(MeToo, Ronin, iFruit)

scala> models.getClass
res24: Class[_ <: Iterable[String]] = class scala.collection.immutable.$colon$colon

The iterator method returns an Iterator object, which provides a way to traverse each element in sequence, one time
scala> val modelIter = models.iterator
modelIter: Iterator[String] = non-empty iterator

scala> modelIter.getClass
res26: Class[_ <: Iterator[String]] = class scala.collection.LinearSeqLike$$anon$1

scala> modelIter.next
res27: String = MeToo

scala> modelIter.next
res28: String = Ronin

scala> modelIter.next
res29: String = iFruit

Seq: Using an Index to Access Specific Elements
- Seq adds the ability to access each element at a fixed offset (index)
- First element is at index 0
- Seq(n) returns the value of the element at offset n

scala> val mySeq = Seq("MeToo", "Ronin", "iFruit")
mySeq: Seq[String] = List(MeToo, Ronin, iFruit)

scala> mySeq(1)
res0: String = Ronin

Set: Storing Data with Automatic De-duplication
- Set removes duplicates
- Does not change ordering
- Set(value) returns true or false

scala> val mySet = Set("MeToo", "Ronin", "iFruit")
mySet: scala.collection.immutable.Set[String] = Set(MeToo, Ronin, iFruit)

scala> mySet.getClass
res1: Class[_ <: scala.collection.immutable.Set[String]] = class scala.collection.immutable.Set$Set3

scala> mySet("Banana")
res2: Boolean = false

scala> mySet("MeToo")
res3: Boolean = true

Map: Storing Key-Value Pairs
- Map stores (key -> value) pairs

scala> val wifiStatus = Map("disabled" -> "Wifi off",
     | "enabled" -> "Wifi on but disconected",
     | "conected" -> "Wifi on and connected"
     | )
wifiStatus: scala.collection.immutable.Map[String,String] = Map(disabled -> Wifi off, enabled -> Wifi on but disconected, conected -> Wifi on and connected)

scala> wifiStatus("enabled")
res4: String = Wifi on but disconected

Collection Variable Declaration
- We have covered the different collection types, but the elements of a collection also have a type
- Element type may be specified explicitly or inferred

scala> val myMap: Map[Int,String] = Map(1 -> "a", 2 -> "b")
myMap: Map[Int,String] = Map(1 -> a, 2 -> b)

scala> myMap.getClass
res5: Class[_ <: Map[Int,String]] = class scala.collection.immutable.Map$Map2

scala> val myMap1 = Map(1 -> "a", 2 -> "b")
myMap1: scala.collection.immutable.Map[Int,String] = Map(1 -> a, 2 -> b)

scala> myMap.getClass
res6: Class[_ <: Map[Int,String]] = class scala.collection.immutable.Map$Map2

Scalability in Collection Processing
- Scala collections include methods for processing all items in a collection without returning each item to the calling program
- By processing all items and only returning the result, Scala can optimize the program for distributed processing
How to iterate, process data
collection.foreach()

Unique Values with Lookup Convenience
- A Set is an Iterable that contains no duplicate elements
scala> mySet.size
res7: Int = 3

scala> mySet("Ronin")
res8: Boolean = true

Dropping Elements from a Set
- drop removes the first n elements

scala> val mySet = Set("Titanic", "Sorrento", "Ronin")
mySet: scala.collection.immutable.Set[String] = Set(Titanic, Sorrento, Ronin)

scala> mySet
res9: scala.collection.immutable.Set[String] = Set(Titanic, Sorrento, Ronin)

scala> val mySet2 = mySet.drop(1)
mySet2: scala.collection.immutable.Set[String] = Set(Sorrento, Ronin)

scala> mySet2
res11: scala.collection.immutable.Set[String] = Set(Sorrento, Ronin)

Storing and Processing Data of Varying Types
- A List is a finite immutable sequence
  – Very commonly used in Scala programming
  – Accessing the first element and adding an element to the front of the list are constant-time operations
- A List literal can be constructed using :: (cons operator) and Nil

scala> val newList = "a" :: "b" :: "c" :: Nil
newList: List[String] = List(a, b, c)

scala> newList.getClass
res13: Class[_ <: List[String]] = class scala.collection.immutable.$colon$colon

esto lo toma como una tupla:
scala> val newList = ("a", "b", "c")
newList: (String, String, String) = (a,b,c)

scala> newList.getClass
res12: Class[_ <: (String, String, String)] = class scala.Tuple3

List Elements
- Create a list using the List keyword
  – An alternative to using the cons operator and Nil
- Elements of a List can be accessed using an index

scala> val models = List("Titanic", "Sorrento", "Ronin")
models: List[String] = List(Titanic, Sorrento, Ronin)

scala> models
res14: List[String] = List(Titanic, Sorrento, Ronin)

scala> models(0)
res15: String = Titanic

scala> models(1)
res16: String = Sorrento

scala> models(3)
java.lang.IndexOutOfBoundsException: 3
  at scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:65)
  at scala.collection.immutable.List.apply(List.scala:84)
  ... 32 elided

scala> models(2)
res18: String = Ronin

Element Types
- Lists can contain a single data type or type Any

scala> val randomlist = List("iFruit", 3, "Ronin", 5.2)
randomlist: List[Any] = List(iFruit, 3, Ronin, 5.2)

- Lists can contain Collection and Tuple elements as well as simple types

scala> val devices = List(("Sorrento", 10), ("Sorrento", 20), ("iFruit", 30))
devices: List[(String, Int)] = List((Sorrento,10), (Sorrento,20), (iFruit,30))

Convenient List Methods
scala> val myList: List[Int] = List(1, 5, 7, 1, 3, 2)
myList: List[Int] = List(1, 5, 7, 1, 3, 2)

scala> myList.sum
res0: Int = 19

scala> myList.max
res1: Int = 7

scala> myList.min
res2: Int = 1

scala> myList.take(2)
res3: List[Int] = List(1, 5)

scala> myList.sorted
res4: List[Int] = List(1, 1, 2, 3, 5, 7)

scala> myList.reverse
res5: List[Int] = List(2, 3, 1, 7, 5, 1)

the Union or Intersection of Lists
scala> val myListA = List("iFruit", "Sorrento", "Ronin")
myListA: List[String] = List(iFruit, Sorrento, Ronin)

scala> val myListB = List("iFruit", "MeToo", "Ronin")
myListB: List[String] = List(iFruit, MeToo, Ronin)

scala> val myListC = myListA.union(myListB)
myListC: List[String] = List(iFruit, Sorrento, Ronin, iFruit, MeToo, Ronin)

scala> val myListD = myListA++myListB
myListD: List[String] = List(iFruit, Sorrento, Ronin, iFruit, MeToo, Ronin)

scala> myListC == myListD
res6: Boolean = true

scala> val myListE = myListA.intersect(myListB)
myListE: List[String] = List(iFruit, Ronin)

Appending Values to a List
- Operations using the lists leave the original lists unchanged

scala> myListA ++ myListB
res7: List[String] = List(iFruit, Sorrento, Ronin, iFruit, MeToo, Ronin)

scala> myListA ++ myListB
res7: List[String] = List(iFruit, Sorrento, Ronin, iFruit, MeToo, Ronin)

scala> myListA
res9: List[String] = List(iFruit, Sorrento, Ronin)

scala> myListB
res10: List[String] = List(iFruit, MeToo, Ronin)

- Use :+ to append to a list

scala> val myListF = myListA :+ "xPhone"
myListF: List[String] = List(iFruit, Sorrento, Ronin, xPhone)

Lists that Can Be Modified
- A ListBuffer is the mutable form of a List
- A ListBuffer provides constant time prepend and append operations

scala> val listBuf =
     | scala.collection.mutable.ListBuffer.empty[Int]
listBuf: scala.collection.mutable.ListBuffer[Int] = ListBuffer()

scala> listBuf += 17
res12: listBuf.type = ListBuffer(17)

scala> listBuf += 29
res13: listBuf.type = ListBuffer(17, 29)

scala> listBuf += 45
res14: listBuf.type = ListBuffer(17, 29, 45)

scala> listBuf
res16: scala.collection.mutable.ListBuffer[Int] = ListBuffer(17, 29, 45)

scala> listBuf.size
res17: Int = 3

scala> listBuf-=29
res18: listBuf.type = ListBuffer(17, 45)

scala> listBuf.size
res19: Int = 2

Mutable, but not Reassignable
- ListBuffer is mutable with respect to its elements, however, attempts to reassign the pointer address are not allowed if it was declared with val

scala> import scala.collection.mutable.ListBuffer
import scala.collection.mutable.ListBuffer

scala> val listBuf2 = ListBuffer("abc")
listBuf2: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc)

scala> listBuf2 += "def"
res23: listBuf2.type = ListBuffer(abc, def)

scala> listBuf = listBuf2
<console>:14: error: reassignment to val
       listBuf = listBuf2
               ^
Mutable and Reassignable
- Use var to create a mutable and reassignable ListBuffer 

scala> var listBufVar = ListBuffer("one")
listBufVar: scala.collection.mutable.ListBuffer[String] = ListBuffer(one)

scala> listBufVar += "banana"
res26: scala.collection.mutable.ListBuffer[String] = ListBuffer(one, banana)

scala> listBuf2
res28: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc, def)

scala> listBufVar =listBuf2
listBufVar: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc, def)

scala> listBufVar
res27: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc, def)

Warning for Reassignable Collection Variables
- Review this example carefully
  – What is happening when listBuf2 is modified

Cuando se reasigna y se modifica algun valor en la lista, se actualiza el valor en las dos listas
AQUI listBuf2 es creado con val

scala> val listBuf2 = ListBuffer("abc")
listBuf2: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc)

scala> listBuf2 += "def"
res34: listBuf2.type = ListBuffer(abc, def)

scala> var listBufVar = ListBuffer("one")
listBufVar: scala.collection.mutable.ListBuffer[String] = ListBuffer(one)

scala> listBufVar += "banana"
res35: scala.collection.mutable.ListBuffer[String] = ListBuffer(one, banana)

scala> listBufVar = listBuf2
listBufVar: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc, def)

scala> listBuf2 += "xyz"
res36: listBuf2.type = ListBuffer(abc, def, xyz)

scala> listBufVar
res37: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc, def, xyz)

scala> listBuf2
res38: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc, def, xyz)

scala> listBufVar += "jkl"
res39: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc, def, xyz, jkl)

scala> listBuf2
res40: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc, def, xyz, jkl)

AQUI listBuf2 se crea con var
scala> var listBuf2 = ListBuffer("abc")
listBuf2: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc)

scala> listBuf2 += "def"
res41: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc, def)

scala> var listBufVar = ListBuffer("one")
listBufVar: scala.collection.mutable.ListBuffer[String] = ListBuffer(one)

scala> listBufVar += "banana"
res42: scala.collection.mutable.ListBuffer[String] = ListBuffer(one, banana)

scala> listBufVar = listBuf2
listBufVar: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc, def)

scala> listBuf2 += "xyz"
res43: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc, def, xyz)

scala> listBufVar
res44: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc, def, xyz)

scala> listBufVar += "jkl"
res45: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc, def, xyz, jkl)

scala> listBuf2
res46: scala.collection.mutable.ListBuffer[String] = ListBuffer(abc, def, xyz, jkl)

Storing Data of a Known Size
- An Array is mutable but not resizable
  – Created with a fixed number of elements
  – You cannot change the number of elements in the array
  – You can update the value of an existing element
  – Array elements can be of a single type or Any 

scala> val devs = Array("iFruit", "MeToo", "Ronin")
devs: Array[String] = Array(iFruit, MeToo, Ronin)

scala> devs(2)
res47: String = Ronin

scala> devs(2) = "Titanic"

scala> devs
res49: Array[String] = Array(iFruit, MeToo, Titanic)

scala> devs(2)
res50: String = Titanic

Updating Array Elements
- Arrays are fixed in both size and type 

scala> val devices: Array[String] = new Array[String](4)
devices: Array[String] = Array(null, null, null, null)

scala> devices.update(0, "Sorrento")

scala> devices
res52: Array[String] = Array(Sorrento, null, null, null)

scala> devices(0) = "Titanic"

scala> devices
res54: Array[String] = Array(Titanic, null, null, null)

scala> devices(1)
res55: String = null

scala> devices(1) = 256
<console>:14: error: type mismatch;
 found   : Int(256)
 required: String
       devices(1) = 256
                    ^

scala> devices.length
length   lengthCompare

scala> devices.length
res57: Int = 4

scala> devices
res58: Array[String] = Array(Titanic, null, null, null)

Vector: Random Access, Flexible Size
- Vector, Array, and List all inherit from the Seq type
  – List belongs to to the LinearSeq branch of Seq
  – Vector, Array, and String belong to the IndexedSeq branch
- A Vector is more efficient for random access than a List
  – Allow access to any element in effectively constant time
  – Strikes a good balance between random selection and update speed

- Vector is immutable, modifications are not made in place

scala> val vec = Vector(1, 18, 6)
vec: scala.collection.immutable.Vector[Int] = Vector(1, 18, 6)

scala> vec.updated(1, 30)
res59: scala.collection.immutable.Vector[Int] = Vector(1, 30, 6)

scala> vec
res60: scala.collection.immutable.Vector[Int] = Vector(1, 18, 6)

- Unlike Array, a Vector has flexible size

scala> var vec = Vector(1, 6, 21)
vec: scala.collection.immutable.Vector[Int] = Vector(1, 6, 21)

scala> vec = vec :+ 5
vec: scala.collection.immutable.Vector[Int] = Vector(1, 6, 21, 5)

scala> vec
res61: scala.collection.immutable.Vector[Int] = Vector(1, 6, 21, 5)

scala> vec = 77 +: vec
vec: scala.collection.immutable.Vector[Int] = Vector(77, 1, 6, 21, 5)

scala> vec
res62: scala.collection.immutable.Vector[Int] = Vector(77, 1, 6, 21, 5)

scala> vec.updated(1, 30)
res63: scala.collection.immutable.Vector[Int] = Vector(77, 30, 6, 21, 5)

scala> vec
res64: scala.collection.immutable.Vector[Int] = Vector(77, 1, 6, 21, 5)

Storing and Processing Key-Value Pairs
- A Map is a collection of key-value pairs
  – Immutable by default – values are not modified in place
- Declare a map variable using either of these techniques
  – Map((key1, value1), (key2, value2))
  – Map(key1 -> value1, key2 -> value2)
- Keys and values
  – Keys are unique and may only appear once; values are not unique

Storing and Processing Key-Value Pairs
- A Map is a collection of key-value pairs
  – Immutable by default – values are not modified in place
- Declare a map variable using either of these techniques
  – Map((key1, value1), (key2, value2))
  – Map(key1 -> value1, key2 -> value2)
- Keys and values
  – Keys are unique and may only appear once; values are not unique
• The values are associated with keys that are easily understood string names.
• For example, to determine if the WiFi is turned on, access phoneStatus("WiFi")

val phoneStatus = Map(
("DTS" -> "2014-03-15:10:10:31"),
("Brand" -> "Titanic"),
("Model" -> "4000"),
("UID" -> "1882b564-c7e0-4315-aa24-228c0155ee1b"),
("DevTemp" -> 58),
("AmbTemp" -> 36),
("Battery" -> 39),
("Signal" -> 31),
("CPU" -> 15),
("Memory" -> 0),
("GPS" -> true ),
("Bluetooth" -> "enabled"),
("WiFi" -> "enabled"),
("Latitude" -> 40.69206648),
("Longitude" -> -119.4216429))

scala> val phoneStatus = Map(
     | ("DTS" -> "2014-03-15:10:10:31"),
     | ("Brand" -> "Titanic"),
     | ("Model" -> "4000"),
     | ("UID" -> "1882b564-c7e0-4315-aa24-228c0155ee1b"),
     | ("DevTemp" -> 58),
     | ("AmbTemp" -> 36),
     | ("Battery" -> 39),
     | ("Signal" -> 31),
     | ("CPU" -> 15),
     | ("Memory" -> 0),
     | ("GPS" -> true ),
     | ("Bluetooth" -> "enabled"),
     | ("WiFi" -> "enabled"),
     | ("Latitude" -> 40.69206648),
     | ("Longitude" -> -119.4216429))
phoneStatus: scala.collection.immutable.Map[String,Any] = Map(AmbTemp -> 36, GPS -> true, Memory -> 0, Battery -> 39, Latitude -> 40.69206648, Signal -> 31, Longitude -> -119.4216429, DevTemp -> 58, Model -> 4000, WiFi -> enabled, UID -> 1882b564-c7e0-4315-aa24-228c0155ee1b, CPU -> 15, DTS -> 2014-03-15:10:10:31, Brand -> Titanic, Bluetooth -> enabled)

Map Access Methods
scala> phoneStatus.contains("DTS")
res65: Boolean = true

scala> phoneStatus.keyS
keySet   keys   keysIterator

scala> phoneStatus.keys
res67: Iterable[String] = Set(AmbTemp, GPS, Memory, Battery, Latitude, Signal, Longitude, DevTemp, Model, WiFi, UID, CPU, DTS, Brand, Bluetooth)

scala> phoneStatus.keysIterator
res68: Iterator[String] = non-empty iterator

scala> phoneStatus.values
res69: Iterable[Any] = MapLike(36, true, 0, 39, 40.69206648, 31, -119.4216429, 58, 4000, enabled, 1882b564-c7e0-4315-aa24-228c0155ee1b, 15, 2014-03-15:10:10:31, Titanic, enabled)

- Use get or getOrElse to avoid an exception for non-existent keys

scala> phoneStatus
res70: scala.collection.immutable.Map[String,Any] = Map(AmbTemp -> 36, GPS -> true, Memory -> 0, Battery -> 39, Latitude -> 40.69206648, Signal -> 31, Longitude -> -119.4216429, DevTemp -> 58, Model -> 4000, WiFi -> enabled, UID -> 1882b564-c7e0-4315-aa24-228c0155ee1b, CPU -> 15, DTS -> 2014-03-15:10:10:31, Brand -> Titanic, Bluetooth -> enabled)

scala> phoneStatus("DTS")
res71: Any = 2014-03-15:10:10:31

scala> phoneStatus("key_does_not_exist")
java.util.NoSuchElementException: key not found: key_does_not_exist
  at scala.collection.MapLike$class.default(MapLike.scala:228)
  at scala.collection.AbstractMap.default(Map.scala:59)
  at scala.collection.MapLike$class.apply(MapLike.scala:141)
  at scala.collection.AbstractMap.apply(Map.scala:59)
  ... 32 elided

scala> phoneStatus.get("key_does_not_exist")
res73: Option[Any] = None

scala> phoneStatus.get("DTS")
res74: Option[Any] = Some(2014-03-15:10:10:31)

scala> phoneStatus.getOrElse("key_does_not_exist", "No Key")
res76: Any = No Key

Mutable Map
- We can not change Wireless to disabled

scala> phoneStatus("Wireless") = "disabled"
<console>:14: error: value update is not a member of scala.collection.immutable.Map[String,Any]
       phoneStatus("Wireless") = "disabled"
       ^
- Changing a value requires explicitly creating a mutable map

scala> val mutRec = scala.collection.mutable.Map(("Brand" ->
     | "Titanic"), ("Model" -> "4000"), ("Wireless" -> "enabled"))
mutRec: scala.collection.mutable.Map[String,String] = Map(Wireless -> enabled, Model -> 4000, Brand -> Titanic)

scala> mutRec("Wireless") = "disabled"

scala> mutRec
res79: scala.collection.mutable.Map[String,String] = Map(Wireless -> disabled, Model -> 4000, Brand -> Titanic)

Converting Between Collection Types
- Scala provides several methods for converting between collection types

scala> val myList = List("Titanic", "F01L", "enabled", 32)
myList: List[Any] = List(Titanic, F01L, enabled, 32)

scala> val myArray = myList.toArray
myArray: Array[Any] = Array(Titanic, F01L, enabled, 32)

scala> val myIterable = myList.toIterable
myIterable: Iterable[Any] = List(Titanic, F01L, enabled, 32)

scala> val myList2 = myIterable.toList
myList2: List[Any] = List(Titanic, F01L, enabled, 32)

scala> val myList3 = myArray.toList
myList3: List[Any] = List(Titanic, F01L, enabled, 32)

scala> val myList4 = myList.toString
myList4: String = List(Titanic, F01L, enabled, 32)

Converting a Tuple to a List

scala> val myTup = (4, "MeToo", "1.0", 37.5, 41.3, "Enabled")
myTup: (Int, String, String, Double, Double, String) = (4,MeToo,1.0,37.5,41.3,Enabled)

scala> myTup.getClass
res80: Class[_ <: (Int, String, String, Double, Double, String)] = class scala.Tuple6

scala> val myList = myTup.toList
<console>:13: error: value toList is not a member of (Int, String, String, Double, Double, String)
       val myList = myTup.toList
                          ^

scala> val myList = myTup.productIterator.toList
myList: List[Any] = List(4, MeToo, 1.0, 37.5, 41.3, Enabled)

String Conversions
- Strings in Scala are treated as collections similar to Arrays
- Strings can be converted to other Collection types

scala> val myStr = "A Banana"
myStr: String = A Banana

scala> myStr(2)
res81: Char = B

scala> myStr.toArray
res82: Array[Char] = Array(A,  , B, a, n, a, n, a)

scala> myStr.toList
res83: List[Char] = List(A,  , B, a, n, a, n, a)

scala> myStr.to
to        toBoolean   toByte        toDouble   toIndexedSeq   toIterable   toList   toLowerCase   toSeq   toShort    toString        toUpperCase   
toArray   toBuffer    toCharArray   toFloat    toInt          toIterator   toLong   toMap         toSet   toStream   toTraversable   toVector      

scala> myStr.toSet
res84: scala.collection.immutable.Set[Char] = Set(n, A, a,  , B)

scala> myStr.toVector
res85: Vector[Char] = Vector(A,  , B, a, n, a, n, a)
- Tuple
  – Fixed size: Tuple2, Tuple3, …, Tuple22
  – Not part of the collection library
  – Created at compile time, which restricts their flexibility
- List
  – Flexible size
  – Elements are immutable, so they cannot be changed by assignment
  – Fast addition and removal at head
  – Slow access to arbitrary indexes
- ListBuffer
  – Flexible size
  – Elements are mutable
  – Constant time append and prepend operations
- Array
  – Created with a fixed number of elements and not resizable
  – Fast access to arbitrary indexes
- Map
  – For working with key-value pairs
    To create a mutable Map, import scala.collection.mutable explicitly and declare the Map as mutable.Map

https://docs.scala-lang.org/overviews/collections/
https://docs.scala-lang.org/overviews/collections/seqs.html

Controlling Program Flow
§ Functional programming flow control is different from imperative
programming flow control and object oriented flow control
– Scala supports all three
- Imperative: program explicitly operates on data
- Object-oriented: program explicitly invokes a method
- Functional: program implies what needs to be done

Looping that Limits Distributed Processing
Looping through Data
- while
  – Loop construct that tests for an exit condition are typical of imperative programming
- for
  – Loop for a range of values

NOT BEST PRACTICE
- You can do this in Scala but it is not best practice. Do you know why?
scala> val sorrentoPhones = List("F00L", "F01L", "F10L", "F11L",
     | "F20L", "F21L", "F22L", "F23L", "F24L")
sorrentoPhones: List[String] = List(F00L, F01L, F10L, F11L, F20L, F21L, F22L, F23L, F24L)

scala> var i = 0
i: Int = 0

scala> while (i < sorrentoPhones.length){
     | println(sorrentoPhones(i))
     | i = i + 1}
F00L
F01L
F10L
F11L
F20L
F21L
F22L
F23L
F24L

for Loops with Ranges
- The <- syntax is called an enumerator generator
– You must adjust the number of iterations when using to
– Use until to avoid this extra math to adjust for length
– The by keyword allows you to increment by a custom value

scala> for (i <- 0 to sorrentoPhones.length - 1) {
     | println(sorrentoPhones(i))
     | }
F00L
F01L
F10L
F11L
F20L
F21L
F22L
F23L
F24L

scala> for (i <- 0 until sorrentoPhones.length) {
     | println(sorrentoPhones(i))
     | }
F00L
F01L
F10L
F11L
F20L
F21L
F22L
F23L
F24L

scala> for (i <- 0 until sorrentoPhones.length by 2) {
     | println(sorrentoPhones(i))
     | }
F00L
F10L
F20L
F22L
F24L

• In this example, the index is required by the application, but in many cases it is not
• Eliminating the local counting variable would remove a common source of scalability issues

scala> for (i <- 0 until sorrentoPhones.length) {
     | println(i.toString + ": " + sorrentoPhones(i))
     | }
0: F00L
1: F01L
2: F10L
3: F11L
4: F20L
5: F21L
6: F22L
7: F23L
8: F24L

for Iteration Over a Collection
- This is the preferred form of explicit iteration in Scala
- Note: no counting variable
  – No bounds issues, no mutability issue to limit scalability
- The generator already knows to process each item in the collection

scala> for (model <- sorrentoPhones) {
     | print(model + " ")
     | }
F00L F01L F10L F11L F20L F21L F22L F23L F24L 

for with Multiple Generators
- Generators within the for() must be separated by semicolons (;)
- They are treated as if they were nested for loops, left to right

scala> val phonebrands = List("iFruit", "MeToo")
phonebrands: List[String] = List(iFruit, MeToo)

scala> val newmodels = List("Z1", "Z-Pro")
newmodels: List[String] = List(Z1, Z-Pro)

scala> for (brand <- phonebrands; model <- newmodels) {
     | println(brand + " " + model)
     | }

iFruit Z1
iFruit Z-Pro
MeToo Z1
MeToo Z-Pro

Selecting a Subset of the Collection
- if is used to discard items that do not match
- However, this loop is generating each item and then only printing those items that match the criteria

scala> val sorrentoPhones = List("F00L", "F01L", "F10L", "F11L",
     | "F20L", "F21L", "F22L", "F23L", "F24L")
sorrentoPhones: List[String] = List(F00L, F01L, F10L, F11L, F20L, F21L, F22L, F23L, F24L)

scala> for (model <- sorrentoPhones) {
     | if (model.contains("2")) print(model + " ")
     | }
F20L F21L F22L F23L F24L 

Using for Filters
- This example moves the if condition inside the for loop
  – This is called a generator filter
- Scala will only generate items that match the filter criteria

scala> val sorrentoPhones = List("F00L", "F01L", "F10L", "F11L",
     | "F20L", "F21L", "F22L", "F23L", "F24L")
sorrentoPhones: List[String] = List(F00L, F01L, F10L, F11L, F20L, F21L, F22L, F23L, F24L)

scala> for (model <- sorrentoPhones; if( model.contains("2"))) {
     | print(model + " ")
     | }
F20L F21L F22L F23L F24L 

Collecting Data into a new Collection
- yield returns a new collection of items rather than processing each item one at a time

scala> val phonebrands = List("iFruit", "MeToo")
phonebrands: List[String] = List(iFruit, MeToo)

scala> val newmodels = List("Z1", "Z-Pro")
newmodels: List[String] = List(Z1, Z-Pro)

scala> val newlist =
     | for (brand <- phonebrands; model <- newmodels)
     | yield brand + " " + model
newlist: List[String] = List(iFruit Z1, iFruit Z-Pro, MeToo Z1, MeToo Z-Pro)

Iterating Over Elements in a Collection
- Iterators provide a way of iterating over elements in a collection
- Iterators can refer to distributed elements
- Iterators are scalable, making them ideal for Big Data applications

next to Iterate
- Create an Iterator from a collection using toIterator
  – For a tuple use productIterator
- The Iterator is used one time – use is “destructive”

scala> val phones = Array("iFruit", "MeToo")
phones: Array[String] = Array(iFruit, MeToo)

scala> val iter = phones.toIterator
iter: Iterator[String] = non-empty iterator

scala> iter.next
res95: String = iFruit

scala> iter.next
res96: String = MeToo

scala> iter.next
java.util.NoSuchElementException: next on empty iterator
  at scala.collection.Iterator$$anon$2.next(Iterator.scala:39)
  at scala.collection.Iterator$$anon$2.next(Iterator.scala:37)
  at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63)
  ... 32 elided

Iterators in a while Loop
- This example shows the preferred use of while in Scala
  – There are no counting variables or I/O dependencies

scala> val phones = Array("iFruit", "MeToo")
phones: Array[String] = Array(iFruit, MeToo)

scala> val iter = phones.toIterator
iter: Iterator[String] = non-empty iterator

scala> iter.next
res95: String = iFruit

scala> iter.next
res96: String = MeToo

scala> iter.next
java.util.NoSuchElementException: next on empty iterator
  at scala.collection.Iterator$$anon$2.next(Iterator.scala:39)
  at scala.collection.Iterator$$anon$2.next(Iterator.scala:37)
  at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63)
  ... 32 elided

scala> val titanicPhones = List("1000", "2000", "3000", "Bananas")
titanicPhones: List[String] = List(1000, 2000, 3000, Bananas)

scala> val iter = titanicPhones.toIterator
iter: Iterator[String] = non-empty iterator

scala> print(iter.next)
1000
scala> print(iter.next)
2000
scala> while (iter.hasNext) {
     | print(iter.next + " ")
     | }
3000 Bananas 
scala> print(iter.next)
java.util.NoSuchElementException: next on empty iterator
  at scala.collection.Iterator$$anon$2.next(Iterator.scala:39)
  at scala.collection.Iterator$$anon$2.next(Iterator.scala:37)
  at scala.collection.LinearSeqLike$$anon$1.next(LinearSeqLike.scala:47)
  ... 32 elided

scala> val titanicPhones = List("1000", "2000", "3000", "Bananas")
titanicPhones: List[String] = List(1000, 2000, 3000, Bananas)

scala> val iter = titanicPhones.toIterator
iter: Iterator[String] = non-empty iterator

scala> while (iter.hasNext) {
     | print(iter.next)
     | }
100020003000Bananas

Some Iterator Methods
+-----------------------------------------------------------------------+
| Method                       | Description                            |
+-----------------------------------------------------------------------+
| size                         | The remaining number of elements       |
| isEmpty                      | true if there are remaining elements   |
| exists(element)              | true if the element exists in the list |
| take(n)                      | Returns a new Iterator with just the   |
|                              | next n elements                        |
| filter(boolean-expression)   | Returns a new Iterator with elements   |
|                              | for which the expression is true       |
| foreach(function)            | Execute function for each element      |
|                              | provided by the iterato                |
+-----------------------------------------------------------------------+

def
- Variable types and values are evaluated immediately upon assignment
- Contrast with the function definition where only the type is evaluated
  – The value will be evaluated when the function is called

scala> val myConstant = 10
myConstant: Int = 10

scala> var myVariable = 24
myVariable: Int = 24

scala> def myFunction = myConstant + myVariable
myFunction: Int

scala> myFunction
res104: Int = 34

scala> myVariable = 9
myVariable: Int = 9

scala> myFunction
res105: Int = 19

myFunction evaluates to a different result when myVariable is reassigned to 20 because the value is passed in by reference
scala> myVariable = 20
myVariable: Int = 20

However, when myConstant is reassigned to 3, there is no change to the result returned by myFunction because myConstant was passed
by value, not by reference

scala> val myConstant = 3
myConstant: Int = 3

scala> myFunction
res107: Int = 30

Using an Expression Block with def
- The multi-line function definition uses curly braces
- All functions return something
  – If there is no explicit return type, Scala returns Unit
- Parentheses are only required if the function accepts parameters

scala> def listPhones {
     | println("MeToo")
     | println("Titanic")
     | println("iFruit")
     | }
listPhones: Unit

Special Unit Type
  – When a function passes back “nothing” in Scala, it passes back Unit - Cuando una función devuelve "nada" en Scala, devuelve Unit
  – This is equivalent to the void return type in a Java method
  – There is only one Unit in Scala; it is non-instantiable

scala> listPhones
MeToo
Titanic
iFruit

Function Example with Parameter and Return Value

• Use = to define a function with a return value
• No return keyword
• The evaluation of the final expression is returned

scala> def CtoF(celsius: Double) = {
     | (celsius * 9 / 5) + 32
     | }
CtoF: (celsius: Double)Double

scala> CtoF(34.00)
res109: Double = 93.2

- For simple expressions, the curly braces are not needed

def CtoF(celsius: Double) = (celsius * 9 / 5 ) + 32

- Return type may be explicit or inferred

def CtoF(celsius: Double) = (celsius * 9 / 5 ) + 32 : Double

Passing a Function as a Parameter
- convertList is called a higher-order function because it takes another function as a parameter
- convert is the name of the parameter that accepts a function
  – convert specifies the type for the input parameter to the left of the => transformation symbol
  – It specifies the return type to the right of =>

scala> def CtoF(celsius: Double) = (celsius * 9 / 5) + 32
CtoF: (celsius: Double)Double

scala> def convertList(myList:List[Double],
     |                 convert:(Double) => Double) {
     |     for(n <- myList)
     |        println(n,convert(n))
     | }
convertList: (myList: List[Double], convert: Double => Double)Unit

In this case, CtoF is the function passed into the convert parameter

scala> val phoneCelsius = List(34.0, 23.5, 12.2)
phoneCelsius: List[Double] = List(34.0, 23.5, 12.2)

scala> convertList(phoneCelsius, CtoF)
(34.0,93.2)
(23.5,74.3)
(12.2,53.96)

Anonymous Functions
- Anonymous functions are an alternate syntax for defining functions
  – They do not require a function name or label
  – Also referred to as lambda functions
- Anonymous functions in source code are called function literals
  – Often used when a function will be called only once

Anonymous Function
- An anonymous function is a way to define a function inline,
  – In other words, it is embedded in other code
    (parameter: type) => {function_definition: type}
            |                    |               |
            |                    v               |
            |                  code              |
            v                                    v
  parameter names and types                   return type

an Anonymous Function
- The example below shows an anonymous function for converting temperature from Celsius to Fahrenheit

A function literal can be used in the call to a higher-order function as an anonymous function.

scala> def convertList(myList:List[Double],
     | convert:(Double)=>Double) {
     | for (n <- myList)
     | println(n, convert(n))
     | }
convertList: (myList: List[Double], convert: Double => Double)Unit

scala> val phoneCelsius = List(34.0, 23.5, 12.2)
phoneCelsius: List[Double] = List(34.0, 23.5, 12.2)

scala> convertList(phoneCelsius, cc => (cc * 9 / 5) + 32)
(34.0,93.2)
(23.5,74.3)
(12.2,53.96)

Higher-Order Collection Methods
- Commonly used collection methods include
  – foreach
  – map
  – filter
- Using these methods help your program to scale
  – They delegate control over iteration to the framework

foreach Method
- List inherits the foreach method
- The _ (underscore) is a placeholder variable
  – It is a reference to the current element being operated on by foreach

scala> val phones = List("MeToo", "Titanic", "Ronin")
phones: List[String] = List(MeToo, Titanic, Ronin)

scala> phones.foreach(println(_))
MeToo
Titanic
Ronin

scala> phones.foreach(println)
MeToo
Titanic
Ronin

Placeholder _ - marcador de tipo
- Using _ may create ambiguity that prevents Scala from inferring the type - scala no infiere el tipo

scala> val phones = List("MeToo", "Titanic", "Ronin")
phones: List[String] = List(MeToo, Titanic, Ronin)

scala> phones.foreach(println(_).toUpperCase)
<console>:14: error: value toUpperCase is not a member of Unit
       phones.foreach(println(_).toUpperCase)
                                 ^

- In these cases, the type must either be specified or made more inferable
  – The second example hints to Scala that the list contains Strings

scala> phones.foreach(println(_).toString.toUpperCase)
MeToo
Titanic
Ronin

The map Method to Apply a Function to Each Element

scala> def CtoF(celsius: Double) = celsius * 9 / 5 + 32
CtoF: (celsius: Double)Double

scala> val phoneCelsius = List(34.0, 23.5, 12.2)
phoneCelsius: List[Double] = List(34.0, 23.5, 12.2)

Passing a named function
scala> phoneCelsius.map(c => CtoF(c))
res117: List[Double] = List(93.2, 74.3, 53.96)

Using a placeholder como parámetro:
scala> phoneCelsius.map(CtoF(_))
res118: List[Double] = List(93.2, 74.3, 53.96)

Passing an anonymous function (function literal)
scala> phoneCelsius.map(c => c * 9 / 5 + 32)
res119: List[Double] = List(93.2, 74.3, 53.96)

Passing an expression with a placeholder parameter
scala> phoneCelsius.map(_ * 9 / 5 + 32)
res1: List[Double] = List(93.2, 74.3, 53.96)

Filtering Numeric Values

scala> val phoneCelsius = List(34.0, 23.5, 12.2)
phoneCelsius: List[Double] = List(34.0, 23.5, 12.2)

- Create the filter condition using relational operators
scala> phoneCelsius.filter(val1 => val1 < 23)
res2: List[Double] = List(12.2)

scala> phoneCelsius.filter(val1 => val1 > 23)
res3: List[Double] = List(34.0, 23.5)

- In this example, the underscore placeholder refers to a numeric
- In the example, there is an implicit conversion of the integer literal to a floating point value

scala> phoneCelsius.filter(_ < 23)
res4: List[Double] = List(12.2)

scala> phoneCelsius.filter(_ > 23)
res5: List[Double] = List(34.0, 23.5)

filter Method
- Since the placeholder in this case refers to a String, we can call string methods like startsWith and length on the placeholder

scala> val phones = List("1000", "2000", "2500", "Bananas")
phones: List[String] = List(1000, 2000, 2500, Bananas)

scala> phones.filter(_.startsWith("2"))
res7: List[String] = List(2000, 2500)

scala> phones.filter(_.length > 4)
res8: List[String] = List(Bananas)

scala> phones.length
res10: Int = 4

scala> phones.filter(_.length > 4)
res11: List[String] = List(Bananas)

Providing an Operator as an Argument Using sortWith

scala> val phoneCelsius = List(34.0, 23.5, 12.2)
phoneCelsius: List[Double] = List(34.0, 23.5, 12.2)

- sortWith uses the passed in operator to compare the two elements
scala> phoneCelsius.sortWith((val1, val2) => val1 < val2)
res12: List[Double] = List(12.2, 23.5, 34.0)

  – The first underscore refers to the first parameter, the second one refers to the second parameter
scala> phoneCelsius.sortWith(_ < _)
res13: List[Double] = List(12.2, 23.5, 34.0)

Chaining Collection Methods
scala> var myList: List[Int] = List(1, 5, 7, 3, 2, 1)
myList: List[Int] = List(1, 5, 7, 3, 2, 1)

scala> myList.map(_ + 10)
res0: List[Int] = List(11, 15, 17, 13, 12, 11)

scala> myList.filter(_ > 4)
res1: List[Int] = List(5, 7)

scala> myList.map(_ + 1).filter(_ > 4)
res2: List[Int] = List(6, 8)

scala> val titanicPhones = List("1000", "2000", "3000", "Bananas")
titanicPhones: List[String] = List(1000, 2000, 3000, Bananas)

scala> titanicPhones.filter(_.endsWith("00")).sortWith(_ > _)
res4: List[String] = List(3000, 2000, 1000)


Comparing Literals Using match..case
- case can match any literal of any type

scala> val phoneWireless = "enabled"
phoneWireless: String = enabled

scala> var msg = "Radio state Unknown"
msg: String = Radio state Unknown

scala> phoneWireless match {
     | case "enabled" => msg = "Radio is On"
     | case "disabled" => msg = "Radio is Off"
     | case "connected" => msg = "Radio On, Protocol Up"
     | }

scala> println(msg)
Radio is On

Handling Non-Matching Literals
- A match can implicitly return a value
  – msg is assigned the result of the match…case

scala> val phoneWireless = "happy"
phoneWireless: String = happy

scala> var msg = "unknown"
msg: String = unknown

scala> phoneWireless match {
     |       case "enabled" => msg = "Radio is On"
     |       case "disabled" => msg = "Radio is Off"
     |       case "connected" => msg = "Radio On, Protocol Up"
     |       case default => msg = "Radio estate unknow"
     | }

scala> println(msg)
Radio estate unknow

match … case with Mixed Types (1)
- This array has a mix of types, use match...case to process each type
- Do you expect 'F' to be reported as a Char?

scala> val mixedArr = Array("11", 12, "thirteen", 14.0, 'F', null)
mixedArr: Array[Any] = Array(11, 12, thirteen, 14.0, F, null)

- The ordering of case statements within a match is significant
  – The first case that matches is executed

scala> for (elem <- mixedArr) {
     | elem match {
     |     case elem:String => println("String: " + elem)
     |     case elem:Int => println("Integer: " + elem)
     |     case elem:Double => println("Float: " + elem)
     |     case elem:AnyRef => println("Unknown: " + elem)
     |     case elem:Char => println("Char: " + elem)
     |     case null => println("Found null") 
     |     }
     | }
String: 11
Integer: 12
String: thirteen
Float: 14.0
Unknown: F        - 'F' is reported as “Unknown”
Found null

- Reorder the case statements to get the intended result
  – In this case, elem:Char must precede elem:AnyRef

scala>  for (elem <- mixedArr) {
     |      elem match {
     |      case elem:String => println("String: " + elem)
     |      case elem:Int => println("Integer: " + elem)
     |      case elem:Double => println("Float: " + elem)
     |      case elem:Char => println("Char: " + elem)
     |      case elem:AnyRef => println("Unknown: " + elem)
     |      case null => println("Found null") 
     |      }
     | }
String: 11
Integer: 12
String: thirteen
Float: 14.0
Char: F
Found null

Managing Response to Illegal Input Data
- An Option is a special type with a value of Some(n) or None
- An Option can be used to “wrap” a function that would potentially throw an error if it produced an illegal value
- If the value is good, then it is returned wrapped in Some
- Option can be used in a match…case by the caller

Using getOrElse to Control Program Flow
- Some(x) contains the value, where x is the returned value
- Some and None can be explicitly set, as illustrated
- getOrElse
  – Returns the wrapped value if Some, otherwise it performs the action

scala> val superPhone = Some("Model 6")
superPhone: Some[String] = Some(Model 6)

scala> superPhone.getOrElse("Not found")
res0: String = Model 6

scala> val superPhone = None
superPhone: None.type = None

scala> superPhone.getOrElse("Not found")
res1: String = Not found

Using Option in a Function
- This example shows a common use of Option in functions
  – The function returns a value encapsulated in a Some / None (Option) so that the caller can take appropriate action

scala> def str2Double(in: String): Option[Double] = {
     |     try {
     |         Some(in.toDouble)
     |     } catch {
     |          case e: NumberFormatException => None
     |     }
     | }


scala> str2Double("35.2")
res2: Option[Double] = Some(35.2)

scala> str2Double("Whatever")
res4: Option[Double] = None

scala> str2Double(35.2)
<console>:13: error: type mismatch;
 found   : Double(35.2)
 required: String
       str2Double(35.2)
                  ^

Option with match and case
- Process Some(x) inputs
  – In this example, we use typed pattern matching

scala> def convert2Float(x: Option[Any]) = x match {
     |     case Some(d: Double) => d.toFloat
     |     case Some(i: Int) => i.toFloat
     |     case Some(f: Float) => f
     |     case Some(_: Any) => println("Invalid. data provided")
     |     case None => println("No data provided.")
     | }
convert2Float: (x: Option[Any])AnyVal

scala> convert2Float(Some(25.0))
res0: AnyVal = 25.0

scala> convert2Float(Some("25.0"))
Invalid. data provided
res1: AnyVal = ()

scala> convert2Float(Some(25F))
res2: AnyVal = 25.0

scala> convert2Float(Some(25))
res3: AnyVal = 25.0

scala> convert2Float(Some("25"))
Invalid. data provided
res4: AnyVal = ()

scala> convert2Float(Some("twenty-five"))
Invalid. data provided
res13: AnyVal = ()

scala> convert2Float(None)
No data provided.
res14: AnyVal = ()

scala> convert2Float(Some())
warning: there was one deprecation warning; re-run with -deprecation for details
Invalid. data provided
res10: AnyVal = ()

scala> convert2Float(Some(""))
Invalid. data provided
res11: AnyVal = ()

scala> convert2Float(Some(" "))
Invalid. data provided
res12: AnyVal = ()

What are Partial Functions?
- A partial function is used when an answer should be returned only for a subset of possible input values
  – Defines the (partial) data it can handle
  – Can be queried to determine whether a given value can be handled
- Simple examples where partial functions can be useful
  – Division by zero
  – Square root of a negative number

Why Partial Functions?
- Take divide by zero as an example

- Providing a zero for x will cause an arithmetic exception
  – Partial functions can offer a way to avoid such an exception

Implementing a Partial Function
- Must be declared as a PartialFunction
- PartialFunction defines two methods that you must implement
  – apply performs the actual processing for your method
  – isDefinedAt evaluates whether the supplied input is valid

Partial Functions vs. Exception Handlers
- Partial functions allow a caller to test input before using it in a parameter

Partial Functions with Pattern Match
- When a partial function includes one or more case statements, the apply and isDefinedAt methods are generated automatically

- Use complete functions whenever possible
- A partial function may compile fine, but you may experience runtime errors for unhandled values
- Partial functions are useful when you are certain that
  – An unhandled value will never be supplied
  – Values are always checked with isDefinedAt before an explicit or implicit call to the apply method

- Scala supports imperative programming and functional programming
- Scala provides iterative methods for scalability
- Scala supports higher-order functions
- If possible, use Collection methods rather than imperative programming
- Pattern matching behaves differently from “switch” in other languages

scala> div(25)
res15: Int = 0

scala> div(0)
java.lang.ArithmeticException: / by zero
  at $anonfun$1.apply$mcII$sp(<console>:11)
  ... 32 elided


val div = new PartialFunction[Int, Int] {
    def apply(x: Int) = 24 / x
    def isDefinedAt(x: Int) = x != 0
}

scala> val div = new PartialFunction[Int, Int] {
     |     def apply(x: Int) = 24 / x
     |     def isDefinedAt(x: Int) = x != 0
     | }
div: PartialFunction[Int,Int] = <function1>

scala> div.isDefinedAt(0)
res18: Boolean = false

scala> div.isDefinedAt(2)
res19: Boolean = true

scala> if (div.isDefinedAt(2)) div(2)
res20: AnyVal = 12

scala> val getThirdItem: PartialFunction[List[Int], Int] = {
     |     case x :: y :: z :: _ => z
     | }
getThirdItem: PartialFunction[List[Int],Int] = <function1>

scala> getThirdItem.isDefinedAt(List(25))
res21: Boolean = false

scala> getThirdItem.isDefinedAt(List(25, 35, 45, 85))
res22: Boolean = true

scala> getThirdItem(List(25, 35, 45, 85))
res23: Int = 45

https://blog.bruchez.name/posts/scala-partial-functions-without-phd/
https://docs.scala-lang.org/sips/converters-among-optional-functions-partialfunctions-and-extractor-objects.html#inner-main

Working with Scala Libraries
- Code is organized by classes
  – Object-oriented code organization
  – Contains data elements and methods
  – Establishes local scope within the class
  – Enables instantiation of objects
- Classes are grouped together into packages
  – Separate namespaces to avoid collision of classes
- Packages are defined in Scala source files
  – Multiple packages can be contained in a single file
- Use the import keyword to import libraries to use in your code

Defining and Instantiating Classes

scala> class Device(name: String) {
     |       val phoneName = name
     |        def display = s"Phone is $phoneName"
     | }
defined class Device

scala> val a = new Device("Sorrento")
a: Device = Device@30b075b9

scala> a.display
res25: String = Phone is Sorrento

Defining Member Variables in a Constructor

scala> class Device(val phoneName: String) {
     |   def display = s"Phone is $phoneName"
     | }
defined class Device

scala> val a = new Device("Ronin")
a: Device = Device@50c442a5                    -----
                                                   |
scala> a.display                                   |
res28: String = Phone is Ronin                     |
                                                   | toString is an inherited method for all classes
Overriding Inherited Methods                       |
                                                   |
scala> a.toString                                  |
res29: String = Device@50c442a5                -----

scala> class Device(val phoneName: String) {
     | def display = s"Phone is $phoneName"
     | override def toString = s"$phoneName"
     | }
defined class Device

scala> val a = new Device("Titanic")
a: Device = Titanic

scala> a.toString
res30: String = Titanic

Singleton Objects
- Singleton objects are created without an explicit class
- Scala generates a class automatically, the object is a companion of the class


vi Device.scala 
class Device(val phoneName: String) {
      def display = s"Phone is $phoneName"
      override def toString = s"$phoneName"
}

vi TestDevice.scala

object TestDevice {
   def main(args: Array[String]) {
       val a = new Device("iFruit 3000")
       println(a.display)
   }
}

scalac Device.scala TestDevice.scala
ls *Dev*
/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src
4,0K -rw-rw-r-- 1 hadoop hadoop  765 oct 11 12:23 'TestDevice$.class'
4,0K -rw-rw-r-- 1 hadoop hadoop  586 oct 11 12:23  TestDevice.class
4,0K -rw-rw-r-- 1 hadoop hadoop 1,5K oct 11 12:23  Device.class
4,0K -rw-rw-r-- 1 hadoop hadoop  127 oct 11 12:21  Device.scala
4,0K -rw-rw-r-- 1 hadoop hadoop  129 oct 11 12:19  TestDevice.scala

Class: TestDevice.class
Companion Object: TestDevice$.class
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala TestDevice
Phone is iFruit 3000

Packages in Scala
- Packages are used to group classes into a library
- Package naming follows Java conventions
  – Begin with the reverse domain name of the organization
  – Additional names can be appended, following a dot
  – Example: com.loudacre.libraries becomes: com/loudacre/libraries
- Default packages imported during compilation
  – java.lang
  – scala
  – scala.Predef

Defining a Package

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ vi Device2.scala

vi Device2.scala

package com.loudacre.phonelib

class Device2(val phoneName: String) {
      def display = s"Phone is $phoneName"
      override def toString = s"$phoneName"
}

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scalac Device2.scala

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ cd com
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src/com$ cd loudacre/
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src/com/loudacre$ cd phonelib/
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src/com/loudacre/phonelib$ ls
/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src/com/loudacre/phonelib
total 12K
4,0K drwxrwxr-x 2 hadoop hadoop 4,0K oct 11 12:32 .
4,0K drwxrwxr-x 3 hadoop hadoop 4,0K oct 11 12:32 ..
4,0K -rw-rw-r-- 1 hadoop hadoop 1,6K oct 11 12:32 Device2.class

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ vi TestDevice2.scala 
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scalac TestDevice2.scala 

package com.cloudera.training
import com.loudacre.phonelib.Device2

object TestDevice2{
       def main(args: Array[String]) {
           val a = new Device2("iFruit 3000")
           println(a.display)
       }
}

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ cd com
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src/com$ cd cloudera/
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src/com/cloudera$ cd training/
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src/com/cloudera/training$ ls
/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src/com/cloudera/training
total 16K
4,0K drwxrwxr-x 2 hadoop hadoop 4,0K oct 11 12:38  .
4,0K -rw-rw-r-- 1 hadoop hadoop  858 oct 11 12:38 'TestDevice2$.class'
4,0K drwxrwxr-x 3 hadoop hadoop 4,0K oct 11 12:38  ..
4,0K -rw-rw-r-- 1 hadoop hadoop  686 oct 11 12:38  TestDevice2.class

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ scala com.cloudera.training.TestDevice2
Phone is iFruit 3000

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ ls
/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src
total 68K
4,0K drwxrwxr-x 3 hadoop hadoop 4,0K oct 11 12:55  .
4,0K drwxrwxr-x 4 hadoop hadoop 4,0K oct 11 12:38  com
4,0K -rw-rw-r-- 1 hadoop hadoop  214 oct 11 12:38  TestDevice2.scala
4,0K -rw-rw-r-- 1 hadoop hadoop  159 oct 11 12:32  Device2.scala

(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ ls com/cloudera/training/
/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src
total 16K
4,0K drwxrwxr-x 2 hadoop hadoop 4,0K oct 11 12:55  .
4,0K -rw-rw-r-- 1 hadoop hadoop  858 oct 11 12:55 'TestDevice2$.class'
4,0K -rw-rw-r-- 1 hadoop hadoop  686 oct 11 12:55  TestDevice2.class
4,0K drwxrwxr-x 3 hadoop hadoop 4,0K oct 11 12:38  ..
(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ ls com/loudacre/phonelib/
/home/hadoop/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src
total 12K
4,0K drwxrwxr-x 2 hadoop hadoop 4,0K oct 11 12:51 .
4,0K -rw-rw-r-- 1 hadoop hadoop 1,6K oct 11 12:51 Device2.class
4,0K drwxrwxr-x 3 hadoop hadoop 4,0K oct 11 12:48 ..

Importing Part of a Package
Importing All Classes
- import pack1._
  – Imports all classes from pack1
  – Equivalent to the Java code import pack1.*
- import pack1._, pack2._
  – Imports all classes from pack1 and from pack2

Import Subset of Classes
- import pack1.Class1
  – Imports only Class1 from pack1
- import pack1.{Class1, Class3}
  – Imports only Class1 and Class3 from pack1
- import pack1.Class1._
  – Imports all members of Class1 including implicit definitions
  – No equivalent in Java
- import pack1.{Class1 => MyClass}
  – Imports Class1 from pack1 and renames it MyClass
  – This avoids collision with an existing function named Class1

- Libraries in Scala are implemented as packages
  – Similar to Java
- Packages consist of a set of related classes and objects

