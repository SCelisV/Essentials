#------------------
#Hands-On Exercise: Transforming Data Using RDDs
#------------------

#export PYSPARK_DRIVER_PYTHON=ipython
#PYSPARK_DRIVER_PYTHON_OPTS=ipython
#pyspark

spark


#cat $DEVDATA/weblogs/2013-09-15.log |more

#hdfs dfs -put $DEVDATA/weblogs /devsh_loudacre/weblogs

#RDDs - ok - sc context
sc



file = ("/devsh_loudacre/weblogs/")
rdd = sc.textFile(file)

logsRDD = sc.textFile( file )
jpglogsRDD = logsRDD. filter(lambda i: ".jpg" in i)

jpgLines = jpglogsRDD. take(5)
for i in jpglogsRDD. take(5) : print (i)

lineLengthsRDD = logsRDD. map(lambda i: len(i))
lineLengthsRDD. isEmpty()
lineLengthsRDD. count()
lenLines = lineLengthsRDD. take(5)
for i in lenLines: print(i)


lineFieldsRDD = logsRDD. map(lambda i: i. split(' '))
for i in lineFieldsRDD. take(5): print (i)

# devuelve una lista
lineFields = lineFieldsRDD. take(10)
splitLine = lineFieldsRDD. take(10)
for i in splitLine: print(i)

printRDD ( lineFields )

"""
 ['203.229.179.227',
  '-',
  '66340',
  '[15/Sep/2013:02:31:01',
  '+0100]',
  '"GET',
  '/theme.css',
  'HTTP/1.0"',
  '200',
  '5870',
  '"http://www.loudacre.com"',
  '',
  '"Loudacre',
  'Mobile',
  'Browser',
  'MeeToo',
  '3.0"'],
 ['38.226.242.199',
  '-',
  '27',
  '[15/Sep/2013:02:29:32',
  '+0100]',
  '"GET',
  '/KBDOC-00154.html',
  'HTTP/1.0"',
  '200',
  '7652',
  '"http://www.loudacre.com"',
  '',
  '"Loudacre',
  'CSR',
  'Browser"'],
"""

for i in lineFields:
    print("-------")
    for j in i: print(j)


ipsRDD = logsRDD. map(lambda i: i. split(' ')[0])
type(ipsRDD)
# take (devuelve una lista)
for i in ipsRDD. take(10): print(i)

ipsRDD. saveAsTextFile("/devsh_loudacre/iplist")

#scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/iplist

#scelisdev02@cca175-m:~$ hdfs dfs -rm -r /devsh_loudacre/iplist
#scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/iplist

#scelisdev02@cca175-m:~$ hdfs dfs -cat /devsh_loudacre/iplist/part-00000

ipusersRDD = logsRDD. filter(lambda i: ".html" in i). map(lambda i: i. split(' ')[0] + "," + i. split(' ')[2])

htmllogsRDD = logsRDD. filter(lambda i: ".html" in i)
# htmlLines es una lista
htmlLines = htmllogsRDD. take(5)
for i in htmlLines: print(i)

ipusersRDD = logsRDD. map(lambda i: i. split(' ')[0] + "," + i. split(' ')[2])
for i in ipusersRDD. take(10): 
    print("---------")
    print(i)

#hdfs dfs -rm -r /devsh_loudacre/userips_csv
ipusersRDD. saveAsTextFile("/devsh_loudacre/userips_csv")
#hdfs dfs -ls /devsh_loudacre/userips_csv
#hdfs dfs -cat /devsh_loudacre/userips_csv

#hdfs dfs -rm -r /devsh_loudacre/userips_csv 

#hdfs dfs -ls /devsh_loudacre/userips_csv/

#scelisdev02@cca175-m:~/training_materials/devsh/exercises/transform-rdds/solution$ hdfs dfs -cat /devsh_loudacre/userips_csv/part-00000 |more

usersipDF =  spark.read.option("inferschema", "true").csv("/devsh_loudacre/userips_csv")
usersipDF. printSchema()

usersipDF. show()

#Bonus Exercise 1: Clean Device Status Data

#$DEVDATA/devicestatus.txt

#scelisdev02@cca175-m:~/training_materials$  sc.textFile("/devsh_loudacre/devicestatus.tx
#scelisdev02@cca175-m:~/training_materials$ hdfs dfs -ls /devsh_loudacre/

devstatusRDD = sc.textFile("/devsh_loudacre/devicestatus.txt")
type(devstatusRDD)
for i in devstatusRDD. take(10):
    print(i)

# delimitadores estan en la columna 19
delimitadorRDD = devstatusRDD. filter(lambda i: len(i) > 20)
for i in delimitadorRDD. take(10):
    print(i)

plist = delimitadorRDD. take(10)
for i in plist: print(i)

# hago un split por el separador que se encuentra en (20th characterâ€”position 19) - 2014-03-15:10:10:20|MeeToo 1.0|e0|MeeToo 1.0|
lineFieldsRDD = devstatusRDD. map(lambda i: i. split(i[19:20]))
for i in lineFieldsRDD. take(10):
    print("-------")
    print (i)

# longitud de la lista en este caso 14 campos
valuesFieldsRDD = lineFieldsRDD. filter(lambda i: len(i) == 14)
for i in valuesFieldsRDD. take(10):
    print(i)

# selecciono algunos campos de cada linea
dataRDD = valuesFieldsRDD. map(lambda i: (i[0], i[1] .split(' ')[0], i[2], i[12], i[13]) )
for data in dataRDD. take(10): print(data)

dataRDD. saveAsTextFile("/devsh_loudacre/devicestatus_etl")
dataRDD. map(lambda line: ','. join(line)). saveAsTextFile("/devsh_loudacre/devicestatus_etl")

#scelisdev02@cca175-m:~/training_materials$ hdfs dfs -ls /devsh_loudacre/devicestatus_etl

#scelisdev02@cca175-m:~/training_materials$ hdfs dfs -cat /devsh_loudacre/devicestatus_etl/part-00000

#scelisdev02@cca175-m:~/training_materials$ hdfs dfs -cat /devsh_loudacre/devicestatus_etl/part-00000

devstatusRDD = sc.textFile("/devsh_loudacre/devicestatus.txt")
cleanstatusRDD = devstatusRDD. filter(lambda line: len(line) > 20). map(lambda line: line. split(line[19:20])). filter(lambda values: len(values) == 14)
for line in cleanstatusRDD. take(2): print(line)

dataRDD = cleanstatusRDD. map(lambda line: (line[0], line[1]. split(' ')[0], line[2], line[12], line[13]) )
dataRDD. map(lambda line: ','. join(line)). saveAsTextFile("/devsh_loudacre/devicestatus_etl")

#Bonus Exercise 2: Convert Multi-line XML files to CSV files 
#scelisdev02@cca175-m:~/training_materials$ ls $DEVDATA/activations

#scelisdev02@cca175-m:~/training_materials$ cat $DEVDATA/activations/2008-10.xml

#scelisdev02@cca175-m:~/training_materials$ hdfs dfs -put $DEVDATA/activations /devsh_loudacre

#$DEVSH/exercises/transform-rdds/bonus-xml. 

#scelisdev02@cca175-m:~/training_materials$ vi $DEVSH/exercises/transform-rdds/bonus-xml/ActivationModels.pyspark


import xml.etree.ElementTree as ElementTree
def getActivations(s):
    filetree = ElementTree. fromstring(s)
    return filetree. getiterator('activation')

def getModel(activation):
    return activation. find('model'). text

def getAccount(activation):
    return activation. find('account-number'). text

# Read XML files into an RDD 

files="/devsh_loudacre/activations"
activationFiles = sc. wholeTextFiles(files)
activationFiles
for i in activationFiles. take(2): print (i)

activationRecords = activationFiles. flatMap(lambda pair: getActivations(pair[1]))
activationRecords
for i in activationRecords. take(2): print (i)

models = activationRecords. map(lambda i: getAccount(i) + ":" + getModel(i))
models
for i in models. take(2): print (i)

#scelisdev02@cca175-m:~$ hdfs dfs -rm -R /devsh_loudacre/account-models
models. saveAsTextFile("/devsh_loudacre/account-models")

#scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/account-models
#scelisdev02@cca175-m:~$ hdfs dfs -cat /devsh_loudacre/account-models/part-00000 |more
