
#export PYSPARK_DRIVER_PYTHON=ipython
#PYSPARK_DRIVER_PYTHON_OPTS=ipython
#pyspark


# hdfs dfs -put $HOME/18* /user/scelisdev03/

from pyspark.sql.types import *
from pyspark.sql.functions import *

devColumns = [
StructField("estacion",StringType()),
StructField("fecha",StringType()),
StructField("tipo",StringType()),
StructField("temperatura",DoubleType())]

devSchema = StructType(devColumns)

myDF = spark.read.schema(devSchema) .csv("/user/scelisdev03/1800_1801.csv") 
myDF.show(5)
myDF.printSchema()

for i in spark.catalog.listDatabases(): print(i)
# spark.catalog.setCurrentDatabase("devsh")
for i in spark.catalog.listTables("devsh"): print(i)

"""
listColumns - da error porque no le asigna una db a la vista

# Use listColumns(tablename,database)
if dbName is None:
   dbName = self.currentDatabase()
   iter = self._jcatalog.listColumns(dbName, tableName).toLocalIterator()
   columns = []
   while iter.hasNext():

# todo esto me da el mismo error
# AnalysisException: Table 'v_1800_1801' does not exist  - NPI
for i in spark.catalog.listColumns("v_1800_1801"): print(i) 
for i in spark.catalog.listColumns("v_1800_1801", None): print(i)
for i in spark.catalog.listColumns(None, "v_1800_1801"): print(i)

"""
myDF.createOrReplaceTempView("v_1800_1801")

spark.sql(" DESCRIBE v_1800_1801 ").show()

(spark.sql(
  """
  SELECT estacion, substring(fecha, 1, 4 ) as anyo,
         format_number( MIN( temperatura ) * 0.1 * 9.0 / 5.0 + 32.0 , 2 ) as min
  FROM v_1800_1801 
  WHERE tipo = 'TMIN'
  GROUP BY estacion, substring(fecha, 1, 4 )
  ORDER BY anyo DESC, estacion
  """).show())

spark.catalog.dropTempView ("v_1800_1801")