spark.catalog.listTables("devsh").show
spark.catalog.listColumns("devsh","accounts").show

val myDF = spark.read.table("devsh.accounts")
myDF.printSchema

val myDF = spark.sql("SELECT * FROM devsh.accounts")
myDF.printSchema
myDF.show(5)

val myDF = spark.sql("SELECT first_name, last_name FROM devsh.accounts")
myDF.printSchema
myDF.show(5)

/*
DataFrame API
*/

val myDF2 = myDF.select("first_name","last_name")
myDF2.printSchema()
myDF2.show(5)

// hdfs dfs -put $DATADEV/accountdevice /devsh_loudacre/accountdevice
val myDF = spark.read.option("header","true").option("inferSchema","true").csv("/devsh_loudacre/accountdevice")
myDF.printSchema()

// Crea una vista - view - temporal - TEMPORARY
myDF.createOrReplaceTempView("account_dev")
spark.catalog.listTables("devsh").show
spark.catalog.listColumns( "account_dev" ).show

val myDF = spark.sql("SELECT * FROM account_dev LIMIT 5").show()
spark.catalog.setCurrentDatabase("devsh")

val myDF = spark.sql("SELECT acct_num, first_name, last_name, account_device_id FROM devsh.accounts JOIN account_dev ON acct_num = account_id")
myDF.printSchema
myDF.show


// write a hive table
// scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/name_dev
myDF.write.mode("append").option("path","/devsh_loudacre/name_dev").saveAsTable("name_dev")
myDF.write.mode("overwrite").option("path","/devsh_loudacre/name_dev").saveAsTable("name_dev")

// Use the Catalog API to confirm that the table was created correctly with the right schema.
spark.catalog.listTables("devsh").show
spark.sql("DESCRIBE devsh.name_dev").show()
spark.sql("SELECT * FROM name_dev LIMIT 5").show()


// write a parquet file
// scelisdev02@cca175-m:~$ hdfs dfs -ls /devsh_loudacre/name_dev_parquet
myDF.write.mode("overwrite").save("/devsh_loudacre/name_dev_parquet")
// confirm that the parquet file was created correctly with the right schema.
val myDFparquet = spark.read.parquet("/devsh_loudacre/name_dev_parquet")
myDFparquet.printSchema
myDFparquet.show 


myDFparquet.show (false)