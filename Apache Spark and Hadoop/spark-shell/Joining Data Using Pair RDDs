//------------------
//Hands-On Exercise: Joining Data Using Pair RDDs
//------------------

//scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -put weblogs /devsh_loudacre/weblogs/
//scelisdev02@cca175-m:~/training_materials/devsh/data$ hdfs dfs -cat weblogs /devsh_loudacre/weblogs/2014-03-15.log |more
//Paso 0. Crear el RDD con los ficheros

//weblogfiles: String = /devsh_loudacre/weblogs/*2.log
val weblogfiles = ( "/devsh_loudacre/weblogs/*2.log" )

//weblogsRDD: org.apache.spark.rdd.RDD[String] - 
val weblogsRDD = sc.textFile( weblogfiles )
for ( i <- weblogsRDD. take(2) ) print( i )

//Paso 1. Para extraer la key que en nuestro caso es el usuario - split devuelve org.apache.spark.rdd.RDD[Array[String]]
val weblogs1RDD = weblogsRDD. map( _.split(' ') )
for ( i <- weblogs1RDD. take(10) ) print( i )

//Paso 2. crear la k,v:  usuario [2], 1 - weblogs2RDD: org.apache.spark.rdd.RDD[(String, Int)] - (67858,1)
val weblogs2RDD = weblogs1RDD. map( i => ( i(2), 1) )
for ( i <- weblogs2RDD. take(10) ) print( i )

//Paso 3. hacer el reduce, esto me crea un RDD por userID - org.apache.spark.rdd.RDD[(String, Int)] - (99066,2)
val weblogs3RDD = weblogs2RDD. reduceByKey( ( v1,v2 ) => v1 + v2 )
for ( i <- weblogs3RDD. take(10) ) print( i )
weblogs3RDD. getClass

//Paso 4. crear un dictionaries con la frecuencia, count por key - devuelve un scala.collection.Map[Int,Long]] 
val weblogs4collection = weblogs3RDD.map( i => ( i._2, i._1 ) ). countByKey()
weblogs4collection. getClass()

// (138,6)(170,3)(5,26)
for ( i <- weblogs4collection ){
     print ( i )
} 

//Paso 5. Para extraer la key que en este caso es la ip y el usuario - org.apache.spark.rdd.RDD[Array[String]]
val weblogs5RDD = weblogsRDD. map( i => i.split(' ') )
for ( i <- weblogs5RDD. take(10) ) print( i )

//Paso 6. extraer la ip y el usuario - org.apache.spark.rdd.RDD[(String, String)] - (67858,131.166.169.114)(67858,131.166.169.114)
val weblogs6RDD = weblogs1RDD. map( i => ( i(2), i(0) ) )
for ( i <- weblogs6RDD. take(10) ) print( i )

//Paso 7. Group by por key -  org.apache.spark.rdd.RDD[(String, Iterable[String])] - (99066,CompactBuffer(195.67.16.237, 195.67.16.237))
val weblogs7RDD = weblogs6RDD. groupByKey()
for ( i <- weblogs7RDD. take(10) ) print( i )

//Paso 8. Para poder imprimir el iterable: 
for ( k <- weblogs7RDD. take(2) ) {
     println( k._1 + ":" )
     for ( i <- k._2 ) 
         println( "\t" + i )
}
     
//scelisdev02@cca175-m:~/training_materials/devsh/exercises/pair-rdds/solution$ hdfs dfs -ls /user/hive/warehouse/devsh.db/
//scelisdev02@cca175-m:~/training_materials/devsh/exercises/pair-rdds/solution$ 

//Paso 0. Crear el RDD con los datos de la tabla accounts de hive - accountsdata: String = /user/hive/warehouse/devsh.db/accounts
val accountsdata = ( "/user/hive/warehouse/devsh.db/accounts" )

// org.apache.spark.rdd.RDD[String]
val accountsRDD = sc. textFile( accountsdata )
for ( i <- accountsRDD. take(10) ) print( i )

//Paso 1. Crear la lista a partir del RDD y hacer el split por campos - org.apache.spark.rdd.RDD[Array[String]]
val accounts1RDD = accountsRDD. map( _.split(',') )
for ( i <- accounts1RDD. take(10) ) print( i )

//Paso 2. Crea el RDD por userID, campo [0] - org.apache.spark.rdd.RDD[(String, Array[String])] - (1,[Ljava.lang.String;@4c591b5f)
val accounts2RDD = accounts1RDD. map( i => ( i(0), i ) )
for ( i <- accounts2RDD. take(10) ) print( i )

// org.apache.spark.rdd.RDD[(String, (Array[String], Int))] - 27120,([Ljava.lang.String;@49d4e622,6))
val accountsjoinRDD = accounts2RDD. join( weblogs3RDD )
for ( i <- accountsjoinRDD. take(10) ) print( i )

for ( x <- accountsjoinRDD. take(10) ) {
     printf( "%s %s %s %s\n", x._1, x._2._2,  x._2._1(3), x._2._1(4) )
}

//(base) hadoop@sc-ubuntu-20-04-2-lts:~/0_DEVELOP__0_DEVELOP__0_DEVELOP__/Scala/IntelliJProjects/scalaPUE/src$ spark-shell

val weblogfiles = ( "/devsh_loudacre/weblogs/*2.log" )
// org.apache.spark.rdd.RDD[String]
val weblogsRDD = sc.textFile( weblogfiles )
// weblogsRDD.getClass
for (i <- weblogsRDD.take(2)) print(i)

// org.apache.spark.rdd.RDD[String] - 200.31.229.239-26069
val weblogsRDDFlat = weblogsRDD.flatMap(i => i.split(' '))
for (i <- weblogsRDDFlat.take(20)) print(i)
// weblogsRDDFlat.getClass

// org.apache.spark.rdd.RDD[(String, Int)] - (200.31.229.239,2)(-,2)(26069,2)
val weblogsRDDMap = weblogsRDDFlat.map(i => (i,2))
for (i <- weblogsRDDMap.take(20)) print(i)
//weblogsRDDMap.getClass

// org.apache.spark.rdd.RDD[(String, Int)] - (27120,12)(3492,12)(27.110.232.235,4)
val weblogsRDDReduce = weblogsRDDMap.reduceByKey((v1,v2) => v1+v2)
for (i <- weblogsRDDReduce.take(20)) print(i)
//weblogsRDDReduce.getClass


// -------------

val weblogfiles = ( "/devsh_loudacre/weblogs/*2.log" 

// org.apache.spark.rdd.RDD[(String, Int)] - (27120,6)(3492,6)(27.110.232.235,2)
val weblogsRDD = sc. textFile(weblogfiles). flatMap(_.split(' ')). map((_,1)). reduceByKey(_+_)
// print(weblogsRDD)
for ( i <- weblogsRDD. take(10) ) print(i)

// otra forma de escribirlo
// val weblogsRDD = sc.textFile(weblogfiles).flatMap(i => i.split(' ')).map(word => (word,1)).reduceByKey((v1,v2) => v1+v2)

// Group IPs by user IDa - org.apache.spark.rdd.RDD[(String, Iterable[String])] - (99066,CompactBuffer(195.67.16.237, 195.67.16.237))
val userIPsRDD = sc. textFile(weblogfiles). map( _.split(' ') ). map( i => ( i(2),i(0) ) ). groupByKey()
for ( i <- userIPsRDD. take(10) ) print(i)

// print out the first 10 user ids, and their IP list
/*
99066:                                                                          
        195.67.16.237
        195.67.16.237
*/
for ( i <- userIPsRDD.take(10) ) {
   println( i._1 + ":" )
   for ( j <- i._2 ) println( "\t" + j)
}

// Bonus Exercises

//filename: String
val filename = "/user/hive/warehouse/devsh.db/accounts" 
// org.apache.spark.rdd.RDD[(String, Array[String])] - (94660,[Ljava.lang.String;@78075142)
val bonusRDD = sc.textFile( filename ). map( _.split(',') ) . keyBy( _(8) ) 
for ( i <- bonusRDD.take(10) ) print( i ) 

// org.apache.spark.rdd.RDD[(String, String)] - (94660,Becton,Donald)(94171,Jones,Donna)
val bonusRDD2 = bonusRDD. mapValues( i => i(4) + ',' + i(3) )
for ( i <- bonusRDD2.take(10) ) print( i ) 

/*
org.apache.spark.rdd.RDD[(String, Iterable[String])] - 
(95461,CompactBuffer(Dorazio,Charles, Malec,Derrick, Toney,Carolyn, Wicker,Eileen, Hollier,Harold, Rush,Eddie, Copley,Elizabeth, Cowles,Terry, Stewart,Monique, Boston,Shana, Bradford,Robert, Evans,Michael, Walker,James, Strange,Michael, Benally,Josephine, Sanchez,Steven, Johnson,Rebecca))
*/
val bonusRDD2 = bonusRDD. mapValues( i => i(4) + ',' + i(3) ) .groupByKey()
for ( i <- bonusRDD2.take(10) ) print( i ) 

/*
(
---,85000)      Allen,Harvey
        Prinz,Daniel
        Pascale,Robert
        Brookes,Donna
        Mackenzie,James
        Chamberlain,Robert
        Cunningham,Richard
        Sewell,Bailey
        Marin,Daniel
(
---,85001)
*/

for ( i <- bonusRDD2. sortByKey(). take( 10 ) ){
     print ("\n---", i._1)
     for ( j <- i._2 ) println( "\t" + j)
}

val accountsdata = "/user/hive/warehouse/devsh.db/accounts"
//org.apache.spark.rdd.RDD[String]
val accountsRDD = sc. textFile(accountsdata)
for ( i <- accountsRDD. take(2) ) print(i)

val accounts1RDD = accountsRDD. map( _.split(',') )
for ( i <- accountsRDD. take(2) ) print(i)


val filename = "/user/hive/warehouse/devsh.db/accounts"
// org.apache.spark.rdd.RDD[(String, Array[String])] - (94660,[Ljava.lang.String;@4ade8f0)
val bonusRDD = sc. textFile(filename). map( _.split(',') ) . keyBy(_(8))
for ( i <- bonusRDD. take(10) ) print( i )

// org.apache.spark.rdd.RDD[(String, Iterable[String])] - (95461,CompactBuffer(Dorazio,Charles, Malec,Derrick, Toney ....
val bonusRDD2 = bonusRDD. mapValues( x => x(4) + ',' + x(3) ) .groupByKey()
for ( i <- bonusRDD2. take(10) ) print( i )
/*
(---,85000)     Allen,Harvey
        Prinz,Daniel
        Pascale,Robert
        Brookes,Donna
        Mackenzie,James
        Chamberlain,Robert
        Cunningham,Richard
        Sewell,Bailey
        Marin,Daniel
*/
for ( i <- bonusRDD2. sortByKey(). take( 10 ) ){
     print ("---", i._1)
     for ( j <- i._2 ) println( "\t" + j)
}

val filename = "/user/hive/warehouse/devsh.db/accounts"
// org.apache.spark.rdd.RDD[(String, Array[String])] 
val bonusRDD = sc. textFile(filename). map( _.split(',') ) .keyBy( _(8) )
// org.apache.spark.rdd.RDD[(String, Iterable[String])]
val bonusRDD2 = bonusRDD. mapValues( x => x(4) + ',' + x(3) ) .groupByKey()
for (x <- bonusRDD2.sortByKey().take(5)) {
   println("---" + x._1)
   x._2.foreach(println)
}

/*
(---,85000)Allen,Harvey
Prinz,Daniel
Pascale,Robert
Brookes,Donna
Mackenzie,James
Chamberlain,Robert
Cunningham,Richard
Sewell,Bailey
Marin,Daniel
*/
for ( i <- bonusRDD2. sortByKey(). take( 10 ) ){
     print ("---", i._1)
     i._2.foreach(println)
}
