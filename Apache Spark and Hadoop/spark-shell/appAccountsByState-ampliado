/*

Scala application 

$ sudo apt install maven

cd /home/hadoop/SCProjects/0_SCProjects_github.com_SCelisV/scala/SC.Code/pue-cca175/accounts-by-state_project

scelisdev03@cluster-cca175-m:~/accounts-by-state_project$ mvn package -e

to execute:
scelisdev03@cluster-cca175-m:~/accounts-by-state_project$ mvn package

scelisdev03@cluster-cca175-m:~/accounts-by-state_project$ spark-submit --class stubs.AccountsByState target/accounts-by-state-1.0.jar CA

scelisdev03@cluster-cca175-m:~/accounts-by-state_project$ spark-submit --class solution.AccountsByState target/accounts-by-state-1.0.jar CA

definition
The Spark application will take a single argument—a state code (such as CA). 
The program should read the data from the devsh.accounts Hive table and save the rows whose state column value matches the specified state code. 

Write the results to /devsh_loudacre/accounts_by_state/state-code (such as accounts_by_state/CA).

OJO - no me funciono revisar

*/

package stubs 

import org.apache.spark.sql.SparkSession

object AccountsByState {
        
    // salvar el resultado
    // hdfs dfs -rm -R /devsh_loudacre/accounts_by_state/

    def processState( state ):
        val outdir = "/devsh_loudacre/accounts_by_state/" + state
        val stateAccountsDF = accountsDF. filter( accountsDF.state === state ). write.mode( "overwrite" ).  save( outdir ) 

    def main(args: Array[String]) {
 
        // Create a SparkSession object
        val spark = SparkSession.builder.appName("Accounts by State Ampliado").getOrCreate()

        // Change the application log level from INFO (the default) to WARN
        spark.sparkContext.setLogLevel("WARN")
  
        // Leer la tabla Hive
        val accountsDF = spark.read.table("devsh.accounts").persist()

        // Obtener una colección que contenga los estados distintos que hay en las cuentas.  
        // Por ejemplo, en Scala tendríamos: Array["AZ", "CA", "NV" , "OR"] .

        statesRow = ( accountsDF.  select("state").  distinct().  collect() )
        states = map( r: r.getString(0), statesRow )

        for (s <- states)
            println (s)
            processState( s )

    //stop the Spark session:
    spark.stop()

  }
}

/*
Revisar los ficheros en:

#hdfs dfs -ls /devsh_loudacre/
#hdfs dfs -ls /devsh_loudacre/accounts_by_state/

*/